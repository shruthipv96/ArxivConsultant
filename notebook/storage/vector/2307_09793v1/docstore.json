{"docstore/data": {"dcc97cc2-36d8-468e-b92f-941593b108e6": {"__data__": {"id_": "dcc97cc2-36d8-468e-b92f-941593b108e6", "embedding": null, "metadata": {"page_label": "1", "file_name": "2307_09793v1.pdf", "Title of this paper": "On the Origin of LLMs: An Evolutionary Tree and Graph for 15,821 Large Language Models", "Authors": "Sarah Gao, Andrew Kean Gao", "Date published": "07/19/2023", "URL": "http://arxiv.org/abs/2307.09793v1", "summary": "Since late 2022, Large Language Models (LLMs) have become very prominent with\nLLMs like ChatGPT and Bard receiving millions of users. Hundreds of new LLMs\nare announced each week, many of which are deposited to Hugging Face, a\nrepository of machine learning models and datasets. To date, nearly 16,000 Text\nGeneration models have been uploaded to the site. Given the huge influx of\nLLMs, it is of interest to know which LLM backbones, settings, training\nmethods, and families are popular or trending. However, there is no\ncomprehensive index of LLMs available. We take advantage of the relatively\nsystematic nomenclature of Hugging Face LLMs to perform hierarchical clustering\nand identify communities amongst LLMs using n-grams and term frequency-inverse\ndocument frequency. Our methods successfully identify families of LLMs and\naccurately cluster LLMs into meaningful subgroups. We present a public web\napplication to navigate and explore Constellation, our atlas of 15,821 LLMs.\nConstellation rapidly generates a variety of visualizations, namely\ndendrograms, graphs, word clouds, and scatter plots. Constellation is available\nat the following link: https://constellation.sites.stanford.edu/."}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "be09d8bd-d254-44d3-959f-ed610c5fd1ec", "node_type": "4", "metadata": {"page_label": "1", "file_name": "2307_09793v1.pdf", "Title of this paper": "On the Origin of LLMs: An Evolutionary Tree and Graph for 15,821 Large Language Models", "Authors": "Sarah Gao, Andrew Kean Gao", "Date published": "07/19/2023", "URL": "http://arxiv.org/abs/2307.09793v1", "summary": "Since late 2022, Large Language Models (LLMs) have become very prominent with\nLLMs like ChatGPT and Bard receiving millions of users. Hundreds of new LLMs\nare announced each week, many of which are deposited to Hugging Face, a\nrepository of machine learning models and datasets. To date, nearly 16,000 Text\nGeneration models have been uploaded to the site. Given the huge influx of\nLLMs, it is of interest to know which LLM backbones, settings, training\nmethods, and families are popular or trending. However, there is no\ncomprehensive index of LLMs available. We take advantage of the relatively\nsystematic nomenclature of Hugging Face LLMs to perform hierarchical clustering\nand identify communities amongst LLMs using n-grams and term frequency-inverse\ndocument frequency. Our methods successfully identify families of LLMs and\naccurately cluster LLMs into meaningful subgroups. We present a public web\napplication to navigate and explore Constellation, our atlas of 15,821 LLMs.\nConstellation rapidly generates a variety of visualizations, namely\ndendrograms, graphs, word clouds, and scatter plots. Constellation is available\nat the following link: https://constellation.sites.stanford.edu/."}, "hash": "0f719dfae9ca6b2d6d803193cfd07f022eff30ba608cd35b627f74fa8b1f6769", "class_name": "RelatedNodeInfo"}}, "text": "1\nOn\nthe\nOrigin\nof\nLLMs:\nAn\nEvolutionary\nTree\nand\nGraph\nfor\n15,821\nLarge\nLanguage\nModels\nSarah\nR\nGao,\nAndrew\nK\nGao\nCanyon\nCrest\nAcademy ,\nStanford\nUniversity", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 157, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8bf0dde0-156d-42a7-b03a-853e5fb40828": {"__data__": {"id_": "8bf0dde0-156d-42a7-b03a-853e5fb40828", "embedding": null, "metadata": {"page_label": "2", "file_name": "2307_09793v1.pdf", "Title of this paper": "On the Origin of LLMs: An Evolutionary Tree and Graph for 15,821 Large Language Models", "Authors": "Sarah Gao, Andrew Kean Gao", "Date published": "07/19/2023", "URL": "http://arxiv.org/abs/2307.09793v1", "summary": "Since late 2022, Large Language Models (LLMs) have become very prominent with\nLLMs like ChatGPT and Bard receiving millions of users. Hundreds of new LLMs\nare announced each week, many of which are deposited to Hugging Face, a\nrepository of machine learning models and datasets. To date, nearly 16,000 Text\nGeneration models have been uploaded to the site. Given the huge influx of\nLLMs, it is of interest to know which LLM backbones, settings, training\nmethods, and families are popular or trending. However, there is no\ncomprehensive index of LLMs available. We take advantage of the relatively\nsystematic nomenclature of Hugging Face LLMs to perform hierarchical clustering\nand identify communities amongst LLMs using n-grams and term frequency-inverse\ndocument frequency. Our methods successfully identify families of LLMs and\naccurately cluster LLMs into meaningful subgroups. We present a public web\napplication to navigate and explore Constellation, our atlas of 15,821 LLMs.\nConstellation rapidly generates a variety of visualizations, namely\ndendrograms, graphs, word clouds, and scatter plots. Constellation is available\nat the following link: https://constellation.sites.stanford.edu/."}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "00e0297f-659f-4379-a7cb-a1457f8396e8", "node_type": "4", "metadata": {"page_label": "2", "file_name": "2307_09793v1.pdf", "Title of this paper": "On the Origin of LLMs: An Evolutionary Tree and Graph for 15,821 Large Language Models", "Authors": "Sarah Gao, Andrew Kean Gao", "Date published": "07/19/2023", "URL": "http://arxiv.org/abs/2307.09793v1", "summary": "Since late 2022, Large Language Models (LLMs) have become very prominent with\nLLMs like ChatGPT and Bard receiving millions of users. Hundreds of new LLMs\nare announced each week, many of which are deposited to Hugging Face, a\nrepository of machine learning models and datasets. To date, nearly 16,000 Text\nGeneration models have been uploaded to the site. Given the huge influx of\nLLMs, it is of interest to know which LLM backbones, settings, training\nmethods, and families are popular or trending. However, there is no\ncomprehensive index of LLMs available. We take advantage of the relatively\nsystematic nomenclature of Hugging Face LLMs to perform hierarchical clustering\nand identify communities amongst LLMs using n-grams and term frequency-inverse\ndocument frequency. Our methods successfully identify families of LLMs and\naccurately cluster LLMs into meaningful subgroups. We present a public web\napplication to navigate and explore Constellation, our atlas of 15,821 LLMs.\nConstellation rapidly generates a variety of visualizations, namely\ndendrograms, graphs, word clouds, and scatter plots. Constellation is available\nat the following link: https://constellation.sites.stanford.edu/."}, "hash": "e889d90530b20a11da106aed017a08996b8df75207f3e17ad4f4714c1699b497", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "686c3ce4-2356-4d59-9aaf-b60e636862ec", "node_type": "1", "metadata": {}, "hash": "bbbacef0da011ff0a89f0b0c12727ea4312b6c5e21defc43b57959c7c0b00443", "class_name": "RelatedNodeInfo"}}, "text": "2\nAbstract\nSince\nlate\n2022,\nLarge\nLanguage\nModels\n(LLMs)\nhave\nbecome\nvery\nprominent\nwith\nLLMs\nlike\nChatGPT\nand\nBard\nreceiving\nmillions\nof\nusers.\nHundreds\nof\nnew\nLLMs\nare\nannounced\neach\nweek,\nmany\nof\nwhich\nare\ndeposited\nto\nHugging\nFace,\na\nrepository\nof\nmachine\nlearning\nmodels\nand\ndatasets.\nTo\ndate,\nnearly\n16,000\nText\nGeneration\nmodels\nhave\nbeen\nuploaded\nto\nthe\nsite.\nGiven\nthe\nhuge\ninflux\nof\nLLMs,\nit\nis\nof\ninterest\nto\nknow\nwhich\nLLM\nbackbones,\nsettings,\ntraining\nmethods,\nand\nfamilies\nare\npopular\nor\ntrending.\nHowever ,\nthere\nis\nno\ncomprehensive\nindex\nof\nLLMs\navailable.\nWe\ntake\nadvantage\nof\nthe\nrelatively\nsystematic\nnomenclature\nof\nHugging\nFace\nLLMs\nto\nperform\nhierarchical\nclustering\nand\nidentify\ncommunities\namongst\nLLMs\nusing\nn-grams\nand\nterm\nfrequency-inverse\ndocument\nfrequency .\nOur\nmethods\nsuccessfully\nidentify\nfamilies\nof\nLLMs\nand\naccurately\ncluster\nLLMs\ninto\nmeaningful\nsubgroups.\nWe\npresent\na\npublic\nweb\napplication\nto\nnavigate\nand\nexplore\nConstellation,\nour\natlas\nof\n15,821\nLLMs.\nConstellation\nrapidly\ngenerates\na\nvariety\nof\nvisualizations,\nnamely\ndendrograms,\ngraphs,\nword\nclouds,\nand\nscatter\nplots.\nConstellation\nis\navailable\nat\nthe\nfollowing\nlink:\nhttps://constellation.sites.stanford.edu/\n.\nThe\ndataset\nwe\ncreated\nwill\nbe\nshared\npublicly\non\nGithub,\nunder\n@andrewgcodes\n(\nhttps://github.com/andrewgcodes\n).\nIntroduction\nLarge\nlanguage\nmodels\n(LLMs)\nare\ntrained\nto\ngenerate\nrealistic\ntext\ngiven\na\nuser\nprompt\n[1].\nPopular\nLLMs\ninclude\nChatGPT ,\nBard,\nand\nthe\nLLaMa\nfamily\nof\nmodels\n[2].\nIn\naddition\nto\nlarge\ncompanies\nlike\nOpenAI\nand\nGoogle,\nsmaller\nresearch\ngroups\nand\nindividuals\ncan\nalso\ntrain\nLLMs\nand\nshare\nthem\nthrough\nHugging\nFace,\na\npopular\nmachine\nlearning\nrepository\n[3,4].\nAs\nof\nJuly\n18,\n2023\nat\n12\nPM\n(GMT\n-5),\n15,821\nLLMs\n(or\nat\nleast,\nText\nGeneration\nmodels)\nwere\navailable\npublicly\non\nHugging\nFace.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1831, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "686c3ce4-2356-4d59-9aaf-b60e636862ec": {"__data__": {"id_": "686c3ce4-2356-4d59-9aaf-b60e636862ec", "embedding": null, "metadata": {"page_label": "2", "file_name": "2307_09793v1.pdf", "Title of this paper": "On the Origin of LLMs: An Evolutionary Tree and Graph for 15,821 Large Language Models", "Authors": "Sarah Gao, Andrew Kean Gao", "Date published": "07/19/2023", "URL": "http://arxiv.org/abs/2307.09793v1", "summary": "Since late 2022, Large Language Models (LLMs) have become very prominent with\nLLMs like ChatGPT and Bard receiving millions of users. Hundreds of new LLMs\nare announced each week, many of which are deposited to Hugging Face, a\nrepository of machine learning models and datasets. To date, nearly 16,000 Text\nGeneration models have been uploaded to the site. Given the huge influx of\nLLMs, it is of interest to know which LLM backbones, settings, training\nmethods, and families are popular or trending. However, there is no\ncomprehensive index of LLMs available. We take advantage of the relatively\nsystematic nomenclature of Hugging Face LLMs to perform hierarchical clustering\nand identify communities amongst LLMs using n-grams and term frequency-inverse\ndocument frequency. Our methods successfully identify families of LLMs and\naccurately cluster LLMs into meaningful subgroups. We present a public web\napplication to navigate and explore Constellation, our atlas of 15,821 LLMs.\nConstellation rapidly generates a variety of visualizations, namely\ndendrograms, graphs, word clouds, and scatter plots. Constellation is available\nat the following link: https://constellation.sites.stanford.edu/."}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "00e0297f-659f-4379-a7cb-a1457f8396e8", "node_type": "4", "metadata": {"page_label": "2", "file_name": "2307_09793v1.pdf", "Title of this paper": "On the Origin of LLMs: An Evolutionary Tree and Graph for 15,821 Large Language Models", "Authors": "Sarah Gao, Andrew Kean Gao", "Date published": "07/19/2023", "URL": "http://arxiv.org/abs/2307.09793v1", "summary": "Since late 2022, Large Language Models (LLMs) have become very prominent with\nLLMs like ChatGPT and Bard receiving millions of users. Hundreds of new LLMs\nare announced each week, many of which are deposited to Hugging Face, a\nrepository of machine learning models and datasets. To date, nearly 16,000 Text\nGeneration models have been uploaded to the site. Given the huge influx of\nLLMs, it is of interest to know which LLM backbones, settings, training\nmethods, and families are popular or trending. However, there is no\ncomprehensive index of LLMs available. We take advantage of the relatively\nsystematic nomenclature of Hugging Face LLMs to perform hierarchical clustering\nand identify communities amongst LLMs using n-grams and term frequency-inverse\ndocument frequency. Our methods successfully identify families of LLMs and\naccurately cluster LLMs into meaningful subgroups. We present a public web\napplication to navigate and explore Constellation, our atlas of 15,821 LLMs.\nConstellation rapidly generates a variety of visualizations, namely\ndendrograms, graphs, word clouds, and scatter plots. Constellation is available\nat the following link: https://constellation.sites.stanford.edu/."}, "hash": "e889d90530b20a11da106aed017a08996b8df75207f3e17ad4f4714c1699b497", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8bf0dde0-156d-42a7-b03a-853e5fb40828", "node_type": "1", "metadata": {"page_label": "2", "file_name": "2307_09793v1.pdf", "Title of this paper": "On the Origin of LLMs: An Evolutionary Tree and Graph for 15,821 Large Language Models", "Authors": "Sarah Gao, Andrew Kean Gao", "Date published": "07/19/2023", "URL": "http://arxiv.org/abs/2307.09793v1", "summary": "Since late 2022, Large Language Models (LLMs) have become very prominent with\nLLMs like ChatGPT and Bard receiving millions of users. Hundreds of new LLMs\nare announced each week, many of which are deposited to Hugging Face, a\nrepository of machine learning models and datasets. To date, nearly 16,000 Text\nGeneration models have been uploaded to the site. Given the huge influx of\nLLMs, it is of interest to know which LLM backbones, settings, training\nmethods, and families are popular or trending. However, there is no\ncomprehensive index of LLMs available. We take advantage of the relatively\nsystematic nomenclature of Hugging Face LLMs to perform hierarchical clustering\nand identify communities amongst LLMs using n-grams and term frequency-inverse\ndocument frequency. Our methods successfully identify families of LLMs and\naccurately cluster LLMs into meaningful subgroups. We present a public web\napplication to navigate and explore Constellation, our atlas of 15,821 LLMs.\nConstellation rapidly generates a variety of visualizations, namely\ndendrograms, graphs, word clouds, and scatter plots. Constellation is available\nat the following link: https://constellation.sites.stanford.edu/."}, "hash": "41204b8b740bd09b3310aac4f682892aeb06cd7bcf8ae48243a8d60d76b198ca", "class_name": "RelatedNodeInfo"}}, "text": "Introduction\nLarge\nlanguage\nmodels\n(LLMs)\nare\ntrained\nto\ngenerate\nrealistic\ntext\ngiven\na\nuser\nprompt\n[1].\nPopular\nLLMs\ninclude\nChatGPT ,\nBard,\nand\nthe\nLLaMa\nfamily\nof\nmodels\n[2].\nIn\naddition\nto\nlarge\ncompanies\nlike\nOpenAI\nand\nGoogle,\nsmaller\nresearch\ngroups\nand\nindividuals\ncan\nalso\ntrain\nLLMs\nand\nshare\nthem\nthrough\nHugging\nFace,\na\npopular\nmachine\nlearning\nrepository\n[3,4].\nAs\nof\nJuly\n18,\n2023\nat\n12\nPM\n(GMT\n-5),\n15,821\nLLMs\n(or\nat\nleast,\nText\nGeneration\nmodels)\nwere\navailable\npublicly\non\nHugging\nFace.\nTo\nour\nknowledge,\nfew\nattempts\nhave\nbeen\nmade\nto\norganize\nthese\nLLMs,\nperhaps\ndue\nto\nthe\nimmense\nnumber\nof\nmodels.\nInspired\nby\nthe\nbioinformatics\ntechnique\nof\nusing\nhierarchical\nclustering\non\nDNA\nsequences,\nwe\napply\nhierarchical\nclustering\nto\nthe\nHugging\nFace\nmodel\nnames,\nassuming\nthat\nsimilar\nnames\nindicate\nsimilarity\n[5].\nWe\nalso\nconstruct\na\ngraph\nof\nLLMs\nand\ndetect\ncommunities\nusing\nthe\nLouvain\nmethod.\nAdditionally ,\nwe\ngenerate\nother\nvisualizations\nand\nexplore\nthe\ndata.", "mimetype": "text/plain", "start_char_idx": 1326, "end_char_idx": 2310, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a6385ae7-d112-49c7-a369-b30e4a11347e": {"__data__": {"id_": "a6385ae7-d112-49c7-a369-b30e4a11347e", "embedding": null, "metadata": {"page_label": "3", "file_name": "2307_09793v1.pdf", "Title of this paper": "On the Origin of LLMs: An Evolutionary Tree and Graph for 15,821 Large Language Models", "Authors": "Sarah Gao, Andrew Kean Gao", "Date published": "07/19/2023", "URL": "http://arxiv.org/abs/2307.09793v1", "summary": "Since late 2022, Large Language Models (LLMs) have become very prominent with\nLLMs like ChatGPT and Bard receiving millions of users. Hundreds of new LLMs\nare announced each week, many of which are deposited to Hugging Face, a\nrepository of machine learning models and datasets. To date, nearly 16,000 Text\nGeneration models have been uploaded to the site. Given the huge influx of\nLLMs, it is of interest to know which LLM backbones, settings, training\nmethods, and families are popular or trending. However, there is no\ncomprehensive index of LLMs available. We take advantage of the relatively\nsystematic nomenclature of Hugging Face LLMs to perform hierarchical clustering\nand identify communities amongst LLMs using n-grams and term frequency-inverse\ndocument frequency. Our methods successfully identify families of LLMs and\naccurately cluster LLMs into meaningful subgroups. We present a public web\napplication to navigate and explore Constellation, our atlas of 15,821 LLMs.\nConstellation rapidly generates a variety of visualizations, namely\ndendrograms, graphs, word clouds, and scatter plots. Constellation is available\nat the following link: https://constellation.sites.stanford.edu/."}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c7bb9a83-f55f-4569-8d47-413a0b60a196", "node_type": "4", "metadata": {"page_label": "3", "file_name": "2307_09793v1.pdf", "Title of this paper": "On the Origin of LLMs: An Evolutionary Tree and Graph for 15,821 Large Language Models", "Authors": "Sarah Gao, Andrew Kean Gao", "Date published": "07/19/2023", "URL": "http://arxiv.org/abs/2307.09793v1", "summary": "Since late 2022, Large Language Models (LLMs) have become very prominent with\nLLMs like ChatGPT and Bard receiving millions of users. Hundreds of new LLMs\nare announced each week, many of which are deposited to Hugging Face, a\nrepository of machine learning models and datasets. To date, nearly 16,000 Text\nGeneration models have been uploaded to the site. Given the huge influx of\nLLMs, it is of interest to know which LLM backbones, settings, training\nmethods, and families are popular or trending. However, there is no\ncomprehensive index of LLMs available. We take advantage of the relatively\nsystematic nomenclature of Hugging Face LLMs to perform hierarchical clustering\nand identify communities amongst LLMs using n-grams and term frequency-inverse\ndocument frequency. Our methods successfully identify families of LLMs and\naccurately cluster LLMs into meaningful subgroups. We present a public web\napplication to navigate and explore Constellation, our atlas of 15,821 LLMs.\nConstellation rapidly generates a variety of visualizations, namely\ndendrograms, graphs, word clouds, and scatter plots. Constellation is available\nat the following link: https://constellation.sites.stanford.edu/."}, "hash": "49cb72862f7bdcec0838ac66b8e12f01d5c7f87a1ed377d0de805b8fd751c5af", "class_name": "RelatedNodeInfo"}}, "text": "3\nMethods\nLibraries\n\u25cf\nBeautifulSoup\n[6]\n\u25cf\nPandas\n[7]\n\u25cf\nStreamlit\n[8]\n\u25cf\nScipy\n[9]\n\u25cf\nPlotly\n[10]\n\u25cf\nNumpy\n[11]\n\u25cf\nScikit-learn\n[12]\n\u25cf\nRadial\nTree\n[13]\n\u25cf\nNLTK\n[14]\n\u25cf\nMatplotlib\n[15]\n\u25cf\nPython-Louvain\n[16]\n\u25cf\nNetworkX\n[17]\n\u25cf\nWordcloud\n[18]\n\u25cf\nRegEx\n[19]\nData\nCollection\nPython's\nBeautifulSoup\nlibrary\nwas\nused\nto\nretrieve\nthe\nnames,\nnumber\nof\nlikes,\nand\nnumber\nof\ndownloads\nof\nHugging\nFace\nmodels\nlabeled\nwith\n\u201cText\nGeneration\u201d.\nData\ncollected\nincluded\nmodel\nnames,\nReadme\nlinks,\nnumber\nof\ndownloads,\nand\nthe\nnumber\nof\nlikes.\nData\ncollection\nwas\nperformed\non\nJuly\n18,\n2023\naround\n12\nPM\n(US\nET;\nGMT\n-5).\nNote\nthat\ndata\ncollection\nwas\nnot\ninstantaneous.\nIn\nsome\ninstances,\nwe\nfailed\nto\nretrieve\na\nnumber\nof\nlikes\nor\ndownloads\nfor\na\nmodel.\nParameter\nExtraction\nIn\naddition\nto\nthe\nabove\nattributes,\nmodel\nparameters\nwere\ninferred\nfrom\nthe\nmodel\nname\nusing\na\nregular\nexpression\n(RegEx)\npattern\n(\\d+(\\.\\d+)?)(B|M|b|m).\nThis\npattern\nmatches\ndigit\nsequences\nfollowed\nby\n\"B\",\n\"M\",\n\"b\",\nor\n\"m\",\nas\nmodel\nsizes\nare\noften\nincluded\nin\nthe\nname\n(e.g.,\n\"falcon-7b\").\nThe\nnumber\nof\nparameters\nin\nmillions\nwas\nrecorded\nin\na\ncolumn\nnamed\n'params_millions'\nin\nthe\ndataset.\nIf\nparameters\ncouldn't\nbe\ninferred,\nthe\ncorresponding\nfield\nwas\nmarked\nas\n'NaN'.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1225, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f09e5fe8-d050-42bf-88e2-59e66df420c1": {"__data__": {"id_": "f09e5fe8-d050-42bf-88e2-59e66df420c1", "embedding": null, "metadata": {"page_label": "4", "file_name": "2307_09793v1.pdf", "Title of this paper": "On the Origin of LLMs: An Evolutionary Tree and Graph for 15,821 Large Language Models", "Authors": "Sarah Gao, Andrew Kean Gao", "Date published": "07/19/2023", "URL": "http://arxiv.org/abs/2307.09793v1", "summary": "Since late 2022, Large Language Models (LLMs) have become very prominent with\nLLMs like ChatGPT and Bard receiving millions of users. Hundreds of new LLMs\nare announced each week, many of which are deposited to Hugging Face, a\nrepository of machine learning models and datasets. To date, nearly 16,000 Text\nGeneration models have been uploaded to the site. Given the huge influx of\nLLMs, it is of interest to know which LLM backbones, settings, training\nmethods, and families are popular or trending. However, there is no\ncomprehensive index of LLMs available. We take advantage of the relatively\nsystematic nomenclature of Hugging Face LLMs to perform hierarchical clustering\nand identify communities amongst LLMs using n-grams and term frequency-inverse\ndocument frequency. Our methods successfully identify families of LLMs and\naccurately cluster LLMs into meaningful subgroups. We present a public web\napplication to navigate and explore Constellation, our atlas of 15,821 LLMs.\nConstellation rapidly generates a variety of visualizations, namely\ndendrograms, graphs, word clouds, and scatter plots. Constellation is available\nat the following link: https://constellation.sites.stanford.edu/."}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c8560985-7ab4-440b-9b4e-81e61e7d93dd", "node_type": "4", "metadata": {"page_label": "4", "file_name": "2307_09793v1.pdf", "Title of this paper": "On the Origin of LLMs: An Evolutionary Tree and Graph for 15,821 Large Language Models", "Authors": "Sarah Gao, Andrew Kean Gao", "Date published": "07/19/2023", "URL": "http://arxiv.org/abs/2307.09793v1", "summary": "Since late 2022, Large Language Models (LLMs) have become very prominent with\nLLMs like ChatGPT and Bard receiving millions of users. Hundreds of new LLMs\nare announced each week, many of which are deposited to Hugging Face, a\nrepository of machine learning models and datasets. To date, nearly 16,000 Text\nGeneration models have been uploaded to the site. Given the huge influx of\nLLMs, it is of interest to know which LLM backbones, settings, training\nmethods, and families are popular or trending. However, there is no\ncomprehensive index of LLMs available. We take advantage of the relatively\nsystematic nomenclature of Hugging Face LLMs to perform hierarchical clustering\nand identify communities amongst LLMs using n-grams and term frequency-inverse\ndocument frequency. Our methods successfully identify families of LLMs and\naccurately cluster LLMs into meaningful subgroups. We present a public web\napplication to navigate and explore Constellation, our atlas of 15,821 LLMs.\nConstellation rapidly generates a variety of visualizations, namely\ndendrograms, graphs, word clouds, and scatter plots. Constellation is available\nat the following link: https://constellation.sites.stanford.edu/."}, "hash": "0ad73477b92139c2cc730b464cd79ec205ab6eb2b42f3af67874c509ccc75f32", "class_name": "RelatedNodeInfo"}}, "text": "4\nData\nAnalysis\nand\nVisualization\nWe\nused\nlibraries\nsuch\nas\nScipy ,\nPlotly ,\nNumpy ,\nScikit-learn,\nRadial\nTree,\nNLTK,\nand\nMatplotlib\nto\nanalyze\nthe\ndataset\nand\ngenerate\nvisualizations.\nA\nfull\nlist\nof\nimports\nis\nprovided\nat\nthe\nbeginning\nof\nMethods.\nText\nFeature\nExtraction:\nThe\nmodel\nnames\nwere\nconverted\ninto\na\nmatrix\nof\nTerm\nFrequency-Inverse\nDocument\nFrequency\n(TF-IDF)\nfeatures\nusing\nScikit-learn's\nTfidfV ectorizer .\nThe\nvectorizer\nwas\nconfigured\nto\nbreak\ndown\nthe\nmodel\nnames\ninto\nn-grams\nranging\nfrom\n2\nto\n8\ncharacters.\nHierarchical\nClustering:\nHierarchical\nclustering\nwith\nsingle\nlinkage\nwas\nperformed\nusing\nthe\nmatrix\nof\nTF-IDF\nfeatures.\nCosine\ndistance\nwas\nused\nas\na\nsimilarity\nmeasure\nbetween\nthe\nmodel\nnames.\nThe\nclustering\nresult\nwas\nvisualized\nas\nan\ninteractive\ndendrogram\nusing\nthe\nPlotly\nlibrary .\nAgglomerative\nClustering:\nIn\naddition\nto\nhierarchical\nclustering,\nagglomerative\nclustering\nwas\nalso\nperformed\nwith\na\nspecified\nnumber\nof\nclusters.\nThe\nsize\nof\neach\ncluster\nwas\ncalculated\nand\nvisualized\nas\na\nbar\nchart.\nWord\nClouds\nTo\nunderstand\nthe\ncontents\nof\nour\nagglomerative\nclusters,\nwe\ngenerate\nWord\nClouds\nthe\nmost\ncommon\nn-grams\nin\neach\ncluster ,\nwith\nmore\nfrequent\nn-grams\nbeing\npresented\nwith\nlarger\ntext\nsize.\nGraph\nVisualization\nwith\nCommunities\nA\ngraph-based\nvisualization\nwas\nconstructed\nto\nprovide\nan\nintuitive\nunderstanding\nof\nthe\nrelationship\nand\nsimilarity\namong\nthe\nmodels.\nNetworkX,\na\nPython\nlibrary\nfor\nthe\ncreation,\nmanipulation,\nand\nstudy\nof\nthe\nstructure,\ndynamics,\nand\nfunctions\nof\ncomplex\nnetworks,\nwas\nused\nto\ngenerate\nthe\ngraph.\nNode\nCreation:\nEach\nmodel\nname\nwas\nrepresented\nas\na\nnode\nin\nthe\ngraph.\nThe\ngraph\nwas\ninitialized\nas\nan\nundirected\ngraph,\nand\neach\nmodel\nname\nwas\nadded\nas\na\nnode\nusing\nthe\n'add_node'\nfunction.\nThe\nmodel\nname\nserved\nas\nthe\nnode\nlabel.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1802, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c0d9c9ac-5437-45ca-b6ab-b88cc06dccd9": {"__data__": {"id_": "c0d9c9ac-5437-45ca-b6ab-b88cc06dccd9", "embedding": null, "metadata": {"page_label": "5", "file_name": "2307_09793v1.pdf", "Title of this paper": "On the Origin of LLMs: An Evolutionary Tree and Graph for 15,821 Large Language Models", "Authors": "Sarah Gao, Andrew Kean Gao", "Date published": "07/19/2023", "URL": "http://arxiv.org/abs/2307.09793v1", "summary": "Since late 2022, Large Language Models (LLMs) have become very prominent with\nLLMs like ChatGPT and Bard receiving millions of users. Hundreds of new LLMs\nare announced each week, many of which are deposited to Hugging Face, a\nrepository of machine learning models and datasets. To date, nearly 16,000 Text\nGeneration models have been uploaded to the site. Given the huge influx of\nLLMs, it is of interest to know which LLM backbones, settings, training\nmethods, and families are popular or trending. However, there is no\ncomprehensive index of LLMs available. We take advantage of the relatively\nsystematic nomenclature of Hugging Face LLMs to perform hierarchical clustering\nand identify communities amongst LLMs using n-grams and term frequency-inverse\ndocument frequency. Our methods successfully identify families of LLMs and\naccurately cluster LLMs into meaningful subgroups. We present a public web\napplication to navigate and explore Constellation, our atlas of 15,821 LLMs.\nConstellation rapidly generates a variety of visualizations, namely\ndendrograms, graphs, word clouds, and scatter plots. Constellation is available\nat the following link: https://constellation.sites.stanford.edu/."}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "0d9ee52b-e573-452b-a07d-fd2adda8bfef", "node_type": "4", "metadata": {"page_label": "5", "file_name": "2307_09793v1.pdf", "Title of this paper": "On the Origin of LLMs: An Evolutionary Tree and Graph for 15,821 Large Language Models", "Authors": "Sarah Gao, Andrew Kean Gao", "Date published": "07/19/2023", "URL": "http://arxiv.org/abs/2307.09793v1", "summary": "Since late 2022, Large Language Models (LLMs) have become very prominent with\nLLMs like ChatGPT and Bard receiving millions of users. Hundreds of new LLMs\nare announced each week, many of which are deposited to Hugging Face, a\nrepository of machine learning models and datasets. To date, nearly 16,000 Text\nGeneration models have been uploaded to the site. Given the huge influx of\nLLMs, it is of interest to know which LLM backbones, settings, training\nmethods, and families are popular or trending. However, there is no\ncomprehensive index of LLMs available. We take advantage of the relatively\nsystematic nomenclature of Hugging Face LLMs to perform hierarchical clustering\nand identify communities amongst LLMs using n-grams and term frequency-inverse\ndocument frequency. Our methods successfully identify families of LLMs and\naccurately cluster LLMs into meaningful subgroups. We present a public web\napplication to navigate and explore Constellation, our atlas of 15,821 LLMs.\nConstellation rapidly generates a variety of visualizations, namely\ndendrograms, graphs, word clouds, and scatter plots. Constellation is available\nat the following link: https://constellation.sites.stanford.edu/."}, "hash": "b2c066a57afae5844467c0dd7cd51dd2aa3fd92bb0f3afc5c19e6d7b4b9c7221", "class_name": "RelatedNodeInfo"}}, "text": "5\nEdge\nCreation:\nEdges\nin\nthe\ngraph\nwere\nused\nto\nrepresent\nthe\nsimilarity\nbetween\npairs\nof\nmodel\nnames.\nAfter\ncalculating\nthe\ncosine\nsimilarity\nmatrix,\nan\nedge\nwas\nadded\nbetween\ntwo\nnodes\n(model\nnames)\nif\ntheir\ncosine\nsimilarity\nwas\nabove\na\nspecific\nthreshold\n(0.2\nin\nthis\ncase).\nThe\ncosine\nsimilarity\nvalue\nwas\nset\nas\nthe\nweight\nof\nthe\nedge.\nCommunity\nDetection:\nThe\nLouvain\nmethod,\na\npopular\ncommunity\ndetection\nalgorithm,\nwas\nused\nto\nfind\ncommunities\nwithin\nthe\nconstructed\ngraph.\nCommunities\nrepresent\ngroups\nof\nmodels\nthat\nare\nmore\nsimilar\nto\neach\nother\nthan\nto\nmodels\nin\nother\ngroups.\nThe\ndetected\ncommunities\nwere\nused\nfor\nsubsequent\nvisual\nenhancements.\nLayout\nCalculation:\nThe\nFruchterman-Reingold\nforce-directed\nalgorithm\nwas\nemployed\nto\ncalculate\nthe\nlayout\nof\nnodes\nin\nthe\ngraph.\nThis\nalgorithm\narranges\nthe\nnodes\nin\nsuch\na\nway\nthat\nall\nthe\nedges\nare\nmore\nor\nless\nequally\nlong\nand\nthere\nare\nas\nfew\ncrossing\nedges\nas\npossible.\nInteractive\nVisualization:\nThe\ngenerated\ngraph\nwas\nvisualized\ninteractively\nusing\nthe\nPlotly\nlibrary .\nEach\nnode\nrepresented\na\nmodel\nand\nwas\ncolor -coded\nbased\non\nthe\ncommunity\nit\nbelonged\nto.\nEdges\nbetween\nthe\nnodes\nindicated\nsimilarity ,\nwith\ntheir\nthickness\ncorresponding\nto\nthe\ncosine\nsimilarity\nscore.\nHovering\nover\nthe\nnodes\ndisplayed\nmore\ndetails\nabout\nthe\nmodel.\nAdditional\nEnhancements:\nThe\ncentroid\n(center\npoint)\nof\neach\ncommunity\nwas\ncomputed\nto\nadd\na\ncolored\nbackground\nfor\neach\ncommunity\ncluster .\nThe\nsize\nof\nthe\nbackground\ncolor\npatch\nrepresented\nthe\nsize\nof\nthe\ncommunity .\nWeb\nApplication\nWe\nbuilt\na\npublic\nweb\napplication\nusing\nthe\nStreamlit\nframework\nto\ngenerate\ninteractive\ndendrograms,\nword\nclouds,\nand\ngraphs\nfor\nthe\ndata,\nwhich\nis\navailable\nhere:\nhttps://constellation.sites.stanford.edu/\n.\nResults\nThere\nwere\n15,821\npublic\nmodels\nlabeled\nwith\nText\nGeneration\non\nHugging\nFace\nat\nthe\ntime\nof\ndata\ncollection.\nWe\nassembled\na\nfinal\nPandas\ndataframe\ncontaining\nseven\ncolumns:\nrank,", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1939, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2651af6b-d63f-487c-bfc3-b8fe9a3012c3": {"__data__": {"id_": "2651af6b-d63f-487c-bfc3-b8fe9a3012c3", "embedding": null, "metadata": {"page_label": "6", "file_name": "2307_09793v1.pdf", "Title of this paper": "On the Origin of LLMs: An Evolutionary Tree and Graph for 15,821 Large Language Models", "Authors": "Sarah Gao, Andrew Kean Gao", "Date published": "07/19/2023", "URL": "http://arxiv.org/abs/2307.09793v1", "summary": "Since late 2022, Large Language Models (LLMs) have become very prominent with\nLLMs like ChatGPT and Bard receiving millions of users. Hundreds of new LLMs\nare announced each week, many of which are deposited to Hugging Face, a\nrepository of machine learning models and datasets. To date, nearly 16,000 Text\nGeneration models have been uploaded to the site. Given the huge influx of\nLLMs, it is of interest to know which LLM backbones, settings, training\nmethods, and families are popular or trending. However, there is no\ncomprehensive index of LLMs available. We take advantage of the relatively\nsystematic nomenclature of Hugging Face LLMs to perform hierarchical clustering\nand identify communities amongst LLMs using n-grams and term frequency-inverse\ndocument frequency. Our methods successfully identify families of LLMs and\naccurately cluster LLMs into meaningful subgroups. We present a public web\napplication to navigate and explore Constellation, our atlas of 15,821 LLMs.\nConstellation rapidly generates a variety of visualizations, namely\ndendrograms, graphs, word clouds, and scatter plots. Constellation is available\nat the following link: https://constellation.sites.stanford.edu/."}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "4e5bb8b1-c29b-4b39-af9f-9778183178bc", "node_type": "4", "metadata": {"page_label": "6", "file_name": "2307_09793v1.pdf", "Title of this paper": "On the Origin of LLMs: An Evolutionary Tree and Graph for 15,821 Large Language Models", "Authors": "Sarah Gao, Andrew Kean Gao", "Date published": "07/19/2023", "URL": "http://arxiv.org/abs/2307.09793v1", "summary": "Since late 2022, Large Language Models (LLMs) have become very prominent with\nLLMs like ChatGPT and Bard receiving millions of users. Hundreds of new LLMs\nare announced each week, many of which are deposited to Hugging Face, a\nrepository of machine learning models and datasets. To date, nearly 16,000 Text\nGeneration models have been uploaded to the site. Given the huge influx of\nLLMs, it is of interest to know which LLM backbones, settings, training\nmethods, and families are popular or trending. However, there is no\ncomprehensive index of LLMs available. We take advantage of the relatively\nsystematic nomenclature of Hugging Face LLMs to perform hierarchical clustering\nand identify communities amongst LLMs using n-grams and term frequency-inverse\ndocument frequency. Our methods successfully identify families of LLMs and\naccurately cluster LLMs into meaningful subgroups. We present a public web\napplication to navigate and explore Constellation, our atlas of 15,821 LLMs.\nConstellation rapidly generates a variety of visualizations, namely\ndendrograms, graphs, word clouds, and scatter plots. Constellation is available\nat the following link: https://constellation.sites.stanford.edu/."}, "hash": "03cb5825038fc36218b580cc3deaef502048aea1317cccbacae8a80330ea2728", "class_name": "RelatedNodeInfo"}}, "text": "6\nmodel_name,\nlink,\ndownloads,\nlikes,\nReadMeLink,\nand\nparams_millions.\nRank\nis\nassigned\nin\norder\nof\nnumber\nof\ndownloads.\nFor\ninstance,\n\u201cgpt2\u201d\nhas\nthe\nmost\ndownloads.\nNote\nthat\n\u201cgpt2\u201d\ndoes\nnot\nhave\nan\ninferred\nnumber\nof\nparameters\nbecause\nthe\nmodel\nname\ndoes\nnot\ncontain\nany\nevidence\nof\nparameter\nsize.\nWe\nwere\nable\nto\ninfer\nmodel\nparameters\nfor\n4,560\nmodels\n(28.8%).\nWe\nexpect\nour\nRegEx\nexpression\nto\nresult\nin\nfew\nfalse\npositives.\nNot\nall\nlinks\nin\nReadMeLink\nlead\nto\na\nvalid\nReadme\nfile.\nThe\nlinks\nwere\nautomatically\ncomputed\nby\nappending\n\u201c/raw/main/README.md\u201d\nto\nthe\nmodel\nlink.\nAll\nmodel\nlinks\nshould\nlead\nto\na\nvalid\nHugging\nFace\npage.\nFigur e\n1.\nFirst\nfive\nrows\nof\nour\ndataset\nin\norder\nof\nnumber\nof\ndownloads.\nWe\ncomputed\na\nPearson\ncorrelation\ncoefficient\nof\n0.242\nbetween\nthe\nnumber\nof\nlikes\nand\ndownloads\na\nmodel\nreceives.\nThere\nis\na\nclear\npositive\nbut\nweak\nrelationship.\nIt\nis\npossible\nthat\nthis\nweakness\nindicates\na\ndisparity\nbetween\nmodel\nusefulness\nand\npopularity .\nAlternatively ,\nlarger,\nmore\npowerful\nmodels\nmay\nattract\nmore\nattention\n(receiving\nmore\nlikes)\nbut\nwill\ngarner\nrelatively\nfew\ndownloads\nbecause\nthey\nare\ntoo\nlarge\nfor\nmost\nHugging\nFace\nusers\nto\nuse.\nIn\ngeneral,\nmodels\ntend\nto\nreceive\nfar\nmore\ndownloads\nthan\nlikes.\nThis\ncould\nbe\nbecause\nthere\nis\nno\nbenefit\nto\nthe\nuser\nto\nlike\na\nmodel\non\nHugging\nFace,\nwhile\ndownloading\nthe\nmodel\nis\nbeneficial.\nWe\ngenerate\na\nradial\ndendrogram\non\nall\nmodels\nwith\nover\n5,000\ndownloads\nto\ncompactly\nvisualize\nrelationships\nand\nfamilies.\nFrom\nthe\ndendrogram,\nfamilies\nof\nLLMs\nlike\nWizard,\nPythia,\nCausalLM,\nand\nBloom\ncan\nbe\nobserved.\nWe\nsuggest\nusing\nthe\nweb\napplication\nto\nview\nthe\ndendrogram\nsince\nthe\nlarge\nnumber\nof\nleaves\nmakes\nit\ndifficult\nto\nrender\non\na\nsingle\nstatic\nimage\nclearly .", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1746, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "26641d65-4c2b-4ebb-9da1-3d8488f51ca8": {"__data__": {"id_": "26641d65-4c2b-4ebb-9da1-3d8488f51ca8", "embedding": null, "metadata": {"page_label": "7", "file_name": "2307_09793v1.pdf", "Title of this paper": "On the Origin of LLMs: An Evolutionary Tree and Graph for 15,821 Large Language Models", "Authors": "Sarah Gao, Andrew Kean Gao", "Date published": "07/19/2023", "URL": "http://arxiv.org/abs/2307.09793v1", "summary": "Since late 2022, Large Language Models (LLMs) have become very prominent with\nLLMs like ChatGPT and Bard receiving millions of users. Hundreds of new LLMs\nare announced each week, many of which are deposited to Hugging Face, a\nrepository of machine learning models and datasets. To date, nearly 16,000 Text\nGeneration models have been uploaded to the site. Given the huge influx of\nLLMs, it is of interest to know which LLM backbones, settings, training\nmethods, and families are popular or trending. However, there is no\ncomprehensive index of LLMs available. We take advantage of the relatively\nsystematic nomenclature of Hugging Face LLMs to perform hierarchical clustering\nand identify communities amongst LLMs using n-grams and term frequency-inverse\ndocument frequency. Our methods successfully identify families of LLMs and\naccurately cluster LLMs into meaningful subgroups. We present a public web\napplication to navigate and explore Constellation, our atlas of 15,821 LLMs.\nConstellation rapidly generates a variety of visualizations, namely\ndendrograms, graphs, word clouds, and scatter plots. Constellation is available\nat the following link: https://constellation.sites.stanford.edu/."}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a4679295-7258-4592-8f65-95a6220bb594", "node_type": "4", "metadata": {"page_label": "7", "file_name": "2307_09793v1.pdf", "Title of this paper": "On the Origin of LLMs: An Evolutionary Tree and Graph for 15,821 Large Language Models", "Authors": "Sarah Gao, Andrew Kean Gao", "Date published": "07/19/2023", "URL": "http://arxiv.org/abs/2307.09793v1", "summary": "Since late 2022, Large Language Models (LLMs) have become very prominent with\nLLMs like ChatGPT and Bard receiving millions of users. Hundreds of new LLMs\nare announced each week, many of which are deposited to Hugging Face, a\nrepository of machine learning models and datasets. To date, nearly 16,000 Text\nGeneration models have been uploaded to the site. Given the huge influx of\nLLMs, it is of interest to know which LLM backbones, settings, training\nmethods, and families are popular or trending. However, there is no\ncomprehensive index of LLMs available. We take advantage of the relatively\nsystematic nomenclature of Hugging Face LLMs to perform hierarchical clustering\nand identify communities amongst LLMs using n-grams and term frequency-inverse\ndocument frequency. Our methods successfully identify families of LLMs and\naccurately cluster LLMs into meaningful subgroups. We present a public web\napplication to navigate and explore Constellation, our atlas of 15,821 LLMs.\nConstellation rapidly generates a variety of visualizations, namely\ndendrograms, graphs, word clouds, and scatter plots. Constellation is available\nat the following link: https://constellation.sites.stanford.edu/."}, "hash": "2f8b4a008f761c9c95a6d8a45e8e7a1c08ffc417aac709351be84148fe917a00", "class_name": "RelatedNodeInfo"}}, "text": "7\nFigur e\n2.\nScatter\nplot\nshowing\nthe\nrelationship\nbetween\nthe\nnumber\nof\nlikes\nand\ndownloads\na\nmodel\nreceives.\nBoth\naxes\nreceived\na\nlog\nscale\ntransformation.\n  \nFigur e\n3.\nRadial\ndendrogram\nof\nmodels\nwith\nover\n5,000\ndownloads.\nHigh\nresolution\nimage\navailable\non\nConstellation\nweb\nsite.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 285, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "07d317d5-863f-4137-9848-4e1b69a580e4": {"__data__": {"id_": "07d317d5-863f-4137-9848-4e1b69a580e4", "embedding": null, "metadata": {"page_label": "8", "file_name": "2307_09793v1.pdf", "Title of this paper": "On the Origin of LLMs: An Evolutionary Tree and Graph for 15,821 Large Language Models", "Authors": "Sarah Gao, Andrew Kean Gao", "Date published": "07/19/2023", "URL": "http://arxiv.org/abs/2307.09793v1", "summary": "Since late 2022, Large Language Models (LLMs) have become very prominent with\nLLMs like ChatGPT and Bard receiving millions of users. Hundreds of new LLMs\nare announced each week, many of which are deposited to Hugging Face, a\nrepository of machine learning models and datasets. To date, nearly 16,000 Text\nGeneration models have been uploaded to the site. Given the huge influx of\nLLMs, it is of interest to know which LLM backbones, settings, training\nmethods, and families are popular or trending. However, there is no\ncomprehensive index of LLMs available. We take advantage of the relatively\nsystematic nomenclature of Hugging Face LLMs to perform hierarchical clustering\nand identify communities amongst LLMs using n-grams and term frequency-inverse\ndocument frequency. Our methods successfully identify families of LLMs and\naccurately cluster LLMs into meaningful subgroups. We present a public web\napplication to navigate and explore Constellation, our atlas of 15,821 LLMs.\nConstellation rapidly generates a variety of visualizations, namely\ndendrograms, graphs, word clouds, and scatter plots. Constellation is available\nat the following link: https://constellation.sites.stanford.edu/."}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "39fe3ec6-746d-4d8f-9b2f-e51a464704cb", "node_type": "4", "metadata": {"page_label": "8", "file_name": "2307_09793v1.pdf", "Title of this paper": "On the Origin of LLMs: An Evolutionary Tree and Graph for 15,821 Large Language Models", "Authors": "Sarah Gao, Andrew Kean Gao", "Date published": "07/19/2023", "URL": "http://arxiv.org/abs/2307.09793v1", "summary": "Since late 2022, Large Language Models (LLMs) have become very prominent with\nLLMs like ChatGPT and Bard receiving millions of users. Hundreds of new LLMs\nare announced each week, many of which are deposited to Hugging Face, a\nrepository of machine learning models and datasets. To date, nearly 16,000 Text\nGeneration models have been uploaded to the site. Given the huge influx of\nLLMs, it is of interest to know which LLM backbones, settings, training\nmethods, and families are popular or trending. However, there is no\ncomprehensive index of LLMs available. We take advantage of the relatively\nsystematic nomenclature of Hugging Face LLMs to perform hierarchical clustering\nand identify communities amongst LLMs using n-grams and term frequency-inverse\ndocument frequency. Our methods successfully identify families of LLMs and\naccurately cluster LLMs into meaningful subgroups. We present a public web\napplication to navigate and explore Constellation, our atlas of 15,821 LLMs.\nConstellation rapidly generates a variety of visualizations, namely\ndendrograms, graphs, word clouds, and scatter plots. Constellation is available\nat the following link: https://constellation.sites.stanford.edu/."}, "hash": "889d161ac2dddb516f93c8a5739074a1983cf21708423b75443614fb37303084", "class_name": "RelatedNodeInfo"}}, "text": "8\nWe\ndo\nnot\nshow\nall\nthe\nword\nclouds\nhere\ndue\nto\nspace,\nbut\nhere\nare\nsome\nof\nthe\nexample\nword\nclouds\nfor\nclusters\ngenerated\nfrom\nall\nmodels\nwith\nover\n1,000\ndownloads\n(clusters\n=\n20).\nThe\nword\nclouds\nare\nhelpful\nin\nunderstanding\nwhich\nmodel\nfamilies\nare\nprominent.\nFigur e\n4.\nWord\nclouds\nshow\nclusterization\nof\nLLaMa\nmodels\nand\ncode-specific\nLLMs.\nWe\ngenerate\na\ngraph\nof\nthe\nmodels,\nwith\nsimilar\nmodels\nreceiving\nan\nedge.\nWe\nuse\nthe\nLouvain\nmethod\nto\ndetect\ncommunities.\nFigur e\n5.\nGraph\nof\nmodels\nwith\nmore\nthan\n10,000\ndownloads,\nwith\ncommunities\ndetected\nusing\nthe\nLouvain\nmethod.\nWe\npresent\na\npublicly\navailable\nweb\napplication\n(\nhttps://constellation.sites.stanford.edu/\n)\nto\ndynamically\nexplore\nthe\ndata.\nThe\nweb\napplication\nenables\nthe\nuser\nto\nspecify\nthe\nminimum\nnumber\nof\ndownloads\nan\nLLM\nmust\nhave\nto\nbe\nconsidered\nin\nthe\nanalysis.\nThe\nweb\napp\nquickly\ngenerates\na\ndendrogram,\nword\nclouds,\nand\ngraph.\nHovering\nover\ngraph\nnodes\nreveals\nadditional\nmetadata\nabout\nthe\nmodel\nlocated\nat\nthat\nnode.\nThe\nweb\napplication\nalso\ndisplays\nuseful\nstatistics\nand\nan\ninteractive\nscatter\nplot\nof\nlikes\nversus\ndownloads.\nHovering\nover\npoints\nreveals\nthe\nmodel\nname.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1155, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "658dc46b-77d4-4a97-b444-ee5ec3455b36": {"__data__": {"id_": "658dc46b-77d4-4a97-b444-ee5ec3455b36", "embedding": null, "metadata": {"page_label": "9", "file_name": "2307_09793v1.pdf", "Title of this paper": "On the Origin of LLMs: An Evolutionary Tree and Graph for 15,821 Large Language Models", "Authors": "Sarah Gao, Andrew Kean Gao", "Date published": "07/19/2023", "URL": "http://arxiv.org/abs/2307.09793v1", "summary": "Since late 2022, Large Language Models (LLMs) have become very prominent with\nLLMs like ChatGPT and Bard receiving millions of users. Hundreds of new LLMs\nare announced each week, many of which are deposited to Hugging Face, a\nrepository of machine learning models and datasets. To date, nearly 16,000 Text\nGeneration models have been uploaded to the site. Given the huge influx of\nLLMs, it is of interest to know which LLM backbones, settings, training\nmethods, and families are popular or trending. However, there is no\ncomprehensive index of LLMs available. We take advantage of the relatively\nsystematic nomenclature of Hugging Face LLMs to perform hierarchical clustering\nand identify communities amongst LLMs using n-grams and term frequency-inverse\ndocument frequency. Our methods successfully identify families of LLMs and\naccurately cluster LLMs into meaningful subgroups. We present a public web\napplication to navigate and explore Constellation, our atlas of 15,821 LLMs.\nConstellation rapidly generates a variety of visualizations, namely\ndendrograms, graphs, word clouds, and scatter plots. Constellation is available\nat the following link: https://constellation.sites.stanford.edu/."}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "196a9988-356c-40ae-b363-ca90fb4f3e62", "node_type": "4", "metadata": {"page_label": "9", "file_name": "2307_09793v1.pdf", "Title of this paper": "On the Origin of LLMs: An Evolutionary Tree and Graph for 15,821 Large Language Models", "Authors": "Sarah Gao, Andrew Kean Gao", "Date published": "07/19/2023", "URL": "http://arxiv.org/abs/2307.09793v1", "summary": "Since late 2022, Large Language Models (LLMs) have become very prominent with\nLLMs like ChatGPT and Bard receiving millions of users. Hundreds of new LLMs\nare announced each week, many of which are deposited to Hugging Face, a\nrepository of machine learning models and datasets. To date, nearly 16,000 Text\nGeneration models have been uploaded to the site. Given the huge influx of\nLLMs, it is of interest to know which LLM backbones, settings, training\nmethods, and families are popular or trending. However, there is no\ncomprehensive index of LLMs available. We take advantage of the relatively\nsystematic nomenclature of Hugging Face LLMs to perform hierarchical clustering\nand identify communities amongst LLMs using n-grams and term frequency-inverse\ndocument frequency. Our methods successfully identify families of LLMs and\naccurately cluster LLMs into meaningful subgroups. We present a public web\napplication to navigate and explore Constellation, our atlas of 15,821 LLMs.\nConstellation rapidly generates a variety of visualizations, namely\ndendrograms, graphs, word clouds, and scatter plots. Constellation is available\nat the following link: https://constellation.sites.stanford.edu/."}, "hash": "dd426ec9031661c488fbcf094463a3281e40f833050478811a748a157ec0171b", "class_name": "RelatedNodeInfo"}}, "text": "9\nFigur e\n6.\nScreenshot\nof\nthe\nweb\napplication.\nConclusion\nThe\nincreasing\nnumber\nand\ndiversity\nof\nLarge\nLanguage\nModels\n(LLMs)\nnecessitate\na\ncomprehensive\nand\nsystematic\napproach\nto\norganize,\nclassify ,\nand\nunderstand\nthese\nmodels.\nIn\nthis\nstudy ,\nwe\nhave\nproposed\nan\neffective\nsolution\nby\ncreating\nConstellation,\na\nuser-friendly\nweb\napplication\nthat\nvisualizes\nthe\nhierarchical\nrelationships\namong\nLLMs,\nhelping\nto\nreveal\nprominent\nLLM\nfamilies\nand\nunderlying\nstructures.\nOur\napproach\nis\ngenerally\ninspired\nby\nbioinformatics\nand\nsequence\nsimilarity .\nIt\nutilizes\nhierarchical\nand\nagglomerative\nclustering,\ncombined\nwith\nan\narray\nof\ntechniques\nsuch\nas\ndendrograms,\nword\nclouds,\nand\ngraph-based\nrepresentations.\nThe\nword\nclouds\nprovide\na\nhigh-level\nview\nof\nprominent\nmodel\nfamilies,\nwhile\nthe\ngraph-based\nvisualization\ndepicts\nthe\nrelationships\nand\nsimilarities\namong\nmodels\nin\na\nmore\nflexible\nformat\nthan\nthe\ndendrogram.\nThe\nmajor\nlimitation\nof\nour\nstudy\nis\nthat\nit\nassumes\nthat\nLLMs\nare\nonly\nsimilar\nif\nthey\nhave\nsimilar\nnames.\nThis\nis\nnot\ncompletely\ntrue:\nLLMs\ncan\nbe\nnamed\nanything\nby\nthe\ncreator\nwho\ndeposits\nit\nto\nHugging\nFace.\nHowever ,\nin\ngeneral,\nwe\nnote\nthat\nLLMs\ntend\nto\nbe\nnamed\nin\na\nstructured,\nlogical\nfashion.\nOur\nresults\nindicate\nthat\nour\nassumption\nthat\nin\ngeneral\nsimilar\nLLMs\nshare\nsimilar\nnames\nis\nsound.\nWe\nacknowledge\nthat\nour\napproach\ncan\nmiss\nsimilar\nLLMs,\nespecially\nif\none\nof\nthe\nLLMs\nis\narbitrarily\nnamed.\nAnother\nlimitation\nis\nthat\nnot\nall\nmodels\nlabeled\n\u201cText\nGeneration\u201d\nare\nnecessarily\nLLMs.\nFinally ,\na\nfurther\ncaveat\nis\nthat\nthe\ndendrogram\nis\nnot\na\ntrue\n\u201cevolutionary\u201d\ntree.\nWhile\nmodels\nin\nthe\nsame\nlow-level\ncluster\nare\ngenerally\nreliably\nrelated,\nthis\ndoes\nnot\nhold\nfor\nhigher -level\nclusters.\nBy\nmaking\nConstellation\npublicly\navailable,\nwe\nhope\nto\nencourage\nmore\nsystematic\nand\ninformed\nengagement\nwith\nLLMs.\nAs\nthe\nlandscape\nof\nLLMs\ncontinues\nto\nevolve\nrapidly ,\ntools", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1905, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "50da2edf-82ef-4859-a1e3-a6e622707bbf": {"__data__": {"id_": "50da2edf-82ef-4859-a1e3-a6e622707bbf", "embedding": null, "metadata": {"page_label": "10", "file_name": "2307_09793v1.pdf", "Title of this paper": "On the Origin of LLMs: An Evolutionary Tree and Graph for 15,821 Large Language Models", "Authors": "Sarah Gao, Andrew Kean Gao", "Date published": "07/19/2023", "URL": "http://arxiv.org/abs/2307.09793v1", "summary": "Since late 2022, Large Language Models (LLMs) have become very prominent with\nLLMs like ChatGPT and Bard receiving millions of users. Hundreds of new LLMs\nare announced each week, many of which are deposited to Hugging Face, a\nrepository of machine learning models and datasets. To date, nearly 16,000 Text\nGeneration models have been uploaded to the site. Given the huge influx of\nLLMs, it is of interest to know which LLM backbones, settings, training\nmethods, and families are popular or trending. However, there is no\ncomprehensive index of LLMs available. We take advantage of the relatively\nsystematic nomenclature of Hugging Face LLMs to perform hierarchical clustering\nand identify communities amongst LLMs using n-grams and term frequency-inverse\ndocument frequency. Our methods successfully identify families of LLMs and\naccurately cluster LLMs into meaningful subgroups. We present a public web\napplication to navigate and explore Constellation, our atlas of 15,821 LLMs.\nConstellation rapidly generates a variety of visualizations, namely\ndendrograms, graphs, word clouds, and scatter plots. Constellation is available\nat the following link: https://constellation.sites.stanford.edu/."}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1c1efa70-0466-4ffc-a9a9-32677bc0fab4", "node_type": "4", "metadata": {"page_label": "10", "file_name": "2307_09793v1.pdf", "Title of this paper": "On the Origin of LLMs: An Evolutionary Tree and Graph for 15,821 Large Language Models", "Authors": "Sarah Gao, Andrew Kean Gao", "Date published": "07/19/2023", "URL": "http://arxiv.org/abs/2307.09793v1", "summary": "Since late 2022, Large Language Models (LLMs) have become very prominent with\nLLMs like ChatGPT and Bard receiving millions of users. Hundreds of new LLMs\nare announced each week, many of which are deposited to Hugging Face, a\nrepository of machine learning models and datasets. To date, nearly 16,000 Text\nGeneration models have been uploaded to the site. Given the huge influx of\nLLMs, it is of interest to know which LLM backbones, settings, training\nmethods, and families are popular or trending. However, there is no\ncomprehensive index of LLMs available. We take advantage of the relatively\nsystematic nomenclature of Hugging Face LLMs to perform hierarchical clustering\nand identify communities amongst LLMs using n-grams and term frequency-inverse\ndocument frequency. Our methods successfully identify families of LLMs and\naccurately cluster LLMs into meaningful subgroups. We present a public web\napplication to navigate and explore Constellation, our atlas of 15,821 LLMs.\nConstellation rapidly generates a variety of visualizations, namely\ndendrograms, graphs, word clouds, and scatter plots. Constellation is available\nat the following link: https://constellation.sites.stanford.edu/."}, "hash": "001e79c138061c7cba91bddbfaeab900241f31f32a23bbc1c8093021159d8709", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8d236519-9926-452b-a267-53776b45b8ad", "node_type": "1", "metadata": {}, "hash": "3810a65fce5a4a7b5bfd5979f57bd28244971cd5cedcba3ca4f9978e959132e9", "class_name": "RelatedNodeInfo"}}, "text": "10\nsuch\nas\nConstellation\nwill\nbe\ninstrumental\nin\nassisting\nthe\nresearcher\nand\ndeveloper\ncommunities\nin\nkeeping\npace\nwith\nthese\ndevelopments.\nReferences\n1.\nGao,\nA.\n(2023,\nJuly\n8).\nPrompt\nEngineering\nfor\nLarge\nLanguage\nModels.\nSSRN;\nSSRN.\nhttps://doi.org/10.2139/ssrn.4504303\n2.\nArancio,\nJ.\n(2023,\nApril\n17).\nLlama,\nAlpaca\nand\nVicuna:\nthe\nnew\nChatgpt\nrunning\non\nyour\nlaptop\n.\nMedium.\nhttps://medium.com/@jeremyarancio/exploring-llamas-family-models-how-we-achieved-running-llms-on-l\naptops-16bf2539a1bb\n3.\nHiter,\nS.\n(2023,\nJune\n6).\nWhat\nIs\na\nLarge\nLanguage\nModel?\n|\nGuide\nto\nLLMs\n.\nEWEEK.\nhttps://www.eweek.com/artificial-intelligence/large-language-model/\n4.\nHugging\nFace.\n(n.d.).\nHugging\nFace\n\u2013\nOn\na\nmission\nto\nsolve\nNLP,\none\ncommit\nat\na\ntime.\nHuggingface.co.\nhttps://huggingface.co/\n5.\nWei,\nD.,\nJiang,\nQ.,\nWei,\nY.,\n&\nWang,\nS.\n(2012).\nA\nnovel\nhierarchical\nclustering\nalgorithm\nfor\ngene\nsequences.\nBMC\nBioinformatics\n,\n13\n(1).\nhttps://doi.org/10.1186/1471-2105-13-174\n6.\nBeautiful\nSoup\nDocumentation\n.\n(n.d.).\nTedboy.github.io.\nRetrieved\nJuly\n19,\n2023,\nfrom\nhttps://tedboy.github.io/bs4_doc/\n7.\npandas\ndocumentation\n\u2014\npandas\n1.0.1\ndocumentation\n.\n(2023,\nJune\n28).\nPandas.pydata.org.\nhttps://pandas.pydata.org/docs/\n8.\nStreamlit\nDocs\n.\n(n.d.).\nDocs.streamlit.io.\nhttps://docs.streamlit.io/\n9.\nscipy.\n(2020,\nFebruary\n3).\nscipy/scipy\n.\nGitHub.\nhttps://github.com/scipy/scipy\n10.\nplotly.py\n.\n(2021,\nSeptember\n28).\nGitHub.\nhttps://github.com/plotly/plotly.py\n11.\nnumpy/numpy\n.\n(2021,\nOctober\n9).\nGitHub.\nhttps://github.com/numpy/numpy\n12.\nScikit-Learn.\n(2019).\nUser\nguide:\ncontents\n\u2014\nscikit-learn\n0.22.1\ndocumentation\n.\nScikit-Learn.org.\nhttps://scikit-learn.org/stable/user_guide.html\n13.\nkoonimaru.\n(2023,\nJune\n15).\nradialtree.\nGitHub.\nhttps://github.com/koonimaru/radialtree\n14.\nNLTK.\n(2009).", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1789, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8d236519-9926-452b-a267-53776b45b8ad": {"__data__": {"id_": "8d236519-9926-452b-a267-53776b45b8ad", "embedding": null, "metadata": {"page_label": "10", "file_name": "2307_09793v1.pdf", "Title of this paper": "On the Origin of LLMs: An Evolutionary Tree and Graph for 15,821 Large Language Models", "Authors": "Sarah Gao, Andrew Kean Gao", "Date published": "07/19/2023", "URL": "http://arxiv.org/abs/2307.09793v1", "summary": "Since late 2022, Large Language Models (LLMs) have become very prominent with\nLLMs like ChatGPT and Bard receiving millions of users. Hundreds of new LLMs\nare announced each week, many of which are deposited to Hugging Face, a\nrepository of machine learning models and datasets. To date, nearly 16,000 Text\nGeneration models have been uploaded to the site. Given the huge influx of\nLLMs, it is of interest to know which LLM backbones, settings, training\nmethods, and families are popular or trending. However, there is no\ncomprehensive index of LLMs available. We take advantage of the relatively\nsystematic nomenclature of Hugging Face LLMs to perform hierarchical clustering\nand identify communities amongst LLMs using n-grams and term frequency-inverse\ndocument frequency. Our methods successfully identify families of LLMs and\naccurately cluster LLMs into meaningful subgroups. We present a public web\napplication to navigate and explore Constellation, our atlas of 15,821 LLMs.\nConstellation rapidly generates a variety of visualizations, namely\ndendrograms, graphs, word clouds, and scatter plots. Constellation is available\nat the following link: https://constellation.sites.stanford.edu/."}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1c1efa70-0466-4ffc-a9a9-32677bc0fab4", "node_type": "4", "metadata": {"page_label": "10", "file_name": "2307_09793v1.pdf", "Title of this paper": "On the Origin of LLMs: An Evolutionary Tree and Graph for 15,821 Large Language Models", "Authors": "Sarah Gao, Andrew Kean Gao", "Date published": "07/19/2023", "URL": "http://arxiv.org/abs/2307.09793v1", "summary": "Since late 2022, Large Language Models (LLMs) have become very prominent with\nLLMs like ChatGPT and Bard receiving millions of users. Hundreds of new LLMs\nare announced each week, many of which are deposited to Hugging Face, a\nrepository of machine learning models and datasets. To date, nearly 16,000 Text\nGeneration models have been uploaded to the site. Given the huge influx of\nLLMs, it is of interest to know which LLM backbones, settings, training\nmethods, and families are popular or trending. However, there is no\ncomprehensive index of LLMs available. We take advantage of the relatively\nsystematic nomenclature of Hugging Face LLMs to perform hierarchical clustering\nand identify communities amongst LLMs using n-grams and term frequency-inverse\ndocument frequency. Our methods successfully identify families of LLMs and\naccurately cluster LLMs into meaningful subgroups. We present a public web\napplication to navigate and explore Constellation, our atlas of 15,821 LLMs.\nConstellation rapidly generates a variety of visualizations, namely\ndendrograms, graphs, word clouds, and scatter plots. Constellation is available\nat the following link: https://constellation.sites.stanford.edu/."}, "hash": "001e79c138061c7cba91bddbfaeab900241f31f32a23bbc1c8093021159d8709", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "50da2edf-82ef-4859-a1e3-a6e622707bbf", "node_type": "1", "metadata": {"page_label": "10", "file_name": "2307_09793v1.pdf", "Title of this paper": "On the Origin of LLMs: An Evolutionary Tree and Graph for 15,821 Large Language Models", "Authors": "Sarah Gao, Andrew Kean Gao", "Date published": "07/19/2023", "URL": "http://arxiv.org/abs/2307.09793v1", "summary": "Since late 2022, Large Language Models (LLMs) have become very prominent with\nLLMs like ChatGPT and Bard receiving millions of users. Hundreds of new LLMs\nare announced each week, many of which are deposited to Hugging Face, a\nrepository of machine learning models and datasets. To date, nearly 16,000 Text\nGeneration models have been uploaded to the site. Given the huge influx of\nLLMs, it is of interest to know which LLM backbones, settings, training\nmethods, and families are popular or trending. However, there is no\ncomprehensive index of LLMs available. We take advantage of the relatively\nsystematic nomenclature of Hugging Face LLMs to perform hierarchical clustering\nand identify communities amongst LLMs using n-grams and term frequency-inverse\ndocument frequency. Our methods successfully identify families of LLMs and\naccurately cluster LLMs into meaningful subgroups. We present a public web\napplication to navigate and explore Constellation, our atlas of 15,821 LLMs.\nConstellation rapidly generates a variety of visualizations, namely\ndendrograms, graphs, word clouds, and scatter plots. Constellation is available\nat the following link: https://constellation.sites.stanford.edu/."}, "hash": "f96b929c96105033ed1048a1e5bbb43f25e8c6b4313cc463aa21c15ff04edaa6", "class_name": "RelatedNodeInfo"}}, "text": "(n.d.).\nDocs.streamlit.io.\nhttps://docs.streamlit.io/\n9.\nscipy.\n(2020,\nFebruary\n3).\nscipy/scipy\n.\nGitHub.\nhttps://github.com/scipy/scipy\n10.\nplotly.py\n.\n(2021,\nSeptember\n28).\nGitHub.\nhttps://github.com/plotly/plotly.py\n11.\nnumpy/numpy\n.\n(2021,\nOctober\n9).\nGitHub.\nhttps://github.com/numpy/numpy\n12.\nScikit-Learn.\n(2019).\nUser\nguide:\ncontents\n\u2014\nscikit-learn\n0.22.1\ndocumentation\n.\nScikit-Learn.org.\nhttps://scikit-learn.org/stable/user_guide.html\n13.\nkoonimaru.\n(2023,\nJune\n15).\nradialtree.\nGitHub.\nhttps://github.com/koonimaru/radialtree\n14.\nNLTK.\n(2009).\nNatural\nLanguage\nToolkit\n\u2014\nNLTK\n3.4.4\ndocumentation\n.\nNltk.org.\nhttps://www.nltk.org/\n15.\nMatplotlib.\n(n.d.).\nMatplotlib:\nPython\nplotting\n\u2014\nMatplotlib\n3.3.4\ndocumentation\n.\nMatplotlib.org.\nhttps://matplotlib.org/stable/index.html", "mimetype": "text/plain", "start_char_idx": 1234, "end_char_idx": 2019, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c2765c2e-247d-4a47-bf3c-f9047ee3cd58": {"__data__": {"id_": "c2765c2e-247d-4a47-bf3c-f9047ee3cd58", "embedding": null, "metadata": {"page_label": "11", "file_name": "2307_09793v1.pdf", "Title of this paper": "On the Origin of LLMs: An Evolutionary Tree and Graph for 15,821 Large Language Models", "Authors": "Sarah Gao, Andrew Kean Gao", "Date published": "07/19/2023", "URL": "http://arxiv.org/abs/2307.09793v1", "summary": "Since late 2022, Large Language Models (LLMs) have become very prominent with\nLLMs like ChatGPT and Bard receiving millions of users. Hundreds of new LLMs\nare announced each week, many of which are deposited to Hugging Face, a\nrepository of machine learning models and datasets. To date, nearly 16,000 Text\nGeneration models have been uploaded to the site. Given the huge influx of\nLLMs, it is of interest to know which LLM backbones, settings, training\nmethods, and families are popular or trending. However, there is no\ncomprehensive index of LLMs available. We take advantage of the relatively\nsystematic nomenclature of Hugging Face LLMs to perform hierarchical clustering\nand identify communities amongst LLMs using n-grams and term frequency-inverse\ndocument frequency. Our methods successfully identify families of LLMs and\naccurately cluster LLMs into meaningful subgroups. We present a public web\napplication to navigate and explore Constellation, our atlas of 15,821 LLMs.\nConstellation rapidly generates a variety of visualizations, namely\ndendrograms, graphs, word clouds, and scatter plots. Constellation is available\nat the following link: https://constellation.sites.stanford.edu/."}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "993a1a85-08e5-4220-8ef1-b5078f918edf", "node_type": "4", "metadata": {"page_label": "11", "file_name": "2307_09793v1.pdf", "Title of this paper": "On the Origin of LLMs: An Evolutionary Tree and Graph for 15,821 Large Language Models", "Authors": "Sarah Gao, Andrew Kean Gao", "Date published": "07/19/2023", "URL": "http://arxiv.org/abs/2307.09793v1", "summary": "Since late 2022, Large Language Models (LLMs) have become very prominent with\nLLMs like ChatGPT and Bard receiving millions of users. Hundreds of new LLMs\nare announced each week, many of which are deposited to Hugging Face, a\nrepository of machine learning models and datasets. To date, nearly 16,000 Text\nGeneration models have been uploaded to the site. Given the huge influx of\nLLMs, it is of interest to know which LLM backbones, settings, training\nmethods, and families are popular or trending. However, there is no\ncomprehensive index of LLMs available. We take advantage of the relatively\nsystematic nomenclature of Hugging Face LLMs to perform hierarchical clustering\nand identify communities amongst LLMs using n-grams and term frequency-inverse\ndocument frequency. Our methods successfully identify families of LLMs and\naccurately cluster LLMs into meaningful subgroups. We present a public web\napplication to navigate and explore Constellation, our atlas of 15,821 LLMs.\nConstellation rapidly generates a variety of visualizations, namely\ndendrograms, graphs, word clouds, and scatter plots. Constellation is available\nat the following link: https://constellation.sites.stanford.edu/."}, "hash": "851104304c680d832cd5cb70e0f60c548bc6fba287893fb9ee6befdb0540b3a8", "class_name": "RelatedNodeInfo"}}, "text": "11\n16.\nAynaud,\nT.\n(2023,\nJuly\n11).\nLouvain\nCommunity\nDetection\n.\nGitHub.\nhttps://github.com/taynaud/python-louvain\n17.\nHapberg,\nA.,\nSchult,\nD.,\n&\nSwart,\nP.\n(2008,\nAugust).\nExploring\nnetwork\nstructure,\ndynamics,\nand\nfunction\nusing\nNetworkX\n.\nConference.scipy.org.\nhttps://conference.scipy.org/proceedings/SciPy2008/paper_2\n18.\nMueller,\nA.\n(2020,\nMay\n7).\namueller/word_cloud\n.\nGitHub.\nhttps://github.com/amueller/word_cloud\n19.\nAhmad,\nZ.\n(2023,\nJuly\n19).\nziishaned/learn-regex\n.\nGitHub.\nhttps://github.com/ziishaned/learn-regex\nAppendix\nWord\nOccurrences\ngpt2\n1597\n7b\n889\n13b\n770\ngpt\n756\nfinetuned\n611\nllama\n475\ngptq\n393\ndistilgpt2\n383\npythia\n381\nmodel\n309\nwikitext2\n297\nsmall\n294\nbase\n285\ninstruct\n262\nneo\n261\nopt\n252\nvicuna\n238\n4bit\n224\nbloom\n215\nv2\n214\n30b\n203\n6b\n191", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 767, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0b5f6cd5-e375-4d72-93c3-ffc74e242f0a": {"__data__": {"id_": "0b5f6cd5-e375-4d72-93c3-ffc74e242f0a", "embedding": null, "metadata": {"page_label": "12", "file_name": "2307_09793v1.pdf", "Title of this paper": "On the Origin of LLMs: An Evolutionary Tree and Graph for 15,821 Large Language Models", "Authors": "Sarah Gao, Andrew Kean Gao", "Date published": "07/19/2023", "URL": "http://arxiv.org/abs/2307.09793v1", "summary": "Since late 2022, Large Language Models (LLMs) have become very prominent with\nLLMs like ChatGPT and Bard receiving millions of users. Hundreds of new LLMs\nare announced each week, many of which are deposited to Hugging Face, a\nrepository of machine learning models and datasets. To date, nearly 16,000 Text\nGeneration models have been uploaded to the site. Given the huge influx of\nLLMs, it is of interest to know which LLM backbones, settings, training\nmethods, and families are popular or trending. However, there is no\ncomprehensive index of LLMs available. We take advantage of the relatively\nsystematic nomenclature of Hugging Face LLMs to perform hierarchical clustering\nand identify communities amongst LLMs using n-grams and term frequency-inverse\ndocument frequency. Our methods successfully identify families of LLMs and\naccurately cluster LLMs into meaningful subgroups. We present a public web\napplication to navigate and explore Constellation, our atlas of 15,821 LLMs.\nConstellation rapidly generates a variety of visualizations, namely\ndendrograms, graphs, word clouds, and scatter plots. Constellation is available\nat the following link: https://constellation.sites.stanford.edu/."}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a08baf9e-8ce4-4752-b973-81f7c9b8af52", "node_type": "4", "metadata": {"page_label": "12", "file_name": "2307_09793v1.pdf", "Title of this paper": "On the Origin of LLMs: An Evolutionary Tree and Graph for 15,821 Large Language Models", "Authors": "Sarah Gao, Andrew Kean Gao", "Date published": "07/19/2023", "URL": "http://arxiv.org/abs/2307.09793v1", "summary": "Since late 2022, Large Language Models (LLMs) have become very prominent with\nLLMs like ChatGPT and Bard receiving millions of users. Hundreds of new LLMs\nare announced each week, many of which are deposited to Hugging Face, a\nrepository of machine learning models and datasets. To date, nearly 16,000 Text\nGeneration models have been uploaded to the site. Given the huge influx of\nLLMs, it is of interest to know which LLM backbones, settings, training\nmethods, and families are popular or trending. However, there is no\ncomprehensive index of LLMs available. We take advantage of the relatively\nsystematic nomenclature of Hugging Face LLMs to perform hierarchical clustering\nand identify communities amongst LLMs using n-grams and term frequency-inverse\ndocument frequency. Our methods successfully identify families of LLMs and\naccurately cluster LLMs into meaningful subgroups. We present a public web\napplication to navigate and explore Constellation, our atlas of 15,821 LLMs.\nConstellation rapidly generates a variety of visualizations, namely\ndendrograms, graphs, word clouds, and scatter plots. Constellation is available\nat the following link: https://constellation.sites.stanford.edu/."}, "hash": "1d9f2b09f5a96424d0ec1d6b9283378edffdf89188746d619198218ffa2a1163", "class_name": "RelatedNodeInfo"}}, "text": "12\nalpaca\n190\n125m\n182\ncodeparrot\n178\nrarity\n172\nv1\n171\nfalcon\n168\n8k\n167\nsft\n167\nlarge\n166\ndialogpt\n160\ntest\n157\n2\n156\nall\n155\nmedium\n154\nlora\n153\nds\n146\nmerged\n145\nsuperhot\n143\nj\n141\nhf\n141\nfp16\n133\nchat\n129\nopen\n126\nconcat\n126\nowt2\n126\n350m\n123\n70m\n123\nchinese\n122\n128g\n121\nmpt\n118\ngpt4\n118\n3b\n116\nmyawesomeeli5c\nlm\n108\ntiny\n106", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 331, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "03dd9090-9250-43c3-bd74-13154963e134": {"__data__": {"id_": "03dd9090-9250-43c3-bd74-13154963e134", "embedding": null, "metadata": {"page_label": "13", "file_name": "2307_09793v1.pdf", "Title of this paper": "On the Origin of LLMs: An Evolutionary Tree and Graph for 15,821 Large Language Models", "Authors": "Sarah Gao, Andrew Kean Gao", "Date published": "07/19/2023", "URL": "http://arxiv.org/abs/2307.09793v1", "summary": "Since late 2022, Large Language Models (LLMs) have become very prominent with\nLLMs like ChatGPT and Bard receiving millions of users. Hundreds of new LLMs\nare announced each week, many of which are deposited to Hugging Face, a\nrepository of machine learning models and datasets. To date, nearly 16,000 Text\nGeneration models have been uploaded to the site. Given the huge influx of\nLLMs, it is of interest to know which LLM backbones, settings, training\nmethods, and families are popular or trending. However, there is no\ncomprehensive index of LLMs available. We take advantage of the relatively\nsystematic nomenclature of Hugging Face LLMs to perform hierarchical clustering\nand identify communities amongst LLMs using n-grams and term frequency-inverse\ndocument frequency. Our methods successfully identify families of LLMs and\naccurately cluster LLMs into meaningful subgroups. We present a public web\napplication to navigate and explore Constellation, our atlas of 15,821 LLMs.\nConstellation rapidly generates a variety of visualizations, namely\ndendrograms, graphs, word clouds, and scatter plots. Constellation is available\nat the following link: https://constellation.sites.stanford.edu/."}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "36dcac16-219c-4453-a2d5-bcb9035c4603", "node_type": "4", "metadata": {"page_label": "13", "file_name": "2307_09793v1.pdf", "Title of this paper": "On the Origin of LLMs: An Evolutionary Tree and Graph for 15,821 Large Language Models", "Authors": "Sarah Gao, Andrew Kean Gao", "Date published": "07/19/2023", "URL": "http://arxiv.org/abs/2307.09793v1", "summary": "Since late 2022, Large Language Models (LLMs) have become very prominent with\nLLMs like ChatGPT and Bard receiving millions of users. Hundreds of new LLMs\nare announced each week, many of which are deposited to Hugging Face, a\nrepository of machine learning models and datasets. To date, nearly 16,000 Text\nGeneration models have been uploaded to the site. Given the huge influx of\nLLMs, it is of interest to know which LLM backbones, settings, training\nmethods, and families are popular or trending. However, there is no\ncomprehensive index of LLMs available. We take advantage of the relatively\nsystematic nomenclature of Hugging Face LLMs to perform hierarchical clustering\nand identify communities amongst LLMs using n-grams and term frequency-inverse\ndocument frequency. Our methods successfully identify families of LLMs and\naccurately cluster LLMs into meaningful subgroups. We present a public web\napplication to navigate and explore Constellation, our atlas of 15,821 LLMs.\nConstellation rapidly generates a variety of visualizations, namely\ndendrograms, graphs, word clouds, and scatter plots. Constellation is available\nat the following link: https://constellation.sites.stanford.edu/."}, "hash": "b8e446f27e53af65d53b9da1883721e47ca0606934bcc7092608d40a266141b0", "class_name": "RelatedNodeInfo"}}, "text": "13\n1\n106\n4\n105\n8bit\n103\nheadless\n101\ncodegen\n97\n33b\n97\ndeduped\n95\nsharded\n89\nairoboros\n88\nfinetunedgpt2\n85\nv3\n83\nwizardlm\n83\ngenerator\n83\nno\n82\n560m\n81\nrandom\n79\nuncensored\n74\nfinetune\n74\nxl\n72\n1b\n71\nmod\n71\n27b\n69\n65b\n68\nelonmusk\n68\nbloomz\n66\nv0\n66\nbert\n65\nguanaco\n64\nft\n64\nimdb\n63\ngptj\n62\n160m\n61\ngptneo\n61\nredpajama\n58\nneox\n57", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 328, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4a9e88dd-8706-4180-b1cc-c5e3e964d7d5": {"__data__": {"id_": "4a9e88dd-8706-4180-b1cc-c5e3e964d7d5", "embedding": null, "metadata": {"page_label": "14", "file_name": "2307_09793v1.pdf", "Title of this paper": "On the Origin of LLMs: An Evolutionary Tree and Graph for 15,821 Large Language Models", "Authors": "Sarah Gao, Andrew Kean Gao", "Date published": "07/19/2023", "URL": "http://arxiv.org/abs/2307.09793v1", "summary": "Since late 2022, Large Language Models (LLMs) have become very prominent with\nLLMs like ChatGPT and Bard receiving millions of users. Hundreds of new LLMs\nare announced each week, many of which are deposited to Hugging Face, a\nrepository of machine learning models and datasets. To date, nearly 16,000 Text\nGeneration models have been uploaded to the site. Given the huge influx of\nLLMs, it is of interest to know which LLM backbones, settings, training\nmethods, and families are popular or trending. However, there is no\ncomprehensive index of LLMs available. We take advantage of the relatively\nsystematic nomenclature of Hugging Face LLMs to perform hierarchical clustering\nand identify communities amongst LLMs using n-grams and term frequency-inverse\ndocument frequency. Our methods successfully identify families of LLMs and\naccurately cluster LLMs into meaningful subgroups. We present a public web\napplication to navigate and explore Constellation, our atlas of 15,821 LLMs.\nConstellation rapidly generates a variety of visualizations, namely\ndendrograms, graphs, word clouds, and scatter plots. Constellation is available\nat the following link: https://constellation.sites.stanford.edu/."}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "97359e44-5937-41ed-a57c-2db6413b7b32", "node_type": "4", "metadata": {"page_label": "14", "file_name": "2307_09793v1.pdf", "Title of this paper": "On the Origin of LLMs: An Evolutionary Tree and Graph for 15,821 Large Language Models", "Authors": "Sarah Gao, Andrew Kean Gao", "Date published": "07/19/2023", "URL": "http://arxiv.org/abs/2307.09793v1", "summary": "Since late 2022, Large Language Models (LLMs) have become very prominent with\nLLMs like ChatGPT and Bard receiving millions of users. Hundreds of new LLMs\nare announced each week, many of which are deposited to Hugging Face, a\nrepository of machine learning models and datasets. To date, nearly 16,000 Text\nGeneration models have been uploaded to the site. Given the huge influx of\nLLMs, it is of interest to know which LLM backbones, settings, training\nmethods, and families are popular or trending. However, there is no\ncomprehensive index of LLMs available. We take advantage of the relatively\nsystematic nomenclature of Hugging Face LLMs to perform hierarchical clustering\nand identify communities amongst LLMs using n-grams and term frequency-inverse\ndocument frequency. Our methods successfully identify families of LLMs and\naccurately cluster LLMs into meaningful subgroups. We present a public web\napplication to navigate and explore Constellation, our atlas of 15,821 LLMs.\nConstellation rapidly generates a variety of visualizations, namely\ndendrograms, graphs, word clouds, and scatter plots. Constellation is available\nat the following link: https://constellation.sites.stanford.edu/."}, "hash": "9c283f3d3d15afb1c00ed44e63234facba4a424c6b0628b78bdb07252a3f4d02", "class_name": "RelatedNodeInfo"}}, "text": "14\nggml\n57\nthe\n55\n5\n55\ndolly\n53\n12b\n53\ncut\n53\nguten\n53\n67b\n52\ndelta\n52\nTable\n1.\nMost\ncommon\nwords/phrases\namong\nall\nHugging\nFace\nLLMs", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 133, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}}, "docstore/metadata": {"dcc97cc2-36d8-468e-b92f-941593b108e6": {"doc_hash": "193cd132324930293cfa6e59a1092c8c2f37b94e60a4461a289b532b2d3bba2b", "ref_doc_id": "be09d8bd-d254-44d3-959f-ed610c5fd1ec"}, "8bf0dde0-156d-42a7-b03a-853e5fb40828": {"doc_hash": "41204b8b740bd09b3310aac4f682892aeb06cd7bcf8ae48243a8d60d76b198ca", "ref_doc_id": "00e0297f-659f-4379-a7cb-a1457f8396e8"}, "686c3ce4-2356-4d59-9aaf-b60e636862ec": {"doc_hash": "cd33591b817917df95fbde30c9b1545c975f917b6b413de17e89ea34e47cfca3", "ref_doc_id": "00e0297f-659f-4379-a7cb-a1457f8396e8"}, "a6385ae7-d112-49c7-a369-b30e4a11347e": {"doc_hash": "49cb72862f7bdcec0838ac66b8e12f01d5c7f87a1ed377d0de805b8fd751c5af", "ref_doc_id": "c7bb9a83-f55f-4569-8d47-413a0b60a196"}, "f09e5fe8-d050-42bf-88e2-59e66df420c1": {"doc_hash": "0ad73477b92139c2cc730b464cd79ec205ab6eb2b42f3af67874c509ccc75f32", "ref_doc_id": "c8560985-7ab4-440b-9b4e-81e61e7d93dd"}, "c0d9c9ac-5437-45ca-b6ab-b88cc06dccd9": {"doc_hash": "b2c066a57afae5844467c0dd7cd51dd2aa3fd92bb0f3afc5c19e6d7b4b9c7221", "ref_doc_id": "0d9ee52b-e573-452b-a07d-fd2adda8bfef"}, "2651af6b-d63f-487c-bfc3-b8fe9a3012c3": {"doc_hash": "7e95f59987834f70bd642d838265e4d62f961b5e5932071cab68e95258aaaa7b", "ref_doc_id": "4e5bb8b1-c29b-4b39-af9f-9778183178bc"}, "26641d65-4c2b-4ebb-9da1-3d8488f51ca8": {"doc_hash": "0d77e92c60cf47638dac593def31ca0ba8884972685022664a4c44bdd9c48474", "ref_doc_id": "a4679295-7258-4592-8f65-95a6220bb594"}, "07d317d5-863f-4137-9848-4e1b69a580e4": {"doc_hash": "2139d5e874d3e025da5a66a8c9d6c2fbd51e0fce2231b33c081478e4894a321e", "ref_doc_id": "39fe3ec6-746d-4d8f-9b2f-e51a464704cb"}, "658dc46b-77d4-4a97-b444-ee5ec3455b36": {"doc_hash": "1cc3c43f11edc4324c665cdb8d23e969a5c7035d437a64916c4a528183a81193", "ref_doc_id": "196a9988-356c-40ae-b363-ca90fb4f3e62"}, "50da2edf-82ef-4859-a1e3-a6e622707bbf": {"doc_hash": "f96b929c96105033ed1048a1e5bbb43f25e8c6b4313cc463aa21c15ff04edaa6", "ref_doc_id": "1c1efa70-0466-4ffc-a9a9-32677bc0fab4"}, "8d236519-9926-452b-a267-53776b45b8ad": {"doc_hash": "2b6f1399da3648edc35a4822a9f7b3b2d641c28de6676ed649f36a4f5bbf03be", "ref_doc_id": "1c1efa70-0466-4ffc-a9a9-32677bc0fab4"}, "c2765c2e-247d-4a47-bf3c-f9047ee3cd58": {"doc_hash": "851104304c680d832cd5cb70e0f60c548bc6fba287893fb9ee6befdb0540b3a8", "ref_doc_id": "993a1a85-08e5-4220-8ef1-b5078f918edf"}, "0b5f6cd5-e375-4d72-93c3-ffc74e242f0a": {"doc_hash": "1d9f2b09f5a96424d0ec1d6b9283378edffdf89188746d619198218ffa2a1163", "ref_doc_id": "a08baf9e-8ce4-4752-b973-81f7c9b8af52"}, "03dd9090-9250-43c3-bd74-13154963e134": {"doc_hash": "b8e446f27e53af65d53b9da1883721e47ca0606934bcc7092608d40a266141b0", "ref_doc_id": "36dcac16-219c-4453-a2d5-bcb9035c4603"}, "4a9e88dd-8706-4180-b1cc-c5e3e964d7d5": {"doc_hash": "9c283f3d3d15afb1c00ed44e63234facba4a424c6b0628b78bdb07252a3f4d02", "ref_doc_id": "97359e44-5937-41ed-a57c-2db6413b7b32"}}, "docstore/ref_doc_info": {"be09d8bd-d254-44d3-959f-ed610c5fd1ec": {"node_ids": ["dcc97cc2-36d8-468e-b92f-941593b108e6"], "metadata": {"page_label": "1", "file_name": "2307_09793v1.pdf", "Title of this paper": "On the Origin of LLMs: An Evolutionary Tree and Graph for 15,821 Large Language Models", "Authors": "Sarah Gao, Andrew Kean Gao", "Date published": "07/19/2023", "URL": "http://arxiv.org/abs/2307.09793v1", "summary": "Since late 2022, Large Language Models (LLMs) have become very prominent with\nLLMs like ChatGPT and Bard receiving millions of users. Hundreds of new LLMs\nare announced each week, many of which are deposited to Hugging Face, a\nrepository of machine learning models and datasets. To date, nearly 16,000 Text\nGeneration models have been uploaded to the site. Given the huge influx of\nLLMs, it is of interest to know which LLM backbones, settings, training\nmethods, and families are popular or trending. However, there is no\ncomprehensive index of LLMs available. We take advantage of the relatively\nsystematic nomenclature of Hugging Face LLMs to perform hierarchical clustering\nand identify communities amongst LLMs using n-grams and term frequency-inverse\ndocument frequency. Our methods successfully identify families of LLMs and\naccurately cluster LLMs into meaningful subgroups. We present a public web\napplication to navigate and explore Constellation, our atlas of 15,821 LLMs.\nConstellation rapidly generates a variety of visualizations, namely\ndendrograms, graphs, word clouds, and scatter plots. Constellation is available\nat the following link: https://constellation.sites.stanford.edu/."}}, "00e0297f-659f-4379-a7cb-a1457f8396e8": {"node_ids": ["8bf0dde0-156d-42a7-b03a-853e5fb40828", "686c3ce4-2356-4d59-9aaf-b60e636862ec"], "metadata": {"page_label": "2", "file_name": "2307_09793v1.pdf", "Title of this paper": "On the Origin of LLMs: An Evolutionary Tree and Graph for 15,821 Large Language Models", "Authors": "Sarah Gao, Andrew Kean Gao", "Date published": "07/19/2023", "URL": "http://arxiv.org/abs/2307.09793v1", "summary": "Since late 2022, Large Language Models (LLMs) have become very prominent with\nLLMs like ChatGPT and Bard receiving millions of users. Hundreds of new LLMs\nare announced each week, many of which are deposited to Hugging Face, a\nrepository of machine learning models and datasets. To date, nearly 16,000 Text\nGeneration models have been uploaded to the site. Given the huge influx of\nLLMs, it is of interest to know which LLM backbones, settings, training\nmethods, and families are popular or trending. However, there is no\ncomprehensive index of LLMs available. We take advantage of the relatively\nsystematic nomenclature of Hugging Face LLMs to perform hierarchical clustering\nand identify communities amongst LLMs using n-grams and term frequency-inverse\ndocument frequency. Our methods successfully identify families of LLMs and\naccurately cluster LLMs into meaningful subgroups. We present a public web\napplication to navigate and explore Constellation, our atlas of 15,821 LLMs.\nConstellation rapidly generates a variety of visualizations, namely\ndendrograms, graphs, word clouds, and scatter plots. Constellation is available\nat the following link: https://constellation.sites.stanford.edu/."}}, "c7bb9a83-f55f-4569-8d47-413a0b60a196": {"node_ids": ["a6385ae7-d112-49c7-a369-b30e4a11347e"], "metadata": {"page_label": "3", "file_name": "2307_09793v1.pdf", "Title of this paper": "On the Origin of LLMs: An Evolutionary Tree and Graph for 15,821 Large Language Models", "Authors": "Sarah Gao, Andrew Kean Gao", "Date published": "07/19/2023", "URL": "http://arxiv.org/abs/2307.09793v1", "summary": "Since late 2022, Large Language Models (LLMs) have become very prominent with\nLLMs like ChatGPT and Bard receiving millions of users. Hundreds of new LLMs\nare announced each week, many of which are deposited to Hugging Face, a\nrepository of machine learning models and datasets. To date, nearly 16,000 Text\nGeneration models have been uploaded to the site. Given the huge influx of\nLLMs, it is of interest to know which LLM backbones, settings, training\nmethods, and families are popular or trending. However, there is no\ncomprehensive index of LLMs available. We take advantage of the relatively\nsystematic nomenclature of Hugging Face LLMs to perform hierarchical clustering\nand identify communities amongst LLMs using n-grams and term frequency-inverse\ndocument frequency. Our methods successfully identify families of LLMs and\naccurately cluster LLMs into meaningful subgroups. We present a public web\napplication to navigate and explore Constellation, our atlas of 15,821 LLMs.\nConstellation rapidly generates a variety of visualizations, namely\ndendrograms, graphs, word clouds, and scatter plots. Constellation is available\nat the following link: https://constellation.sites.stanford.edu/."}}, "c8560985-7ab4-440b-9b4e-81e61e7d93dd": {"node_ids": ["f09e5fe8-d050-42bf-88e2-59e66df420c1"], "metadata": {"page_label": "4", "file_name": "2307_09793v1.pdf", "Title of this paper": "On the Origin of LLMs: An Evolutionary Tree and Graph for 15,821 Large Language Models", "Authors": "Sarah Gao, Andrew Kean Gao", "Date published": "07/19/2023", "URL": "http://arxiv.org/abs/2307.09793v1", "summary": "Since late 2022, Large Language Models (LLMs) have become very prominent with\nLLMs like ChatGPT and Bard receiving millions of users. Hundreds of new LLMs\nare announced each week, many of which are deposited to Hugging Face, a\nrepository of machine learning models and datasets. To date, nearly 16,000 Text\nGeneration models have been uploaded to the site. Given the huge influx of\nLLMs, it is of interest to know which LLM backbones, settings, training\nmethods, and families are popular or trending. However, there is no\ncomprehensive index of LLMs available. We take advantage of the relatively\nsystematic nomenclature of Hugging Face LLMs to perform hierarchical clustering\nand identify communities amongst LLMs using n-grams and term frequency-inverse\ndocument frequency. Our methods successfully identify families of LLMs and\naccurately cluster LLMs into meaningful subgroups. We present a public web\napplication to navigate and explore Constellation, our atlas of 15,821 LLMs.\nConstellation rapidly generates a variety of visualizations, namely\ndendrograms, graphs, word clouds, and scatter plots. Constellation is available\nat the following link: https://constellation.sites.stanford.edu/."}}, "0d9ee52b-e573-452b-a07d-fd2adda8bfef": {"node_ids": ["c0d9c9ac-5437-45ca-b6ab-b88cc06dccd9"], "metadata": {"page_label": "5", "file_name": "2307_09793v1.pdf", "Title of this paper": "On the Origin of LLMs: An Evolutionary Tree and Graph for 15,821 Large Language Models", "Authors": "Sarah Gao, Andrew Kean Gao", "Date published": "07/19/2023", "URL": "http://arxiv.org/abs/2307.09793v1", "summary": "Since late 2022, Large Language Models (LLMs) have become very prominent with\nLLMs like ChatGPT and Bard receiving millions of users. Hundreds of new LLMs\nare announced each week, many of which are deposited to Hugging Face, a\nrepository of machine learning models and datasets. To date, nearly 16,000 Text\nGeneration models have been uploaded to the site. Given the huge influx of\nLLMs, it is of interest to know which LLM backbones, settings, training\nmethods, and families are popular or trending. However, there is no\ncomprehensive index of LLMs available. We take advantage of the relatively\nsystematic nomenclature of Hugging Face LLMs to perform hierarchical clustering\nand identify communities amongst LLMs using n-grams and term frequency-inverse\ndocument frequency. Our methods successfully identify families of LLMs and\naccurately cluster LLMs into meaningful subgroups. We present a public web\napplication to navigate and explore Constellation, our atlas of 15,821 LLMs.\nConstellation rapidly generates a variety of visualizations, namely\ndendrograms, graphs, word clouds, and scatter plots. Constellation is available\nat the following link: https://constellation.sites.stanford.edu/."}}, "4e5bb8b1-c29b-4b39-af9f-9778183178bc": {"node_ids": ["2651af6b-d63f-487c-bfc3-b8fe9a3012c3"], "metadata": {"page_label": "6", "file_name": "2307_09793v1.pdf", "Title of this paper": "On the Origin of LLMs: An Evolutionary Tree and Graph for 15,821 Large Language Models", "Authors": "Sarah Gao, Andrew Kean Gao", "Date published": "07/19/2023", "URL": "http://arxiv.org/abs/2307.09793v1", "summary": "Since late 2022, Large Language Models (LLMs) have become very prominent with\nLLMs like ChatGPT and Bard receiving millions of users. Hundreds of new LLMs\nare announced each week, many of which are deposited to Hugging Face, a\nrepository of machine learning models and datasets. To date, nearly 16,000 Text\nGeneration models have been uploaded to the site. Given the huge influx of\nLLMs, it is of interest to know which LLM backbones, settings, training\nmethods, and families are popular or trending. However, there is no\ncomprehensive index of LLMs available. We take advantage of the relatively\nsystematic nomenclature of Hugging Face LLMs to perform hierarchical clustering\nand identify communities amongst LLMs using n-grams and term frequency-inverse\ndocument frequency. Our methods successfully identify families of LLMs and\naccurately cluster LLMs into meaningful subgroups. We present a public web\napplication to navigate and explore Constellation, our atlas of 15,821 LLMs.\nConstellation rapidly generates a variety of visualizations, namely\ndendrograms, graphs, word clouds, and scatter plots. Constellation is available\nat the following link: https://constellation.sites.stanford.edu/."}}, "a4679295-7258-4592-8f65-95a6220bb594": {"node_ids": ["26641d65-4c2b-4ebb-9da1-3d8488f51ca8"], "metadata": {"page_label": "7", "file_name": "2307_09793v1.pdf", "Title of this paper": "On the Origin of LLMs: An Evolutionary Tree and Graph for 15,821 Large Language Models", "Authors": "Sarah Gao, Andrew Kean Gao", "Date published": "07/19/2023", "URL": "http://arxiv.org/abs/2307.09793v1", "summary": "Since late 2022, Large Language Models (LLMs) have become very prominent with\nLLMs like ChatGPT and Bard receiving millions of users. Hundreds of new LLMs\nare announced each week, many of which are deposited to Hugging Face, a\nrepository of machine learning models and datasets. To date, nearly 16,000 Text\nGeneration models have been uploaded to the site. Given the huge influx of\nLLMs, it is of interest to know which LLM backbones, settings, training\nmethods, and families are popular or trending. However, there is no\ncomprehensive index of LLMs available. We take advantage of the relatively\nsystematic nomenclature of Hugging Face LLMs to perform hierarchical clustering\nand identify communities amongst LLMs using n-grams and term frequency-inverse\ndocument frequency. Our methods successfully identify families of LLMs and\naccurately cluster LLMs into meaningful subgroups. We present a public web\napplication to navigate and explore Constellation, our atlas of 15,821 LLMs.\nConstellation rapidly generates a variety of visualizations, namely\ndendrograms, graphs, word clouds, and scatter plots. Constellation is available\nat the following link: https://constellation.sites.stanford.edu/."}}, "39fe3ec6-746d-4d8f-9b2f-e51a464704cb": {"node_ids": ["07d317d5-863f-4137-9848-4e1b69a580e4"], "metadata": {"page_label": "8", "file_name": "2307_09793v1.pdf", "Title of this paper": "On the Origin of LLMs: An Evolutionary Tree and Graph for 15,821 Large Language Models", "Authors": "Sarah Gao, Andrew Kean Gao", "Date published": "07/19/2023", "URL": "http://arxiv.org/abs/2307.09793v1", "summary": "Since late 2022, Large Language Models (LLMs) have become very prominent with\nLLMs like ChatGPT and Bard receiving millions of users. Hundreds of new LLMs\nare announced each week, many of which are deposited to Hugging Face, a\nrepository of machine learning models and datasets. To date, nearly 16,000 Text\nGeneration models have been uploaded to the site. Given the huge influx of\nLLMs, it is of interest to know which LLM backbones, settings, training\nmethods, and families are popular or trending. However, there is no\ncomprehensive index of LLMs available. We take advantage of the relatively\nsystematic nomenclature of Hugging Face LLMs to perform hierarchical clustering\nand identify communities amongst LLMs using n-grams and term frequency-inverse\ndocument frequency. Our methods successfully identify families of LLMs and\naccurately cluster LLMs into meaningful subgroups. We present a public web\napplication to navigate and explore Constellation, our atlas of 15,821 LLMs.\nConstellation rapidly generates a variety of visualizations, namely\ndendrograms, graphs, word clouds, and scatter plots. Constellation is available\nat the following link: https://constellation.sites.stanford.edu/."}}, "196a9988-356c-40ae-b363-ca90fb4f3e62": {"node_ids": ["658dc46b-77d4-4a97-b444-ee5ec3455b36"], "metadata": {"page_label": "9", "file_name": "2307_09793v1.pdf", "Title of this paper": "On the Origin of LLMs: An Evolutionary Tree and Graph for 15,821 Large Language Models", "Authors": "Sarah Gao, Andrew Kean Gao", "Date published": "07/19/2023", "URL": "http://arxiv.org/abs/2307.09793v1", "summary": "Since late 2022, Large Language Models (LLMs) have become very prominent with\nLLMs like ChatGPT and Bard receiving millions of users. Hundreds of new LLMs\nare announced each week, many of which are deposited to Hugging Face, a\nrepository of machine learning models and datasets. To date, nearly 16,000 Text\nGeneration models have been uploaded to the site. Given the huge influx of\nLLMs, it is of interest to know which LLM backbones, settings, training\nmethods, and families are popular or trending. However, there is no\ncomprehensive index of LLMs available. We take advantage of the relatively\nsystematic nomenclature of Hugging Face LLMs to perform hierarchical clustering\nand identify communities amongst LLMs using n-grams and term frequency-inverse\ndocument frequency. Our methods successfully identify families of LLMs and\naccurately cluster LLMs into meaningful subgroups. We present a public web\napplication to navigate and explore Constellation, our atlas of 15,821 LLMs.\nConstellation rapidly generates a variety of visualizations, namely\ndendrograms, graphs, word clouds, and scatter plots. Constellation is available\nat the following link: https://constellation.sites.stanford.edu/."}}, "1c1efa70-0466-4ffc-a9a9-32677bc0fab4": {"node_ids": ["50da2edf-82ef-4859-a1e3-a6e622707bbf", "8d236519-9926-452b-a267-53776b45b8ad"], "metadata": {"page_label": "10", "file_name": "2307_09793v1.pdf", "Title of this paper": "On the Origin of LLMs: An Evolutionary Tree and Graph for 15,821 Large Language Models", "Authors": "Sarah Gao, Andrew Kean Gao", "Date published": "07/19/2023", "URL": "http://arxiv.org/abs/2307.09793v1", "summary": "Since late 2022, Large Language Models (LLMs) have become very prominent with\nLLMs like ChatGPT and Bard receiving millions of users. Hundreds of new LLMs\nare announced each week, many of which are deposited to Hugging Face, a\nrepository of machine learning models and datasets. To date, nearly 16,000 Text\nGeneration models have been uploaded to the site. Given the huge influx of\nLLMs, it is of interest to know which LLM backbones, settings, training\nmethods, and families are popular or trending. However, there is no\ncomprehensive index of LLMs available. We take advantage of the relatively\nsystematic nomenclature of Hugging Face LLMs to perform hierarchical clustering\nand identify communities amongst LLMs using n-grams and term frequency-inverse\ndocument frequency. Our methods successfully identify families of LLMs and\naccurately cluster LLMs into meaningful subgroups. We present a public web\napplication to navigate and explore Constellation, our atlas of 15,821 LLMs.\nConstellation rapidly generates a variety of visualizations, namely\ndendrograms, graphs, word clouds, and scatter plots. Constellation is available\nat the following link: https://constellation.sites.stanford.edu/."}}, "993a1a85-08e5-4220-8ef1-b5078f918edf": {"node_ids": ["c2765c2e-247d-4a47-bf3c-f9047ee3cd58"], "metadata": {"page_label": "11", "file_name": "2307_09793v1.pdf", "Title of this paper": "On the Origin of LLMs: An Evolutionary Tree and Graph for 15,821 Large Language Models", "Authors": "Sarah Gao, Andrew Kean Gao", "Date published": "07/19/2023", "URL": "http://arxiv.org/abs/2307.09793v1", "summary": "Since late 2022, Large Language Models (LLMs) have become very prominent with\nLLMs like ChatGPT and Bard receiving millions of users. Hundreds of new LLMs\nare announced each week, many of which are deposited to Hugging Face, a\nrepository of machine learning models and datasets. To date, nearly 16,000 Text\nGeneration models have been uploaded to the site. Given the huge influx of\nLLMs, it is of interest to know which LLM backbones, settings, training\nmethods, and families are popular or trending. However, there is no\ncomprehensive index of LLMs available. We take advantage of the relatively\nsystematic nomenclature of Hugging Face LLMs to perform hierarchical clustering\nand identify communities amongst LLMs using n-grams and term frequency-inverse\ndocument frequency. Our methods successfully identify families of LLMs and\naccurately cluster LLMs into meaningful subgroups. We present a public web\napplication to navigate and explore Constellation, our atlas of 15,821 LLMs.\nConstellation rapidly generates a variety of visualizations, namely\ndendrograms, graphs, word clouds, and scatter plots. Constellation is available\nat the following link: https://constellation.sites.stanford.edu/."}}, "a08baf9e-8ce4-4752-b973-81f7c9b8af52": {"node_ids": ["0b5f6cd5-e375-4d72-93c3-ffc74e242f0a"], "metadata": {"page_label": "12", "file_name": "2307_09793v1.pdf", "Title of this paper": "On the Origin of LLMs: An Evolutionary Tree and Graph for 15,821 Large Language Models", "Authors": "Sarah Gao, Andrew Kean Gao", "Date published": "07/19/2023", "URL": "http://arxiv.org/abs/2307.09793v1", "summary": "Since late 2022, Large Language Models (LLMs) have become very prominent with\nLLMs like ChatGPT and Bard receiving millions of users. Hundreds of new LLMs\nare announced each week, many of which are deposited to Hugging Face, a\nrepository of machine learning models and datasets. To date, nearly 16,000 Text\nGeneration models have been uploaded to the site. Given the huge influx of\nLLMs, it is of interest to know which LLM backbones, settings, training\nmethods, and families are popular or trending. However, there is no\ncomprehensive index of LLMs available. We take advantage of the relatively\nsystematic nomenclature of Hugging Face LLMs to perform hierarchical clustering\nand identify communities amongst LLMs using n-grams and term frequency-inverse\ndocument frequency. Our methods successfully identify families of LLMs and\naccurately cluster LLMs into meaningful subgroups. We present a public web\napplication to navigate and explore Constellation, our atlas of 15,821 LLMs.\nConstellation rapidly generates a variety of visualizations, namely\ndendrograms, graphs, word clouds, and scatter plots. Constellation is available\nat the following link: https://constellation.sites.stanford.edu/."}}, "36dcac16-219c-4453-a2d5-bcb9035c4603": {"node_ids": ["03dd9090-9250-43c3-bd74-13154963e134"], "metadata": {"page_label": "13", "file_name": "2307_09793v1.pdf", "Title of this paper": "On the Origin of LLMs: An Evolutionary Tree and Graph for 15,821 Large Language Models", "Authors": "Sarah Gao, Andrew Kean Gao", "Date published": "07/19/2023", "URL": "http://arxiv.org/abs/2307.09793v1", "summary": "Since late 2022, Large Language Models (LLMs) have become very prominent with\nLLMs like ChatGPT and Bard receiving millions of users. Hundreds of new LLMs\nare announced each week, many of which are deposited to Hugging Face, a\nrepository of machine learning models and datasets. To date, nearly 16,000 Text\nGeneration models have been uploaded to the site. Given the huge influx of\nLLMs, it is of interest to know which LLM backbones, settings, training\nmethods, and families are popular or trending. However, there is no\ncomprehensive index of LLMs available. We take advantage of the relatively\nsystematic nomenclature of Hugging Face LLMs to perform hierarchical clustering\nand identify communities amongst LLMs using n-grams and term frequency-inverse\ndocument frequency. Our methods successfully identify families of LLMs and\naccurately cluster LLMs into meaningful subgroups. We present a public web\napplication to navigate and explore Constellation, our atlas of 15,821 LLMs.\nConstellation rapidly generates a variety of visualizations, namely\ndendrograms, graphs, word clouds, and scatter plots. Constellation is available\nat the following link: https://constellation.sites.stanford.edu/."}}, "97359e44-5937-41ed-a57c-2db6413b7b32": {"node_ids": ["4a9e88dd-8706-4180-b1cc-c5e3e964d7d5"], "metadata": {"page_label": "14", "file_name": "2307_09793v1.pdf", "Title of this paper": "On the Origin of LLMs: An Evolutionary Tree and Graph for 15,821 Large Language Models", "Authors": "Sarah Gao, Andrew Kean Gao", "Date published": "07/19/2023", "URL": "http://arxiv.org/abs/2307.09793v1", "summary": "Since late 2022, Large Language Models (LLMs) have become very prominent with\nLLMs like ChatGPT and Bard receiving millions of users. Hundreds of new LLMs\nare announced each week, many of which are deposited to Hugging Face, a\nrepository of machine learning models and datasets. To date, nearly 16,000 Text\nGeneration models have been uploaded to the site. Given the huge influx of\nLLMs, it is of interest to know which LLM backbones, settings, training\nmethods, and families are popular or trending. However, there is no\ncomprehensive index of LLMs available. We take advantage of the relatively\nsystematic nomenclature of Hugging Face LLMs to perform hierarchical clustering\nand identify communities amongst LLMs using n-grams and term frequency-inverse\ndocument frequency. Our methods successfully identify families of LLMs and\naccurately cluster LLMs into meaningful subgroups. We present a public web\napplication to navigate and explore Constellation, our atlas of 15,821 LLMs.\nConstellation rapidly generates a variety of visualizations, namely\ndendrograms, graphs, word clouds, and scatter plots. Constellation is available\nat the following link: https://constellation.sites.stanford.edu/."}}}}