{"docstore/data": {"d7ebcbf1-5d35-4044-a578-e8bb3ab36f46": {"__data__": {"id_": "d7ebcbf1-5d35-4044-a578-e8bb3ab36f46", "embedding": null, "metadata": {"page_label": "1", "file_name": "2405_19888v1.pdf", "Title of this paper": "Parrot: Efficient Serving of LLM-based Applications with Semantic Variable", "Authors": "Chaofan Lin, Zhenhua Han, Chengruidong Zhang, Yuqing Yang, Fan Yang, Chen Chen, Lili Qiu", "Date published": "05/30/2024", "URL": "http://arxiv.org/abs/2405.19888v1", "summary": "The rise of large language models (LLMs) has enabled LLM-based applications\n(a.k.a. AI agents or co-pilots), a new software paradigm that combines the\nstrength of LLM and conventional software. Diverse LLM applications from\ndifferent tenants could design complex workflows using multiple LLM requests to\naccomplish one task. However, they have to use the over-simplified\nrequest-level API provided by today's public LLM services, losing essential\napplication-level information. Public LLM services have to blindly optimize\nindividual LLM requests, leading to sub-optimal end-to-end performance of LLM\napplications.\n  This paper introduces Parrot, an LLM service system that focuses on the\nend-to-end experience of LLM-based applications. Parrot proposes Semantic\nVariable, a unified abstraction to expose application-level knowledge to public\nLLM services. A Semantic Variable annotates an input/output variable in the\nprompt of a request, and creates the data pipeline when connecting multiple LLM\nrequests, providing a natural way to program LLM applications. Exposing\nSemantic Variables to the public LLM service allows it to perform conventional\ndata flow analysis to uncover the correlation across multiple LLM requests.\nThis correlation opens a brand-new optimization space for the end-to-end\nperformance of LLM-based applications. Extensive evaluations demonstrate that\nParrot can achieve up to an order-of-magnitude improvement for popular and\npractical use cases of LLM applications."}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "fa5c2731-1ef9-497c-bc3b-557fd958eeb4", "node_type": "4", "metadata": {"page_label": "1", "file_name": "2405_19888v1.pdf", "Title of this paper": "Parrot: Efficient Serving of LLM-based Applications with Semantic Variable", "Authors": "Chaofan Lin, Zhenhua Han, Chengruidong Zhang, Yuqing Yang, Fan Yang, Chen Chen, Lili Qiu", "Date published": "05/30/2024", "URL": "http://arxiv.org/abs/2405.19888v1", "summary": "The rise of large language models (LLMs) has enabled LLM-based applications\n(a.k.a. AI agents or co-pilots), a new software paradigm that combines the\nstrength of LLM and conventional software. Diverse LLM applications from\ndifferent tenants could design complex workflows using multiple LLM requests to\naccomplish one task. However, they have to use the over-simplified\nrequest-level API provided by today's public LLM services, losing essential\napplication-level information. Public LLM services have to blindly optimize\nindividual LLM requests, leading to sub-optimal end-to-end performance of LLM\napplications.\n  This paper introduces Parrot, an LLM service system that focuses on the\nend-to-end experience of LLM-based applications. Parrot proposes Semantic\nVariable, a unified abstraction to expose application-level knowledge to public\nLLM services. A Semantic Variable annotates an input/output variable in the\nprompt of a request, and creates the data pipeline when connecting multiple LLM\nrequests, providing a natural way to program LLM applications. Exposing\nSemantic Variables to the public LLM service allows it to perform conventional\ndata flow analysis to uncover the correlation across multiple LLM requests.\nThis correlation opens a brand-new optimization space for the end-to-end\nperformance of LLM-based applications. Extensive evaluations demonstrate that\nParrot can achieve up to an order-of-magnitude improvement for popular and\npractical use cases of LLM applications."}, "hash": "05de79e7bef19b3d3806df7d52176a9def3fb34796d5f64496676b287ac3eef5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "52d41ace-99fb-46fc-a8e8-383e1b54f674", "node_type": "1", "metadata": {}, "hash": "a5f1be848b6474316c2d849472b374f38b7fcf29c61b43932ffbba453d01704d", "class_name": "RelatedNodeInfo"}}, "text": "Parrot: Efficient Serving of LLM-based Applications with Semantic Variable\nChaofan Lin1\u2217, Zhenhua Han2, Chengruidong Zhang2, Yuqing Yang2\nFan Yang2, Chen Chen1\u2217, Lili Qiu2\n1Shanghai Jiao Tong University,2Microsoft Research\nAbstract\nThe rise of large language models (LLMs) has enabled\nLLM-based applications (a.k.a. AI agents or co-pilots), a new\nsoftware paradigm that combines the strength of LLM and\nconventional software. Diverse LLM applications from differ-\nent tenants could design complex workflows using multiple\nLLM requests to accomplish one task. However, they have\nto use the over-simplified request-level API provided by to-\nday\u2019s public LLM services, losing essential application-level\ninformation. Public LLM services have to blindly optimize\nindividual LLM requests, leading to sub-optimal end-to-end\nperformance of LLM applications.\nThis paper introduces Parrot, an LLM service system that\nfocuses on the end-to-end experience of LLM-based applica-\ntions. Parrot proposes Semantic Variable , a unified abstrac-\ntion to expose application-level knowledge to public LLM\nservices. A Semantic Variable annotates an input/output vari-\nable in the prompt of a request, and creates the data pipeline\nwhen connecting multiple LLM requests, providing a natu-\nral way to program LLM applications. Exposing Semantic\nVariables to the public LLM service allows it to perform con-\nventional data flow analysis to uncover the correlation across\nmultiple LLM requests. This correlation opens a brand-new\noptimization space for the end-to-end performance of LLM-\nbased applications. Extensive evaluations demonstrate that\nParrot can achieve up to an order-of-magnitude improvement\nfor popular and practical use cases of LLM applications.\n1 Introduction\nLarge language models (LLMs) have demonstrated a remark-\nable language understanding capability [7, 41]. This enables\na paradigm shift in application development. In this new\nparadigm, one or multiple application entities, known as AI\nagents or co-pilots, communicate with LLMs via natural lan-\nguage, known as \u201cprompts\u201d, to accomplish a task collabo-\nratively. For example, Meeting applications like Microsoft\nTeams or Google Meet can summarize meeting discussions\nthrough LLMs [33]. Search engines like Google and Bing\ncan be enhanced with Chat ability through LLMs [14, 34].\nIt is believed such LLM-based applications will become the\nmainstream applications in the near future [13].\n\u2217This work is partially done while Chaofan Lin\u2019s internship and Dr. Chen\nChen\u2019s visting scholar in Microsoft Research.To accomplish a task, LLM-based applications typically\nrequire multiple rounds of conversation. The conversation, im-\nplemented through multiple API calls to LLM, demonstrates\ncomplex workflow patterns.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2760, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "52d41ace-99fb-46fc-a8e8-383e1b54f674": {"__data__": {"id_": "52d41ace-99fb-46fc-a8e8-383e1b54f674", "embedding": null, "metadata": {"page_label": "1", "file_name": "2405_19888v1.pdf", "Title of this paper": "Parrot: Efficient Serving of LLM-based Applications with Semantic Variable", "Authors": "Chaofan Lin, Zhenhua Han, Chengruidong Zhang, Yuqing Yang, Fan Yang, Chen Chen, Lili Qiu", "Date published": "05/30/2024", "URL": "http://arxiv.org/abs/2405.19888v1", "summary": "The rise of large language models (LLMs) has enabled LLM-based applications\n(a.k.a. AI agents or co-pilots), a new software paradigm that combines the\nstrength of LLM and conventional software. Diverse LLM applications from\ndifferent tenants could design complex workflows using multiple LLM requests to\naccomplish one task. However, they have to use the over-simplified\nrequest-level API provided by today's public LLM services, losing essential\napplication-level information. Public LLM services have to blindly optimize\nindividual LLM requests, leading to sub-optimal end-to-end performance of LLM\napplications.\n  This paper introduces Parrot, an LLM service system that focuses on the\nend-to-end experience of LLM-based applications. Parrot proposes Semantic\nVariable, a unified abstraction to expose application-level knowledge to public\nLLM services. A Semantic Variable annotates an input/output variable in the\nprompt of a request, and creates the data pipeline when connecting multiple LLM\nrequests, providing a natural way to program LLM applications. Exposing\nSemantic Variables to the public LLM service allows it to perform conventional\ndata flow analysis to uncover the correlation across multiple LLM requests.\nThis correlation opens a brand-new optimization space for the end-to-end\nperformance of LLM-based applications. Extensive evaluations demonstrate that\nParrot can achieve up to an order-of-magnitude improvement for popular and\npractical use cases of LLM applications."}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "fa5c2731-1ef9-497c-bc3b-557fd958eeb4", "node_type": "4", "metadata": {"page_label": "1", "file_name": "2405_19888v1.pdf", "Title of this paper": "Parrot: Efficient Serving of LLM-based Applications with Semantic Variable", "Authors": "Chaofan Lin, Zhenhua Han, Chengruidong Zhang, Yuqing Yang, Fan Yang, Chen Chen, Lili Qiu", "Date published": "05/30/2024", "URL": "http://arxiv.org/abs/2405.19888v1", "summary": "The rise of large language models (LLMs) has enabled LLM-based applications\n(a.k.a. AI agents or co-pilots), a new software paradigm that combines the\nstrength of LLM and conventional software. Diverse LLM applications from\ndifferent tenants could design complex workflows using multiple LLM requests to\naccomplish one task. However, they have to use the over-simplified\nrequest-level API provided by today's public LLM services, losing essential\napplication-level information. Public LLM services have to blindly optimize\nindividual LLM requests, leading to sub-optimal end-to-end performance of LLM\napplications.\n  This paper introduces Parrot, an LLM service system that focuses on the\nend-to-end experience of LLM-based applications. Parrot proposes Semantic\nVariable, a unified abstraction to expose application-level knowledge to public\nLLM services. A Semantic Variable annotates an input/output variable in the\nprompt of a request, and creates the data pipeline when connecting multiple LLM\nrequests, providing a natural way to program LLM applications. Exposing\nSemantic Variables to the public LLM service allows it to perform conventional\ndata flow analysis to uncover the correlation across multiple LLM requests.\nThis correlation opens a brand-new optimization space for the end-to-end\nperformance of LLM-based applications. Extensive evaluations demonstrate that\nParrot can achieve up to an order-of-magnitude improvement for popular and\npractical use cases of LLM applications."}, "hash": "05de79e7bef19b3d3806df7d52176a9def3fb34796d5f64496676b287ac3eef5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d7ebcbf1-5d35-4044-a578-e8bb3ab36f46", "node_type": "1", "metadata": {"page_label": "1", "file_name": "2405_19888v1.pdf", "Title of this paper": "Parrot: Efficient Serving of LLM-based Applications with Semantic Variable", "Authors": "Chaofan Lin, Zhenhua Han, Chengruidong Zhang, Yuqing Yang, Fan Yang, Chen Chen, Lili Qiu", "Date published": "05/30/2024", "URL": "http://arxiv.org/abs/2405.19888v1", "summary": "The rise of large language models (LLMs) has enabled LLM-based applications\n(a.k.a. AI agents or co-pilots), a new software paradigm that combines the\nstrength of LLM and conventional software. Diverse LLM applications from\ndifferent tenants could design complex workflows using multiple LLM requests to\naccomplish one task. However, they have to use the over-simplified\nrequest-level API provided by today's public LLM services, losing essential\napplication-level information. Public LLM services have to blindly optimize\nindividual LLM requests, leading to sub-optimal end-to-end performance of LLM\napplications.\n  This paper introduces Parrot, an LLM service system that focuses on the\nend-to-end experience of LLM-based applications. Parrot proposes Semantic\nVariable, a unified abstraction to expose application-level knowledge to public\nLLM services. A Semantic Variable annotates an input/output variable in the\nprompt of a request, and creates the data pipeline when connecting multiple LLM\nrequests, providing a natural way to program LLM applications. Exposing\nSemantic Variables to the public LLM service allows it to perform conventional\ndata flow analysis to uncover the correlation across multiple LLM requests.\nThis correlation opens a brand-new optimization space for the end-to-end\nperformance of LLM-based applications. Extensive evaluations demonstrate that\nParrot can achieve up to an order-of-magnitude improvement for popular and\npractical use cases of LLM applications."}, "hash": "259940c26088d614fee7c935257dc0208e733911429abae6028cd114a021bea7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f8a2dc6c-72eb-4c22-858c-ebde310dbc4b", "node_type": "1", "metadata": {}, "hash": "372241c63ea3c36e198e88912f146ce7374ffc7170efcc6174f9f31b70626739", "class_name": "RelatedNodeInfo"}}, "text": "In this new\nparadigm, one or multiple application entities, known as AI\nagents or co-pilots, communicate with LLMs via natural lan-\nguage, known as \u201cprompts\u201d, to accomplish a task collabo-\nratively. For example, Meeting applications like Microsoft\nTeams or Google Meet can summarize meeting discussions\nthrough LLMs [33]. Search engines like Google and Bing\ncan be enhanced with Chat ability through LLMs [14, 34].\nIt is believed such LLM-based applications will become the\nmainstream applications in the near future [13].\n\u2217This work is partially done while Chaofan Lin\u2019s internship and Dr. Chen\nChen\u2019s visting scholar in Microsoft Research.To accomplish a task, LLM-based applications typically\nrequire multiple rounds of conversation. The conversation, im-\nplemented through multiple API calls to LLM, demonstrates\ncomplex workflow patterns. Figure 1 illustrates several popu-\nlar conversation patterns. For example, a meeting summary\napplication [8, 33] often divides a lengthy document into mul-\ntiple shorter sections, each satisfying the length constraint\nof the LLM conversation and thus can be summarized and\ncombined into the final summary through the Map-Reduce\nor chaining summary patterns. Chat-based applications, e.g.,\nBing Copilot [34], call LLM APIs multiple times to generate\nanswers based on user queries. Multiple agents, each repre-\nsenting a different role played by different LLM calls, can\ncollaborate to achieve a task [22, 47, 54].\nPublic LLM service providers have to face diverse tenants\nand applications, each with different workflows and perfor-\nmance preference. However, existing API design for LLM\nservice provision is still request-centric. Public LLM services\nonly observe tons of individual requests, without knowing any\napplication-level information, e.g., which requests belong to\nthe same application, how different requests are connected, or\nwhether there are any similarities. The lost application-level\ninformation makes public LLM service blindly optimize the\nperformance of individual requests, leading to sub-optimal\nend-to-end performance of LLM applications. In this paper,\nwe observe there exist significant opportunities to improve\ntheend-to-end experience of LLM applications by exploiting\nthe application-level information, especially the correlation\nof multiple LLM requests.\nFirst, multiple consecutive LLM requests may be depen-\ndent: the result of one request could be the direct input of\nthe next request.", "mimetype": "text/plain", "start_char_idx": 1917, "end_char_idx": 4377, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f8a2dc6c-72eb-4c22-858c-ebde310dbc4b": {"__data__": {"id_": "f8a2dc6c-72eb-4c22-858c-ebde310dbc4b", "embedding": null, "metadata": {"page_label": "1", "file_name": "2405_19888v1.pdf", "Title of this paper": "Parrot: Efficient Serving of LLM-based Applications with Semantic Variable", "Authors": "Chaofan Lin, Zhenhua Han, Chengruidong Zhang, Yuqing Yang, Fan Yang, Chen Chen, Lili Qiu", "Date published": "05/30/2024", "URL": "http://arxiv.org/abs/2405.19888v1", "summary": "The rise of large language models (LLMs) has enabled LLM-based applications\n(a.k.a. AI agents or co-pilots), a new software paradigm that combines the\nstrength of LLM and conventional software. Diverse LLM applications from\ndifferent tenants could design complex workflows using multiple LLM requests to\naccomplish one task. However, they have to use the over-simplified\nrequest-level API provided by today's public LLM services, losing essential\napplication-level information. Public LLM services have to blindly optimize\nindividual LLM requests, leading to sub-optimal end-to-end performance of LLM\napplications.\n  This paper introduces Parrot, an LLM service system that focuses on the\nend-to-end experience of LLM-based applications. Parrot proposes Semantic\nVariable, a unified abstraction to expose application-level knowledge to public\nLLM services. A Semantic Variable annotates an input/output variable in the\nprompt of a request, and creates the data pipeline when connecting multiple LLM\nrequests, providing a natural way to program LLM applications. Exposing\nSemantic Variables to the public LLM service allows it to perform conventional\ndata flow analysis to uncover the correlation across multiple LLM requests.\nThis correlation opens a brand-new optimization space for the end-to-end\nperformance of LLM-based applications. Extensive evaluations demonstrate that\nParrot can achieve up to an order-of-magnitude improvement for popular and\npractical use cases of LLM applications."}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "fa5c2731-1ef9-497c-bc3b-557fd958eeb4", "node_type": "4", "metadata": {"page_label": "1", "file_name": "2405_19888v1.pdf", "Title of this paper": "Parrot: Efficient Serving of LLM-based Applications with Semantic Variable", "Authors": "Chaofan Lin, Zhenhua Han, Chengruidong Zhang, Yuqing Yang, Fan Yang, Chen Chen, Lili Qiu", "Date published": "05/30/2024", "URL": "http://arxiv.org/abs/2405.19888v1", "summary": "The rise of large language models (LLMs) has enabled LLM-based applications\n(a.k.a. AI agents or co-pilots), a new software paradigm that combines the\nstrength of LLM and conventional software. Diverse LLM applications from\ndifferent tenants could design complex workflows using multiple LLM requests to\naccomplish one task. However, they have to use the over-simplified\nrequest-level API provided by today's public LLM services, losing essential\napplication-level information. Public LLM services have to blindly optimize\nindividual LLM requests, leading to sub-optimal end-to-end performance of LLM\napplications.\n  This paper introduces Parrot, an LLM service system that focuses on the\nend-to-end experience of LLM-based applications. Parrot proposes Semantic\nVariable, a unified abstraction to expose application-level knowledge to public\nLLM services. A Semantic Variable annotates an input/output variable in the\nprompt of a request, and creates the data pipeline when connecting multiple LLM\nrequests, providing a natural way to program LLM applications. Exposing\nSemantic Variables to the public LLM service allows it to perform conventional\ndata flow analysis to uncover the correlation across multiple LLM requests.\nThis correlation opens a brand-new optimization space for the end-to-end\nperformance of LLM-based applications. Extensive evaluations demonstrate that\nParrot can achieve up to an order-of-magnitude improvement for popular and\npractical use cases of LLM applications."}, "hash": "05de79e7bef19b3d3806df7d52176a9def3fb34796d5f64496676b287ac3eef5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "52d41ace-99fb-46fc-a8e8-383e1b54f674", "node_type": "1", "metadata": {"page_label": "1", "file_name": "2405_19888v1.pdf", "Title of this paper": "Parrot: Efficient Serving of LLM-based Applications with Semantic Variable", "Authors": "Chaofan Lin, Zhenhua Han, Chengruidong Zhang, Yuqing Yang, Fan Yang, Chen Chen, Lili Qiu", "Date published": "05/30/2024", "URL": "http://arxiv.org/abs/2405.19888v1", "summary": "The rise of large language models (LLMs) has enabled LLM-based applications\n(a.k.a. AI agents or co-pilots), a new software paradigm that combines the\nstrength of LLM and conventional software. Diverse LLM applications from\ndifferent tenants could design complex workflows using multiple LLM requests to\naccomplish one task. However, they have to use the over-simplified\nrequest-level API provided by today's public LLM services, losing essential\napplication-level information. Public LLM services have to blindly optimize\nindividual LLM requests, leading to sub-optimal end-to-end performance of LLM\napplications.\n  This paper introduces Parrot, an LLM service system that focuses on the\nend-to-end experience of LLM-based applications. Parrot proposes Semantic\nVariable, a unified abstraction to expose application-level knowledge to public\nLLM services. A Semantic Variable annotates an input/output variable in the\nprompt of a request, and creates the data pipeline when connecting multiple LLM\nrequests, providing a natural way to program LLM applications. Exposing\nSemantic Variables to the public LLM service allows it to perform conventional\ndata flow analysis to uncover the correlation across multiple LLM requests.\nThis correlation opens a brand-new optimization space for the end-to-end\nperformance of LLM-based applications. Extensive evaluations demonstrate that\nParrot can achieve up to an order-of-magnitude improvement for popular and\npractical use cases of LLM applications."}, "hash": "62f97548ad831f4f1f46db6239dfee6c410141e40e751faa24dc78492dbe6528", "class_name": "RelatedNodeInfo"}}, "text": "Public LLM service providers have to face diverse tenants\nand applications, each with different workflows and perfor-\nmance preference. However, existing API design for LLM\nservice provision is still request-centric. Public LLM services\nonly observe tons of individual requests, without knowing any\napplication-level information, e.g., which requests belong to\nthe same application, how different requests are connected, or\nwhether there are any similarities. The lost application-level\ninformation makes public LLM service blindly optimize the\nperformance of individual requests, leading to sub-optimal\nend-to-end performance of LLM applications. In this paper,\nwe observe there exist significant opportunities to improve\ntheend-to-end experience of LLM applications by exploiting\nthe application-level information, especially the correlation\nof multiple LLM requests.\nFirst, multiple consecutive LLM requests may be depen-\ndent: the result of one request could be the direct input of\nthe next request. Therefore, it is desirable to colocate those\nChunk 1\nChunk 2\u2026\u2026\nChunk NLLM\nLLM\nLLM\u2026\u2026S1\nS2\nSN \u2026\u2026LLMFinal \nSummary\n(a) Map-Reduce Summary\nChunk 1\nChunk 2\u2026\u2026\nChunk NLLM\nS1 + LLM\nS1 + S2 +\n\u2026\u2026SN-1 LLMFinal \nSummaryLLM Request\nMessage Passing (b) Chain Summary\nQuery Rewriter\nLLM -powered Search\nQA w/ search result\nSafety CheckerLLM\nFinal \nAnswerUser \nQuery\n(c) LLM-Powered Search\nProduct Manger\nArchitect\nEngineer\nQA TesterCode ReviewerLLM\nFinal \nCodeTask (d) Multi-agent Coding\nFigure 1: The workflow of popular LLM-based applications.\nThe final result requires multiple LLM requests.arXiv:2405.19888v1  [cs.LG]  30 May 2024", "mimetype": "text/plain", "start_char_idx": 3374, "end_char_idx": 4997, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "260ccb22-9083-4c8e-8236-5de334684fcc": {"__data__": {"id_": "260ccb22-9083-4c8e-8236-5de334684fcc", "embedding": null, "metadata": {"page_label": "2", "file_name": "2405_19888v1.pdf", "Title of this paper": "Parrot: Efficient Serving of LLM-based Applications with Semantic Variable", "Authors": "Chaofan Lin, Zhenhua Han, Chengruidong Zhang, Yuqing Yang, Fan Yang, Chen Chen, Lili Qiu", "Date published": "05/30/2024", "URL": "http://arxiv.org/abs/2405.19888v1", "summary": "The rise of large language models (LLMs) has enabled LLM-based applications\n(a.k.a. AI agents or co-pilots), a new software paradigm that combines the\nstrength of LLM and conventional software. Diverse LLM applications from\ndifferent tenants could design complex workflows using multiple LLM requests to\naccomplish one task. However, they have to use the over-simplified\nrequest-level API provided by today's public LLM services, losing essential\napplication-level information. Public LLM services have to blindly optimize\nindividual LLM requests, leading to sub-optimal end-to-end performance of LLM\napplications.\n  This paper introduces Parrot, an LLM service system that focuses on the\nend-to-end experience of LLM-based applications. Parrot proposes Semantic\nVariable, a unified abstraction to expose application-level knowledge to public\nLLM services. A Semantic Variable annotates an input/output variable in the\nprompt of a request, and creates the data pipeline when connecting multiple LLM\nrequests, providing a natural way to program LLM applications. Exposing\nSemantic Variables to the public LLM service allows it to perform conventional\ndata flow analysis to uncover the correlation across multiple LLM requests.\nThis correlation opens a brand-new optimization space for the end-to-end\nperformance of LLM-based applications. Extensive evaluations demonstrate that\nParrot can achieve up to an order-of-magnitude improvement for popular and\npractical use cases of LLM applications."}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e80fb7f0-ffbb-4071-b1ea-28d614ada6b6", "node_type": "4", "metadata": {"page_label": "2", "file_name": "2405_19888v1.pdf", "Title of this paper": "Parrot: Efficient Serving of LLM-based Applications with Semantic Variable", "Authors": "Chaofan Lin, Zhenhua Han, Chengruidong Zhang, Yuqing Yang, Fan Yang, Chen Chen, Lili Qiu", "Date published": "05/30/2024", "URL": "http://arxiv.org/abs/2405.19888v1", "summary": "The rise of large language models (LLMs) has enabled LLM-based applications\n(a.k.a. AI agents or co-pilots), a new software paradigm that combines the\nstrength of LLM and conventional software. Diverse LLM applications from\ndifferent tenants could design complex workflows using multiple LLM requests to\naccomplish one task. However, they have to use the over-simplified\nrequest-level API provided by today's public LLM services, losing essential\napplication-level information. Public LLM services have to blindly optimize\nindividual LLM requests, leading to sub-optimal end-to-end performance of LLM\napplications.\n  This paper introduces Parrot, an LLM service system that focuses on the\nend-to-end experience of LLM-based applications. Parrot proposes Semantic\nVariable, a unified abstraction to expose application-level knowledge to public\nLLM services. A Semantic Variable annotates an input/output variable in the\nprompt of a request, and creates the data pipeline when connecting multiple LLM\nrequests, providing a natural way to program LLM applications. Exposing\nSemantic Variables to the public LLM service allows it to perform conventional\ndata flow analysis to uncover the correlation across multiple LLM requests.\nThis correlation opens a brand-new optimization space for the end-to-end\nperformance of LLM-based applications. Extensive evaluations demonstrate that\nParrot can achieve up to an order-of-magnitude improvement for popular and\npractical use cases of LLM applications."}, "hash": "b3627b052feba9743035dd0be2dafc8dd20bf36f634e737f0ad052c265390dec", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0abda68a-949c-4163-bf0d-c764f0361133", "node_type": "1", "metadata": {}, "hash": "92cd5f0e17ff5cff0a58c877ba962533fd417180f6d75b91914a0d760fc4236f", "class_name": "RelatedNodeInfo"}}, "text": "requests together and execute them consecutively on the LLM\nservice side. However, unaware of their dependencies, these\nrequests have to be executed interactively between the client\nside of LLM-based applications and the public LLM ser-\nvices. These clients, often located on the other end of the\nInternet, can only issue the second request after they receive\nthe result of the first request. This unnecessarily incurs extra\noverhead of consecutive requests on network latency as well\nas losing the opportunity of co-scheduling these consecutive\nrequests (\u00a73).\nSecond, LLM requests may have diverse scheduling pref-\nerence , even within a single application. For example, in Fig-\nure 1a, to reduce the end-to-end latency, the requests represent-\ning multiple Map tasks should be batched more aggressively\nto increase the throughput of the Map tasks; while the Re-\nduce task, due to its scarcity, should be optimized for latency.\nUnfortunately, public LLM services cannot discriminate the\ndifference between the two types of tasks. As a result, the\ncurrent practice is to blindly optimize the latency for individ-\nual requests, which might not be desirable for the end-to-end\nexperience.\nThird, there exists a high degree of commonality across\nLLM requests. Popular LLM applications (e.g., Bing Copi-\nlot [32], GPTs [42]) use a long system prompt, including task\ndefinitions, examples, and safety rules, to guide the behavior\nof LLM applications. The long system prompt is usually static\nand common for all users. As existing public LLM services\ntreat each request individually, these common prefix prompts\nare provided repeatedly in each request, leading to a great\nwaste of storage, computation, and memory bandwidth. Our\nanalysis of a production LLM-based search engine shows\nthat over 94% of tokens in the requests are repeated across\ndifferent users.\nAlthough we have seen some emerging engine-level tech-\nniques [25,56,63] proposed to optimize the above three cases,\nthey all work based on certain application-level knowledge,\nwhich is lost in nowadays public LLM services. In a nut-\nshell, due to the lack of understanding of the correlations of\nLLM requests, existing LLM services cannot leverage the\nthree opportunities, leading to high end-to-end service latency\nand reduced throughput. Based on the above facts and in-\nsights, we introduce Parrot, an LLM service system that treats\nLLM applications as first-class citizens. Parrot retains most of\napplication-level information by a simple abstraction Seman-\ntic Variable, achieving a perfect balance between increasing\nsystem complexity and bringing new information for opti-\nmization. A Semantic Variable is a text region in the prompt\nwith a specific semantic purpose, such as a task instruction, a\nlist of few-shot examples, an input, or an output.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2811, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0abda68a-949c-4163-bf0d-c764f0361133": {"__data__": {"id_": "0abda68a-949c-4163-bf0d-c764f0361133", "embedding": null, "metadata": {"page_label": "2", "file_name": "2405_19888v1.pdf", "Title of this paper": "Parrot: Efficient Serving of LLM-based Applications with Semantic Variable", "Authors": "Chaofan Lin, Zhenhua Han, Chengruidong Zhang, Yuqing Yang, Fan Yang, Chen Chen, Lili Qiu", "Date published": "05/30/2024", "URL": "http://arxiv.org/abs/2405.19888v1", "summary": "The rise of large language models (LLMs) has enabled LLM-based applications\n(a.k.a. AI agents or co-pilots), a new software paradigm that combines the\nstrength of LLM and conventional software. Diverse LLM applications from\ndifferent tenants could design complex workflows using multiple LLM requests to\naccomplish one task. However, they have to use the over-simplified\nrequest-level API provided by today's public LLM services, losing essential\napplication-level information. Public LLM services have to blindly optimize\nindividual LLM requests, leading to sub-optimal end-to-end performance of LLM\napplications.\n  This paper introduces Parrot, an LLM service system that focuses on the\nend-to-end experience of LLM-based applications. Parrot proposes Semantic\nVariable, a unified abstraction to expose application-level knowledge to public\nLLM services. A Semantic Variable annotates an input/output variable in the\nprompt of a request, and creates the data pipeline when connecting multiple LLM\nrequests, providing a natural way to program LLM applications. Exposing\nSemantic Variables to the public LLM service allows it to perform conventional\ndata flow analysis to uncover the correlation across multiple LLM requests.\nThis correlation opens a brand-new optimization space for the end-to-end\nperformance of LLM-based applications. Extensive evaluations demonstrate that\nParrot can achieve up to an order-of-magnitude improvement for popular and\npractical use cases of LLM applications."}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e80fb7f0-ffbb-4071-b1ea-28d614ada6b6", "node_type": "4", "metadata": {"page_label": "2", "file_name": "2405_19888v1.pdf", "Title of this paper": "Parrot: Efficient Serving of LLM-based Applications with Semantic Variable", "Authors": "Chaofan Lin, Zhenhua Han, Chengruidong Zhang, Yuqing Yang, Fan Yang, Chen Chen, Lili Qiu", "Date published": "05/30/2024", "URL": "http://arxiv.org/abs/2405.19888v1", "summary": "The rise of large language models (LLMs) has enabled LLM-based applications\n(a.k.a. AI agents or co-pilots), a new software paradigm that combines the\nstrength of LLM and conventional software. Diverse LLM applications from\ndifferent tenants could design complex workflows using multiple LLM requests to\naccomplish one task. However, they have to use the over-simplified\nrequest-level API provided by today's public LLM services, losing essential\napplication-level information. Public LLM services have to blindly optimize\nindividual LLM requests, leading to sub-optimal end-to-end performance of LLM\napplications.\n  This paper introduces Parrot, an LLM service system that focuses on the\nend-to-end experience of LLM-based applications. Parrot proposes Semantic\nVariable, a unified abstraction to expose application-level knowledge to public\nLLM services. A Semantic Variable annotates an input/output variable in the\nprompt of a request, and creates the data pipeline when connecting multiple LLM\nrequests, providing a natural way to program LLM applications. Exposing\nSemantic Variables to the public LLM service allows it to perform conventional\ndata flow analysis to uncover the correlation across multiple LLM requests.\nThis correlation opens a brand-new optimization space for the end-to-end\nperformance of LLM-based applications. Extensive evaluations demonstrate that\nParrot can achieve up to an order-of-magnitude improvement for popular and\npractical use cases of LLM applications."}, "hash": "b3627b052feba9743035dd0be2dafc8dd20bf36f634e737f0ad052c265390dec", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "260ccb22-9083-4c8e-8236-5de334684fcc", "node_type": "1", "metadata": {"page_label": "2", "file_name": "2405_19888v1.pdf", "Title of this paper": "Parrot: Efficient Serving of LLM-based Applications with Semantic Variable", "Authors": "Chaofan Lin, Zhenhua Han, Chengruidong Zhang, Yuqing Yang, Fan Yang, Chen Chen, Lili Qiu", "Date published": "05/30/2024", "URL": "http://arxiv.org/abs/2405.19888v1", "summary": "The rise of large language models (LLMs) has enabled LLM-based applications\n(a.k.a. AI agents or co-pilots), a new software paradigm that combines the\nstrength of LLM and conventional software. Diverse LLM applications from\ndifferent tenants could design complex workflows using multiple LLM requests to\naccomplish one task. However, they have to use the over-simplified\nrequest-level API provided by today's public LLM services, losing essential\napplication-level information. Public LLM services have to blindly optimize\nindividual LLM requests, leading to sub-optimal end-to-end performance of LLM\napplications.\n  This paper introduces Parrot, an LLM service system that focuses on the\nend-to-end experience of LLM-based applications. Parrot proposes Semantic\nVariable, a unified abstraction to expose application-level knowledge to public\nLLM services. A Semantic Variable annotates an input/output variable in the\nprompt of a request, and creates the data pipeline when connecting multiple LLM\nrequests, providing a natural way to program LLM applications. Exposing\nSemantic Variables to the public LLM service allows it to perform conventional\ndata flow analysis to uncover the correlation across multiple LLM requests.\nThis correlation opens a brand-new optimization space for the end-to-end\nperformance of LLM-based applications. Extensive evaluations demonstrate that\nParrot can achieve up to an order-of-magnitude improvement for popular and\npractical use cases of LLM applications."}, "hash": "23289684e1e2aa6e0de4ca64928336be908b3ad4ad177d28cd03874fecf8cb64", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9bb3ca12-d623-42c5-9f41-dda20e1aba71", "node_type": "1", "metadata": {}, "hash": "adda621b2245d99898c155a5001b0ce94a1819908973f93bf89e13eb8681095d", "class_name": "RelatedNodeInfo"}}, "text": "In a nut-\nshell, due to the lack of understanding of the correlations of\nLLM requests, existing LLM services cannot leverage the\nthree opportunities, leading to high end-to-end service latency\nand reduced throughput. Based on the above facts and in-\nsights, we introduce Parrot, an LLM service system that treats\nLLM applications as first-class citizens. Parrot retains most of\napplication-level information by a simple abstraction Seman-\ntic Variable, achieving a perfect balance between increasing\nsystem complexity and bringing new information for opti-\nmization. A Semantic Variable is a text region in the prompt\nwith a specific semantic purpose, such as a task instruction, a\nlist of few-shot examples, an input, or an output. A Semantic\nVariable can also work as the data pipeline that connects mul-\ntiple LLM requests. Semantic Variable naturally exposes the\ninformation of prompt structures and correlations of requests\nto LLM services. By inspecting Semantic Variable at runtime,\nParrot can perform conventional data flow analysis to derive\nFigure 2: The communication of consecutive LLM requests\nin multi-agent applications.\nthe data dependency between LLM requests just-in-time.\nBy analyzing the application-level information, Parrot\u2019s\nunified abstraction naturally enables joint optimizations,\nwhich bring better global optimality. The same data pipeline\nbuilt by Semantic Variables can enable multiple optimizations\nsimultaneously, including hiding data pipeline\u2019s latency, ob-\njective deduction for a better scheduling and commonality\nanalysis to perform de-duplication. Parrot\u2019s scheduling also\ntakes different opportunities into accounts under the unified\nabstraction. Our extensive evaluation of Parrot on popular\nLLM-based applications, including the production and open-\nsource projects, shows Parrot achieves up to 11.7\u00d7speedup\nor12\u00d7higher throughput compared with the state-of-the-art\nsolutions.\n2 Background\nLLM Service. Most LLM services are provisioned as a\nconditional generation service via a text completion API.\nCompletion (prompt : str )\u2212 \u2192generated_text : str .\nThe application client provides a text prompt, and the LLM\nservice responds with the generated text. Behind the API,\nan LLM service provider runs one or multiple clusters of\nLLM inference engines. A request scheduler dispatches LLM\nrequests from a queue to an LLM inference engine, which\nuses a set of GPUs to conduct the LLM inference.\nLLM-based Applications. Figure 1 highlights the repre-\nsentative workflows of how LLM is used in the applications.\nDue to the limited context window of LLMs (e.g., 4,096 for\nGPT-3.5-Turbo [40]), data analytics on long documents fol-\nlow a map-reduce style (Figure 1a) or chain style (Figure 1b)\nworkflow to generate the final results.", "mimetype": "text/plain", "start_char_idx": 2079, "end_char_idx": 4843, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9bb3ca12-d623-42c5-9f41-dda20e1aba71": {"__data__": {"id_": "9bb3ca12-d623-42c5-9f41-dda20e1aba71", "embedding": null, "metadata": {"page_label": "2", "file_name": "2405_19888v1.pdf", "Title of this paper": "Parrot: Efficient Serving of LLM-based Applications with Semantic Variable", "Authors": "Chaofan Lin, Zhenhua Han, Chengruidong Zhang, Yuqing Yang, Fan Yang, Chen Chen, Lili Qiu", "Date published": "05/30/2024", "URL": "http://arxiv.org/abs/2405.19888v1", "summary": "The rise of large language models (LLMs) has enabled LLM-based applications\n(a.k.a. AI agents or co-pilots), a new software paradigm that combines the\nstrength of LLM and conventional software. Diverse LLM applications from\ndifferent tenants could design complex workflows using multiple LLM requests to\naccomplish one task. However, they have to use the over-simplified\nrequest-level API provided by today's public LLM services, losing essential\napplication-level information. Public LLM services have to blindly optimize\nindividual LLM requests, leading to sub-optimal end-to-end performance of LLM\napplications.\n  This paper introduces Parrot, an LLM service system that focuses on the\nend-to-end experience of LLM-based applications. Parrot proposes Semantic\nVariable, a unified abstraction to expose application-level knowledge to public\nLLM services. A Semantic Variable annotates an input/output variable in the\nprompt of a request, and creates the data pipeline when connecting multiple LLM\nrequests, providing a natural way to program LLM applications. Exposing\nSemantic Variables to the public LLM service allows it to perform conventional\ndata flow analysis to uncover the correlation across multiple LLM requests.\nThis correlation opens a brand-new optimization space for the end-to-end\nperformance of LLM-based applications. Extensive evaluations demonstrate that\nParrot can achieve up to an order-of-magnitude improvement for popular and\npractical use cases of LLM applications."}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e80fb7f0-ffbb-4071-b1ea-28d614ada6b6", "node_type": "4", "metadata": {"page_label": "2", "file_name": "2405_19888v1.pdf", "Title of this paper": "Parrot: Efficient Serving of LLM-based Applications with Semantic Variable", "Authors": "Chaofan Lin, Zhenhua Han, Chengruidong Zhang, Yuqing Yang, Fan Yang, Chen Chen, Lili Qiu", "Date published": "05/30/2024", "URL": "http://arxiv.org/abs/2405.19888v1", "summary": "The rise of large language models (LLMs) has enabled LLM-based applications\n(a.k.a. AI agents or co-pilots), a new software paradigm that combines the\nstrength of LLM and conventional software. Diverse LLM applications from\ndifferent tenants could design complex workflows using multiple LLM requests to\naccomplish one task. However, they have to use the over-simplified\nrequest-level API provided by today's public LLM services, losing essential\napplication-level information. Public LLM services have to blindly optimize\nindividual LLM requests, leading to sub-optimal end-to-end performance of LLM\napplications.\n  This paper introduces Parrot, an LLM service system that focuses on the\nend-to-end experience of LLM-based applications. Parrot proposes Semantic\nVariable, a unified abstraction to expose application-level knowledge to public\nLLM services. A Semantic Variable annotates an input/output variable in the\nprompt of a request, and creates the data pipeline when connecting multiple LLM\nrequests, providing a natural way to program LLM applications. Exposing\nSemantic Variables to the public LLM service allows it to perform conventional\ndata flow analysis to uncover the correlation across multiple LLM requests.\nThis correlation opens a brand-new optimization space for the end-to-end\nperformance of LLM-based applications. Extensive evaluations demonstrate that\nParrot can achieve up to an order-of-magnitude improvement for popular and\npractical use cases of LLM applications."}, "hash": "b3627b052feba9743035dd0be2dafc8dd20bf36f634e737f0ad052c265390dec", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0abda68a-949c-4163-bf0d-c764f0361133", "node_type": "1", "metadata": {"page_label": "2", "file_name": "2405_19888v1.pdf", "Title of this paper": "Parrot: Efficient Serving of LLM-based Applications with Semantic Variable", "Authors": "Chaofan Lin, Zhenhua Han, Chengruidong Zhang, Yuqing Yang, Fan Yang, Chen Chen, Lili Qiu", "Date published": "05/30/2024", "URL": "http://arxiv.org/abs/2405.19888v1", "summary": "The rise of large language models (LLMs) has enabled LLM-based applications\n(a.k.a. AI agents or co-pilots), a new software paradigm that combines the\nstrength of LLM and conventional software. Diverse LLM applications from\ndifferent tenants could design complex workflows using multiple LLM requests to\naccomplish one task. However, they have to use the over-simplified\nrequest-level API provided by today's public LLM services, losing essential\napplication-level information. Public LLM services have to blindly optimize\nindividual LLM requests, leading to sub-optimal end-to-end performance of LLM\napplications.\n  This paper introduces Parrot, an LLM service system that focuses on the\nend-to-end experience of LLM-based applications. Parrot proposes Semantic\nVariable, a unified abstraction to expose application-level knowledge to public\nLLM services. A Semantic Variable annotates an input/output variable in the\nprompt of a request, and creates the data pipeline when connecting multiple LLM\nrequests, providing a natural way to program LLM applications. Exposing\nSemantic Variables to the public LLM service allows it to perform conventional\ndata flow analysis to uncover the correlation across multiple LLM requests.\nThis correlation opens a brand-new optimization space for the end-to-end\nperformance of LLM-based applications. Extensive evaluations demonstrate that\nParrot can achieve up to an order-of-magnitude improvement for popular and\npractical use cases of LLM applications."}, "hash": "542152b35e77329042ea236e96a1a403f267be78834cc7628314ec4932a8bee1", "class_name": "RelatedNodeInfo"}}, "text": "Completion (prompt : str )\u2212 \u2192generated_text : str .\nThe application client provides a text prompt, and the LLM\nservice responds with the generated text. Behind the API,\nan LLM service provider runs one or multiple clusters of\nLLM inference engines. A request scheduler dispatches LLM\nrequests from a queue to an LLM inference engine, which\nuses a set of GPUs to conduct the LLM inference.\nLLM-based Applications. Figure 1 highlights the repre-\nsentative workflows of how LLM is used in the applications.\nDue to the limited context window of LLMs (e.g., 4,096 for\nGPT-3.5-Turbo [40]), data analytics on long documents fol-\nlow a map-reduce style (Figure 1a) or chain style (Figure 1b)\nworkflow to generate the final results. It splits the long tran-\nscript into chunks, uses multiple requests to generate partial\nresults for each chunk (the Map task), and combines them\naltogether (a Reduce task) or incrementally (the chain style)\nto generate the final result. Chat-based search engine in Fig-\nure 1c may use consecutive LLM requests to discern query", "mimetype": "text/plain", "start_char_idx": 4120, "end_char_idx": 5170, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9e2b072e-c6f8-4cd0-89c3-934f10c1c63a": {"__data__": {"id_": "9e2b072e-c6f8-4cd0-89c3-934f10c1c63a", "embedding": null, "metadata": {"page_label": "3", "file_name": "2405_19888v1.pdf", "Title of this paper": "Parrot: Efficient Serving of LLM-based Applications with Semantic Variable", "Authors": "Chaofan Lin, Zhenhua Han, Chengruidong Zhang, Yuqing Yang, Fan Yang, Chen Chen, Lili Qiu", "Date published": "05/30/2024", "URL": "http://arxiv.org/abs/2405.19888v1", "summary": "The rise of large language models (LLMs) has enabled LLM-based applications\n(a.k.a. AI agents or co-pilots), a new software paradigm that combines the\nstrength of LLM and conventional software. Diverse LLM applications from\ndifferent tenants could design complex workflows using multiple LLM requests to\naccomplish one task. However, they have to use the over-simplified\nrequest-level API provided by today's public LLM services, losing essential\napplication-level information. Public LLM services have to blindly optimize\nindividual LLM requests, leading to sub-optimal end-to-end performance of LLM\napplications.\n  This paper introduces Parrot, an LLM service system that focuses on the\nend-to-end experience of LLM-based applications. Parrot proposes Semantic\nVariable, a unified abstraction to expose application-level knowledge to public\nLLM services. A Semantic Variable annotates an input/output variable in the\nprompt of a request, and creates the data pipeline when connecting multiple LLM\nrequests, providing a natural way to program LLM applications. Exposing\nSemantic Variables to the public LLM service allows it to perform conventional\ndata flow analysis to uncover the correlation across multiple LLM requests.\nThis correlation opens a brand-new optimization space for the end-to-end\nperformance of LLM-based applications. Extensive evaluations demonstrate that\nParrot can achieve up to an order-of-magnitude improvement for popular and\npractical use cases of LLM applications."}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d9baf175-0877-4dac-997b-5c89b78c24fb", "node_type": "4", "metadata": {"page_label": "3", "file_name": "2405_19888v1.pdf", "Title of this paper": "Parrot: Efficient Serving of LLM-based Applications with Semantic Variable", "Authors": "Chaofan Lin, Zhenhua Han, Chengruidong Zhang, Yuqing Yang, Fan Yang, Chen Chen, Lili Qiu", "Date published": "05/30/2024", "URL": "http://arxiv.org/abs/2405.19888v1", "summary": "The rise of large language models (LLMs) has enabled LLM-based applications\n(a.k.a. AI agents or co-pilots), a new software paradigm that combines the\nstrength of LLM and conventional software. Diverse LLM applications from\ndifferent tenants could design complex workflows using multiple LLM requests to\naccomplish one task. However, they have to use the over-simplified\nrequest-level API provided by today's public LLM services, losing essential\napplication-level information. Public LLM services have to blindly optimize\nindividual LLM requests, leading to sub-optimal end-to-end performance of LLM\napplications.\n  This paper introduces Parrot, an LLM service system that focuses on the\nend-to-end experience of LLM-based applications. Parrot proposes Semantic\nVariable, a unified abstraction to expose application-level knowledge to public\nLLM services. A Semantic Variable annotates an input/output variable in the\nprompt of a request, and creates the data pipeline when connecting multiple LLM\nrequests, providing a natural way to program LLM applications. Exposing\nSemantic Variables to the public LLM service allows it to perform conventional\ndata flow analysis to uncover the correlation across multiple LLM requests.\nThis correlation opens a brand-new optimization space for the end-to-end\nperformance of LLM-based applications. Extensive evaluations demonstrate that\nParrot can achieve up to an order-of-magnitude improvement for popular and\npractical use cases of LLM applications."}, "hash": "c77c46beb0ad3cc66617be97ffb1013b5439427f81f88aa2484c99d356dd7eb9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1667c3c7-4008-4211-a2cd-352fd219db94", "node_type": "1", "metadata": {}, "hash": "13f318b095c888c6b16704bd4834ee3be9e2b4a3a0bc3539ca4df2164ab49173", "class_name": "RelatedNodeInfo"}}, "text": "0 1000 2000 3000 4000\nPrompt Length (# of tokens)010002000300040005000Time (ms)\nEnd-to-end Time (P99))\nGPU Inference Time\nOther Overhead (median)(a) Latency Breakdown\nLLM Step  A\nLLM Step B\nSchedulerInternetLLM  App\nOther LLM Apps\nA B\nLLM Engine LLM Engine LLM Engine\nAQueue\nB\u2460\n\u2461\u2462\nQuery\nResponse\n\u2463 (b) Current LLM Services\nLLM Engine LLM Engine LLM EngineSchedulerLLM Step A\nLLM Step B\n\u2460\n\u2461Internet\nA BA B Queue\nQuery\nResponseLLM  App\nOther LLM Apps (c) Our system: Parrot\nFigure 3: The end-to-end latency breakdown of current LLM services. The source of the overhead comes from network and\nqueuing due to chatty interaction between LLM application and LLM services, which is eliminated in our system Parrot.\nintention, enrich the query with supplementary information,\nretrieve related data, undergo a safety check, and finally gen-\nerate the response. Multi-agent in Figure 1d and Figure 2 is\nanother type of workflow using multiple LLM requests, each\nwith a designated role. Different roles work collaboratively on\nthe same task, e.g., AutoGen [54] and MetaGPT [22] use the\nroles like product manager, architect, engineer, and QA tester.\nThey communicate with each other on a software project.\nEach role is supported by one or multiple LLM requests to\nact as the designed role to generate their responses.\n3 Problems of Serving LLM Applications\nAlthough LLM\u2019s text completion API provides a flexible way\nof building LLM applications, it loses the application-level\ninformation to public LLM services, leading to the following\nchallenges.\nExcessive Overhead of Consecutive Requests. As demon-\nstrated in Figure 1, LLM applications frequently make multi-\nple LLM calls to complete a single task. Due to the request-\ncentric design of existing public LLM services, which gener-\nate responses for each request individually, developers have to\nparse the output of an LLM request and compose the prompts\nfor subsequent LLM requests on the client side. Figure 3a\nshows our empirical study of the latency breakdown of the\nLLM calls from a popular LLM application in our production,\nwhich uses a chain-style workflow. The prompt lengths range\nfrom 150 to 4000 tokens and the output length is around 50\ntokens. We find there is a significant portion of the latency of\nLLM API call originates outside the LLM engine ( 30\u223c50%\non average and over 70% in the worst cases). The overhead in-\ncreases with the growing length of prompts. The high latency\ncan sometimes result in API timeouts and resubmissions.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2493, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1667c3c7-4008-4211-a2cd-352fd219db94": {"__data__": {"id_": "1667c3c7-4008-4211-a2cd-352fd219db94", "embedding": null, "metadata": {"page_label": "3", "file_name": "2405_19888v1.pdf", "Title of this paper": "Parrot: Efficient Serving of LLM-based Applications with Semantic Variable", "Authors": "Chaofan Lin, Zhenhua Han, Chengruidong Zhang, Yuqing Yang, Fan Yang, Chen Chen, Lili Qiu", "Date published": "05/30/2024", "URL": "http://arxiv.org/abs/2405.19888v1", "summary": "The rise of large language models (LLMs) has enabled LLM-based applications\n(a.k.a. AI agents or co-pilots), a new software paradigm that combines the\nstrength of LLM and conventional software. Diverse LLM applications from\ndifferent tenants could design complex workflows using multiple LLM requests to\naccomplish one task. However, they have to use the over-simplified\nrequest-level API provided by today's public LLM services, losing essential\napplication-level information. Public LLM services have to blindly optimize\nindividual LLM requests, leading to sub-optimal end-to-end performance of LLM\napplications.\n  This paper introduces Parrot, an LLM service system that focuses on the\nend-to-end experience of LLM-based applications. Parrot proposes Semantic\nVariable, a unified abstraction to expose application-level knowledge to public\nLLM services. A Semantic Variable annotates an input/output variable in the\nprompt of a request, and creates the data pipeline when connecting multiple LLM\nrequests, providing a natural way to program LLM applications. Exposing\nSemantic Variables to the public LLM service allows it to perform conventional\ndata flow analysis to uncover the correlation across multiple LLM requests.\nThis correlation opens a brand-new optimization space for the end-to-end\nperformance of LLM-based applications. Extensive evaluations demonstrate that\nParrot can achieve up to an order-of-magnitude improvement for popular and\npractical use cases of LLM applications."}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d9baf175-0877-4dac-997b-5c89b78c24fb", "node_type": "4", "metadata": {"page_label": "3", "file_name": "2405_19888v1.pdf", "Title of this paper": "Parrot: Efficient Serving of LLM-based Applications with Semantic Variable", "Authors": "Chaofan Lin, Zhenhua Han, Chengruidong Zhang, Yuqing Yang, Fan Yang, Chen Chen, Lili Qiu", "Date published": "05/30/2024", "URL": "http://arxiv.org/abs/2405.19888v1", "summary": "The rise of large language models (LLMs) has enabled LLM-based applications\n(a.k.a. AI agents or co-pilots), a new software paradigm that combines the\nstrength of LLM and conventional software. Diverse LLM applications from\ndifferent tenants could design complex workflows using multiple LLM requests to\naccomplish one task. However, they have to use the over-simplified\nrequest-level API provided by today's public LLM services, losing essential\napplication-level information. Public LLM services have to blindly optimize\nindividual LLM requests, leading to sub-optimal end-to-end performance of LLM\napplications.\n  This paper introduces Parrot, an LLM service system that focuses on the\nend-to-end experience of LLM-based applications. Parrot proposes Semantic\nVariable, a unified abstraction to expose application-level knowledge to public\nLLM services. A Semantic Variable annotates an input/output variable in the\nprompt of a request, and creates the data pipeline when connecting multiple LLM\nrequests, providing a natural way to program LLM applications. Exposing\nSemantic Variables to the public LLM service allows it to perform conventional\ndata flow analysis to uncover the correlation across multiple LLM requests.\nThis correlation opens a brand-new optimization space for the end-to-end\nperformance of LLM-based applications. Extensive evaluations demonstrate that\nParrot can achieve up to an order-of-magnitude improvement for popular and\npractical use cases of LLM applications."}, "hash": "c77c46beb0ad3cc66617be97ffb1013b5439427f81f88aa2484c99d356dd7eb9", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9e2b072e-c6f8-4cd0-89c3-934f10c1c63a", "node_type": "1", "metadata": {"page_label": "3", "file_name": "2405_19888v1.pdf", "Title of this paper": "Parrot: Efficient Serving of LLM-based Applications with Semantic Variable", "Authors": "Chaofan Lin, Zhenhua Han, Chengruidong Zhang, Yuqing Yang, Fan Yang, Chen Chen, Lili Qiu", "Date published": "05/30/2024", "URL": "http://arxiv.org/abs/2405.19888v1", "summary": "The rise of large language models (LLMs) has enabled LLM-based applications\n(a.k.a. AI agents or co-pilots), a new software paradigm that combines the\nstrength of LLM and conventional software. Diverse LLM applications from\ndifferent tenants could design complex workflows using multiple LLM requests to\naccomplish one task. However, they have to use the over-simplified\nrequest-level API provided by today's public LLM services, losing essential\napplication-level information. Public LLM services have to blindly optimize\nindividual LLM requests, leading to sub-optimal end-to-end performance of LLM\napplications.\n  This paper introduces Parrot, an LLM service system that focuses on the\nend-to-end experience of LLM-based applications. Parrot proposes Semantic\nVariable, a unified abstraction to expose application-level knowledge to public\nLLM services. A Semantic Variable annotates an input/output variable in the\nprompt of a request, and creates the data pipeline when connecting multiple LLM\nrequests, providing a natural way to program LLM applications. Exposing\nSemantic Variables to the public LLM service allows it to perform conventional\ndata flow analysis to uncover the correlation across multiple LLM requests.\nThis correlation opens a brand-new optimization space for the end-to-end\nperformance of LLM-based applications. Extensive evaluations demonstrate that\nParrot can achieve up to an order-of-magnitude improvement for popular and\npractical use cases of LLM applications."}, "hash": "a3affb1f27b0d2fef1e0c4c5799443d31325d7438d0cbae64e7bab7ea1828bb6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "668a598d-f2b4-4b2f-b0f3-d0c148fed797", "node_type": "1", "metadata": {}, "hash": "a1e2989e40c5871e08d2c53f8852607aee50eeedfc6d0e571595457b7d184e10", "class_name": "RelatedNodeInfo"}}, "text": "Due to the request-\ncentric design of existing public LLM services, which gener-\nate responses for each request individually, developers have to\nparse the output of an LLM request and compose the prompts\nfor subsequent LLM requests on the client side. Figure 3a\nshows our empirical study of the latency breakdown of the\nLLM calls from a popular LLM application in our production,\nwhich uses a chain-style workflow. The prompt lengths range\nfrom 150 to 4000 tokens and the output length is around 50\ntokens. We find there is a significant portion of the latency of\nLLM API call originates outside the LLM engine ( 30\u223c50%\non average and over 70% in the worst cases). The overhead in-\ncreases with the growing length of prompts. The high latency\ncan sometimes result in API timeouts and resubmissions.\nSuch overhead is due to the chatty interaction between\nLLM services and clients. Figure 3b illustrates the overhead\nof a simple two-step LLM application (e.g., chain-style sum-\nmary of two text chunks). Existing LLM services are unaware\nof the dependency among such requests, where the output of\nthe previous request may be the direct input of the next one.\nFor such consecutive and dependent requests, the client has\nChunk 1Chunk 2\nChunk 3Chunk 4\u2026\u2026Final Summary Chunk 5Chunk 6\nChunk 15Chunk 16Batch =2\nTime\nChunk 1 Final SummaryBatch=8 Chunk 9Time(1) Per-request latency optimized\n(2) End -to-end latency  optimizedMaximize Throughput\nMinimize LatencyLatency=2700 ms\nLatency=1100 msReduce StageMap Stage\nReduce Stage\nChunk 2 Chunk 10Chunk 7 Chunk 15Chunk 8 Chunk 16Map Stage \u2026\u2026\n\u2026\u2026Figure 4: Request-centric scheduling v.s. application-centric\nscheduling for the map-reduce style document summary task.\nto wait for the arrival of the response to the first LLM request\n(2) before submitting the next LLM request ( 3). This un-\nnecessarily incurs heavy network latency because clients and\nLLM services are typically in different data centers. More-\nover, the next LLM request has to suffer extra queuing delays\n(4), because requests from other applications may arrive\nbetween the consecutive LLM requests.\nIn Table 1, we evaluated four popular LLM applications.\nThe first two are from our production, and the last two are\npopular open-source projects. They all require tens of LLM\ncalls to complete a single task, which results in high user-\nperceived latency. Our evaluation in \u00a78.2 shows LLM services\nthat treat requests individually could slow down the end-to-\nend latency by over 2\u00d7. An LLM service can eliminate the\noverhead if it can handle consecutive requests in a batch.\nParrot adopts such an approach.", "mimetype": "text/plain", "start_char_idx": 1695, "end_char_idx": 4302, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "668a598d-f2b4-4b2f-b0f3-d0c148fed797": {"__data__": {"id_": "668a598d-f2b4-4b2f-b0f3-d0c148fed797", "embedding": null, "metadata": {"page_label": "3", "file_name": "2405_19888v1.pdf", "Title of this paper": "Parrot: Efficient Serving of LLM-based Applications with Semantic Variable", "Authors": "Chaofan Lin, Zhenhua Han, Chengruidong Zhang, Yuqing Yang, Fan Yang, Chen Chen, Lili Qiu", "Date published": "05/30/2024", "URL": "http://arxiv.org/abs/2405.19888v1", "summary": "The rise of large language models (LLMs) has enabled LLM-based applications\n(a.k.a. AI agents or co-pilots), a new software paradigm that combines the\nstrength of LLM and conventional software. Diverse LLM applications from\ndifferent tenants could design complex workflows using multiple LLM requests to\naccomplish one task. However, they have to use the over-simplified\nrequest-level API provided by today's public LLM services, losing essential\napplication-level information. Public LLM services have to blindly optimize\nindividual LLM requests, leading to sub-optimal end-to-end performance of LLM\napplications.\n  This paper introduces Parrot, an LLM service system that focuses on the\nend-to-end experience of LLM-based applications. Parrot proposes Semantic\nVariable, a unified abstraction to expose application-level knowledge to public\nLLM services. A Semantic Variable annotates an input/output variable in the\nprompt of a request, and creates the data pipeline when connecting multiple LLM\nrequests, providing a natural way to program LLM applications. Exposing\nSemantic Variables to the public LLM service allows it to perform conventional\ndata flow analysis to uncover the correlation across multiple LLM requests.\nThis correlation opens a brand-new optimization space for the end-to-end\nperformance of LLM-based applications. Extensive evaluations demonstrate that\nParrot can achieve up to an order-of-magnitude improvement for popular and\npractical use cases of LLM applications."}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d9baf175-0877-4dac-997b-5c89b78c24fb", "node_type": "4", "metadata": {"page_label": "3", "file_name": "2405_19888v1.pdf", "Title of this paper": "Parrot: Efficient Serving of LLM-based Applications with Semantic Variable", "Authors": "Chaofan Lin, Zhenhua Han, Chengruidong Zhang, Yuqing Yang, Fan Yang, Chen Chen, Lili Qiu", "Date published": "05/30/2024", "URL": "http://arxiv.org/abs/2405.19888v1", "summary": "The rise of large language models (LLMs) has enabled LLM-based applications\n(a.k.a. AI agents or co-pilots), a new software paradigm that combines the\nstrength of LLM and conventional software. Diverse LLM applications from\ndifferent tenants could design complex workflows using multiple LLM requests to\naccomplish one task. However, they have to use the over-simplified\nrequest-level API provided by today's public LLM services, losing essential\napplication-level information. Public LLM services have to blindly optimize\nindividual LLM requests, leading to sub-optimal end-to-end performance of LLM\napplications.\n  This paper introduces Parrot, an LLM service system that focuses on the\nend-to-end experience of LLM-based applications. Parrot proposes Semantic\nVariable, a unified abstraction to expose application-level knowledge to public\nLLM services. A Semantic Variable annotates an input/output variable in the\nprompt of a request, and creates the data pipeline when connecting multiple LLM\nrequests, providing a natural way to program LLM applications. Exposing\nSemantic Variables to the public LLM service allows it to perform conventional\ndata flow analysis to uncover the correlation across multiple LLM requests.\nThis correlation opens a brand-new optimization space for the end-to-end\nperformance of LLM-based applications. Extensive evaluations demonstrate that\nParrot can achieve up to an order-of-magnitude improvement for popular and\npractical use cases of LLM applications."}, "hash": "c77c46beb0ad3cc66617be97ffb1013b5439427f81f88aa2484c99d356dd7eb9", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1667c3c7-4008-4211-a2cd-352fd219db94", "node_type": "1", "metadata": {"page_label": "3", "file_name": "2405_19888v1.pdf", "Title of this paper": "Parrot: Efficient Serving of LLM-based Applications with Semantic Variable", "Authors": "Chaofan Lin, Zhenhua Han, Chengruidong Zhang, Yuqing Yang, Fan Yang, Chen Chen, Lili Qiu", "Date published": "05/30/2024", "URL": "http://arxiv.org/abs/2405.19888v1", "summary": "The rise of large language models (LLMs) has enabled LLM-based applications\n(a.k.a. AI agents or co-pilots), a new software paradigm that combines the\nstrength of LLM and conventional software. Diverse LLM applications from\ndifferent tenants could design complex workflows using multiple LLM requests to\naccomplish one task. However, they have to use the over-simplified\nrequest-level API provided by today's public LLM services, losing essential\napplication-level information. Public LLM services have to blindly optimize\nindividual LLM requests, leading to sub-optimal end-to-end performance of LLM\napplications.\n  This paper introduces Parrot, an LLM service system that focuses on the\nend-to-end experience of LLM-based applications. Parrot proposes Semantic\nVariable, a unified abstraction to expose application-level knowledge to public\nLLM services. A Semantic Variable annotates an input/output variable in the\nprompt of a request, and creates the data pipeline when connecting multiple LLM\nrequests, providing a natural way to program LLM applications. Exposing\nSemantic Variables to the public LLM service allows it to perform conventional\ndata flow analysis to uncover the correlation across multiple LLM requests.\nThis correlation opens a brand-new optimization space for the end-to-end\nperformance of LLM-based applications. Extensive evaluations demonstrate that\nParrot can achieve up to an order-of-magnitude improvement for popular and\npractical use cases of LLM applications."}, "hash": "b8fc8a837f50aa9dd258f666746c403cf25581bb6fe20ba3ec292507aae75a87", "class_name": "RelatedNodeInfo"}}, "text": "This un-\nnecessarily incurs heavy network latency because clients and\nLLM services are typically in different data centers. More-\nover, the next LLM request has to suffer extra queuing delays\n(4), because requests from other applications may arrive\nbetween the consecutive LLM requests.\nIn Table 1, we evaluated four popular LLM applications.\nThe first two are from our production, and the last two are\npopular open-source projects. They all require tens of LLM\ncalls to complete a single task, which results in high user-\nperceived latency. Our evaluation in \u00a78.2 shows LLM services\nthat treat requests individually could slow down the end-to-\nend latency by over 2\u00d7. An LLM service can eliminate the\noverhead if it can handle consecutive requests in a batch.\nParrot adopts such an approach. As shown in Figure 3c, the\ntwo steps of the same application are scheduled together, thus\nallowing the output of Step A to be fed directly into Step\nB\u2014with the network and queuing overhead bypassed.\nMisaligned Scheduling Objectives. Due to the lost appli-\nLLM-based App. # Calls Tokens Repeated (%)\u2217\nLong Doc. Analytics 2\u223c40 3.5k\u223c80k 3%\nChat Search 2\u223c10 5k 94%\nMetaGPT [22] 14 17k 72%\nAutoGen [54] 17 57k 99%\n\u2217We count a paragraph as repeated if it appears in at least two LLM requests.\nTable 1: Statistics of LLM calls of LLM applications.", "mimetype": "text/plain", "start_char_idx": 3510, "end_char_idx": 4843, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a94d38c7-a0e0-4e6a-a35e-273cf47c8a05": {"__data__": {"id_": "a94d38c7-a0e0-4e6a-a35e-273cf47c8a05", "embedding": null, "metadata": {"page_label": "4", "file_name": "2405_19888v1.pdf", "Title of this paper": "Parrot: Efficient Serving of LLM-based Applications with Semantic Variable", "Authors": "Chaofan Lin, Zhenhua Han, Chengruidong Zhang, Yuqing Yang, Fan Yang, Chen Chen, Lili Qiu", "Date published": "05/30/2024", "URL": "http://arxiv.org/abs/2405.19888v1", "summary": "The rise of large language models (LLMs) has enabled LLM-based applications\n(a.k.a. AI agents or co-pilots), a new software paradigm that combines the\nstrength of LLM and conventional software. Diverse LLM applications from\ndifferent tenants could design complex workflows using multiple LLM requests to\naccomplish one task. However, they have to use the over-simplified\nrequest-level API provided by today's public LLM services, losing essential\napplication-level information. Public LLM services have to blindly optimize\nindividual LLM requests, leading to sub-optimal end-to-end performance of LLM\napplications.\n  This paper introduces Parrot, an LLM service system that focuses on the\nend-to-end experience of LLM-based applications. Parrot proposes Semantic\nVariable, a unified abstraction to expose application-level knowledge to public\nLLM services. A Semantic Variable annotates an input/output variable in the\nprompt of a request, and creates the data pipeline when connecting multiple LLM\nrequests, providing a natural way to program LLM applications. Exposing\nSemantic Variables to the public LLM service allows it to perform conventional\ndata flow analysis to uncover the correlation across multiple LLM requests.\nThis correlation opens a brand-new optimization space for the end-to-end\nperformance of LLM-based applications. Extensive evaluations demonstrate that\nParrot can achieve up to an order-of-magnitude improvement for popular and\npractical use cases of LLM applications."}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "9e9820f0-efe5-4358-9ddd-fea783d89323", "node_type": "4", "metadata": {"page_label": "4", "file_name": "2405_19888v1.pdf", "Title of this paper": "Parrot: Efficient Serving of LLM-based Applications with Semantic Variable", "Authors": "Chaofan Lin, Zhenhua Han, Chengruidong Zhang, Yuqing Yang, Fan Yang, Chen Chen, Lili Qiu", "Date published": "05/30/2024", "URL": "http://arxiv.org/abs/2405.19888v1", "summary": "The rise of large language models (LLMs) has enabled LLM-based applications\n(a.k.a. AI agents or co-pilots), a new software paradigm that combines the\nstrength of LLM and conventional software. Diverse LLM applications from\ndifferent tenants could design complex workflows using multiple LLM requests to\naccomplish one task. However, they have to use the over-simplified\nrequest-level API provided by today's public LLM services, losing essential\napplication-level information. Public LLM services have to blindly optimize\nindividual LLM requests, leading to sub-optimal end-to-end performance of LLM\napplications.\n  This paper introduces Parrot, an LLM service system that focuses on the\nend-to-end experience of LLM-based applications. Parrot proposes Semantic\nVariable, a unified abstraction to expose application-level knowledge to public\nLLM services. A Semantic Variable annotates an input/output variable in the\nprompt of a request, and creates the data pipeline when connecting multiple LLM\nrequests, providing a natural way to program LLM applications. Exposing\nSemantic Variables to the public LLM service allows it to perform conventional\ndata flow analysis to uncover the correlation across multiple LLM requests.\nThis correlation opens a brand-new optimization space for the end-to-end\nperformance of LLM-based applications. Extensive evaluations demonstrate that\nParrot can achieve up to an order-of-magnitude improvement for popular and\npractical use cases of LLM applications."}, "hash": "768d5952fdfafb917faf3d317dabfa4a4148c0c2a734c95afc94f47f7b2f0d02", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1539fe43-e4a4-4320-9019-560ebb9ac7a1", "node_type": "1", "metadata": {}, "hash": "df20ec6144fda9d1588af2fbbe4f8848dc9894fb291e53184f5a29e16c2c635a", "class_name": "RelatedNodeInfo"}}, "text": "[system](#instructions) \n## You are the chat mode \nof Microsoft Bing search: \n- You identify as Microsoft \nBing search to users, \n**not** an assistant. \n- You should \u2026\u2026[system](#context) \n- New conversation with user A. \n- Time at the start of this conversation is \nSun, 30 Oct 2022 16:13:49 GMT. The \nuser is located in  Redmond, Washington, \nUnited States. \n[user](#message) Hi. \u2026\u2026[system](#context) \n- New conversation with user B. \n- Time at the start of this conversation is \nMon, 20 Nov 2023 16:13:49 GMT. The \nuser is located in  London, UK. \n[user](#message)\nExplain AI agent  for a kid.\nTask Role (static) Few -shot Examples (quasi -static) User Input (dynamic)+ +Figure 5: The prompt structure of Bing Copilot shows a long\nprompt reused by different user queries.\ncation information (workflow and application performance\nobjective), existing public LLM services have to blindly use\na universal treatment for all requests, e.g., optimizing per-\nrequest latency [44]. However, LLM-based applications are\nmore concerned about the end-to-end experience, rather than\nindividual requests. This misaligned optimization objectives\nmay negatively impact end-to-end performance. Considering\nthe map-reduce document summary in Figure 1a, the system\nshould minimize the end-to-end time it takes to receive the\nfinal summary, rather than the latency of individual requests.\nThe LLM services optimized for individual requests are not\noptimal for end-to-end latency.\nAs depicted in Figure 4, current LLM services must limit\nthe number of concurrent requests running on each LLM en-\ngine to control the latency of individual requests. However,\nthere is a trade-off between latency and throughput in LLM in-\nference. Increasing the batch size can bring up to 8.2\u00d7higher\nthroughput but lead to 95% higher latency [9]. Yet, if we un-\nderstand the application-level performance objective, which\nin this case is the end-to-end latency, we can determine that\nthe ideal scheduling strategy should maximize the throughput\n(using higher batch sizes) during the map stage and minimize\nrequest latency during the reduce stage. This strategy reduces\nend-to-end latency by 2.4\u00d7. Moreover, it uncovers the po-\ntential to enhance cluster throughput without compromising\nthe end-to-end latency of LLM applications. This insight is\nessential for addressing the conflict between rising demand\nand limited hardware resources. It underscores the necessity\nof scheduling LLM requests from the perspective of LLM\napplications, but it also presents the challenge of managing\ndiverse LLM requests with varying performance objectives.\nRedundant Computations.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2627, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1539fe43-e4a4-4320-9019-560ebb9ac7a1": {"__data__": {"id_": "1539fe43-e4a4-4320-9019-560ebb9ac7a1", "embedding": null, "metadata": {"page_label": "4", "file_name": "2405_19888v1.pdf", "Title of this paper": "Parrot: Efficient Serving of LLM-based Applications with Semantic Variable", "Authors": "Chaofan Lin, Zhenhua Han, Chengruidong Zhang, Yuqing Yang, Fan Yang, Chen Chen, Lili Qiu", "Date published": "05/30/2024", "URL": "http://arxiv.org/abs/2405.19888v1", "summary": "The rise of large language models (LLMs) has enabled LLM-based applications\n(a.k.a. AI agents or co-pilots), a new software paradigm that combines the\nstrength of LLM and conventional software. Diverse LLM applications from\ndifferent tenants could design complex workflows using multiple LLM requests to\naccomplish one task. However, they have to use the over-simplified\nrequest-level API provided by today's public LLM services, losing essential\napplication-level information. Public LLM services have to blindly optimize\nindividual LLM requests, leading to sub-optimal end-to-end performance of LLM\napplications.\n  This paper introduces Parrot, an LLM service system that focuses on the\nend-to-end experience of LLM-based applications. Parrot proposes Semantic\nVariable, a unified abstraction to expose application-level knowledge to public\nLLM services. A Semantic Variable annotates an input/output variable in the\nprompt of a request, and creates the data pipeline when connecting multiple LLM\nrequests, providing a natural way to program LLM applications. Exposing\nSemantic Variables to the public LLM service allows it to perform conventional\ndata flow analysis to uncover the correlation across multiple LLM requests.\nThis correlation opens a brand-new optimization space for the end-to-end\nperformance of LLM-based applications. Extensive evaluations demonstrate that\nParrot can achieve up to an order-of-magnitude improvement for popular and\npractical use cases of LLM applications."}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "9e9820f0-efe5-4358-9ddd-fea783d89323", "node_type": "4", "metadata": {"page_label": "4", "file_name": "2405_19888v1.pdf", "Title of this paper": "Parrot: Efficient Serving of LLM-based Applications with Semantic Variable", "Authors": "Chaofan Lin, Zhenhua Han, Chengruidong Zhang, Yuqing Yang, Fan Yang, Chen Chen, Lili Qiu", "Date published": "05/30/2024", "URL": "http://arxiv.org/abs/2405.19888v1", "summary": "The rise of large language models (LLMs) has enabled LLM-based applications\n(a.k.a. AI agents or co-pilots), a new software paradigm that combines the\nstrength of LLM and conventional software. Diverse LLM applications from\ndifferent tenants could design complex workflows using multiple LLM requests to\naccomplish one task. However, they have to use the over-simplified\nrequest-level API provided by today's public LLM services, losing essential\napplication-level information. Public LLM services have to blindly optimize\nindividual LLM requests, leading to sub-optimal end-to-end performance of LLM\napplications.\n  This paper introduces Parrot, an LLM service system that focuses on the\nend-to-end experience of LLM-based applications. Parrot proposes Semantic\nVariable, a unified abstraction to expose application-level knowledge to public\nLLM services. A Semantic Variable annotates an input/output variable in the\nprompt of a request, and creates the data pipeline when connecting multiple LLM\nrequests, providing a natural way to program LLM applications. Exposing\nSemantic Variables to the public LLM service allows it to perform conventional\ndata flow analysis to uncover the correlation across multiple LLM requests.\nThis correlation opens a brand-new optimization space for the end-to-end\nperformance of LLM-based applications. Extensive evaluations demonstrate that\nParrot can achieve up to an order-of-magnitude improvement for popular and\npractical use cases of LLM applications."}, "hash": "768d5952fdfafb917faf3d317dabfa4a4148c0c2a734c95afc94f47f7b2f0d02", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a94d38c7-a0e0-4e6a-a35e-273cf47c8a05", "node_type": "1", "metadata": {"page_label": "4", "file_name": "2405_19888v1.pdf", "Title of this paper": "Parrot: Efficient Serving of LLM-based Applications with Semantic Variable", "Authors": "Chaofan Lin, Zhenhua Han, Chengruidong Zhang, Yuqing Yang, Fan Yang, Chen Chen, Lili Qiu", "Date published": "05/30/2024", "URL": "http://arxiv.org/abs/2405.19888v1", "summary": "The rise of large language models (LLMs) has enabled LLM-based applications\n(a.k.a. AI agents or co-pilots), a new software paradigm that combines the\nstrength of LLM and conventional software. Diverse LLM applications from\ndifferent tenants could design complex workflows using multiple LLM requests to\naccomplish one task. However, they have to use the over-simplified\nrequest-level API provided by today's public LLM services, losing essential\napplication-level information. Public LLM services have to blindly optimize\nindividual LLM requests, leading to sub-optimal end-to-end performance of LLM\napplications.\n  This paper introduces Parrot, an LLM service system that focuses on the\nend-to-end experience of LLM-based applications. Parrot proposes Semantic\nVariable, a unified abstraction to expose application-level knowledge to public\nLLM services. A Semantic Variable annotates an input/output variable in the\nprompt of a request, and creates the data pipeline when connecting multiple LLM\nrequests, providing a natural way to program LLM applications. Exposing\nSemantic Variables to the public LLM service allows it to perform conventional\ndata flow analysis to uncover the correlation across multiple LLM requests.\nThis correlation opens a brand-new optimization space for the end-to-end\nperformance of LLM-based applications. Extensive evaluations demonstrate that\nParrot can achieve up to an order-of-magnitude improvement for popular and\npractical use cases of LLM applications."}, "hash": "c55d150a9ee47d377ab1ff6a2aa2b12d2874fe4f5bb25b916b163794d312f14d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5891bbdf-ac44-4840-8d3c-a86747da6898", "node_type": "1", "metadata": {}, "hash": "b35e9f66092b9d3ad1492f2e3361a8c14560dcfb5c7cd77f6e93e5b9405d408d", "class_name": "RelatedNodeInfo"}}, "text": "Increasing the batch size can bring up to 8.2\u00d7higher\nthroughput but lead to 95% higher latency [9]. Yet, if we un-\nderstand the application-level performance objective, which\nin this case is the end-to-end latency, we can determine that\nthe ideal scheduling strategy should maximize the throughput\n(using higher batch sizes) during the map stage and minimize\nrequest latency during the reduce stage. This strategy reduces\nend-to-end latency by 2.4\u00d7. Moreover, it uncovers the po-\ntential to enhance cluster throughput without compromising\nthe end-to-end latency of LLM applications. This insight is\nessential for addressing the conflict between rising demand\nand limited hardware resources. It underscores the necessity\nof scheduling LLM requests from the perspective of LLM\napplications, but it also presents the challenge of managing\ndiverse LLM requests with varying performance objectives.\nRedundant Computations. Currently, most LLM-based\napplications exhibit a high degree of redundancy in the\nprompts of their requests. For instance, Bing Chat [32] has\nhandled more than 1 billion chat prompts. These prompts\nshare the same system prompts that defines the functionality\nof Bing Chat. OpenAI introduces GPTs [42] to let users cus-\ntomize a ChatGPT for a specific purpose whose prompt tem-\nplate is the same across users. The commonality in prompts\nis crucial as it delineates the functionality and restrictions\nof LLM-based applications. The prompt structure in Fig-\nure 5 [52] includes a role definition, several examples to\nenhance the precision of LLM\u2019s behaviors and user query\ndetails. While the user input is dynamic, the task role is al-\nParrot LLM EngineParrot LLM EngineParrot LLM EngineParrot APIs w/ \n Semantic Variables\nParrot Manager  w/ Inter -Request Analysis\nParrot \nApp -centric  \nLLM ServiceApplications\nInternetApplications (front -end)\nParrot Front -end Others (LangChain ,SK, etc.)\nPerf. Objective Deduction\nSharing Prompt Prefix App -centric Scheduling\nEfficient GPU Kernels Context ManagementContextual Fill / GenInter -Request Comm.Figure 6: Parrot system overview.\nways fixed, and the few-shot examples could be quasi-static in\nthat the same type of tasks use the same examples. This is why\nmore than 94% of prefix tokens could be repetitively used\nacross LLM requests for various users (Table 1). Such com-\nmonality also exists in multi-agent applications. For example,\nMetaGPT [22] and AutoGen [54] recurrently incorporate con-\nversation history into the prompt over several rounds of LLM\nrequests, leading to 72% and99% redundancy respectively.\nThese redundant sections excessively utilize GPU memory\nbandwidth and are computed for multiple times.", "mimetype": "text/plain", "start_char_idx": 1710, "end_char_idx": 4391, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5891bbdf-ac44-4840-8d3c-a86747da6898": {"__data__": {"id_": "5891bbdf-ac44-4840-8d3c-a86747da6898", "embedding": null, "metadata": {"page_label": "4", "file_name": "2405_19888v1.pdf", "Title of this paper": "Parrot: Efficient Serving of LLM-based Applications with Semantic Variable", "Authors": "Chaofan Lin, Zhenhua Han, Chengruidong Zhang, Yuqing Yang, Fan Yang, Chen Chen, Lili Qiu", "Date published": "05/30/2024", "URL": "http://arxiv.org/abs/2405.19888v1", "summary": "The rise of large language models (LLMs) has enabled LLM-based applications\n(a.k.a. AI agents or co-pilots), a new software paradigm that combines the\nstrength of LLM and conventional software. Diverse LLM applications from\ndifferent tenants could design complex workflows using multiple LLM requests to\naccomplish one task. However, they have to use the over-simplified\nrequest-level API provided by today's public LLM services, losing essential\napplication-level information. Public LLM services have to blindly optimize\nindividual LLM requests, leading to sub-optimal end-to-end performance of LLM\napplications.\n  This paper introduces Parrot, an LLM service system that focuses on the\nend-to-end experience of LLM-based applications. Parrot proposes Semantic\nVariable, a unified abstraction to expose application-level knowledge to public\nLLM services. A Semantic Variable annotates an input/output variable in the\nprompt of a request, and creates the data pipeline when connecting multiple LLM\nrequests, providing a natural way to program LLM applications. Exposing\nSemantic Variables to the public LLM service allows it to perform conventional\ndata flow analysis to uncover the correlation across multiple LLM requests.\nThis correlation opens a brand-new optimization space for the end-to-end\nperformance of LLM-based applications. Extensive evaluations demonstrate that\nParrot can achieve up to an order-of-magnitude improvement for popular and\npractical use cases of LLM applications."}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "9e9820f0-efe5-4358-9ddd-fea783d89323", "node_type": "4", "metadata": {"page_label": "4", "file_name": "2405_19888v1.pdf", "Title of this paper": "Parrot: Efficient Serving of LLM-based Applications with Semantic Variable", "Authors": "Chaofan Lin, Zhenhua Han, Chengruidong Zhang, Yuqing Yang, Fan Yang, Chen Chen, Lili Qiu", "Date published": "05/30/2024", "URL": "http://arxiv.org/abs/2405.19888v1", "summary": "The rise of large language models (LLMs) has enabled LLM-based applications\n(a.k.a. AI agents or co-pilots), a new software paradigm that combines the\nstrength of LLM and conventional software. Diverse LLM applications from\ndifferent tenants could design complex workflows using multiple LLM requests to\naccomplish one task. However, they have to use the over-simplified\nrequest-level API provided by today's public LLM services, losing essential\napplication-level information. Public LLM services have to blindly optimize\nindividual LLM requests, leading to sub-optimal end-to-end performance of LLM\napplications.\n  This paper introduces Parrot, an LLM service system that focuses on the\nend-to-end experience of LLM-based applications. Parrot proposes Semantic\nVariable, a unified abstraction to expose application-level knowledge to public\nLLM services. A Semantic Variable annotates an input/output variable in the\nprompt of a request, and creates the data pipeline when connecting multiple LLM\nrequests, providing a natural way to program LLM applications. Exposing\nSemantic Variables to the public LLM service allows it to perform conventional\ndata flow analysis to uncover the correlation across multiple LLM requests.\nThis correlation opens a brand-new optimization space for the end-to-end\nperformance of LLM-based applications. Extensive evaluations demonstrate that\nParrot can achieve up to an order-of-magnitude improvement for popular and\npractical use cases of LLM applications."}, "hash": "768d5952fdfafb917faf3d317dabfa4a4148c0c2a734c95afc94f47f7b2f0d02", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1539fe43-e4a4-4320-9019-560ebb9ac7a1", "node_type": "1", "metadata": {"page_label": "4", "file_name": "2405_19888v1.pdf", "Title of this paper": "Parrot: Efficient Serving of LLM-based Applications with Semantic Variable", "Authors": "Chaofan Lin, Zhenhua Han, Chengruidong Zhang, Yuqing Yang, Fan Yang, Chen Chen, Lili Qiu", "Date published": "05/30/2024", "URL": "http://arxiv.org/abs/2405.19888v1", "summary": "The rise of large language models (LLMs) has enabled LLM-based applications\n(a.k.a. AI agents or co-pilots), a new software paradigm that combines the\nstrength of LLM and conventional software. Diverse LLM applications from\ndifferent tenants could design complex workflows using multiple LLM requests to\naccomplish one task. However, they have to use the over-simplified\nrequest-level API provided by today's public LLM services, losing essential\napplication-level information. Public LLM services have to blindly optimize\nindividual LLM requests, leading to sub-optimal end-to-end performance of LLM\napplications.\n  This paper introduces Parrot, an LLM service system that focuses on the\nend-to-end experience of LLM-based applications. Parrot proposes Semantic\nVariable, a unified abstraction to expose application-level knowledge to public\nLLM services. A Semantic Variable annotates an input/output variable in the\nprompt of a request, and creates the data pipeline when connecting multiple LLM\nrequests, providing a natural way to program LLM applications. Exposing\nSemantic Variables to the public LLM service allows it to perform conventional\ndata flow analysis to uncover the correlation across multiple LLM requests.\nThis correlation opens a brand-new optimization space for the end-to-end\nperformance of LLM-based applications. Extensive evaluations demonstrate that\nParrot can achieve up to an order-of-magnitude improvement for popular and\npractical use cases of LLM applications."}, "hash": "fb1dfaf4c7eaf046435294b8ff1f98cd9a95f28f5e794255cdfc357c4ed5ba04", "class_name": "RelatedNodeInfo"}}, "text": "Perf. Objective Deduction\nSharing Prompt Prefix App -centric Scheduling\nEfficient GPU Kernels Context ManagementContextual Fill / GenInter -Request Comm.Figure 6: Parrot system overview.\nways fixed, and the few-shot examples could be quasi-static in\nthat the same type of tasks use the same examples. This is why\nmore than 94% of prefix tokens could be repetitively used\nacross LLM requests for various users (Table 1). Such com-\nmonality also exists in multi-agent applications. For example,\nMetaGPT [22] and AutoGen [54] recurrently incorporate con-\nversation history into the prompt over several rounds of LLM\nrequests, leading to 72% and99% redundancy respectively.\nThese redundant sections excessively utilize GPU memory\nbandwidth and are computed for multiple times. Earlier re-\nsults have proposed optimizations in LLM engines to avoid\nredundant GPU memory of shared prompt [25]. However, it is\nhard for public LLM services to swiftly detect and co-locate\nthe prompt-sharing requests, which be dynamically generated,\nfrom tons of diverse requests from diverse applications. With-\nout knowledge about the prompt structure, extensive token-\nby-token matching for every LLM request is expensive at the\ncluster level. Hence, if the cluster scheduler of public LLM\nservice cannot dispatch prompt-sharing requests to the same\nengine, the engine-level redundancy avoidance optimizations\nwould be hard to take effect.\n4 Parrot Design\nFigure 6 depicts the overview of Parrot\u2019s design. Parrot pro-\nvides a natural way of programming LLM applications with\nSemantic Variable annotations (\u00a74.1), which is compatible of\nexisting LLM orchestration frameworks, e.g., LangChain [8].\nCentering on this abstraction, Parrot Manager is designed\nto schedule LLM requests at a cluster-level, by deriving the\napplication-level knowledge (\u00a74.2) and optimizing end-to-end\nperformance of application (\u00a75). The manager will schedule\nthe LLM requests to LLM Engine , which is formed by a GPU\nserver (or a group of servers) in the cluster that can serve LLM\nrequests independently.\n4.1 Semantic Variable\nParrot treats an LLM request as a semantic function1im-\nplemented using natural language and executed by LLMs.\n1The term semantic function is borrowed from Semantic Kernel [36].", "mimetype": "text/plain", "start_char_idx": 3619, "end_char_idx": 5877, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0651355d-edd1-4c2f-8294-ac7261b54d4f": {"__data__": {"id_": "0651355d-edd1-4c2f-8294-ac7261b54d4f", "embedding": null, "metadata": {"page_label": "5", "file_name": "2405_19888v1.pdf", "Title of this paper": "Parrot: Efficient Serving of LLM-based Applications with Semantic Variable", "Authors": "Chaofan Lin, Zhenhua Han, Chengruidong Zhang, Yuqing Yang, Fan Yang, Chen Chen, Lili Qiu", "Date published": "05/30/2024", "URL": "http://arxiv.org/abs/2405.19888v1", "summary": "The rise of large language models (LLMs) has enabled LLM-based applications\n(a.k.a. AI agents or co-pilots), a new software paradigm that combines the\nstrength of LLM and conventional software. Diverse LLM applications from\ndifferent tenants could design complex workflows using multiple LLM requests to\naccomplish one task. However, they have to use the over-simplified\nrequest-level API provided by today's public LLM services, losing essential\napplication-level information. Public LLM services have to blindly optimize\nindividual LLM requests, leading to sub-optimal end-to-end performance of LLM\napplications.\n  This paper introduces Parrot, an LLM service system that focuses on the\nend-to-end experience of LLM-based applications. Parrot proposes Semantic\nVariable, a unified abstraction to expose application-level knowledge to public\nLLM services. A Semantic Variable annotates an input/output variable in the\nprompt of a request, and creates the data pipeline when connecting multiple LLM\nrequests, providing a natural way to program LLM applications. Exposing\nSemantic Variables to the public LLM service allows it to perform conventional\ndata flow analysis to uncover the correlation across multiple LLM requests.\nThis correlation opens a brand-new optimization space for the end-to-end\nperformance of LLM-based applications. Extensive evaluations demonstrate that\nParrot can achieve up to an order-of-magnitude improvement for popular and\npractical use cases of LLM applications."}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6946f5c6-9cfa-4c68-b1ce-2af59fcaebd7", "node_type": "4", "metadata": {"page_label": "5", "file_name": "2405_19888v1.pdf", "Title of this paper": "Parrot: Efficient Serving of LLM-based Applications with Semantic Variable", "Authors": "Chaofan Lin, Zhenhua Han, Chengruidong Zhang, Yuqing Yang, Fan Yang, Chen Chen, Lili Qiu", "Date published": "05/30/2024", "URL": "http://arxiv.org/abs/2405.19888v1", "summary": "The rise of large language models (LLMs) has enabled LLM-based applications\n(a.k.a. AI agents or co-pilots), a new software paradigm that combines the\nstrength of LLM and conventional software. Diverse LLM applications from\ndifferent tenants could design complex workflows using multiple LLM requests to\naccomplish one task. However, they have to use the over-simplified\nrequest-level API provided by today's public LLM services, losing essential\napplication-level information. Public LLM services have to blindly optimize\nindividual LLM requests, leading to sub-optimal end-to-end performance of LLM\napplications.\n  This paper introduces Parrot, an LLM service system that focuses on the\nend-to-end experience of LLM-based applications. Parrot proposes Semantic\nVariable, a unified abstraction to expose application-level knowledge to public\nLLM services. A Semantic Variable annotates an input/output variable in the\nprompt of a request, and creates the data pipeline when connecting multiple LLM\nrequests, providing a natural way to program LLM applications. Exposing\nSemantic Variables to the public LLM service allows it to perform conventional\ndata flow analysis to uncover the correlation across multiple LLM requests.\nThis correlation opens a brand-new optimization space for the end-to-end\nperformance of LLM-based applications. Extensive evaluations demonstrate that\nParrot can achieve up to an order-of-magnitude improvement for popular and\npractical use cases of LLM applications."}, "hash": "73062d890cb47b750b690a0583ed200498c8032f2a366327c9e8e56f28e42f77", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b2681130-7644-43d4-80a1-42a4d02dc1e3", "node_type": "1", "metadata": {}, "hash": "738f912af13c199d95c09f225114cb2c2fb2be604bde7dfbe78e53b2ecf75617", "class_name": "RelatedNodeInfo"}}, "text": "import Parrot asP\nfrom Parrot .PerformanceCriteria import LATENCY\n@P.SemanticFunction\ndef WritePythonCode(task: P.SemanticVariable):\n\"\"\" You are an expert software engineer.\nWrite python code of {{input:task}}.\nCode: {{output:code}}\n\"\"\"\n@P.SemanticFunction\ndef WriteTestCode(\ntask: P.SemanticVariable,\ncode: P.SemanticVariable):\n\"\"\" You are an experienced QA engineer.\nYou write test code for {{input:task}}.\nCode: {{input:code}}.\nYour test code: {{output:test}}\n\"\"\"\ndef WriteSnakeGame():\ntask = P.SemanticVariable(\"a snake game\")\ncode = WritePythonCode(task)\ntest = WriteTestCode(task, code)\nreturn code.get(perf=LATENCY), test.get(perf=LATENCY)\nFigure 7: Example: a multi-agent application in Parrot.\nA Semantic Variable is defined as a input or output vari-\nable of a semantic function, which is referred as a place-\nholder in the prompt. Figure 7 shows a simplified example of\nmulti-agent application like MetaGPT [22]. It contains two\nSemanticFunction s, one for the software engineer to write\ncode and one for the QA engineer to write test code. It has\nthree Semantic Variables: task ,code , and test , for task de-\nscription, the code to be developed by the software engineer,\nand the test code to be developed by the QA engineer, re-\nspectively. Although existing LLM orchestration frameworks\n(e.g., LangChain [8]) also allow placeholders in a prompt,\nhowever, the placeholders are rendered with real data before\nthe submission, hence public LLM services cannot detect such\na structure. Instead, Parrot relies on Semantic Variables to\npreserve the prompt structure for further inter-request analysis\nin public LLM services side.\nIn addition to the semantic functions, LLM application\ndevelopers can further define orchestration functions that con-\nnect multiple semantic functions (e.g., WriteSnakeGame in\nFigure 7). The Semantic Variables connecting multiple se-\nmantic functions form the data pipeline of multiple LLM\nrequests in the public LLM service. A simple data flow\nanalysis of the semantic functions can be done to reveals\nthe connections of multiple LLM requests. E.g., in Figure 7,\nthecode variable connects the two LLM requests originat-\ning from WritePythonCode andWriteTestCode , showing\ntheir sequential dependency. Different from traditional com-\npletion API, Parrot splits a completion request to submit\noperation and get operation (\u00a77). A function calling of\nSemanticFunction will trigger the submit API to submit a\nLLM request with its prompt and input Semantic Variables.\nThe execution of a SemanticFunction is asynchronous\nthus it returns the futures of the output Semantic Variables.\ntask\ncodeWritePythonCode\nWriteTestCode\ntestYou are an expert software engineer.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2694, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b2681130-7644-43d4-80a1-42a4d02dc1e3": {"__data__": {"id_": "b2681130-7644-43d4-80a1-42a4d02dc1e3", "embedding": null, "metadata": {"page_label": "5", "file_name": "2405_19888v1.pdf", "Title of this paper": "Parrot: Efficient Serving of LLM-based Applications with Semantic Variable", "Authors": "Chaofan Lin, Zhenhua Han, Chengruidong Zhang, Yuqing Yang, Fan Yang, Chen Chen, Lili Qiu", "Date published": "05/30/2024", "URL": "http://arxiv.org/abs/2405.19888v1", "summary": "The rise of large language models (LLMs) has enabled LLM-based applications\n(a.k.a. AI agents or co-pilots), a new software paradigm that combines the\nstrength of LLM and conventional software. Diverse LLM applications from\ndifferent tenants could design complex workflows using multiple LLM requests to\naccomplish one task. However, they have to use the over-simplified\nrequest-level API provided by today's public LLM services, losing essential\napplication-level information. Public LLM services have to blindly optimize\nindividual LLM requests, leading to sub-optimal end-to-end performance of LLM\napplications.\n  This paper introduces Parrot, an LLM service system that focuses on the\nend-to-end experience of LLM-based applications. Parrot proposes Semantic\nVariable, a unified abstraction to expose application-level knowledge to public\nLLM services. A Semantic Variable annotates an input/output variable in the\nprompt of a request, and creates the data pipeline when connecting multiple LLM\nrequests, providing a natural way to program LLM applications. Exposing\nSemantic Variables to the public LLM service allows it to perform conventional\ndata flow analysis to uncover the correlation across multiple LLM requests.\nThis correlation opens a brand-new optimization space for the end-to-end\nperformance of LLM-based applications. Extensive evaluations demonstrate that\nParrot can achieve up to an order-of-magnitude improvement for popular and\npractical use cases of LLM applications."}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6946f5c6-9cfa-4c68-b1ce-2af59fcaebd7", "node_type": "4", "metadata": {"page_label": "5", "file_name": "2405_19888v1.pdf", "Title of this paper": "Parrot: Efficient Serving of LLM-based Applications with Semantic Variable", "Authors": "Chaofan Lin, Zhenhua Han, Chengruidong Zhang, Yuqing Yang, Fan Yang, Chen Chen, Lili Qiu", "Date published": "05/30/2024", "URL": "http://arxiv.org/abs/2405.19888v1", "summary": "The rise of large language models (LLMs) has enabled LLM-based applications\n(a.k.a. AI agents or co-pilots), a new software paradigm that combines the\nstrength of LLM and conventional software. Diverse LLM applications from\ndifferent tenants could design complex workflows using multiple LLM requests to\naccomplish one task. However, they have to use the over-simplified\nrequest-level API provided by today's public LLM services, losing essential\napplication-level information. Public LLM services have to blindly optimize\nindividual LLM requests, leading to sub-optimal end-to-end performance of LLM\napplications.\n  This paper introduces Parrot, an LLM service system that focuses on the\nend-to-end experience of LLM-based applications. Parrot proposes Semantic\nVariable, a unified abstraction to expose application-level knowledge to public\nLLM services. A Semantic Variable annotates an input/output variable in the\nprompt of a request, and creates the data pipeline when connecting multiple LLM\nrequests, providing a natural way to program LLM applications. Exposing\nSemantic Variables to the public LLM service allows it to perform conventional\ndata flow analysis to uncover the correlation across multiple LLM requests.\nThis correlation opens a brand-new optimization space for the end-to-end\nperformance of LLM-based applications. Extensive evaluations demonstrate that\nParrot can achieve up to an order-of-magnitude improvement for popular and\npractical use cases of LLM applications."}, "hash": "73062d890cb47b750b690a0583ed200498c8032f2a366327c9e8e56f28e42f77", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0651355d-edd1-4c2f-8294-ac7261b54d4f", "node_type": "1", "metadata": {"page_label": "5", "file_name": "2405_19888v1.pdf", "Title of this paper": "Parrot: Efficient Serving of LLM-based Applications with Semantic Variable", "Authors": "Chaofan Lin, Zhenhua Han, Chengruidong Zhang, Yuqing Yang, Fan Yang, Chen Chen, Lili Qiu", "Date published": "05/30/2024", "URL": "http://arxiv.org/abs/2405.19888v1", "summary": "The rise of large language models (LLMs) has enabled LLM-based applications\n(a.k.a. AI agents or co-pilots), a new software paradigm that combines the\nstrength of LLM and conventional software. Diverse LLM applications from\ndifferent tenants could design complex workflows using multiple LLM requests to\naccomplish one task. However, they have to use the over-simplified\nrequest-level API provided by today's public LLM services, losing essential\napplication-level information. Public LLM services have to blindly optimize\nindividual LLM requests, leading to sub-optimal end-to-end performance of LLM\napplications.\n  This paper introduces Parrot, an LLM service system that focuses on the\nend-to-end experience of LLM-based applications. Parrot proposes Semantic\nVariable, a unified abstraction to expose application-level knowledge to public\nLLM services. A Semantic Variable annotates an input/output variable in the\nprompt of a request, and creates the data pipeline when connecting multiple LLM\nrequests, providing a natural way to program LLM applications. Exposing\nSemantic Variables to the public LLM service allows it to perform conventional\ndata flow analysis to uncover the correlation across multiple LLM requests.\nThis correlation opens a brand-new optimization space for the end-to-end\nperformance of LLM-based applications. Extensive evaluations demonstrate that\nParrot can achieve up to an order-of-magnitude improvement for popular and\npractical use cases of LLM applications."}, "hash": "34d9204c444f905d338334cdc90803a979c02469329d559ed5c54e60ca25ce89", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7b146d9e-6512-4fd1-aa7c-37b24cc6ba70", "node_type": "1", "metadata": {}, "hash": "e7e2d6e7a19e68073f4e9e91dd002a345016f30ae566d6ae6d2268d9b231fd9e", "class_name": "RelatedNodeInfo"}}, "text": "The Semantic Variables connecting multiple se-\nmantic functions form the data pipeline of multiple LLM\nrequests in the public LLM service. A simple data flow\nanalysis of the semantic functions can be done to reveals\nthe connections of multiple LLM requests. E.g., in Figure 7,\nthecode variable connects the two LLM requests originat-\ning from WritePythonCode andWriteTestCode , showing\ntheir sequential dependency. Different from traditional com-\npletion API, Parrot splits a completion request to submit\noperation and get operation (\u00a77). A function calling of\nSemanticFunction will trigger the submit API to submit a\nLLM request with its prompt and input Semantic Variables.\nThe execution of a SemanticFunction is asynchronous\nthus it returns the futures of the output Semantic Variables.\ntask\ncodeWritePythonCode\nWriteTestCode\ntestYou are an expert software engineer. Write python code of\nYou are an expert ...... code of: {{input:task }}. Code:Hash( )\nHash( )\u2460 PrefixHash ()\n\u2463 GetPerfObj () Latency   \u2462 GetConsumers () [Request( )]\u2461 GetProducer ()  Request( ) WritePythonCode\nWriteTestCodeFigure 8: Primitives (selected) for Inter-Request Analysis.\nThrough the get API, applications can fetch the value of\nan output Semantic Variable from the public LLM service\nin an on-demand manner. This asynchronous design allows\nParrot-powered LLM service to receive all LLM requests not\nblocked by native functions and analyze their relationships\njust-in-time.\nThegetoperation supports annotation of performance cri-\nteria, showing the end-to-end performance requirement of\nan application, which can be end-to-end latency or through-\nput (extensible to more criteria like per-token latency when\nstreaming, and time-to-first-token). For example, the final out-\nputs, code andtest in Figure 7, are fetched using getwith\nan objective of end-to-end latency. Criteria of middle vari-\nables will be automatically deduced and propagated from final\noutputs (\u00a75.2). After propagation, each variable is attached to\na criterion, which finally works by serving as a hint to Parrot\u2019s\nscheduler (\u00a75.4).\n4.2 Primitives of Inter-Request Analysis\nIn general, Parrot perform inter-request analysis mainly by\ntwo types of application-level information deduced from Se-\nmantic Variable: DAG of requests and prompt structure. Fig-\nure 8 illustrates the DAG workflow of the example shown in\nFigure 7 and the primitives used for inter-request analysis and\noptimizations.\nDAG-based analysis. As requests, or SemanticFunction s,\nare submitted beforehand, Parrot can receive them all at once\nand analyze their correlations just-in-time on the service side.\nParrot maintains a DAG-like data structure in each user\u2019s\nregistered session. Each node is either a request or a Seman-\ntic Variable that connects different requests.", "mimetype": "text/plain", "start_char_idx": 1825, "end_char_idx": 4615, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7b146d9e-6512-4fd1-aa7c-37b24cc6ba70": {"__data__": {"id_": "7b146d9e-6512-4fd1-aa7c-37b24cc6ba70", "embedding": null, "metadata": {"page_label": "5", "file_name": "2405_19888v1.pdf", "Title of this paper": "Parrot: Efficient Serving of LLM-based Applications with Semantic Variable", "Authors": "Chaofan Lin, Zhenhua Han, Chengruidong Zhang, Yuqing Yang, Fan Yang, Chen Chen, Lili Qiu", "Date published": "05/30/2024", "URL": "http://arxiv.org/abs/2405.19888v1", "summary": "The rise of large language models (LLMs) has enabled LLM-based applications\n(a.k.a. AI agents or co-pilots), a new software paradigm that combines the\nstrength of LLM and conventional software. Diverse LLM applications from\ndifferent tenants could design complex workflows using multiple LLM requests to\naccomplish one task. However, they have to use the over-simplified\nrequest-level API provided by today's public LLM services, losing essential\napplication-level information. Public LLM services have to blindly optimize\nindividual LLM requests, leading to sub-optimal end-to-end performance of LLM\napplications.\n  This paper introduces Parrot, an LLM service system that focuses on the\nend-to-end experience of LLM-based applications. Parrot proposes Semantic\nVariable, a unified abstraction to expose application-level knowledge to public\nLLM services. A Semantic Variable annotates an input/output variable in the\nprompt of a request, and creates the data pipeline when connecting multiple LLM\nrequests, providing a natural way to program LLM applications. Exposing\nSemantic Variables to the public LLM service allows it to perform conventional\ndata flow analysis to uncover the correlation across multiple LLM requests.\nThis correlation opens a brand-new optimization space for the end-to-end\nperformance of LLM-based applications. Extensive evaluations demonstrate that\nParrot can achieve up to an order-of-magnitude improvement for popular and\npractical use cases of LLM applications."}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6946f5c6-9cfa-4c68-b1ce-2af59fcaebd7", "node_type": "4", "metadata": {"page_label": "5", "file_name": "2405_19888v1.pdf", "Title of this paper": "Parrot: Efficient Serving of LLM-based Applications with Semantic Variable", "Authors": "Chaofan Lin, Zhenhua Han, Chengruidong Zhang, Yuqing Yang, Fan Yang, Chen Chen, Lili Qiu", "Date published": "05/30/2024", "URL": "http://arxiv.org/abs/2405.19888v1", "summary": "The rise of large language models (LLMs) has enabled LLM-based applications\n(a.k.a. AI agents or co-pilots), a new software paradigm that combines the\nstrength of LLM and conventional software. Diverse LLM applications from\ndifferent tenants could design complex workflows using multiple LLM requests to\naccomplish one task. However, they have to use the over-simplified\nrequest-level API provided by today's public LLM services, losing essential\napplication-level information. Public LLM services have to blindly optimize\nindividual LLM requests, leading to sub-optimal end-to-end performance of LLM\napplications.\n  This paper introduces Parrot, an LLM service system that focuses on the\nend-to-end experience of LLM-based applications. Parrot proposes Semantic\nVariable, a unified abstraction to expose application-level knowledge to public\nLLM services. A Semantic Variable annotates an input/output variable in the\nprompt of a request, and creates the data pipeline when connecting multiple LLM\nrequests, providing a natural way to program LLM applications. Exposing\nSemantic Variables to the public LLM service allows it to perform conventional\ndata flow analysis to uncover the correlation across multiple LLM requests.\nThis correlation opens a brand-new optimization space for the end-to-end\nperformance of LLM-based applications. Extensive evaluations demonstrate that\nParrot can achieve up to an order-of-magnitude improvement for popular and\npractical use cases of LLM applications."}, "hash": "73062d890cb47b750b690a0583ed200498c8032f2a366327c9e8e56f28e42f77", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b2681130-7644-43d4-80a1-42a4d02dc1e3", "node_type": "1", "metadata": {"page_label": "5", "file_name": "2405_19888v1.pdf", "Title of this paper": "Parrot: Efficient Serving of LLM-based Applications with Semantic Variable", "Authors": "Chaofan Lin, Zhenhua Han, Chengruidong Zhang, Yuqing Yang, Fan Yang, Chen Chen, Lili Qiu", "Date published": "05/30/2024", "URL": "http://arxiv.org/abs/2405.19888v1", "summary": "The rise of large language models (LLMs) has enabled LLM-based applications\n(a.k.a. AI agents or co-pilots), a new software paradigm that combines the\nstrength of LLM and conventional software. Diverse LLM applications from\ndifferent tenants could design complex workflows using multiple LLM requests to\naccomplish one task. However, they have to use the over-simplified\nrequest-level API provided by today's public LLM services, losing essential\napplication-level information. Public LLM services have to blindly optimize\nindividual LLM requests, leading to sub-optimal end-to-end performance of LLM\napplications.\n  This paper introduces Parrot, an LLM service system that focuses on the\nend-to-end experience of LLM-based applications. Parrot proposes Semantic\nVariable, a unified abstraction to expose application-level knowledge to public\nLLM services. A Semantic Variable annotates an input/output variable in the\nprompt of a request, and creates the data pipeline when connecting multiple LLM\nrequests, providing a natural way to program LLM applications. Exposing\nSemantic Variables to the public LLM service allows it to perform conventional\ndata flow analysis to uncover the correlation across multiple LLM requests.\nThis correlation opens a brand-new optimization space for the end-to-end\nperformance of LLM-based applications. Extensive evaluations demonstrate that\nParrot can achieve up to an order-of-magnitude improvement for popular and\npractical use cases of LLM applications."}, "hash": "71e3b26f72463773a0b744e2c380627e32541c505f397f554b03262e284e4301", "class_name": "RelatedNodeInfo"}}, "text": "After propagation, each variable is attached to\na criterion, which finally works by serving as a hint to Parrot\u2019s\nscheduler (\u00a75.4).\n4.2 Primitives of Inter-Request Analysis\nIn general, Parrot perform inter-request analysis mainly by\ntwo types of application-level information deduced from Se-\nmantic Variable: DAG of requests and prompt structure. Fig-\nure 8 illustrates the DAG workflow of the example shown in\nFigure 7 and the primitives used for inter-request analysis and\noptimizations.\nDAG-based analysis. As requests, or SemanticFunction s,\nare submitted beforehand, Parrot can receive them all at once\nand analyze their correlations just-in-time on the service side.\nParrot maintains a DAG-like data structure in each user\u2019s\nregistered session. Each node is either a request or a Seman-\ntic Variable that connects different requests. When a request\ncomes, Parrot inserts it to DAG by linking edges with Seman-\ntic Variables it refers through placeholders in the prompts.\nParrot can perform conventional dataflow analysis [1, 38]\nusing the primitives to get the producer and consumers of Se-\nmantic Variables (i.e., GetProducer andGetConsumers ) to\nrecover dependency of LLM requests. Using the request DAG\nand the annotated performance criteria (via GetPerfObj ) of\nfinal output Semantic Variables, Parrot can deduct the request-\nlevel scheduling preference by analyzing the DAG and the\nperformance objective of final outputs (\u00a75.2).", "mimetype": "text/plain", "start_char_idx": 3775, "end_char_idx": 5215, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "11ff1a7a-2f8f-4af4-af4a-92947c724046": {"__data__": {"id_": "11ff1a7a-2f8f-4af4-af4a-92947c724046", "embedding": null, "metadata": {"page_label": "6", "file_name": "2405_19888v1.pdf", "Title of this paper": "Parrot: Efficient Serving of LLM-based Applications with Semantic Variable", "Authors": "Chaofan Lin, Zhenhua Han, Chengruidong Zhang, Yuqing Yang, Fan Yang, Chen Chen, Lili Qiu", "Date published": "05/30/2024", "URL": "http://arxiv.org/abs/2405.19888v1", "summary": "The rise of large language models (LLMs) has enabled LLM-based applications\n(a.k.a. AI agents or co-pilots), a new software paradigm that combines the\nstrength of LLM and conventional software. Diverse LLM applications from\ndifferent tenants could design complex workflows using multiple LLM requests to\naccomplish one task. However, they have to use the over-simplified\nrequest-level API provided by today's public LLM services, losing essential\napplication-level information. Public LLM services have to blindly optimize\nindividual LLM requests, leading to sub-optimal end-to-end performance of LLM\napplications.\n  This paper introduces Parrot, an LLM service system that focuses on the\nend-to-end experience of LLM-based applications. Parrot proposes Semantic\nVariable, a unified abstraction to expose application-level knowledge to public\nLLM services. A Semantic Variable annotates an input/output variable in the\nprompt of a request, and creates the data pipeline when connecting multiple LLM\nrequests, providing a natural way to program LLM applications. Exposing\nSemantic Variables to the public LLM service allows it to perform conventional\ndata flow analysis to uncover the correlation across multiple LLM requests.\nThis correlation opens a brand-new optimization space for the end-to-end\nperformance of LLM-based applications. Extensive evaluations demonstrate that\nParrot can achieve up to an order-of-magnitude improvement for popular and\npractical use cases of LLM applications."}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e7f2314e-8bed-47c9-b43a-309408d62cad", "node_type": "4", "metadata": {"page_label": "6", "file_name": "2405_19888v1.pdf", "Title of this paper": "Parrot: Efficient Serving of LLM-based Applications with Semantic Variable", "Authors": "Chaofan Lin, Zhenhua Han, Chengruidong Zhang, Yuqing Yang, Fan Yang, Chen Chen, Lili Qiu", "Date published": "05/30/2024", "URL": "http://arxiv.org/abs/2405.19888v1", "summary": "The rise of large language models (LLMs) has enabled LLM-based applications\n(a.k.a. AI agents or co-pilots), a new software paradigm that combines the\nstrength of LLM and conventional software. Diverse LLM applications from\ndifferent tenants could design complex workflows using multiple LLM requests to\naccomplish one task. However, they have to use the over-simplified\nrequest-level API provided by today's public LLM services, losing essential\napplication-level information. Public LLM services have to blindly optimize\nindividual LLM requests, leading to sub-optimal end-to-end performance of LLM\napplications.\n  This paper introduces Parrot, an LLM service system that focuses on the\nend-to-end experience of LLM-based applications. Parrot proposes Semantic\nVariable, a unified abstraction to expose application-level knowledge to public\nLLM services. A Semantic Variable annotates an input/output variable in the\nprompt of a request, and creates the data pipeline when connecting multiple LLM\nrequests, providing a natural way to program LLM applications. Exposing\nSemantic Variables to the public LLM service allows it to perform conventional\ndata flow analysis to uncover the correlation across multiple LLM requests.\nThis correlation opens a brand-new optimization space for the end-to-end\nperformance of LLM-based applications. Extensive evaluations demonstrate that\nParrot can achieve up to an order-of-magnitude improvement for popular and\npractical use cases of LLM applications."}, "hash": "c189513be15b1ba9ed16227186a73cacf18071b18a6ae05fe57d84144e4f92cc", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a22fdf36-4f4d-4d92-91d1-80b6a4f818d6", "node_type": "1", "metadata": {}, "hash": "15691546f9eb2587c09b787c3da4796ae9c4db9cf8df14d369b991ce4a69388e", "class_name": "RelatedNodeInfo"}}, "text": "Prompt structure-based analysis. Based on the prompt\nstructure declared by Semantic Variables, Parrot supports ex-\ntracting the hash values of an LLM request at multiple po-\nsitions split by Semantic Variables (i.e., PrefixHash ). For\nexample, the prompt of WritePythonCode has two potential\nsharing prefix: the text before {{input:task}} and the text\nbefore {{output:code}} , thus there will be two prefix hash\nvalues generated. The prefix hashes of LLM requests will\nbe used by swift detection of commonality across multiple\nrequests, supporting both static and dynamically generated\ncontents, as well as within the same type of application or\neven across applications (\u00a75.3).\n5 Optimizations with Semantic Variable\n5.1 Serving Dependent Requests\nTo avoid the unnecessary client-side execution, it requires\nthe dependency of requests at the application level, which\nis lost in today\u2019s public LLM services. With the DAG and\nprimitives illustrated in \u00a74.2, Parrot serves dependent requests\nefficiently through a graph-based executor. The executor polls\nconstantly and sends it to corresponding engine once ready\n(i.e. producer requests are all finished), which allows instant\nexecution and maximizes batching opportunities. For con-\nsecutive execution of dependent requests, materialized value\nis transmitted through a message queue allocated for cor-\nresponding Semantic Variable, avoiding unnecessary chatty\ncommunication between clients and LLM services.\nThe value of a Semantic Variable in a request may require\ntransformation before being exchanged, e.g., the value of a\nSemantic Variable is extracted from the JSON-formatted out-\nput of an LLM request, which is then fed into consecutive\nLLM requests. Similar to existing message queue systems\nthat support message transformation (e.g., Kafka [5]), Parrot\nalso supports string transformation to manipulate Semantic\nVariables during value exchanging among LLM requests. Par-\nrot supports most output parsing methods of LangChain [8],\nwhich covers most use cases of LLM applications.\n5.2 Performance Objective Deduction\nTo optimize the end-to-end performance of applications, we\nneed to know the application-level performance criteria. To\nhelp deriving the request-level scheduling preference from the\nend-to-end application\u2019s performance requirement, we need\nto understand the workflow of the LLM application, which is\nthe DAG of LLM requests derived by Parrot\u2019s primitives.\nWhen an application annotates a Semantic Variable to pre-\nfer higher throughput, all requests generating this Seman-\ntic Variable (both directly or indirectly) will be marked as\nthroughput-preferred when scheduling. This scheduling pref-\nerence is usually beneficial for offline data processing, such\nas bulk document analysis.\n1\n3 54 6\n7x.get (perf =LATENCY )Task \nGroup 0Task \nGroup 1\n2 y.get (perf =LATENCY )Figure 9: Performance deduction for an LLM-based applica-\ntion generating two latency-sensitive Semantic Variable.\nHandling latency-sensitive applications is more intricate.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3014, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a22fdf36-4f4d-4d92-91d1-80b6a4f818d6": {"__data__": {"id_": "a22fdf36-4f4d-4d92-91d1-80b6a4f818d6", "embedding": null, "metadata": {"page_label": "6", "file_name": "2405_19888v1.pdf", "Title of this paper": "Parrot: Efficient Serving of LLM-based Applications with Semantic Variable", "Authors": "Chaofan Lin, Zhenhua Han, Chengruidong Zhang, Yuqing Yang, Fan Yang, Chen Chen, Lili Qiu", "Date published": "05/30/2024", "URL": "http://arxiv.org/abs/2405.19888v1", "summary": "The rise of large language models (LLMs) has enabled LLM-based applications\n(a.k.a. AI agents or co-pilots), a new software paradigm that combines the\nstrength of LLM and conventional software. Diverse LLM applications from\ndifferent tenants could design complex workflows using multiple LLM requests to\naccomplish one task. However, they have to use the over-simplified\nrequest-level API provided by today's public LLM services, losing essential\napplication-level information. Public LLM services have to blindly optimize\nindividual LLM requests, leading to sub-optimal end-to-end performance of LLM\napplications.\n  This paper introduces Parrot, an LLM service system that focuses on the\nend-to-end experience of LLM-based applications. Parrot proposes Semantic\nVariable, a unified abstraction to expose application-level knowledge to public\nLLM services. A Semantic Variable annotates an input/output variable in the\nprompt of a request, and creates the data pipeline when connecting multiple LLM\nrequests, providing a natural way to program LLM applications. Exposing\nSemantic Variables to the public LLM service allows it to perform conventional\ndata flow analysis to uncover the correlation across multiple LLM requests.\nThis correlation opens a brand-new optimization space for the end-to-end\nperformance of LLM-based applications. Extensive evaluations demonstrate that\nParrot can achieve up to an order-of-magnitude improvement for popular and\npractical use cases of LLM applications."}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e7f2314e-8bed-47c9-b43a-309408d62cad", "node_type": "4", "metadata": {"page_label": "6", "file_name": "2405_19888v1.pdf", "Title of this paper": "Parrot: Efficient Serving of LLM-based Applications with Semantic Variable", "Authors": "Chaofan Lin, Zhenhua Han, Chengruidong Zhang, Yuqing Yang, Fan Yang, Chen Chen, Lili Qiu", "Date published": "05/30/2024", "URL": "http://arxiv.org/abs/2405.19888v1", "summary": "The rise of large language models (LLMs) has enabled LLM-based applications\n(a.k.a. AI agents or co-pilots), a new software paradigm that combines the\nstrength of LLM and conventional software. Diverse LLM applications from\ndifferent tenants could design complex workflows using multiple LLM requests to\naccomplish one task. However, they have to use the over-simplified\nrequest-level API provided by today's public LLM services, losing essential\napplication-level information. Public LLM services have to blindly optimize\nindividual LLM requests, leading to sub-optimal end-to-end performance of LLM\napplications.\n  This paper introduces Parrot, an LLM service system that focuses on the\nend-to-end experience of LLM-based applications. Parrot proposes Semantic\nVariable, a unified abstraction to expose application-level knowledge to public\nLLM services. A Semantic Variable annotates an input/output variable in the\nprompt of a request, and creates the data pipeline when connecting multiple LLM\nrequests, providing a natural way to program LLM applications. Exposing\nSemantic Variables to the public LLM service allows it to perform conventional\ndata flow analysis to uncover the correlation across multiple LLM requests.\nThis correlation opens a brand-new optimization space for the end-to-end\nperformance of LLM-based applications. Extensive evaluations demonstrate that\nParrot can achieve up to an order-of-magnitude improvement for popular and\npractical use cases of LLM applications."}, "hash": "c189513be15b1ba9ed16227186a73cacf18071b18a6ae05fe57d84144e4f92cc", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "11ff1a7a-2f8f-4af4-af4a-92947c724046", "node_type": "1", "metadata": {"page_label": "6", "file_name": "2405_19888v1.pdf", "Title of this paper": "Parrot: Efficient Serving of LLM-based Applications with Semantic Variable", "Authors": "Chaofan Lin, Zhenhua Han, Chengruidong Zhang, Yuqing Yang, Fan Yang, Chen Chen, Lili Qiu", "Date published": "05/30/2024", "URL": "http://arxiv.org/abs/2405.19888v1", "summary": "The rise of large language models (LLMs) has enabled LLM-based applications\n(a.k.a. AI agents or co-pilots), a new software paradigm that combines the\nstrength of LLM and conventional software. Diverse LLM applications from\ndifferent tenants could design complex workflows using multiple LLM requests to\naccomplish one task. However, they have to use the over-simplified\nrequest-level API provided by today's public LLM services, losing essential\napplication-level information. Public LLM services have to blindly optimize\nindividual LLM requests, leading to sub-optimal end-to-end performance of LLM\napplications.\n  This paper introduces Parrot, an LLM service system that focuses on the\nend-to-end experience of LLM-based applications. Parrot proposes Semantic\nVariable, a unified abstraction to expose application-level knowledge to public\nLLM services. A Semantic Variable annotates an input/output variable in the\nprompt of a request, and creates the data pipeline when connecting multiple LLM\nrequests, providing a natural way to program LLM applications. Exposing\nSemantic Variables to the public LLM service allows it to perform conventional\ndata flow analysis to uncover the correlation across multiple LLM requests.\nThis correlation opens a brand-new optimization space for the end-to-end\nperformance of LLM-based applications. Extensive evaluations demonstrate that\nParrot can achieve up to an order-of-magnitude improvement for popular and\npractical use cases of LLM applications."}, "hash": "b4d07648d1bc3ab15f2b2790e6957a892835c3710d5ee2d15ebf3bdc9d5c680f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "cd0287d5-964d-4ed6-bdc5-0006d217a25f", "node_type": "1", "metadata": {}, "hash": "4a30559df614af24773a9f354c651af969eeab3f5de68aaef2294bf3091da110", "class_name": "RelatedNodeInfo"}}, "text": "To\nhelp deriving the request-level scheduling preference from the\nend-to-end application\u2019s performance requirement, we need\nto understand the workflow of the LLM application, which is\nthe DAG of LLM requests derived by Parrot\u2019s primitives.\nWhen an application annotates a Semantic Variable to pre-\nfer higher throughput, all requests generating this Seman-\ntic Variable (both directly or indirectly) will be marked as\nthroughput-preferred when scheduling. This scheduling pref-\nerence is usually beneficial for offline data processing, such\nas bulk document analysis.\n1\n3 54 6\n7x.get (perf =LATENCY )Task \nGroup 0Task \nGroup 1\n2 y.get (perf =LATENCY )Figure 9: Performance deduction for an LLM-based applica-\ntion generating two latency-sensitive Semantic Variable.\nHandling latency-sensitive applications is more intricate.\nAs demonstrated in Figure 4, achieving low end-to-end la-\ntency may sometimes require prioritizing throughput at the\nMapping stage. The latency of individual requests can sacri-\nficed so as to reduce the completion time of the entire DAG of\nrequests. Parrot analyzes LLM requests in reverse topological\norder, beginning with those linked to latency-critical Semantic\nVariable, as depicted in Figure 9. With the extracted DAG,\nLLM requests that directly result in latency-critical Seman-\ntic Variables are labeled as latency-sensitive (Request 1 and\n2), as are their immediate predecessors (Request 3). Parallel\nLLM requests at the same stage are grouped into a task group\n(Task Groups 0 and 1). The scheduler should minimize the\nlatency of the entire task group, often leading to a higher batch\ncapacity for higher throughput of token generation.\n5.3 Sharing Prompt Prefix\nWhen an LLM request is scheduled to an LLM engine, a con-\ntext on the engine is created to store the state of the model\nexecution for this request (mainly KV cache). Existing works\nhave proposed to share the KV cache of common prefix of\nprompts in LLM engines to save the GPU memory. However,\nas we have explained in \u00a73, today\u2019s public LLM service face\ndiverse applications and requests, which is hard to identify\nthe commonality at the cluster level. Token-by-token compar-\nison is impractical due to high time complexity, especially for\nvery long context with massive requests. In Parrot, by expos-\ning Semantic Variables to LLM service, we can understand\nthe prompt structure to automatically detect the commonality\nmore efficiently at the granularity of Semantic Variables.\nUsing Parrot\u2019s primitive of PrefixHash , Parrot only needs\nto check the hash value at positions after each Semantic Vari-\nable in a request\u2019s prompt.", "mimetype": "text/plain", "start_char_idx": 2190, "end_char_idx": 4815, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "cd0287d5-964d-4ed6-bdc5-0006d217a25f": {"__data__": {"id_": "cd0287d5-964d-4ed6-bdc5-0006d217a25f", "embedding": null, "metadata": {"page_label": "6", "file_name": "2405_19888v1.pdf", "Title of this paper": "Parrot: Efficient Serving of LLM-based Applications with Semantic Variable", "Authors": "Chaofan Lin, Zhenhua Han, Chengruidong Zhang, Yuqing Yang, Fan Yang, Chen Chen, Lili Qiu", "Date published": "05/30/2024", "URL": "http://arxiv.org/abs/2405.19888v1", "summary": "The rise of large language models (LLMs) has enabled LLM-based applications\n(a.k.a. AI agents or co-pilots), a new software paradigm that combines the\nstrength of LLM and conventional software. Diverse LLM applications from\ndifferent tenants could design complex workflows using multiple LLM requests to\naccomplish one task. However, they have to use the over-simplified\nrequest-level API provided by today's public LLM services, losing essential\napplication-level information. Public LLM services have to blindly optimize\nindividual LLM requests, leading to sub-optimal end-to-end performance of LLM\napplications.\n  This paper introduces Parrot, an LLM service system that focuses on the\nend-to-end experience of LLM-based applications. Parrot proposes Semantic\nVariable, a unified abstraction to expose application-level knowledge to public\nLLM services. A Semantic Variable annotates an input/output variable in the\nprompt of a request, and creates the data pipeline when connecting multiple LLM\nrequests, providing a natural way to program LLM applications. Exposing\nSemantic Variables to the public LLM service allows it to perform conventional\ndata flow analysis to uncover the correlation across multiple LLM requests.\nThis correlation opens a brand-new optimization space for the end-to-end\nperformance of LLM-based applications. Extensive evaluations demonstrate that\nParrot can achieve up to an order-of-magnitude improvement for popular and\npractical use cases of LLM applications."}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e7f2314e-8bed-47c9-b43a-309408d62cad", "node_type": "4", "metadata": {"page_label": "6", "file_name": "2405_19888v1.pdf", "Title of this paper": "Parrot: Efficient Serving of LLM-based Applications with Semantic Variable", "Authors": "Chaofan Lin, Zhenhua Han, Chengruidong Zhang, Yuqing Yang, Fan Yang, Chen Chen, Lili Qiu", "Date published": "05/30/2024", "URL": "http://arxiv.org/abs/2405.19888v1", "summary": "The rise of large language models (LLMs) has enabled LLM-based applications\n(a.k.a. AI agents or co-pilots), a new software paradigm that combines the\nstrength of LLM and conventional software. Diverse LLM applications from\ndifferent tenants could design complex workflows using multiple LLM requests to\naccomplish one task. However, they have to use the over-simplified\nrequest-level API provided by today's public LLM services, losing essential\napplication-level information. Public LLM services have to blindly optimize\nindividual LLM requests, leading to sub-optimal end-to-end performance of LLM\napplications.\n  This paper introduces Parrot, an LLM service system that focuses on the\nend-to-end experience of LLM-based applications. Parrot proposes Semantic\nVariable, a unified abstraction to expose application-level knowledge to public\nLLM services. A Semantic Variable annotates an input/output variable in the\nprompt of a request, and creates the data pipeline when connecting multiple LLM\nrequests, providing a natural way to program LLM applications. Exposing\nSemantic Variables to the public LLM service allows it to perform conventional\ndata flow analysis to uncover the correlation across multiple LLM requests.\nThis correlation opens a brand-new optimization space for the end-to-end\nperformance of LLM-based applications. Extensive evaluations demonstrate that\nParrot can achieve up to an order-of-magnitude improvement for popular and\npractical use cases of LLM applications."}, "hash": "c189513be15b1ba9ed16227186a73cacf18071b18a6ae05fe57d84144e4f92cc", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a22fdf36-4f4d-4d92-91d1-80b6a4f818d6", "node_type": "1", "metadata": {"page_label": "6", "file_name": "2405_19888v1.pdf", "Title of this paper": "Parrot: Efficient Serving of LLM-based Applications with Semantic Variable", "Authors": "Chaofan Lin, Zhenhua Han, Chengruidong Zhang, Yuqing Yang, Fan Yang, Chen Chen, Lili Qiu", "Date published": "05/30/2024", "URL": "http://arxiv.org/abs/2405.19888v1", "summary": "The rise of large language models (LLMs) has enabled LLM-based applications\n(a.k.a. AI agents or co-pilots), a new software paradigm that combines the\nstrength of LLM and conventional software. Diverse LLM applications from\ndifferent tenants could design complex workflows using multiple LLM requests to\naccomplish one task. However, they have to use the over-simplified\nrequest-level API provided by today's public LLM services, losing essential\napplication-level information. Public LLM services have to blindly optimize\nindividual LLM requests, leading to sub-optimal end-to-end performance of LLM\napplications.\n  This paper introduces Parrot, an LLM service system that focuses on the\nend-to-end experience of LLM-based applications. Parrot proposes Semantic\nVariable, a unified abstraction to expose application-level knowledge to public\nLLM services. A Semantic Variable annotates an input/output variable in the\nprompt of a request, and creates the data pipeline when connecting multiple LLM\nrequests, providing a natural way to program LLM applications. Exposing\nSemantic Variables to the public LLM service allows it to perform conventional\ndata flow analysis to uncover the correlation across multiple LLM requests.\nThis correlation opens a brand-new optimization space for the end-to-end\nperformance of LLM-based applications. Extensive evaluations demonstrate that\nParrot can achieve up to an order-of-magnitude improvement for popular and\npractical use cases of LLM applications."}, "hash": "c2303b09e459e38e891b0e067804366a6e661330fd76921347a8c3d6dbd74c79", "class_name": "RelatedNodeInfo"}}, "text": "Existing works\nhave proposed to share the KV cache of common prefix of\nprompts in LLM engines to save the GPU memory. However,\nas we have explained in \u00a73, today\u2019s public LLM service face\ndiverse applications and requests, which is hard to identify\nthe commonality at the cluster level. Token-by-token compar-\nison is impractical due to high time complexity, especially for\nvery long context with massive requests. In Parrot, by expos-\ning Semantic Variables to LLM service, we can understand\nthe prompt structure to automatically detect the commonality\nmore efficiently at the granularity of Semantic Variables.\nUsing Parrot\u2019s primitive of PrefixHash , Parrot only needs\nto check the hash value at positions after each Semantic Vari-\nable in a request\u2019s prompt. Parrot maintains a key-value store,\nwhere each entry maps a (hashed) prefix of tokens to a list of\nrequests, thus the scheduler can quickly check the opportunity\nin an online manner, supporting both static and dynamically-\ngenerated prompt within one application or even across dif-\nferent applications.\nFurthermore, we propose better GPU kernel for the atten-\ntion computation of the requests with a common prefix. We\nfirst leverage vLLM\u2019s paged memory management [25] to\nsave the redundent GPU memory. But vLLM\u2019s kernel still\nsuffers from redundant computation and memory loading\nof the shared tokens. Therefore, we design a new Attention\ndecoding algorithm by combining FlashAttenation [12] and\nPagedAttention [25] that treat the shared and non-shared to-", "mimetype": "text/plain", "start_char_idx": 4054, "end_char_idx": 5574, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "05d8b490-dad4-4b6a-add5-486b4533d430": {"__data__": {"id_": "05d8b490-dad4-4b6a-add5-486b4533d430", "embedding": null, "metadata": {"page_label": "7", "file_name": "2405_19888v1.pdf", "Title of this paper": "Parrot: Efficient Serving of LLM-based Applications with Semantic Variable", "Authors": "Chaofan Lin, Zhenhua Han, Chengruidong Zhang, Yuqing Yang, Fan Yang, Chen Chen, Lili Qiu", "Date published": "05/30/2024", "URL": "http://arxiv.org/abs/2405.19888v1", "summary": "The rise of large language models (LLMs) has enabled LLM-based applications\n(a.k.a. AI agents or co-pilots), a new software paradigm that combines the\nstrength of LLM and conventional software. Diverse LLM applications from\ndifferent tenants could design complex workflows using multiple LLM requests to\naccomplish one task. However, they have to use the over-simplified\nrequest-level API provided by today's public LLM services, losing essential\napplication-level information. Public LLM services have to blindly optimize\nindividual LLM requests, leading to sub-optimal end-to-end performance of LLM\napplications.\n  This paper introduces Parrot, an LLM service system that focuses on the\nend-to-end experience of LLM-based applications. Parrot proposes Semantic\nVariable, a unified abstraction to expose application-level knowledge to public\nLLM services. A Semantic Variable annotates an input/output variable in the\nprompt of a request, and creates the data pipeline when connecting multiple LLM\nrequests, providing a natural way to program LLM applications. Exposing\nSemantic Variables to the public LLM service allows it to perform conventional\ndata flow analysis to uncover the correlation across multiple LLM requests.\nThis correlation opens a brand-new optimization space for the end-to-end\nperformance of LLM-based applications. Extensive evaluations demonstrate that\nParrot can achieve up to an order-of-magnitude improvement for popular and\npractical use cases of LLM applications."}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ffd77891-69dc-49ff-9527-db9920cd7dbb", "node_type": "4", "metadata": {"page_label": "7", "file_name": "2405_19888v1.pdf", "Title of this paper": "Parrot: Efficient Serving of LLM-based Applications with Semantic Variable", "Authors": "Chaofan Lin, Zhenhua Han, Chengruidong Zhang, Yuqing Yang, Fan Yang, Chen Chen, Lili Qiu", "Date published": "05/30/2024", "URL": "http://arxiv.org/abs/2405.19888v1", "summary": "The rise of large language models (LLMs) has enabled LLM-based applications\n(a.k.a. AI agents or co-pilots), a new software paradigm that combines the\nstrength of LLM and conventional software. Diverse LLM applications from\ndifferent tenants could design complex workflows using multiple LLM requests to\naccomplish one task. However, they have to use the over-simplified\nrequest-level API provided by today's public LLM services, losing essential\napplication-level information. Public LLM services have to blindly optimize\nindividual LLM requests, leading to sub-optimal end-to-end performance of LLM\napplications.\n  This paper introduces Parrot, an LLM service system that focuses on the\nend-to-end experience of LLM-based applications. Parrot proposes Semantic\nVariable, a unified abstraction to expose application-level knowledge to public\nLLM services. A Semantic Variable annotates an input/output variable in the\nprompt of a request, and creates the data pipeline when connecting multiple LLM\nrequests, providing a natural way to program LLM applications. Exposing\nSemantic Variables to the public LLM service allows it to perform conventional\ndata flow analysis to uncover the correlation across multiple LLM requests.\nThis correlation opens a brand-new optimization space for the end-to-end\nperformance of LLM-based applications. Extensive evaluations demonstrate that\nParrot can achieve up to an order-of-magnitude improvement for popular and\npractical use cases of LLM applications."}, "hash": "7e2ee1a5256f9a073da30151c53d8bc82327ad1394eb3ed672fd10af43c6ff28", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "23e26e03-e0fd-4f8c-b75c-feecc2600d94", "node_type": "1", "metadata": {}, "hash": "e4d72a3326acd4a6ebbcc63bf71eb3867e3b7e2bddee5ebb6360a3ce91c40980", "class_name": "RelatedNodeInfo"}}, "text": "Algorithm 1: Parrot\u2019s Request Scheduling.\nData: Q: the request queue\n1Q.sort() ; /* Topological order */\n2forr\u2208Qdo\n3 SharedReqsInQueue, CtxInEngine =\nFindSharedPrefix( r);\n4 ifr.TaskGroup \u0338=\u2205then\n5 r\u2217= FindEngine(r.TaskGroup);\n6 else if SharedReqsInQueue \u0338=\u2205then\n7 r\u2217= FindEngine(SharedReqsInQueue);\n8 else if CtxInEngine \u0338=\u2205then\n9 r\u2217= FindEngine(r, filter=CtxInEngine);\n10 ifr\u2217=\u2205then\n11 r\u2217= FindEngine( r);\n12 Q.remove( r\u2217);\nken separately. This significantly accelerates the attention of\nshared contexts (implementation details in \u00a77).\n5.4 Application-Centric Scheduling\nTo fix the problem of existing public LLM service that blindly\noptimize diverse individual requests, Parrot\u2019s scheduling pol-\nicy leverages the application-level knowledge to optimize the\nend-to-end performance. Specifically, the primary goal of Par-\nrot\u2019s scheduler is to meet the varied performance goals of\nLLM applications while optimizing GPU cluster utilization.\nAs explained in \u00a73, a conflict arises when combining through-\nput and latency oriented requests: large batch sizes increase\nthroughput and GPU efficiency but degrade latency, and vice\nversa. Transformer-based LLM inference is largely memory-\nbound, with latency influenced by the count of concurrent\ntokens within the engine. To meet performance targets of\nLLM applications, particularly latency, an LLM engine must\nregulate the token count below a specified threshold, which\nis determined by the LLM request with the most strict la-\ntency constraint. Therefore, Parrot\u2019s scheduling principles are\ntwofold: (1) group LLM requests with similar performance\nrequirements to circumvent the conflict, and (2) maximize\nopportunities for sharing across requests.\nAlgorithm 1 outlines the scheduling process of Parrot. With\nthe extracted DAG, the system arranges the LLM requests\naccording to their topological order (line 1). Parrot tends to\nschedule requests belonging to the same application together\nto avoid the slowing down of interleaved scheduling (\u00a78.2).\nFor requests identified as part of a task group through Parrot\u2019s\nperformance objective deduction, the scheduler attempts to\nallocate the entire task group together (line 4-line 5). Addi-\ntionally, if Parrot detects other queued requests or running\ncontexts with a common prefix, it tries to assign them to\nthe same LLM engine (line 3, line 6-line 9), to utilize Par-\nrot\u2019s context fork to reduce the redundant computation andGPU memory transactions. For an LLM request without the\nabove opportunity, Parrot schedules the request independently\n(line 10-line 11).", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2559, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "23e26e03-e0fd-4f8c-b75c-feecc2600d94": {"__data__": {"id_": "23e26e03-e0fd-4f8c-b75c-feecc2600d94", "embedding": null, "metadata": {"page_label": "7", "file_name": "2405_19888v1.pdf", "Title of this paper": "Parrot: Efficient Serving of LLM-based Applications with Semantic Variable", "Authors": "Chaofan Lin, Zhenhua Han, Chengruidong Zhang, Yuqing Yang, Fan Yang, Chen Chen, Lili Qiu", "Date published": "05/30/2024", "URL": "http://arxiv.org/abs/2405.19888v1", "summary": "The rise of large language models (LLMs) has enabled LLM-based applications\n(a.k.a. AI agents or co-pilots), a new software paradigm that combines the\nstrength of LLM and conventional software. Diverse LLM applications from\ndifferent tenants could design complex workflows using multiple LLM requests to\naccomplish one task. However, they have to use the over-simplified\nrequest-level API provided by today's public LLM services, losing essential\napplication-level information. Public LLM services have to blindly optimize\nindividual LLM requests, leading to sub-optimal end-to-end performance of LLM\napplications.\n  This paper introduces Parrot, an LLM service system that focuses on the\nend-to-end experience of LLM-based applications. Parrot proposes Semantic\nVariable, a unified abstraction to expose application-level knowledge to public\nLLM services. A Semantic Variable annotates an input/output variable in the\nprompt of a request, and creates the data pipeline when connecting multiple LLM\nrequests, providing a natural way to program LLM applications. Exposing\nSemantic Variables to the public LLM service allows it to perform conventional\ndata flow analysis to uncover the correlation across multiple LLM requests.\nThis correlation opens a brand-new optimization space for the end-to-end\nperformance of LLM-based applications. Extensive evaluations demonstrate that\nParrot can achieve up to an order-of-magnitude improvement for popular and\npractical use cases of LLM applications."}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ffd77891-69dc-49ff-9527-db9920cd7dbb", "node_type": "4", "metadata": {"page_label": "7", "file_name": "2405_19888v1.pdf", "Title of this paper": "Parrot: Efficient Serving of LLM-based Applications with Semantic Variable", "Authors": "Chaofan Lin, Zhenhua Han, Chengruidong Zhang, Yuqing Yang, Fan Yang, Chen Chen, Lili Qiu", "Date published": "05/30/2024", "URL": "http://arxiv.org/abs/2405.19888v1", "summary": "The rise of large language models (LLMs) has enabled LLM-based applications\n(a.k.a. AI agents or co-pilots), a new software paradigm that combines the\nstrength of LLM and conventional software. Diverse LLM applications from\ndifferent tenants could design complex workflows using multiple LLM requests to\naccomplish one task. However, they have to use the over-simplified\nrequest-level API provided by today's public LLM services, losing essential\napplication-level information. Public LLM services have to blindly optimize\nindividual LLM requests, leading to sub-optimal end-to-end performance of LLM\napplications.\n  This paper introduces Parrot, an LLM service system that focuses on the\nend-to-end experience of LLM-based applications. Parrot proposes Semantic\nVariable, a unified abstraction to expose application-level knowledge to public\nLLM services. A Semantic Variable annotates an input/output variable in the\nprompt of a request, and creates the data pipeline when connecting multiple LLM\nrequests, providing a natural way to program LLM applications. Exposing\nSemantic Variables to the public LLM service allows it to perform conventional\ndata flow analysis to uncover the correlation across multiple LLM requests.\nThis correlation opens a brand-new optimization space for the end-to-end\nperformance of LLM-based applications. Extensive evaluations demonstrate that\nParrot can achieve up to an order-of-magnitude improvement for popular and\npractical use cases of LLM applications."}, "hash": "7e2ee1a5256f9a073da30151c53d8bc82327ad1394eb3ed672fd10af43c6ff28", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "05d8b490-dad4-4b6a-add5-486b4533d430", "node_type": "1", "metadata": {"page_label": "7", "file_name": "2405_19888v1.pdf", "Title of this paper": "Parrot: Efficient Serving of LLM-based Applications with Semantic Variable", "Authors": "Chaofan Lin, Zhenhua Han, Chengruidong Zhang, Yuqing Yang, Fan Yang, Chen Chen, Lili Qiu", "Date published": "05/30/2024", "URL": "http://arxiv.org/abs/2405.19888v1", "summary": "The rise of large language models (LLMs) has enabled LLM-based applications\n(a.k.a. AI agents or co-pilots), a new software paradigm that combines the\nstrength of LLM and conventional software. Diverse LLM applications from\ndifferent tenants could design complex workflows using multiple LLM requests to\naccomplish one task. However, they have to use the over-simplified\nrequest-level API provided by today's public LLM services, losing essential\napplication-level information. Public LLM services have to blindly optimize\nindividual LLM requests, leading to sub-optimal end-to-end performance of LLM\napplications.\n  This paper introduces Parrot, an LLM service system that focuses on the\nend-to-end experience of LLM-based applications. Parrot proposes Semantic\nVariable, a unified abstraction to expose application-level knowledge to public\nLLM services. A Semantic Variable annotates an input/output variable in the\nprompt of a request, and creates the data pipeline when connecting multiple LLM\nrequests, providing a natural way to program LLM applications. Exposing\nSemantic Variables to the public LLM service allows it to perform conventional\ndata flow analysis to uncover the correlation across multiple LLM requests.\nThis correlation opens a brand-new optimization space for the end-to-end\nperformance of LLM-based applications. Extensive evaluations demonstrate that\nParrot can achieve up to an order-of-magnitude improvement for popular and\npractical use cases of LLM applications."}, "hash": "bf510e02b91575770c530d2d3fc942725ce8ad84b254766aedb147115a64ab2b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b0d5731c-5c33-438e-84f0-c2ca69792b65", "node_type": "1", "metadata": {}, "hash": "887f407da8aba949c7d01ad860f0200fa902bdd7b021bc6a42ee4cfc277505a7", "class_name": "RelatedNodeInfo"}}, "text": "Algorithm 1 outlines the scheduling process of Parrot. With\nthe extracted DAG, the system arranges the LLM requests\naccording to their topological order (line 1). Parrot tends to\nschedule requests belonging to the same application together\nto avoid the slowing down of interleaved scheduling (\u00a78.2).\nFor requests identified as part of a task group through Parrot\u2019s\nperformance objective deduction, the scheduler attempts to\nallocate the entire task group together (line 4-line 5). Addi-\ntionally, if Parrot detects other queued requests or running\ncontexts with a common prefix, it tries to assign them to\nthe same LLM engine (line 3, line 6-line 9), to utilize Par-\nrot\u2019s context fork to reduce the redundant computation andGPU memory transactions. For an LLM request without the\nabove opportunity, Parrot schedules the request independently\n(line 10-line 11). Due to limited space, we omit the details of\nhow Parrot chooses LLM engines (i.e., FindEngine ). Briefly,\nParrot finds the engine that satisfies the scheduling preference\nof a request while minimizing the negative impacts. For in-\nstance, if a latency-sensitive request is scheduled to an LLM\nengine that can run up to 64,000 tokens of throughput-driven\nrequests, its capacity will be significantly reduced to 2,000 to\nsatisfy its strict latency requirement. But, if it is scheduled to\nan engine that has already been running a latency-sensitive\nrequest, the capacity reduction is negligible.\n6 Discussion\nDynamic Applications and Function Calling. Currently,\nParrot only supports cloud-side orchestration of LLM requests\nwithout involving dynamic control flow and native functions\n(e.g., Python Code). They still require client-side execution.\nWe intentionally disable the offloading of these functions\nto public LLM services to minimize the security risks of\nmalicious injection. For private LLM services whose LLM\napplications are trusted or there is a trusted zone to execute\nthese functions, Parrot\u2019s APIs can be easily extended with\nconditional connections and native code submission. More-\nover, these extensions further enable new optimizations, e.g.,\nwe can speculatively pre-launch high-probability branches in\ndynamic applications based on past profiles. This also proves\nthe potential of Parrot\u2019s design when facing new types of\napplications. We leave these extensions as future works.\nOther Applications of Inter-Request Analysis. The inter-\nrequest analysis in Parrot enables a new optimization space\nnot limited to the ones we introduced in \u00a75. A large-scale\nservice has more scheduling features to consider, including\nhandling outliers [3], job failures [58], delay scheduling [57],\nfairness [15,61], starvation [17], or supporting heterogeneous\nclusters [24, 37], which have been widely studied in other\nsystems.", "mimetype": "text/plain", "start_char_idx": 1698, "end_char_idx": 4489, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b0d5731c-5c33-438e-84f0-c2ca69792b65": {"__data__": {"id_": "b0d5731c-5c33-438e-84f0-c2ca69792b65", "embedding": null, "metadata": {"page_label": "7", "file_name": "2405_19888v1.pdf", "Title of this paper": "Parrot: Efficient Serving of LLM-based Applications with Semantic Variable", "Authors": "Chaofan Lin, Zhenhua Han, Chengruidong Zhang, Yuqing Yang, Fan Yang, Chen Chen, Lili Qiu", "Date published": "05/30/2024", "URL": "http://arxiv.org/abs/2405.19888v1", "summary": "The rise of large language models (LLMs) has enabled LLM-based applications\n(a.k.a. AI agents or co-pilots), a new software paradigm that combines the\nstrength of LLM and conventional software. Diverse LLM applications from\ndifferent tenants could design complex workflows using multiple LLM requests to\naccomplish one task. However, they have to use the over-simplified\nrequest-level API provided by today's public LLM services, losing essential\napplication-level information. Public LLM services have to blindly optimize\nindividual LLM requests, leading to sub-optimal end-to-end performance of LLM\napplications.\n  This paper introduces Parrot, an LLM service system that focuses on the\nend-to-end experience of LLM-based applications. Parrot proposes Semantic\nVariable, a unified abstraction to expose application-level knowledge to public\nLLM services. A Semantic Variable annotates an input/output variable in the\nprompt of a request, and creates the data pipeline when connecting multiple LLM\nrequests, providing a natural way to program LLM applications. Exposing\nSemantic Variables to the public LLM service allows it to perform conventional\ndata flow analysis to uncover the correlation across multiple LLM requests.\nThis correlation opens a brand-new optimization space for the end-to-end\nperformance of LLM-based applications. Extensive evaluations demonstrate that\nParrot can achieve up to an order-of-magnitude improvement for popular and\npractical use cases of LLM applications."}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ffd77891-69dc-49ff-9527-db9920cd7dbb", "node_type": "4", "metadata": {"page_label": "7", "file_name": "2405_19888v1.pdf", "Title of this paper": "Parrot: Efficient Serving of LLM-based Applications with Semantic Variable", "Authors": "Chaofan Lin, Zhenhua Han, Chengruidong Zhang, Yuqing Yang, Fan Yang, Chen Chen, Lili Qiu", "Date published": "05/30/2024", "URL": "http://arxiv.org/abs/2405.19888v1", "summary": "The rise of large language models (LLMs) has enabled LLM-based applications\n(a.k.a. AI agents or co-pilots), a new software paradigm that combines the\nstrength of LLM and conventional software. Diverse LLM applications from\ndifferent tenants could design complex workflows using multiple LLM requests to\naccomplish one task. However, they have to use the over-simplified\nrequest-level API provided by today's public LLM services, losing essential\napplication-level information. Public LLM services have to blindly optimize\nindividual LLM requests, leading to sub-optimal end-to-end performance of LLM\napplications.\n  This paper introduces Parrot, an LLM service system that focuses on the\nend-to-end experience of LLM-based applications. Parrot proposes Semantic\nVariable, a unified abstraction to expose application-level knowledge to public\nLLM services. A Semantic Variable annotates an input/output variable in the\nprompt of a request, and creates the data pipeline when connecting multiple LLM\nrequests, providing a natural way to program LLM applications. Exposing\nSemantic Variables to the public LLM service allows it to perform conventional\ndata flow analysis to uncover the correlation across multiple LLM requests.\nThis correlation opens a brand-new optimization space for the end-to-end\nperformance of LLM-based applications. Extensive evaluations demonstrate that\nParrot can achieve up to an order-of-magnitude improvement for popular and\npractical use cases of LLM applications."}, "hash": "7e2ee1a5256f9a073da30151c53d8bc82327ad1394eb3ed672fd10af43c6ff28", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "23e26e03-e0fd-4f8c-b75c-feecc2600d94", "node_type": "1", "metadata": {"page_label": "7", "file_name": "2405_19888v1.pdf", "Title of this paper": "Parrot: Efficient Serving of LLM-based Applications with Semantic Variable", "Authors": "Chaofan Lin, Zhenhua Han, Chengruidong Zhang, Yuqing Yang, Fan Yang, Chen Chen, Lili Qiu", "Date published": "05/30/2024", "URL": "http://arxiv.org/abs/2405.19888v1", "summary": "The rise of large language models (LLMs) has enabled LLM-based applications\n(a.k.a. AI agents or co-pilots), a new software paradigm that combines the\nstrength of LLM and conventional software. Diverse LLM applications from\ndifferent tenants could design complex workflows using multiple LLM requests to\naccomplish one task. However, they have to use the over-simplified\nrequest-level API provided by today's public LLM services, losing essential\napplication-level information. Public LLM services have to blindly optimize\nindividual LLM requests, leading to sub-optimal end-to-end performance of LLM\napplications.\n  This paper introduces Parrot, an LLM service system that focuses on the\nend-to-end experience of LLM-based applications. Parrot proposes Semantic\nVariable, a unified abstraction to expose application-level knowledge to public\nLLM services. A Semantic Variable annotates an input/output variable in the\nprompt of a request, and creates the data pipeline when connecting multiple LLM\nrequests, providing a natural way to program LLM applications. Exposing\nSemantic Variables to the public LLM service allows it to perform conventional\ndata flow analysis to uncover the correlation across multiple LLM requests.\nThis correlation opens a brand-new optimization space for the end-to-end\nperformance of LLM-based applications. Extensive evaluations demonstrate that\nParrot can achieve up to an order-of-magnitude improvement for popular and\npractical use cases of LLM applications."}, "hash": "a1eb4732a5aa567cadcdda5d5b2d1f238aa4250bb8b41673c91c59f16be0769d", "class_name": "RelatedNodeInfo"}}, "text": "More-\nover, these extensions further enable new optimizations, e.g.,\nwe can speculatively pre-launch high-probability branches in\ndynamic applications based on past profiles. This also proves\nthe potential of Parrot\u2019s design when facing new types of\napplications. We leave these extensions as future works.\nOther Applications of Inter-Request Analysis. The inter-\nrequest analysis in Parrot enables a new optimization space\nnot limited to the ones we introduced in \u00a75. A large-scale\nservice has more scheduling features to consider, including\nhandling outliers [3], job failures [58], delay scheduling [57],\nfairness [15,61], starvation [17], or supporting heterogeneous\nclusters [24, 37], which have been widely studied in other\nsystems. Parrot provides a new view from the perspective\nof LLM-based applications: we need to understand the inter-\nconnection and commonality of LLM requests to optimize\napplications\u2019 end-to-end performance. These features can be\nrevisited in the LLM service system by considering the new\ncharacteristics of LLM applications. In this paper, we focus\non Parrot\u2019s mechanisms and a few use cases, leaving other\noptimizations as promising future works.\nParrot with LLM Orchestration Frameworks. There\nhave been several frameworks for developers to build LLM-\nbased applications, e.g., LangChain [8], SemanticKernel [36],\nand PromptFlow [35]. The key function of these frameworks\nis to \u201cglue\u201d different LLM calls to accomplish a complex\ntask (aka. LLM orchestration). Parrot can be integrated with", "mimetype": "text/plain", "start_char_idx": 3751, "end_char_idx": 5275, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f5e52809-757b-454a-85d9-1762efc43e23": {"__data__": {"id_": "f5e52809-757b-454a-85d9-1762efc43e23", "embedding": null, "metadata": {"page_label": "8", "file_name": "2405_19888v1.pdf", "Title of this paper": "Parrot: Efficient Serving of LLM-based Applications with Semantic Variable", "Authors": "Chaofan Lin, Zhenhua Han, Chengruidong Zhang, Yuqing Yang, Fan Yang, Chen Chen, Lili Qiu", "Date published": "05/30/2024", "URL": "http://arxiv.org/abs/2405.19888v1", "summary": "The rise of large language models (LLMs) has enabled LLM-based applications\n(a.k.a. AI agents or co-pilots), a new software paradigm that combines the\nstrength of LLM and conventional software. Diverse LLM applications from\ndifferent tenants could design complex workflows using multiple LLM requests to\naccomplish one task. However, they have to use the over-simplified\nrequest-level API provided by today's public LLM services, losing essential\napplication-level information. Public LLM services have to blindly optimize\nindividual LLM requests, leading to sub-optimal end-to-end performance of LLM\napplications.\n  This paper introduces Parrot, an LLM service system that focuses on the\nend-to-end experience of LLM-based applications. Parrot proposes Semantic\nVariable, a unified abstraction to expose application-level knowledge to public\nLLM services. A Semantic Variable annotates an input/output variable in the\nprompt of a request, and creates the data pipeline when connecting multiple LLM\nrequests, providing a natural way to program LLM applications. Exposing\nSemantic Variables to the public LLM service allows it to perform conventional\ndata flow analysis to uncover the correlation across multiple LLM requests.\nThis correlation opens a brand-new optimization space for the end-to-end\nperformance of LLM-based applications. Extensive evaluations demonstrate that\nParrot can achieve up to an order-of-magnitude improvement for popular and\npractical use cases of LLM applications."}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "62a5c817-91b9-4641-a461-ee893700467a", "node_type": "4", "metadata": {"page_label": "8", "file_name": "2405_19888v1.pdf", "Title of this paper": "Parrot: Efficient Serving of LLM-based Applications with Semantic Variable", "Authors": "Chaofan Lin, Zhenhua Han, Chengruidong Zhang, Yuqing Yang, Fan Yang, Chen Chen, Lili Qiu", "Date published": "05/30/2024", "URL": "http://arxiv.org/abs/2405.19888v1", "summary": "The rise of large language models (LLMs) has enabled LLM-based applications\n(a.k.a. AI agents or co-pilots), a new software paradigm that combines the\nstrength of LLM and conventional software. Diverse LLM applications from\ndifferent tenants could design complex workflows using multiple LLM requests to\naccomplish one task. However, they have to use the over-simplified\nrequest-level API provided by today's public LLM services, losing essential\napplication-level information. Public LLM services have to blindly optimize\nindividual LLM requests, leading to sub-optimal end-to-end performance of LLM\napplications.\n  This paper introduces Parrot, an LLM service system that focuses on the\nend-to-end experience of LLM-based applications. Parrot proposes Semantic\nVariable, a unified abstraction to expose application-level knowledge to public\nLLM services. A Semantic Variable annotates an input/output variable in the\nprompt of a request, and creates the data pipeline when connecting multiple LLM\nrequests, providing a natural way to program LLM applications. Exposing\nSemantic Variables to the public LLM service allows it to perform conventional\ndata flow analysis to uncover the correlation across multiple LLM requests.\nThis correlation opens a brand-new optimization space for the end-to-end\nperformance of LLM-based applications. Extensive evaluations demonstrate that\nParrot can achieve up to an order-of-magnitude improvement for popular and\npractical use cases of LLM applications."}, "hash": "98acb0516a076bd181febf4f52159828e7cea5600afd8fef9d141fc239dd6f08", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "eb4678d4-44a9-4c66-a402-6bd90b65394d", "node_type": "1", "metadata": {}, "hash": "b3fce97d14a0965ade663afa223eec0c38aebb7b90571781bb20830a5c48387f", "class_name": "RelatedNodeInfo"}}, "text": "these frameworks by extending their calling of LLM service\nAPIs with Semantic Variables. Most of these frameworks\nhave already used a template-based approach in which devel-\nopers can design a template with placeholders, and render the\nplaceholders at runtime. These placeholders naturally have\nthe same concept as Parrot\u2019s Semantic Variable. However,\nbecause these frameworks will render the template prompt\nbefore the submission, LLM services lose the information on\nthe prompt structure. To make these frameworks compatible\nwith Parrot, both the template itself and the variables to render\nthe template (using Semantic Variable in Parrot) need to be\nwrapped as a SemanticFunction so the necessary informa-\ntion is exposed to Parrot\u2019s LLM service.\n7 Implementation\nParrot is an end-to-end LLM service for LLM applications,\nimplemented on Python with about 14,000 lines of code. Its\nfront-end provides the abstraction of Semantic Variable, and\nSemanticFunction , which is transformed into Parrot\u2019s APIs\n(implemented with FastAPI [48]) to be submitted as LLM\nrequests. A centralized Parrot manager handles the manage-\nment of LLM requests, including Semantic Variables, com-\nmunication, and scheduling. We also build an LLM engine\nbased on efficient kernels from vLLM [25], xFormers [26],\nand ourselves. The engine supports advanced features for\nLLM serving, including paged memory management [25] and\ncontinues batching [56]. Parrot\u2019s front-end and manager are\nimplemented in 1,600 and 3,200 lines of Python, respectively.\nParrot\u2019s LLM engine is implemented in 5,400 lines of Python\nand 1,600 lines of CUDA. We have implemented OPT [60]\nand LLaMA [51] with PyTorch [45] and Transformers [53].\nAPIs. Applications programmed by SemanticFunction s\nor other frontends are finally lowered to requests to universal\nAPIs through different adapters. Parrot provides OpenAI-like\nAPIs with the extension of Semantic Variables. The request\nbody of two operations mentioned in \u00a74.1 is shown as follows:\n(submit) {\"prompt\": str, \"placeholders\": [{\"name\":\nstr, \"in_out\": bool, \"semantic_var_id\": str,\n\"transforms\": str}, ...], \"session_id\": str},\u2192\n,\u2192\n(get) {\"semantic_var_id\": str, \"criteria\": str,\n\"session_id\": str} ,\u2192\nIn addition to the static string prompt, Parrot preserves the\ninput and output placeholders. A placeholder is associated\nwith a semantic variable either for rendering the input or\nparsing the output. As introduced in \u00a75.1. Parrot supports\ntransformations before the input or after the output. Parrot\nalso supports other APIs for setting and fetching the value of\nSemantic Variables.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2590, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "eb4678d4-44a9-4c66-a402-6bd90b65394d": {"__data__": {"id_": "eb4678d4-44a9-4c66-a402-6bd90b65394d", "embedding": null, "metadata": {"page_label": "8", "file_name": "2405_19888v1.pdf", "Title of this paper": "Parrot: Efficient Serving of LLM-based Applications with Semantic Variable", "Authors": "Chaofan Lin, Zhenhua Han, Chengruidong Zhang, Yuqing Yang, Fan Yang, Chen Chen, Lili Qiu", "Date published": "05/30/2024", "URL": "http://arxiv.org/abs/2405.19888v1", "summary": "The rise of large language models (LLMs) has enabled LLM-based applications\n(a.k.a. AI agents or co-pilots), a new software paradigm that combines the\nstrength of LLM and conventional software. Diverse LLM applications from\ndifferent tenants could design complex workflows using multiple LLM requests to\naccomplish one task. However, they have to use the over-simplified\nrequest-level API provided by today's public LLM services, losing essential\napplication-level information. Public LLM services have to blindly optimize\nindividual LLM requests, leading to sub-optimal end-to-end performance of LLM\napplications.\n  This paper introduces Parrot, an LLM service system that focuses on the\nend-to-end experience of LLM-based applications. Parrot proposes Semantic\nVariable, a unified abstraction to expose application-level knowledge to public\nLLM services. A Semantic Variable annotates an input/output variable in the\nprompt of a request, and creates the data pipeline when connecting multiple LLM\nrequests, providing a natural way to program LLM applications. Exposing\nSemantic Variables to the public LLM service allows it to perform conventional\ndata flow analysis to uncover the correlation across multiple LLM requests.\nThis correlation opens a brand-new optimization space for the end-to-end\nperformance of LLM-based applications. Extensive evaluations demonstrate that\nParrot can achieve up to an order-of-magnitude improvement for popular and\npractical use cases of LLM applications."}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "62a5c817-91b9-4641-a461-ee893700467a", "node_type": "4", "metadata": {"page_label": "8", "file_name": "2405_19888v1.pdf", "Title of this paper": "Parrot: Efficient Serving of LLM-based Applications with Semantic Variable", "Authors": "Chaofan Lin, Zhenhua Han, Chengruidong Zhang, Yuqing Yang, Fan Yang, Chen Chen, Lili Qiu", "Date published": "05/30/2024", "URL": "http://arxiv.org/abs/2405.19888v1", "summary": "The rise of large language models (LLMs) has enabled LLM-based applications\n(a.k.a. AI agents or co-pilots), a new software paradigm that combines the\nstrength of LLM and conventional software. Diverse LLM applications from\ndifferent tenants could design complex workflows using multiple LLM requests to\naccomplish one task. However, they have to use the over-simplified\nrequest-level API provided by today's public LLM services, losing essential\napplication-level information. Public LLM services have to blindly optimize\nindividual LLM requests, leading to sub-optimal end-to-end performance of LLM\napplications.\n  This paper introduces Parrot, an LLM service system that focuses on the\nend-to-end experience of LLM-based applications. Parrot proposes Semantic\nVariable, a unified abstraction to expose application-level knowledge to public\nLLM services. A Semantic Variable annotates an input/output variable in the\nprompt of a request, and creates the data pipeline when connecting multiple LLM\nrequests, providing a natural way to program LLM applications. Exposing\nSemantic Variables to the public LLM service allows it to perform conventional\ndata flow analysis to uncover the correlation across multiple LLM requests.\nThis correlation opens a brand-new optimization space for the end-to-end\nperformance of LLM-based applications. Extensive evaluations demonstrate that\nParrot can achieve up to an order-of-magnitude improvement for popular and\npractical use cases of LLM applications."}, "hash": "98acb0516a076bd181febf4f52159828e7cea5600afd8fef9d141fc239dd6f08", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f5e52809-757b-454a-85d9-1762efc43e23", "node_type": "1", "metadata": {"page_label": "8", "file_name": "2405_19888v1.pdf", "Title of this paper": "Parrot: Efficient Serving of LLM-based Applications with Semantic Variable", "Authors": "Chaofan Lin, Zhenhua Han, Chengruidong Zhang, Yuqing Yang, Fan Yang, Chen Chen, Lili Qiu", "Date published": "05/30/2024", "URL": "http://arxiv.org/abs/2405.19888v1", "summary": "The rise of large language models (LLMs) has enabled LLM-based applications\n(a.k.a. AI agents or co-pilots), a new software paradigm that combines the\nstrength of LLM and conventional software. Diverse LLM applications from\ndifferent tenants could design complex workflows using multiple LLM requests to\naccomplish one task. However, they have to use the over-simplified\nrequest-level API provided by today's public LLM services, losing essential\napplication-level information. Public LLM services have to blindly optimize\nindividual LLM requests, leading to sub-optimal end-to-end performance of LLM\napplications.\n  This paper introduces Parrot, an LLM service system that focuses on the\nend-to-end experience of LLM-based applications. Parrot proposes Semantic\nVariable, a unified abstraction to expose application-level knowledge to public\nLLM services. A Semantic Variable annotates an input/output variable in the\nprompt of a request, and creates the data pipeline when connecting multiple LLM\nrequests, providing a natural way to program LLM applications. Exposing\nSemantic Variables to the public LLM service allows it to perform conventional\ndata flow analysis to uncover the correlation across multiple LLM requests.\nThis correlation opens a brand-new optimization space for the end-to-end\nperformance of LLM-based applications. Extensive evaluations demonstrate that\nParrot can achieve up to an order-of-magnitude improvement for popular and\npractical use cases of LLM applications."}, "hash": "8f641c0aca1d5a6b9f020d8699d74c167789495c7fd72fda38c9635e760526b7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5420325b-29ea-4cc3-b18b-f111ac9069e2", "node_type": "1", "metadata": {}, "hash": "e2682414e1d7e53cb28a46f9713a6768aa14e05133cc64ce4f8909ab0309998a", "class_name": "RelatedNodeInfo"}}, "text": "Parrot provides OpenAI-like\nAPIs with the extension of Semantic Variables. The request\nbody of two operations mentioned in \u00a74.1 is shown as follows:\n(submit) {\"prompt\": str, \"placeholders\": [{\"name\":\nstr, \"in_out\": bool, \"semantic_var_id\": str,\n\"transforms\": str}, ...], \"session_id\": str},\u2192\n,\u2192\n(get) {\"semantic_var_id\": str, \"criteria\": str,\n\"session_id\": str} ,\u2192\nIn addition to the static string prompt, Parrot preserves the\ninput and output placeholders. A placeholder is associated\nwith a semantic variable either for rendering the input or\nparsing the output. As introduced in \u00a75.1. Parrot supports\ntransformations before the input or after the output. Parrot\nalso supports other APIs for setting and fetching the value of\nSemantic Variables. The error message will be returned when\nfetching an Semantic Variable, whose intermediate steps fail\n(including engine, communication, and string transformation).Kernel Optimization. vLLM\u2019s GPU kernel, while capable\nof reusing results cached in GPU memory for shared prefix to-\nkens in a prompt, sometimes excessively reloads these tokens\nfrom global to shared memory, impeding attention score com-\nputations. Using OpenAI Triton [43] and CUDA, we have\ndeveloped a novel GPU kernel, integrating concepts from\nPagedAttention [25] and FlashAttention [11, 12], to acceler-\nate attention decoding computation involving shared prefixes.\nThis kernel retains PagedAttention\u2019s approach of storing the\nkey-value (KV) cache in disparate memory segments and\nutilizes a page table per request to monitor block status and\nplacement. Furthermore, employing FlashAttention princi-\nples, the kernel maximizes data reuse within shared memory.\nUnlike reloading tiles repeatedly in the PagedAttention\u2019s im-\nplementation, it loads KV cache tiles for the shared prefix\nto shared memory only once, diminishing memory transac-\ntions between the L2 Cache and Shared Memory. The kernel\ninitially calculates interim attention metrics (including atten-\ntion scores, qk_max ,exp_sum ) for the shared prefix using the\nloaded tiles and records these back to HBM. Subsequently, it\nprocesses the new tokens\u2019 partial attention beyond the prefix,\namalgamating this with the prefix\u2019s interim results to derive\nthe ultimate attention output.\nUniversal Engine Abstraction. Parrot\u2019s cluster manager\ncontrols multiple engines running various models, tokeniz-\ners, KV cache layouts, etc. To enable Parrot\u2019s optimizations,\nLLM engines need to support (1) stateful generation (e.g.,\nguidance [18]) and (2) sharing KV cache states across dif-\nferent requests. Hence we propose a universal abstraction to\ndescribe the minimal capability required to LLM engines to\nbe integrated into Parrot.", "mimetype": "text/plain", "start_char_idx": 1843, "end_char_idx": 4537, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5420325b-29ea-4cc3-b18b-f111ac9069e2": {"__data__": {"id_": "5420325b-29ea-4cc3-b18b-f111ac9069e2", "embedding": null, "metadata": {"page_label": "8", "file_name": "2405_19888v1.pdf", "Title of this paper": "Parrot: Efficient Serving of LLM-based Applications with Semantic Variable", "Authors": "Chaofan Lin, Zhenhua Han, Chengruidong Zhang, Yuqing Yang, Fan Yang, Chen Chen, Lili Qiu", "Date published": "05/30/2024", "URL": "http://arxiv.org/abs/2405.19888v1", "summary": "The rise of large language models (LLMs) has enabled LLM-based applications\n(a.k.a. AI agents or co-pilots), a new software paradigm that combines the\nstrength of LLM and conventional software. Diverse LLM applications from\ndifferent tenants could design complex workflows using multiple LLM requests to\naccomplish one task. However, they have to use the over-simplified\nrequest-level API provided by today's public LLM services, losing essential\napplication-level information. Public LLM services have to blindly optimize\nindividual LLM requests, leading to sub-optimal end-to-end performance of LLM\napplications.\n  This paper introduces Parrot, an LLM service system that focuses on the\nend-to-end experience of LLM-based applications. Parrot proposes Semantic\nVariable, a unified abstraction to expose application-level knowledge to public\nLLM services. A Semantic Variable annotates an input/output variable in the\nprompt of a request, and creates the data pipeline when connecting multiple LLM\nrequests, providing a natural way to program LLM applications. Exposing\nSemantic Variables to the public LLM service allows it to perform conventional\ndata flow analysis to uncover the correlation across multiple LLM requests.\nThis correlation opens a brand-new optimization space for the end-to-end\nperformance of LLM-based applications. Extensive evaluations demonstrate that\nParrot can achieve up to an order-of-magnitude improvement for popular and\npractical use cases of LLM applications."}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "62a5c817-91b9-4641-a461-ee893700467a", "node_type": "4", "metadata": {"page_label": "8", "file_name": "2405_19888v1.pdf", "Title of this paper": "Parrot: Efficient Serving of LLM-based Applications with Semantic Variable", "Authors": "Chaofan Lin, Zhenhua Han, Chengruidong Zhang, Yuqing Yang, Fan Yang, Chen Chen, Lili Qiu", "Date published": "05/30/2024", "URL": "http://arxiv.org/abs/2405.19888v1", "summary": "The rise of large language models (LLMs) has enabled LLM-based applications\n(a.k.a. AI agents or co-pilots), a new software paradigm that combines the\nstrength of LLM and conventional software. Diverse LLM applications from\ndifferent tenants could design complex workflows using multiple LLM requests to\naccomplish one task. However, they have to use the over-simplified\nrequest-level API provided by today's public LLM services, losing essential\napplication-level information. Public LLM services have to blindly optimize\nindividual LLM requests, leading to sub-optimal end-to-end performance of LLM\napplications.\n  This paper introduces Parrot, an LLM service system that focuses on the\nend-to-end experience of LLM-based applications. Parrot proposes Semantic\nVariable, a unified abstraction to expose application-level knowledge to public\nLLM services. A Semantic Variable annotates an input/output variable in the\nprompt of a request, and creates the data pipeline when connecting multiple LLM\nrequests, providing a natural way to program LLM applications. Exposing\nSemantic Variables to the public LLM service allows it to perform conventional\ndata flow analysis to uncover the correlation across multiple LLM requests.\nThis correlation opens a brand-new optimization space for the end-to-end\nperformance of LLM-based applications. Extensive evaluations demonstrate that\nParrot can achieve up to an order-of-magnitude improvement for popular and\npractical use cases of LLM applications."}, "hash": "98acb0516a076bd181febf4f52159828e7cea5600afd8fef9d141fc239dd6f08", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "eb4678d4-44a9-4c66-a402-6bd90b65394d", "node_type": "1", "metadata": {"page_label": "8", "file_name": "2405_19888v1.pdf", "Title of this paper": "Parrot: Efficient Serving of LLM-based Applications with Semantic Variable", "Authors": "Chaofan Lin, Zhenhua Han, Chengruidong Zhang, Yuqing Yang, Fan Yang, Chen Chen, Lili Qiu", "Date published": "05/30/2024", "URL": "http://arxiv.org/abs/2405.19888v1", "summary": "The rise of large language models (LLMs) has enabled LLM-based applications\n(a.k.a. AI agents or co-pilots), a new software paradigm that combines the\nstrength of LLM and conventional software. Diverse LLM applications from\ndifferent tenants could design complex workflows using multiple LLM requests to\naccomplish one task. However, they have to use the over-simplified\nrequest-level API provided by today's public LLM services, losing essential\napplication-level information. Public LLM services have to blindly optimize\nindividual LLM requests, leading to sub-optimal end-to-end performance of LLM\napplications.\n  This paper introduces Parrot, an LLM service system that focuses on the\nend-to-end experience of LLM-based applications. Parrot proposes Semantic\nVariable, a unified abstraction to expose application-level knowledge to public\nLLM services. A Semantic Variable annotates an input/output variable in the\nprompt of a request, and creates the data pipeline when connecting multiple LLM\nrequests, providing a natural way to program LLM applications. Exposing\nSemantic Variables to the public LLM service allows it to perform conventional\ndata flow analysis to uncover the correlation across multiple LLM requests.\nThis correlation opens a brand-new optimization space for the end-to-end\nperformance of LLM-based applications. Extensive evaluations demonstrate that\nParrot can achieve up to an order-of-magnitude improvement for popular and\npractical use cases of LLM applications."}, "hash": "3d6c43ff647234e08ada4b204dc5887fa1f272b1a02f689e2a308ed1bda08de4", "class_name": "RelatedNodeInfo"}}, "text": "The kernel\ninitially calculates interim attention metrics (including atten-\ntion scores, qk_max ,exp_sum ) for the shared prefix using the\nloaded tiles and records these back to HBM. Subsequently, it\nprocesses the new tokens\u2019 partial attention beyond the prefix,\namalgamating this with the prefix\u2019s interim results to derive\nthe ultimate attention output.\nUniversal Engine Abstraction. Parrot\u2019s cluster manager\ncontrols multiple engines running various models, tokeniz-\ners, KV cache layouts, etc. To enable Parrot\u2019s optimizations,\nLLM engines need to support (1) stateful generation (e.g.,\nguidance [18]) and (2) sharing KV cache states across dif-\nferent requests. Hence we propose a universal abstraction to\ndescribe the minimal capability required to LLM engines to\nbe integrated into Parrot.\ndef Fill(token_ids: List[int], context_id: int,\nparent_context_id: int) ,\u2192\ndef Generate(sampling_configs: Dict, context_id:\nint, parent_context_id: int) ,\u2192\ndef FreeContext(context_id: int)\nThese three methods not only cover the basic completion\nfunctionality of LLM inference engine, but also provide a\nflexible context management interface. The Fill method pro-\ncesses the initial prompt tokens, calculates and fills the KV\ncache into corresponding context. The Generate method pro-\nduces tokens via generative decoding that produces one token\nper iteration until it reaches the length limit, user-defined\ntermination character or EOS (end-of-sequence) token, un-\nder certain sampling configurations (e.g. temperature). Fill s\nandGenerate s are scheduled and batched by engine\u2019s sched-\nuler per iteration using continuous batching [56]. Creating\nand forking contexts can also be realized with these two\nmethods by setting context_id andparent_context_id ,\nrespectively. The FreeContext method explicitly frees a con-\ntext (i.e. free its KV cache in GPU memory). Separating\nFill andGenerate not only fits Semantic Variable naturally:", "mimetype": "text/plain", "start_char_idx": 3741, "end_char_idx": 5671, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2ef43774-b1dc-4346-bf8a-c26ca1b57ed3": {"__data__": {"id_": "2ef43774-b1dc-4346-bf8a-c26ca1b57ed3", "embedding": null, "metadata": {"page_label": "9", "file_name": "2405_19888v1.pdf", "Title of this paper": "Parrot: Efficient Serving of LLM-based Applications with Semantic Variable", "Authors": "Chaofan Lin, Zhenhua Han, Chengruidong Zhang, Yuqing Yang, Fan Yang, Chen Chen, Lili Qiu", "Date published": "05/30/2024", "URL": "http://arxiv.org/abs/2405.19888v1", "summary": "The rise of large language models (LLMs) has enabled LLM-based applications\n(a.k.a. AI agents or co-pilots), a new software paradigm that combines the\nstrength of LLM and conventional software. Diverse LLM applications from\ndifferent tenants could design complex workflows using multiple LLM requests to\naccomplish one task. However, they have to use the over-simplified\nrequest-level API provided by today's public LLM services, losing essential\napplication-level information. Public LLM services have to blindly optimize\nindividual LLM requests, leading to sub-optimal end-to-end performance of LLM\napplications.\n  This paper introduces Parrot, an LLM service system that focuses on the\nend-to-end experience of LLM-based applications. Parrot proposes Semantic\nVariable, a unified abstraction to expose application-level knowledge to public\nLLM services. A Semantic Variable annotates an input/output variable in the\nprompt of a request, and creates the data pipeline when connecting multiple LLM\nrequests, providing a natural way to program LLM applications. Exposing\nSemantic Variables to the public LLM service allows it to perform conventional\ndata flow analysis to uncover the correlation across multiple LLM requests.\nThis correlation opens a brand-new optimization space for the end-to-end\nperformance of LLM-based applications. Extensive evaluations demonstrate that\nParrot can achieve up to an order-of-magnitude improvement for popular and\npractical use cases of LLM applications."}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f242335c-3d9e-4a1c-aa50-287c6dd1ce98", "node_type": "4", "metadata": {"page_label": "9", "file_name": "2405_19888v1.pdf", "Title of this paper": "Parrot: Efficient Serving of LLM-based Applications with Semantic Variable", "Authors": "Chaofan Lin, Zhenhua Han, Chengruidong Zhang, Yuqing Yang, Fan Yang, Chen Chen, Lili Qiu", "Date published": "05/30/2024", "URL": "http://arxiv.org/abs/2405.19888v1", "summary": "The rise of large language models (LLMs) has enabled LLM-based applications\n(a.k.a. AI agents or co-pilots), a new software paradigm that combines the\nstrength of LLM and conventional software. Diverse LLM applications from\ndifferent tenants could design complex workflows using multiple LLM requests to\naccomplish one task. However, they have to use the over-simplified\nrequest-level API provided by today's public LLM services, losing essential\napplication-level information. Public LLM services have to blindly optimize\nindividual LLM requests, leading to sub-optimal end-to-end performance of LLM\napplications.\n  This paper introduces Parrot, an LLM service system that focuses on the\nend-to-end experience of LLM-based applications. Parrot proposes Semantic\nVariable, a unified abstraction to expose application-level knowledge to public\nLLM services. A Semantic Variable annotates an input/output variable in the\nprompt of a request, and creates the data pipeline when connecting multiple LLM\nrequests, providing a natural way to program LLM applications. Exposing\nSemantic Variables to the public LLM service allows it to perform conventional\ndata flow analysis to uncover the correlation across multiple LLM requests.\nThis correlation opens a brand-new optimization space for the end-to-end\nperformance of LLM-based applications. Extensive evaluations demonstrate that\nParrot can achieve up to an order-of-magnitude improvement for popular and\npractical use cases of LLM applications."}, "hash": "6d27c62bc3553a2813aa6b49fb76e63b502e7003ab1f3ec0542dd9b07dbd8978", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "755a9e00-39bd-4a57-8c6f-24ab452a0f7a", "node_type": "1", "metadata": {}, "hash": "0f8cfc1a8c1189573c78ba347016f698bd464eb27c67f8b392d0850c7daa561a", "class_name": "RelatedNodeInfo"}}, "text": "constant text and input values are processed by Fill ; the out-\nput values are generated by Generate , but also breaks the\nrequest-level dependency into a finer granularity, enabling\nmore parallel execution opportunities [2, 21, 46, 64].\n8 Evaluation\n8.1 Experimental Setup\nTestbed. We evaluate Parrot with two separate setups for\nsingle-GPU and multi-GPU experiments. The single-GPU\nevaluations use a server with a 24-core AMD-EPYC-7V13\nCPUs equipped with one NVIDIA A100 (80GB) GPU. The\nmulti-GPU evaluations use a server with 64-core EPYC AMD\nCPU and four NVIDIA A6000 (48GB) GPUs. Both servers\nrun CUDA 12.1 and cuDNN 8.9.2.\nWorkloads. Our evaluations are performed to run four rep-\nresentative LLM applications. Each LLM engine uses one\nGPU and runs a LLaMA 13B or LLaMA 7B model [51] .\nFor LLM-based data analytics on long documents, we use the\nArxiv dataset [27], executing chain and map-reduce summa-\nrizations on an extensive collection of academic papers. To\ninvestigate the sharing opportunities of LLM-based applica-\ntions with many users, we run the prompts from Bing Copilot\nand GPTs [42] with synthesized user queries. For multi-agent\napplications, we build a multi-agent programming application\nusing MetaGPT [22], which contains a system architect to\ndesign APIs, multiple programmers to write code for different\nfiles, reviewers to share review comments. The programmers\nwill also revise the code based on comments. For chat ser-\nvice workloads, we derived scenarios from the ShareGPT\ndataset [50], which mirrors real LLM chat conversations. Ac-\ncording to the distribution of our measurement, we introduced\na random delay of 200\u223c300ms to LLM requests to emulate\ntypical network overhead seen over the Internet. To create\nrealistic workloads, we documented the LLM responses us-\ning GPT-4 [41], ensuring the LLaMA models generated text\nof similar length for system performance analysis. Table 2\npresents the workloads and their optimizations in Parrot.\nBaseline. We benchmark Parrot against sate-of-the-art so-\nlutions for building LLM applications and serving LLM re-\nquests. The majority of LLM applications used in our baseline\nWorkloadServing\nDependent\nRequests.Perf. Obj.\nDeductionSharing\nPromptApp-centric\nScheduling\nData Analytics \u2713 \u2713 \u2713\nServing Popular\nLLM Applications\u2713 \u2713\nMulti-agent App. \u2713 \u2713 \u2713 \u2713\nMixed Workloads \u2713 \u2713 \u2713\nTable 2: The workloads and the optimizations taking effect.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2404, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "755a9e00-39bd-4a57-8c6f-24ab452a0f7a": {"__data__": {"id_": "755a9e00-39bd-4a57-8c6f-24ab452a0f7a", "embedding": null, "metadata": {"page_label": "9", "file_name": "2405_19888v1.pdf", "Title of this paper": "Parrot: Efficient Serving of LLM-based Applications with Semantic Variable", "Authors": "Chaofan Lin, Zhenhua Han, Chengruidong Zhang, Yuqing Yang, Fan Yang, Chen Chen, Lili Qiu", "Date published": "05/30/2024", "URL": "http://arxiv.org/abs/2405.19888v1", "summary": "The rise of large language models (LLMs) has enabled LLM-based applications\n(a.k.a. AI agents or co-pilots), a new software paradigm that combines the\nstrength of LLM and conventional software. Diverse LLM applications from\ndifferent tenants could design complex workflows using multiple LLM requests to\naccomplish one task. However, they have to use the over-simplified\nrequest-level API provided by today's public LLM services, losing essential\napplication-level information. Public LLM services have to blindly optimize\nindividual LLM requests, leading to sub-optimal end-to-end performance of LLM\napplications.\n  This paper introduces Parrot, an LLM service system that focuses on the\nend-to-end experience of LLM-based applications. Parrot proposes Semantic\nVariable, a unified abstraction to expose application-level knowledge to public\nLLM services. A Semantic Variable annotates an input/output variable in the\nprompt of a request, and creates the data pipeline when connecting multiple LLM\nrequests, providing a natural way to program LLM applications. Exposing\nSemantic Variables to the public LLM service allows it to perform conventional\ndata flow analysis to uncover the correlation across multiple LLM requests.\nThis correlation opens a brand-new optimization space for the end-to-end\nperformance of LLM-based applications. Extensive evaluations demonstrate that\nParrot can achieve up to an order-of-magnitude improvement for popular and\npractical use cases of LLM applications."}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f242335c-3d9e-4a1c-aa50-287c6dd1ce98", "node_type": "4", "metadata": {"page_label": "9", "file_name": "2405_19888v1.pdf", "Title of this paper": "Parrot: Efficient Serving of LLM-based Applications with Semantic Variable", "Authors": "Chaofan Lin, Zhenhua Han, Chengruidong Zhang, Yuqing Yang, Fan Yang, Chen Chen, Lili Qiu", "Date published": "05/30/2024", "URL": "http://arxiv.org/abs/2405.19888v1", "summary": "The rise of large language models (LLMs) has enabled LLM-based applications\n(a.k.a. AI agents or co-pilots), a new software paradigm that combines the\nstrength of LLM and conventional software. Diverse LLM applications from\ndifferent tenants could design complex workflows using multiple LLM requests to\naccomplish one task. However, they have to use the over-simplified\nrequest-level API provided by today's public LLM services, losing essential\napplication-level information. Public LLM services have to blindly optimize\nindividual LLM requests, leading to sub-optimal end-to-end performance of LLM\napplications.\n  This paper introduces Parrot, an LLM service system that focuses on the\nend-to-end experience of LLM-based applications. Parrot proposes Semantic\nVariable, a unified abstraction to expose application-level knowledge to public\nLLM services. A Semantic Variable annotates an input/output variable in the\nprompt of a request, and creates the data pipeline when connecting multiple LLM\nrequests, providing a natural way to program LLM applications. Exposing\nSemantic Variables to the public LLM service allows it to perform conventional\ndata flow analysis to uncover the correlation across multiple LLM requests.\nThis correlation opens a brand-new optimization space for the end-to-end\nperformance of LLM-based applications. Extensive evaluations demonstrate that\nParrot can achieve up to an order-of-magnitude improvement for popular and\npractical use cases of LLM applications."}, "hash": "6d27c62bc3553a2813aa6b49fb76e63b502e7003ab1f3ec0542dd9b07dbd8978", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2ef43774-b1dc-4346-bf8a-c26ca1b57ed3", "node_type": "1", "metadata": {"page_label": "9", "file_name": "2405_19888v1.pdf", "Title of this paper": "Parrot: Efficient Serving of LLM-based Applications with Semantic Variable", "Authors": "Chaofan Lin, Zhenhua Han, Chengruidong Zhang, Yuqing Yang, Fan Yang, Chen Chen, Lili Qiu", "Date published": "05/30/2024", "URL": "http://arxiv.org/abs/2405.19888v1", "summary": "The rise of large language models (LLMs) has enabled LLM-based applications\n(a.k.a. AI agents or co-pilots), a new software paradigm that combines the\nstrength of LLM and conventional software. Diverse LLM applications from\ndifferent tenants could design complex workflows using multiple LLM requests to\naccomplish one task. However, they have to use the over-simplified\nrequest-level API provided by today's public LLM services, losing essential\napplication-level information. Public LLM services have to blindly optimize\nindividual LLM requests, leading to sub-optimal end-to-end performance of LLM\napplications.\n  This paper introduces Parrot, an LLM service system that focuses on the\nend-to-end experience of LLM-based applications. Parrot proposes Semantic\nVariable, a unified abstraction to expose application-level knowledge to public\nLLM services. A Semantic Variable annotates an input/output variable in the\nprompt of a request, and creates the data pipeline when connecting multiple LLM\nrequests, providing a natural way to program LLM applications. Exposing\nSemantic Variables to the public LLM service allows it to perform conventional\ndata flow analysis to uncover the correlation across multiple LLM requests.\nThis correlation opens a brand-new optimization space for the end-to-end\nperformance of LLM-based applications. Extensive evaluations demonstrate that\nParrot can achieve up to an order-of-magnitude improvement for popular and\npractical use cases of LLM applications."}, "hash": "91df90bcda0bb8922b5fb86474f0f1df7325ed3ed643cea213e4ec799d26950a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "940d3026-179d-405b-9650-6033dcd18dee", "node_type": "1", "metadata": {}, "hash": "bbb5b90963004f6109f34707c1267ee750c70a322c7ef66b30c4a5a5295d8786", "class_name": "RelatedNodeInfo"}}, "text": "To create\nrealistic workloads, we documented the LLM responses us-\ning GPT-4 [41], ensuring the LLaMA models generated text\nof similar length for system performance analysis. Table 2\npresents the workloads and their optimizations in Parrot.\nBaseline. We benchmark Parrot against sate-of-the-art so-\nlutions for building LLM applications and serving LLM re-\nquests. The majority of LLM applications used in our baseline\nWorkloadServing\nDependent\nRequests.Perf. Obj.\nDeductionSharing\nPromptApp-centric\nScheduling\nData Analytics \u2713 \u2713 \u2713\nServing Popular\nLLM Applications\u2713 \u2713\nMulti-agent App. \u2713 \u2713 \u2713 \u2713\nMixed Workloads \u2713 \u2713 \u2713\nTable 2: The workloads and the optimizations taking effect.\n5 10 15 20 25\nRequests/s2030405060Mean Latency (ms)\nCapacity=2048\nCapacity=4096\nCapacity=6144\nCapacity=8192\nCapacity=10240\nCapacity=12288(a) Mean Latency\n5 10 15 20 25\nRequests/s2030405060P90 Latency (ms)\nCapacity=2048\nCapacity=4096\nCapacity=6144\nCapacity=8192\nCapacity=10240\nCapacity=12288 (b) P90 Latency\nFigure 10: Latency (per output token) of vLLM with varying\ntoken capacities and request rates. Requests are sampled from\nShareGPT [50] and their arrival time follows Poisson distri-\nbutions.\ncomparisons are developed using LangChain [8], which is the\npredominant framework for LLM application development.\nThe LLM applications in baselines leverage OpenAI-style\nchat completion APIs as provided by FastChat [62]. FastChat\nis a widely recognized open-source LLM serving system\nwith over 30,000 stars on its repository. Incoming requests to\nFastChat are allocated to LLM engines that run either Hug-\ngingFace\u2019s Transformers library [53] or vLLM [25], both of\nwhich incorporate cutting-edge enhancements for LLM exe-\ncution, such as FlashAttention [12], PagedAttention [25], and\ncontinuous batching techniques [56]. The default scheduling\nstrategy employed by FastChat assigns incoming requests\nto the LLM engine with the smallest current queue. Since\nexisting LLM services typically expose their functionality\nthrough \"chat\" completion APIs, baseline assessments treat\nall requests as independent and assume a high sensitivity to\nlatency. To manage token generation response times, each\nLLM engine is subject to a capacity threshold, which is the\naggregate token count from all active requests on the engine.\nSince existing LLM token generation is usually bound by\nmemory bandwidth, the per-token generation latency of an\nengine is mainly affected by the number of running tokens in\na batch.", "mimetype": "text/plain", "start_char_idx": 1730, "end_char_idx": 4201, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "940d3026-179d-405b-9650-6033dcd18dee": {"__data__": {"id_": "940d3026-179d-405b-9650-6033dcd18dee", "embedding": null, "metadata": {"page_label": "9", "file_name": "2405_19888v1.pdf", "Title of this paper": "Parrot: Efficient Serving of LLM-based Applications with Semantic Variable", "Authors": "Chaofan Lin, Zhenhua Han, Chengruidong Zhang, Yuqing Yang, Fan Yang, Chen Chen, Lili Qiu", "Date published": "05/30/2024", "URL": "http://arxiv.org/abs/2405.19888v1", "summary": "The rise of large language models (LLMs) has enabled LLM-based applications\n(a.k.a. AI agents or co-pilots), a new software paradigm that combines the\nstrength of LLM and conventional software. Diverse LLM applications from\ndifferent tenants could design complex workflows using multiple LLM requests to\naccomplish one task. However, they have to use the over-simplified\nrequest-level API provided by today's public LLM services, losing essential\napplication-level information. Public LLM services have to blindly optimize\nindividual LLM requests, leading to sub-optimal end-to-end performance of LLM\napplications.\n  This paper introduces Parrot, an LLM service system that focuses on the\nend-to-end experience of LLM-based applications. Parrot proposes Semantic\nVariable, a unified abstraction to expose application-level knowledge to public\nLLM services. A Semantic Variable annotates an input/output variable in the\nprompt of a request, and creates the data pipeline when connecting multiple LLM\nrequests, providing a natural way to program LLM applications. Exposing\nSemantic Variables to the public LLM service allows it to perform conventional\ndata flow analysis to uncover the correlation across multiple LLM requests.\nThis correlation opens a brand-new optimization space for the end-to-end\nperformance of LLM-based applications. Extensive evaluations demonstrate that\nParrot can achieve up to an order-of-magnitude improvement for popular and\npractical use cases of LLM applications."}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f242335c-3d9e-4a1c-aa50-287c6dd1ce98", "node_type": "4", "metadata": {"page_label": "9", "file_name": "2405_19888v1.pdf", "Title of this paper": "Parrot: Efficient Serving of LLM-based Applications with Semantic Variable", "Authors": "Chaofan Lin, Zhenhua Han, Chengruidong Zhang, Yuqing Yang, Fan Yang, Chen Chen, Lili Qiu", "Date published": "05/30/2024", "URL": "http://arxiv.org/abs/2405.19888v1", "summary": "The rise of large language models (LLMs) has enabled LLM-based applications\n(a.k.a. AI agents or co-pilots), a new software paradigm that combines the\nstrength of LLM and conventional software. Diverse LLM applications from\ndifferent tenants could design complex workflows using multiple LLM requests to\naccomplish one task. However, they have to use the over-simplified\nrequest-level API provided by today's public LLM services, losing essential\napplication-level information. Public LLM services have to blindly optimize\nindividual LLM requests, leading to sub-optimal end-to-end performance of LLM\napplications.\n  This paper introduces Parrot, an LLM service system that focuses on the\nend-to-end experience of LLM-based applications. Parrot proposes Semantic\nVariable, a unified abstraction to expose application-level knowledge to public\nLLM services. A Semantic Variable annotates an input/output variable in the\nprompt of a request, and creates the data pipeline when connecting multiple LLM\nrequests, providing a natural way to program LLM applications. Exposing\nSemantic Variables to the public LLM service allows it to perform conventional\ndata flow analysis to uncover the correlation across multiple LLM requests.\nThis correlation opens a brand-new optimization space for the end-to-end\nperformance of LLM-based applications. Extensive evaluations demonstrate that\nParrot can achieve up to an order-of-magnitude improvement for popular and\npractical use cases of LLM applications."}, "hash": "6d27c62bc3553a2813aa6b49fb76e63b502e7003ab1f3ec0542dd9b07dbd8978", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "755a9e00-39bd-4a57-8c6f-24ab452a0f7a", "node_type": "1", "metadata": {"page_label": "9", "file_name": "2405_19888v1.pdf", "Title of this paper": "Parrot: Efficient Serving of LLM-based Applications with Semantic Variable", "Authors": "Chaofan Lin, Zhenhua Han, Chengruidong Zhang, Yuqing Yang, Fan Yang, Chen Chen, Lili Qiu", "Date published": "05/30/2024", "URL": "http://arxiv.org/abs/2405.19888v1", "summary": "The rise of large language models (LLMs) has enabled LLM-based applications\n(a.k.a. AI agents or co-pilots), a new software paradigm that combines the\nstrength of LLM and conventional software. Diverse LLM applications from\ndifferent tenants could design complex workflows using multiple LLM requests to\naccomplish one task. However, they have to use the over-simplified\nrequest-level API provided by today's public LLM services, losing essential\napplication-level information. Public LLM services have to blindly optimize\nindividual LLM requests, leading to sub-optimal end-to-end performance of LLM\napplications.\n  This paper introduces Parrot, an LLM service system that focuses on the\nend-to-end experience of LLM-based applications. Parrot proposes Semantic\nVariable, a unified abstraction to expose application-level knowledge to public\nLLM services. A Semantic Variable annotates an input/output variable in the\nprompt of a request, and creates the data pipeline when connecting multiple LLM\nrequests, providing a natural way to program LLM applications. Exposing\nSemantic Variables to the public LLM service allows it to perform conventional\ndata flow analysis to uncover the correlation across multiple LLM requests.\nThis correlation opens a brand-new optimization space for the end-to-end\nperformance of LLM-based applications. Extensive evaluations demonstrate that\nParrot can achieve up to an order-of-magnitude improvement for popular and\npractical use cases of LLM applications."}, "hash": "7ceed37e38e1eee03d34a7ed1c48ee978bf7bc61966b0a976b32c069b0f715e0", "class_name": "RelatedNodeInfo"}}, "text": "The default scheduling\nstrategy employed by FastChat assigns incoming requests\nto the LLM engine with the smallest current queue. Since\nexisting LLM services typically expose their functionality\nthrough \"chat\" completion APIs, baseline assessments treat\nall requests as independent and assume a high sensitivity to\nlatency. To manage token generation response times, each\nLLM engine is subject to a capacity threshold, which is the\naggregate token count from all active requests on the engine.\nSince existing LLM token generation is usually bound by\nmemory bandwidth, the per-token generation latency of an\nengine is mainly affected by the number of running tokens in\na batch. As depicted in Figure 10, our experiments indicate\nthat the latency per output token, i.e. TPOT (Time-per-output-\ntoken) for vLLM, with continuous batching enabled, experi-\nences a notable uptick when the engine\u2019s workload using a\nbatch capacity beyond 6144. In our evaluation, we use the\nsetting that an LLM engine can keep its generation latency\nunder 40 ms/s for latency-sensitive requests, consistent with\nour experience of OpenAI\u2019s LLM services. When all LLM\nengines hit their maximum capacity, any additional LLM re-\nquests are queued in a FIFO (First In, First Out) manner,\nawaiting the completion and release of resources by ongoing\ntasks. Serving longer context (e.g., 32k or even 1M tokens)\nwithin a satisfactory latency require either more GPUs using\ntensor-parallel [49] or sequence-parallel [6] approaches, or\napproximate attention (e.g., StreamingLLM [55]), which is\nbeyond the scope of this paper.", "mimetype": "text/plain", "start_char_idx": 3525, "end_char_idx": 5114, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "372d39f1-c991-4a83-9902-7961458c59b8": {"__data__": {"id_": "372d39f1-c991-4a83-9902-7961458c59b8", "embedding": null, "metadata": {"page_label": "10", "file_name": "2405_19888v1.pdf", "Title of this paper": "Parrot: Efficient Serving of LLM-based Applications with Semantic Variable", "Authors": "Chaofan Lin, Zhenhua Han, Chengruidong Zhang, Yuqing Yang, Fan Yang, Chen Chen, Lili Qiu", "Date published": "05/30/2024", "URL": "http://arxiv.org/abs/2405.19888v1", "summary": "The rise of large language models (LLMs) has enabled LLM-based applications\n(a.k.a. AI agents or co-pilots), a new software paradigm that combines the\nstrength of LLM and conventional software. Diverse LLM applications from\ndifferent tenants could design complex workflows using multiple LLM requests to\naccomplish one task. However, they have to use the over-simplified\nrequest-level API provided by today's public LLM services, losing essential\napplication-level information. Public LLM services have to blindly optimize\nindividual LLM requests, leading to sub-optimal end-to-end performance of LLM\napplications.\n  This paper introduces Parrot, an LLM service system that focuses on the\nend-to-end experience of LLM-based applications. Parrot proposes Semantic\nVariable, a unified abstraction to expose application-level knowledge to public\nLLM services. A Semantic Variable annotates an input/output variable in the\nprompt of a request, and creates the data pipeline when connecting multiple LLM\nrequests, providing a natural way to program LLM applications. Exposing\nSemantic Variables to the public LLM service allows it to perform conventional\ndata flow analysis to uncover the correlation across multiple LLM requests.\nThis correlation opens a brand-new optimization space for the end-to-end\nperformance of LLM-based applications. Extensive evaluations demonstrate that\nParrot can achieve up to an order-of-magnitude improvement for popular and\npractical use cases of LLM applications."}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "27c11532-acdd-45ca-8762-8191c0f96067", "node_type": "4", "metadata": {"page_label": "10", "file_name": "2405_19888v1.pdf", "Title of this paper": "Parrot: Efficient Serving of LLM-based Applications with Semantic Variable", "Authors": "Chaofan Lin, Zhenhua Han, Chengruidong Zhang, Yuqing Yang, Fan Yang, Chen Chen, Lili Qiu", "Date published": "05/30/2024", "URL": "http://arxiv.org/abs/2405.19888v1", "summary": "The rise of large language models (LLMs) has enabled LLM-based applications\n(a.k.a. AI agents or co-pilots), a new software paradigm that combines the\nstrength of LLM and conventional software. Diverse LLM applications from\ndifferent tenants could design complex workflows using multiple LLM requests to\naccomplish one task. However, they have to use the over-simplified\nrequest-level API provided by today's public LLM services, losing essential\napplication-level information. Public LLM services have to blindly optimize\nindividual LLM requests, leading to sub-optimal end-to-end performance of LLM\napplications.\n  This paper introduces Parrot, an LLM service system that focuses on the\nend-to-end experience of LLM-based applications. Parrot proposes Semantic\nVariable, a unified abstraction to expose application-level knowledge to public\nLLM services. A Semantic Variable annotates an input/output variable in the\nprompt of a request, and creates the data pipeline when connecting multiple LLM\nrequests, providing a natural way to program LLM applications. Exposing\nSemantic Variables to the public LLM service allows it to perform conventional\ndata flow analysis to uncover the correlation across multiple LLM requests.\nThis correlation opens a brand-new optimization space for the end-to-end\nperformance of LLM-based applications. Extensive evaluations demonstrate that\nParrot can achieve up to an order-of-magnitude improvement for popular and\npractical use cases of LLM applications."}, "hash": "7bca58777514206e188f5aec5b3be4c826be14d0bd30a9670a8dcc6a2479fcba", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "767437ef-ae0e-46fb-a914-52e3a8852009", "node_type": "1", "metadata": {}, "hash": "e8449f1d95f94a42ec1dff49cfd256678970ba340663aafde6c4dce7f677f985", "class_name": "RelatedNodeInfo"}}, "text": "25 50 75 100\nOutput Length (# tokens)050100150200250Average Latency (s)1.38x1.21x1.14x1.11x\n1.88x1.64x1.55x1.52x Parrot\nBaseline (vLLM)\nBaseline (HuggingFace)(a) Output lengths\n512 1024 1536 2048\nChunk Size (# tokens)050100150200250Average Latency (s)1.21x\n1.21x\n1.20x\n1.19x1.63x\n1.62x\n1.60x\n1.61xParrot\nBaseline (vLLM)\nBaseline (HuggingFace) (b) Chunk sizes\nFigure 11: Average E2E latency of chain summarization with\nvarying output lengths and chunk sizes.\n8.2 Data Analytics on Long Documents\nOur experimental analysis within data analytics randomly\npicks ten long documents from the Arxiv-March dataset [27],\nusing chain-summary and map-reduce summary. Each docu-\nment has over 20,000 tokens. The results measures the mean\nend-to-end latency across all documents.\nChain-style Applications. Our evaluation demonstrates\nhow Parrot enhances chain summarization by mitigating the\nexcessive communication overhead stemming from client in-\nteractions. Figure 11 presents the average end-to-end latency\nfor summarizing a single document using one LLM engine\n(A100, LLaMA 13B) . We adjust the chunk size (the count of\ntokens per chunk) and the output length, with results shown in\nFigure 11a and Figure 11b, respectively. Parrot achieves a re-\nduction in end-to-end latency by as much as 1.38\u00d7and1.88\u00d7\ncompared to the baselines employing vLLM and Hugging-\nFace, respectively. The efficiency of Parrot primarily stems\nfrom the decreased network latency, which is a consequence\nof reduced client interaction. As the output length increases,\nthe time spent on generation becomes more significant, lead-\ning to a diminishing advantage for Parrot over the baseline. By\nincreasing the chunk size, we decrease the number of chunks,\nyet the extent of the speedup is contingent upon the network\nlatency savings for each chunk. Given that token generation is\nsubstantially more time-consuming than prompt processing,\nwe observe a consistent speedup with variable chunk sizes\nand a fixed output length ( 1.2\u00d7and1.66\u00d7relative to vLLM\nand HuggingFace, respectively). This indicates that Parrot\u2019s\noptimization for dependent LLM requests is particularly bene-\nficial for shorter outputs, which are prevalent in various LLM\napplications such as summarization, short answer generation,\nscoring, and choice provision.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2294, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "767437ef-ae0e-46fb-a914-52e3a8852009": {"__data__": {"id_": "767437ef-ae0e-46fb-a914-52e3a8852009", "embedding": null, "metadata": {"page_label": "10", "file_name": "2405_19888v1.pdf", "Title of this paper": "Parrot: Efficient Serving of LLM-based Applications with Semantic Variable", "Authors": "Chaofan Lin, Zhenhua Han, Chengruidong Zhang, Yuqing Yang, Fan Yang, Chen Chen, Lili Qiu", "Date published": "05/30/2024", "URL": "http://arxiv.org/abs/2405.19888v1", "summary": "The rise of large language models (LLMs) has enabled LLM-based applications\n(a.k.a. AI agents or co-pilots), a new software paradigm that combines the\nstrength of LLM and conventional software. Diverse LLM applications from\ndifferent tenants could design complex workflows using multiple LLM requests to\naccomplish one task. However, they have to use the over-simplified\nrequest-level API provided by today's public LLM services, losing essential\napplication-level information. Public LLM services have to blindly optimize\nindividual LLM requests, leading to sub-optimal end-to-end performance of LLM\napplications.\n  This paper introduces Parrot, an LLM service system that focuses on the\nend-to-end experience of LLM-based applications. Parrot proposes Semantic\nVariable, a unified abstraction to expose application-level knowledge to public\nLLM services. A Semantic Variable annotates an input/output variable in the\nprompt of a request, and creates the data pipeline when connecting multiple LLM\nrequests, providing a natural way to program LLM applications. Exposing\nSemantic Variables to the public LLM service allows it to perform conventional\ndata flow analysis to uncover the correlation across multiple LLM requests.\nThis correlation opens a brand-new optimization space for the end-to-end\nperformance of LLM-based applications. Extensive evaluations demonstrate that\nParrot can achieve up to an order-of-magnitude improvement for popular and\npractical use cases of LLM applications."}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "27c11532-acdd-45ca-8762-8191c0f96067", "node_type": "4", "metadata": {"page_label": "10", "file_name": "2405_19888v1.pdf", "Title of this paper": "Parrot: Efficient Serving of LLM-based Applications with Semantic Variable", "Authors": "Chaofan Lin, Zhenhua Han, Chengruidong Zhang, Yuqing Yang, Fan Yang, Chen Chen, Lili Qiu", "Date published": "05/30/2024", "URL": "http://arxiv.org/abs/2405.19888v1", "summary": "The rise of large language models (LLMs) has enabled LLM-based applications\n(a.k.a. AI agents or co-pilots), a new software paradigm that combines the\nstrength of LLM and conventional software. Diverse LLM applications from\ndifferent tenants could design complex workflows using multiple LLM requests to\naccomplish one task. However, they have to use the over-simplified\nrequest-level API provided by today's public LLM services, losing essential\napplication-level information. Public LLM services have to blindly optimize\nindividual LLM requests, leading to sub-optimal end-to-end performance of LLM\napplications.\n  This paper introduces Parrot, an LLM service system that focuses on the\nend-to-end experience of LLM-based applications. Parrot proposes Semantic\nVariable, a unified abstraction to expose application-level knowledge to public\nLLM services. A Semantic Variable annotates an input/output variable in the\nprompt of a request, and creates the data pipeline when connecting multiple LLM\nrequests, providing a natural way to program LLM applications. Exposing\nSemantic Variables to the public LLM service allows it to perform conventional\ndata flow analysis to uncover the correlation across multiple LLM requests.\nThis correlation opens a brand-new optimization space for the end-to-end\nperformance of LLM-based applications. Extensive evaluations demonstrate that\nParrot can achieve up to an order-of-magnitude improvement for popular and\npractical use cases of LLM applications."}, "hash": "7bca58777514206e188f5aec5b3be4c826be14d0bd30a9670a8dcc6a2479fcba", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "372d39f1-c991-4a83-9902-7961458c59b8", "node_type": "1", "metadata": {"page_label": "10", "file_name": "2405_19888v1.pdf", "Title of this paper": "Parrot: Efficient Serving of LLM-based Applications with Semantic Variable", "Authors": "Chaofan Lin, Zhenhua Han, Chengruidong Zhang, Yuqing Yang, Fan Yang, Chen Chen, Lili Qiu", "Date published": "05/30/2024", "URL": "http://arxiv.org/abs/2405.19888v1", "summary": "The rise of large language models (LLMs) has enabled LLM-based applications\n(a.k.a. AI agents or co-pilots), a new software paradigm that combines the\nstrength of LLM and conventional software. Diverse LLM applications from\ndifferent tenants could design complex workflows using multiple LLM requests to\naccomplish one task. However, they have to use the over-simplified\nrequest-level API provided by today's public LLM services, losing essential\napplication-level information. Public LLM services have to blindly optimize\nindividual LLM requests, leading to sub-optimal end-to-end performance of LLM\napplications.\n  This paper introduces Parrot, an LLM service system that focuses on the\nend-to-end experience of LLM-based applications. Parrot proposes Semantic\nVariable, a unified abstraction to expose application-level knowledge to public\nLLM services. A Semantic Variable annotates an input/output variable in the\nprompt of a request, and creates the data pipeline when connecting multiple LLM\nrequests, providing a natural way to program LLM applications. Exposing\nSemantic Variables to the public LLM service allows it to perform conventional\ndata flow analysis to uncover the correlation across multiple LLM requests.\nThis correlation opens a brand-new optimization space for the end-to-end\nperformance of LLM-based applications. Extensive evaluations demonstrate that\nParrot can achieve up to an order-of-magnitude improvement for popular and\npractical use cases of LLM applications."}, "hash": "6592ab5948277bd3dac9a8594fd008c50df8f86432d14afcdd1346ef6fce422d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0bef3e4c-6a41-4526-ba13-b8d846b954b9", "node_type": "1", "metadata": {}, "hash": "cb47d221d5d27f4151ea48af1b67bd24726cbd83842edb2de246b22b49f79606", "class_name": "RelatedNodeInfo"}}, "text": "The efficiency of Parrot primarily stems\nfrom the decreased network latency, which is a consequence\nof reduced client interaction. As the output length increases,\nthe time spent on generation becomes more significant, lead-\ning to a diminishing advantage for Parrot over the baseline. By\nincreasing the chunk size, we decrease the number of chunks,\nyet the extent of the speedup is contingent upon the network\nlatency savings for each chunk. Given that token generation is\nsubstantially more time-consuming than prompt processing,\nwe observe a consistent speedup with variable chunk sizes\nand a fixed output length ( 1.2\u00d7and1.66\u00d7relative to vLLM\nand HuggingFace, respectively). This indicates that Parrot\u2019s\noptimization for dependent LLM requests is particularly bene-\nficial for shorter outputs, which are prevalent in various LLM\napplications such as summarization, short answer generation,\nscoring, and choice provision. Due to HuggingFace\u2019s slower\nperformance relative to vLLM, subsequent evaluations focus\nsolely on the comparison between Parrot and vLLM.\nFigure 12a extends the evaluation by introducing back-\nground LLM requests at varying rates to examine the capa-\nbility of Parrot in mitigating additional queuing delays for\ndependent requests. Parrot slashes the end-to-end latency by a\nfactor of 2.38\u00d7in comparison to the baseline (vLLM). With\nParrot, as soon as the summary for the first chunk is completed,\n0.00.51.01.52.02.53.03.5\nRequest Rate (reqs/s)50100150200250Average Latency (s)\n1.21x1.19x1.31x1.79x2.38xParrot\nBaseline (vLLM)(a) With background requests\n10 15 20 25\nNumber of Apps0100200300Average Latency (s)1.38x1.52x1.63x1.68xParrot\nBaseline (vLLM) (b) Multiple summary apps.\nFigure 12: Average E2E latency of chain-summary with back-\nground requests or other chain-summary applications.\n12345678910111213141516171819202122232425\nApplication No.050100150200250Latency in Baseline - Latency in Parrot (s)\nFigure 13: The difference in E2E latency of the 25 chain-\nsummary application between Baseline and Parrot. All appli-\ncations finish earlier in Parrot.\nthe subsequent chunk is processed immediately by incorporat-\ning the summaries of previous chunks into the prompt, which\naids in generating the summary for the next chunk. In con-\ntrast, the baseline treats all LLM requests individually. As a\nresult, in addition to the network latency from client interac-\ntions, subsequent requests must re-enter the queue, leading\nto added queuing delays.", "mimetype": "text/plain", "start_char_idx": 1371, "end_char_idx": 3844, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0bef3e4c-6a41-4526-ba13-b8d846b954b9": {"__data__": {"id_": "0bef3e4c-6a41-4526-ba13-b8d846b954b9", "embedding": null, "metadata": {"page_label": "10", "file_name": "2405_19888v1.pdf", "Title of this paper": "Parrot: Efficient Serving of LLM-based Applications with Semantic Variable", "Authors": "Chaofan Lin, Zhenhua Han, Chengruidong Zhang, Yuqing Yang, Fan Yang, Chen Chen, Lili Qiu", "Date published": "05/30/2024", "URL": "http://arxiv.org/abs/2405.19888v1", "summary": "The rise of large language models (LLMs) has enabled LLM-based applications\n(a.k.a. AI agents or co-pilots), a new software paradigm that combines the\nstrength of LLM and conventional software. Diverse LLM applications from\ndifferent tenants could design complex workflows using multiple LLM requests to\naccomplish one task. However, they have to use the over-simplified\nrequest-level API provided by today's public LLM services, losing essential\napplication-level information. Public LLM services have to blindly optimize\nindividual LLM requests, leading to sub-optimal end-to-end performance of LLM\napplications.\n  This paper introduces Parrot, an LLM service system that focuses on the\nend-to-end experience of LLM-based applications. Parrot proposes Semantic\nVariable, a unified abstraction to expose application-level knowledge to public\nLLM services. A Semantic Variable annotates an input/output variable in the\nprompt of a request, and creates the data pipeline when connecting multiple LLM\nrequests, providing a natural way to program LLM applications. Exposing\nSemantic Variables to the public LLM service allows it to perform conventional\ndata flow analysis to uncover the correlation across multiple LLM requests.\nThis correlation opens a brand-new optimization space for the end-to-end\nperformance of LLM-based applications. Extensive evaluations demonstrate that\nParrot can achieve up to an order-of-magnitude improvement for popular and\npractical use cases of LLM applications."}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "27c11532-acdd-45ca-8762-8191c0f96067", "node_type": "4", "metadata": {"page_label": "10", "file_name": "2405_19888v1.pdf", "Title of this paper": "Parrot: Efficient Serving of LLM-based Applications with Semantic Variable", "Authors": "Chaofan Lin, Zhenhua Han, Chengruidong Zhang, Yuqing Yang, Fan Yang, Chen Chen, Lili Qiu", "Date published": "05/30/2024", "URL": "http://arxiv.org/abs/2405.19888v1", "summary": "The rise of large language models (LLMs) has enabled LLM-based applications\n(a.k.a. AI agents or co-pilots), a new software paradigm that combines the\nstrength of LLM and conventional software. Diverse LLM applications from\ndifferent tenants could design complex workflows using multiple LLM requests to\naccomplish one task. However, they have to use the over-simplified\nrequest-level API provided by today's public LLM services, losing essential\napplication-level information. Public LLM services have to blindly optimize\nindividual LLM requests, leading to sub-optimal end-to-end performance of LLM\napplications.\n  This paper introduces Parrot, an LLM service system that focuses on the\nend-to-end experience of LLM-based applications. Parrot proposes Semantic\nVariable, a unified abstraction to expose application-level knowledge to public\nLLM services. A Semantic Variable annotates an input/output variable in the\nprompt of a request, and creates the data pipeline when connecting multiple LLM\nrequests, providing a natural way to program LLM applications. Exposing\nSemantic Variables to the public LLM service allows it to perform conventional\ndata flow analysis to uncover the correlation across multiple LLM requests.\nThis correlation opens a brand-new optimization space for the end-to-end\nperformance of LLM-based applications. Extensive evaluations demonstrate that\nParrot can achieve up to an order-of-magnitude improvement for popular and\npractical use cases of LLM applications."}, "hash": "7bca58777514206e188f5aec5b3be4c826be14d0bd30a9670a8dcc6a2479fcba", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "767437ef-ae0e-46fb-a914-52e3a8852009", "node_type": "1", "metadata": {"page_label": "10", "file_name": "2405_19888v1.pdf", "Title of this paper": "Parrot: Efficient Serving of LLM-based Applications with Semantic Variable", "Authors": "Chaofan Lin, Zhenhua Han, Chengruidong Zhang, Yuqing Yang, Fan Yang, Chen Chen, Lili Qiu", "Date published": "05/30/2024", "URL": "http://arxiv.org/abs/2405.19888v1", "summary": "The rise of large language models (LLMs) has enabled LLM-based applications\n(a.k.a. AI agents or co-pilots), a new software paradigm that combines the\nstrength of LLM and conventional software. Diverse LLM applications from\ndifferent tenants could design complex workflows using multiple LLM requests to\naccomplish one task. However, they have to use the over-simplified\nrequest-level API provided by today's public LLM services, losing essential\napplication-level information. Public LLM services have to blindly optimize\nindividual LLM requests, leading to sub-optimal end-to-end performance of LLM\napplications.\n  This paper introduces Parrot, an LLM service system that focuses on the\nend-to-end experience of LLM-based applications. Parrot proposes Semantic\nVariable, a unified abstraction to expose application-level knowledge to public\nLLM services. A Semantic Variable annotates an input/output variable in the\nprompt of a request, and creates the data pipeline when connecting multiple LLM\nrequests, providing a natural way to program LLM applications. Exposing\nSemantic Variables to the public LLM service allows it to perform conventional\ndata flow analysis to uncover the correlation across multiple LLM requests.\nThis correlation opens a brand-new optimization space for the end-to-end\nperformance of LLM-based applications. Extensive evaluations demonstrate that\nParrot can achieve up to an order-of-magnitude improvement for popular and\npractical use cases of LLM applications."}, "hash": "ec00bb1a48a3cec1fe56857ad4416769c3d0ebe3deb5f6659ab107caa3d6ccbc", "class_name": "RelatedNodeInfo"}}, "text": "Figure 12: Average E2E latency of chain-summary with back-\nground requests or other chain-summary applications.\n12345678910111213141516171819202122232425\nApplication No.050100150200250Latency in Baseline - Latency in Parrot (s)\nFigure 13: The difference in E2E latency of the 25 chain-\nsummary application between Baseline and Parrot. All appli-\ncations finish earlier in Parrot.\nthe subsequent chunk is processed immediately by incorporat-\ning the summaries of previous chunks into the prompt, which\naids in generating the summary for the next chunk. In con-\ntrast, the baseline treats all LLM requests individually. As a\nresult, in addition to the network latency from client interac-\ntions, subsequent requests must re-enter the queue, leading\nto added queuing delays. Figure 12b further illustrates the\nend-to-end latency when multiple chain-summary applica-\ntions are submitted concurrently, with each application tasked\nwith generating a summary for a separate document. Parrot\nmanages to reduce the average end-to-end latency for all ap-\nplications by 1.68\u00d7without slowing down any applications\ncompared to the baseline according to Figure 13. The base-\nline, by interleaving the execution of different applications,\nexacerbates the slowdown of the end-to-end latency for all\napplications. These experiments validate that recognizing the\ninterconnections of LLM requests can significantly enhance\nend-to-end performance, as opposed to processing requests\nin isolation.\nMap-Reduce Applications. An alternative implementation\nof the document summarization application follows the map-\nreduce paradigm as depicted in Figure 1a. This approach\nconsists of multiple parallel mapping LLM requests, where\neach request summarizes a distinct segment of the document,\nfollowed by a reducing LLM request that aggregates these\nindividual summaries into a final summary. As shown in\nFigure 14, Parrot realizes a 2.37\u00d7acceleration over the base-", "mimetype": "text/plain", "start_char_idx": 3073, "end_char_idx": 5010, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5329a245-1753-4c4e-b58c-d9a84b28bd20": {"__data__": {"id_": "5329a245-1753-4c4e-b58c-d9a84b28bd20", "embedding": null, "metadata": {"page_label": "11", "file_name": "2405_19888v1.pdf", "Title of this paper": "Parrot: Efficient Serving of LLM-based Applications with Semantic Variable", "Authors": "Chaofan Lin, Zhenhua Han, Chengruidong Zhang, Yuqing Yang, Fan Yang, Chen Chen, Lili Qiu", "Date published": "05/30/2024", "URL": "http://arxiv.org/abs/2405.19888v1", "summary": "The rise of large language models (LLMs) has enabled LLM-based applications\n(a.k.a. AI agents or co-pilots), a new software paradigm that combines the\nstrength of LLM and conventional software. Diverse LLM applications from\ndifferent tenants could design complex workflows using multiple LLM requests to\naccomplish one task. However, they have to use the over-simplified\nrequest-level API provided by today's public LLM services, losing essential\napplication-level information. Public LLM services have to blindly optimize\nindividual LLM requests, leading to sub-optimal end-to-end performance of LLM\napplications.\n  This paper introduces Parrot, an LLM service system that focuses on the\nend-to-end experience of LLM-based applications. Parrot proposes Semantic\nVariable, a unified abstraction to expose application-level knowledge to public\nLLM services. A Semantic Variable annotates an input/output variable in the\nprompt of a request, and creates the data pipeline when connecting multiple LLM\nrequests, providing a natural way to program LLM applications. Exposing\nSemantic Variables to the public LLM service allows it to perform conventional\ndata flow analysis to uncover the correlation across multiple LLM requests.\nThis correlation opens a brand-new optimization space for the end-to-end\nperformance of LLM-based applications. Extensive evaluations demonstrate that\nParrot can achieve up to an order-of-magnitude improvement for popular and\npractical use cases of LLM applications."}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f799a209-ea73-44e2-8183-2da4ea8ee984", "node_type": "4", "metadata": {"page_label": "11", "file_name": "2405_19888v1.pdf", "Title of this paper": "Parrot: Efficient Serving of LLM-based Applications with Semantic Variable", "Authors": "Chaofan Lin, Zhenhua Han, Chengruidong Zhang, Yuqing Yang, Fan Yang, Chen Chen, Lili Qiu", "Date published": "05/30/2024", "URL": "http://arxiv.org/abs/2405.19888v1", "summary": "The rise of large language models (LLMs) has enabled LLM-based applications\n(a.k.a. AI agents or co-pilots), a new software paradigm that combines the\nstrength of LLM and conventional software. Diverse LLM applications from\ndifferent tenants could design complex workflows using multiple LLM requests to\naccomplish one task. However, they have to use the over-simplified\nrequest-level API provided by today's public LLM services, losing essential\napplication-level information. Public LLM services have to blindly optimize\nindividual LLM requests, leading to sub-optimal end-to-end performance of LLM\napplications.\n  This paper introduces Parrot, an LLM service system that focuses on the\nend-to-end experience of LLM-based applications. Parrot proposes Semantic\nVariable, a unified abstraction to expose application-level knowledge to public\nLLM services. A Semantic Variable annotates an input/output variable in the\nprompt of a request, and creates the data pipeline when connecting multiple LLM\nrequests, providing a natural way to program LLM applications. Exposing\nSemantic Variables to the public LLM service allows it to perform conventional\ndata flow analysis to uncover the correlation across multiple LLM requests.\nThis correlation opens a brand-new optimization space for the end-to-end\nperformance of LLM-based applications. Extensive evaluations demonstrate that\nParrot can achieve up to an order-of-magnitude improvement for popular and\npractical use cases of LLM applications."}, "hash": "9e07a69475878b78b374425df72d68abc0ea3b749d87c9ea002fb336c7824d2d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "de0ac9e9-a8e5-456a-8363-0b4dd995a53c", "node_type": "1", "metadata": {}, "hash": "a3dc6b55c6591f27ad80df80aecb134f342f72aba1f75d755fbc16f1682e7180", "class_name": "RelatedNodeInfo"}}, "text": "25 50 75 100\nOutput Length (# tokens)010203040Average Latency (s)1.70x2.04x2.22x2.37xParrot\nBaseline (vLLM)(a) Output lengths\n512 1024 1536 2048\nChunk Size (# tokens)0102030Average Latency (s)1.96x\n2.07x2.07x 2.16xParrot\nBaseline (vLLM) (b) Chunk sizes\nFigure 14: Average E2E latency of Map-Reduce document\nsummary with varying output lengths and chunk sizes.\nline with one LLM engine (A100, LLaMA 13B). Since the\nmapping LLM requests are independent, they are dispatched\nconcurrently by both Parrot and the baseline. The primary ad-\nvantage of Parrot stems from its deduction of a performance\nobjective that identifies the mapping tasks as a task group.\nBy recognizing this relationship, Parrot is capable of optimiz-\ning the latency of the entire task group through larger batch\nsizes, which in turn enhances throughput. In contrast, the\nbaseline processes each LLM request in isolation, operating\nunder the presumption that they are all sensitive to latency.\nThis constrains the baseline to utilize a limited token capacity\n(4096 tokens) on the LLM engine to achieve optimal latency\nfor individual tasks, which is detrimental to the end-to-end\nperformance of applications. It underscores the necessity for\nLLM services to distinguish LLM requests to optimize the\nend-to-end performance of varied LLM applications.\n8.3 Serving Popular LLM Applications\nProduction applications need to face massive users. As ex-\nplained in Figure 5, developers often need to use a very long\nsystem prompt to define the behavior of LLMs. Therefore,\nusers of the same LLM application often use the shared\nprompt, which can benefit from Parrot\u2019s context fork mech-\nanism and Parrot\u2019s scheduling policy that co-locates LLM\nrequests sharing a long prompt prefix. Because we do not\nhave access to the intermediate steps of Bing Copilot, we only\nevaluate the final request generating the response to users.\nWe synthesized 64 requests from the length distribution we\nmeasured using Bing Copilot. The system prompt length is\nabout 6000 tokens. The output lengths ranges from 180 to\n800 tokens. Figure 15 shows the average request latency of\nBing Copilot of Parrot and the baselines. Because the LLM\nservice in the baseline system does not know the prompt struc-\nture, it is hard to infer the shared prompt from massive LLM\nrequests. Compared to the baseline without sharing prompt,\nParrot achieves 1.8\u00d7 \u223c 2.4\u00d7speedup for batch sizes of 8 and\n16. Further increasing the batch size leads to out-of-memory\ndue to the massive KV cache of shared system prompt.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2530, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "de0ac9e9-a8e5-456a-8363-0b4dd995a53c": {"__data__": {"id_": "de0ac9e9-a8e5-456a-8363-0b4dd995a53c", "embedding": null, "metadata": {"page_label": "11", "file_name": "2405_19888v1.pdf", "Title of this paper": "Parrot: Efficient Serving of LLM-based Applications with Semantic Variable", "Authors": "Chaofan Lin, Zhenhua Han, Chengruidong Zhang, Yuqing Yang, Fan Yang, Chen Chen, Lili Qiu", "Date published": "05/30/2024", "URL": "http://arxiv.org/abs/2405.19888v1", "summary": "The rise of large language models (LLMs) has enabled LLM-based applications\n(a.k.a. AI agents or co-pilots), a new software paradigm that combines the\nstrength of LLM and conventional software. Diverse LLM applications from\ndifferent tenants could design complex workflows using multiple LLM requests to\naccomplish one task. However, they have to use the over-simplified\nrequest-level API provided by today's public LLM services, losing essential\napplication-level information. Public LLM services have to blindly optimize\nindividual LLM requests, leading to sub-optimal end-to-end performance of LLM\napplications.\n  This paper introduces Parrot, an LLM service system that focuses on the\nend-to-end experience of LLM-based applications. Parrot proposes Semantic\nVariable, a unified abstraction to expose application-level knowledge to public\nLLM services. A Semantic Variable annotates an input/output variable in the\nprompt of a request, and creates the data pipeline when connecting multiple LLM\nrequests, providing a natural way to program LLM applications. Exposing\nSemantic Variables to the public LLM service allows it to perform conventional\ndata flow analysis to uncover the correlation across multiple LLM requests.\nThis correlation opens a brand-new optimization space for the end-to-end\nperformance of LLM-based applications. Extensive evaluations demonstrate that\nParrot can achieve up to an order-of-magnitude improvement for popular and\npractical use cases of LLM applications."}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f799a209-ea73-44e2-8183-2da4ea8ee984", "node_type": "4", "metadata": {"page_label": "11", "file_name": "2405_19888v1.pdf", "Title of this paper": "Parrot: Efficient Serving of LLM-based Applications with Semantic Variable", "Authors": "Chaofan Lin, Zhenhua Han, Chengruidong Zhang, Yuqing Yang, Fan Yang, Chen Chen, Lili Qiu", "Date published": "05/30/2024", "URL": "http://arxiv.org/abs/2405.19888v1", "summary": "The rise of large language models (LLMs) has enabled LLM-based applications\n(a.k.a. AI agents or co-pilots), a new software paradigm that combines the\nstrength of LLM and conventional software. Diverse LLM applications from\ndifferent tenants could design complex workflows using multiple LLM requests to\naccomplish one task. However, they have to use the over-simplified\nrequest-level API provided by today's public LLM services, losing essential\napplication-level information. Public LLM services have to blindly optimize\nindividual LLM requests, leading to sub-optimal end-to-end performance of LLM\napplications.\n  This paper introduces Parrot, an LLM service system that focuses on the\nend-to-end experience of LLM-based applications. Parrot proposes Semantic\nVariable, a unified abstraction to expose application-level knowledge to public\nLLM services. A Semantic Variable annotates an input/output variable in the\nprompt of a request, and creates the data pipeline when connecting multiple LLM\nrequests, providing a natural way to program LLM applications. Exposing\nSemantic Variables to the public LLM service allows it to perform conventional\ndata flow analysis to uncover the correlation across multiple LLM requests.\nThis correlation opens a brand-new optimization space for the end-to-end\nperformance of LLM-based applications. Extensive evaluations demonstrate that\nParrot can achieve up to an order-of-magnitude improvement for popular and\npractical use cases of LLM applications."}, "hash": "9e07a69475878b78b374425df72d68abc0ea3b749d87c9ea002fb336c7824d2d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5329a245-1753-4c4e-b58c-d9a84b28bd20", "node_type": "1", "metadata": {"page_label": "11", "file_name": "2405_19888v1.pdf", "Title of this paper": "Parrot: Efficient Serving of LLM-based Applications with Semantic Variable", "Authors": "Chaofan Lin, Zhenhua Han, Chengruidong Zhang, Yuqing Yang, Fan Yang, Chen Chen, Lili Qiu", "Date published": "05/30/2024", "URL": "http://arxiv.org/abs/2405.19888v1", "summary": "The rise of large language models (LLMs) has enabled LLM-based applications\n(a.k.a. AI agents or co-pilots), a new software paradigm that combines the\nstrength of LLM and conventional software. Diverse LLM applications from\ndifferent tenants could design complex workflows using multiple LLM requests to\naccomplish one task. However, they have to use the over-simplified\nrequest-level API provided by today's public LLM services, losing essential\napplication-level information. Public LLM services have to blindly optimize\nindividual LLM requests, leading to sub-optimal end-to-end performance of LLM\napplications.\n  This paper introduces Parrot, an LLM service system that focuses on the\nend-to-end experience of LLM-based applications. Parrot proposes Semantic\nVariable, a unified abstraction to expose application-level knowledge to public\nLLM services. A Semantic Variable annotates an input/output variable in the\nprompt of a request, and creates the data pipeline when connecting multiple LLM\nrequests, providing a natural way to program LLM applications. Exposing\nSemantic Variables to the public LLM service allows it to perform conventional\ndata flow analysis to uncover the correlation across multiple LLM requests.\nThis correlation opens a brand-new optimization space for the end-to-end\nperformance of LLM-based applications. Extensive evaluations demonstrate that\nParrot can achieve up to an order-of-magnitude improvement for popular and\npractical use cases of LLM applications."}, "hash": "1f9accb5da2091debe93c53627ddd2fd4813342f79e0823ec89f5e67c897eba1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ebb694d8-b24b-4e15-a1fb-9cb36ee41f2c", "node_type": "1", "metadata": {}, "hash": "5c73eadd67e43ffa4868ff006ddcf51649e990d8f651d8f700c8ae6b991af7b7", "class_name": "RelatedNodeInfo"}}, "text": "Because we do not\nhave access to the intermediate steps of Bing Copilot, we only\nevaluate the final request generating the response to users.\nWe synthesized 64 requests from the length distribution we\nmeasured using Bing Copilot. The system prompt length is\nabout 6000 tokens. The output lengths ranges from 180 to\n800 tokens. Figure 15 shows the average request latency of\nBing Copilot of Parrot and the baselines. Because the LLM\nservice in the baseline system does not know the prompt struc-\nture, it is hard to infer the shared prompt from massive LLM\nrequests. Compared to the baseline without sharing prompt,\nParrot achieves 1.8\u00d7 \u223c 2.4\u00d7speedup for batch sizes of 8 and\n16. Further increasing the batch size leads to out-of-memory\ndue to the massive KV cache of shared system prompt. We\nalso build an advanced baseline using vLLM\u2019s paged atten-\ntion to support sharing the prompt with a static prefix. Both\n8 16 32 64\nBatch Size010203040Avg. Latency (s)1.1x1.3x1.4x1.7x\n1.8x2.4x\nx xParrot\nBaseline w/ Sharing\nBaseline w/o SharingFigure 15: Latency of Bing Copilot with varying batch sizes.\n200 400 600 800\nOutput Length (# tokens)0.000.020.040.060.080.100.12Latency per token (s)\n1.44x\n1.53x 1.56x 1.58x\nParrot\nBaseline w/ Sharing\n(a) Batch Size = 32\n100 200 300 400 480\nOutput Length (# tokens)0.000.050.100.150.200.25Latency per token (s)\n1.44x\n1.64x1.74x 1.81x1.84x\nParrot\nBaseline w/ Sharing (b) Batch Size = 64\nFigure 16: Latency per output token of Bing Copilot.\nParrot and vLLM use the paged memory management [25],\nthus both systems can hold the same number of tokens in\nan LLM engine (A100, LLaMA 7B). Parrot further achieves\n1.1\u00d7 \u223c 1.7\u00d7speedup over vLLM because of the better GPU\nkernel. Although vLLM can save extra memory usage of the\nshared prompt, its GPU kernel still has to reload the tokens\nrepeatedly. Given that the token generation of LLMs is bound\nby memory bandwidth, such redundant memory loading slows\ndown the end-to-end inference.", "mimetype": "text/plain", "start_char_idx": 1742, "end_char_idx": 3703, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ebb694d8-b24b-4e15-a1fb-9cb36ee41f2c": {"__data__": {"id_": "ebb694d8-b24b-4e15-a1fb-9cb36ee41f2c", "embedding": null, "metadata": {"page_label": "11", "file_name": "2405_19888v1.pdf", "Title of this paper": "Parrot: Efficient Serving of LLM-based Applications with Semantic Variable", "Authors": "Chaofan Lin, Zhenhua Han, Chengruidong Zhang, Yuqing Yang, Fan Yang, Chen Chen, Lili Qiu", "Date published": "05/30/2024", "URL": "http://arxiv.org/abs/2405.19888v1", "summary": "The rise of large language models (LLMs) has enabled LLM-based applications\n(a.k.a. AI agents or co-pilots), a new software paradigm that combines the\nstrength of LLM and conventional software. Diverse LLM applications from\ndifferent tenants could design complex workflows using multiple LLM requests to\naccomplish one task. However, they have to use the over-simplified\nrequest-level API provided by today's public LLM services, losing essential\napplication-level information. Public LLM services have to blindly optimize\nindividual LLM requests, leading to sub-optimal end-to-end performance of LLM\napplications.\n  This paper introduces Parrot, an LLM service system that focuses on the\nend-to-end experience of LLM-based applications. Parrot proposes Semantic\nVariable, a unified abstraction to expose application-level knowledge to public\nLLM services. A Semantic Variable annotates an input/output variable in the\nprompt of a request, and creates the data pipeline when connecting multiple LLM\nrequests, providing a natural way to program LLM applications. Exposing\nSemantic Variables to the public LLM service allows it to perform conventional\ndata flow analysis to uncover the correlation across multiple LLM requests.\nThis correlation opens a brand-new optimization space for the end-to-end\nperformance of LLM-based applications. Extensive evaluations demonstrate that\nParrot can achieve up to an order-of-magnitude improvement for popular and\npractical use cases of LLM applications."}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f799a209-ea73-44e2-8183-2da4ea8ee984", "node_type": "4", "metadata": {"page_label": "11", "file_name": "2405_19888v1.pdf", "Title of this paper": "Parrot: Efficient Serving of LLM-based Applications with Semantic Variable", "Authors": "Chaofan Lin, Zhenhua Han, Chengruidong Zhang, Yuqing Yang, Fan Yang, Chen Chen, Lili Qiu", "Date published": "05/30/2024", "URL": "http://arxiv.org/abs/2405.19888v1", "summary": "The rise of large language models (LLMs) has enabled LLM-based applications\n(a.k.a. AI agents or co-pilots), a new software paradigm that combines the\nstrength of LLM and conventional software. Diverse LLM applications from\ndifferent tenants could design complex workflows using multiple LLM requests to\naccomplish one task. However, they have to use the over-simplified\nrequest-level API provided by today's public LLM services, losing essential\napplication-level information. Public LLM services have to blindly optimize\nindividual LLM requests, leading to sub-optimal end-to-end performance of LLM\napplications.\n  This paper introduces Parrot, an LLM service system that focuses on the\nend-to-end experience of LLM-based applications. Parrot proposes Semantic\nVariable, a unified abstraction to expose application-level knowledge to public\nLLM services. A Semantic Variable annotates an input/output variable in the\nprompt of a request, and creates the data pipeline when connecting multiple LLM\nrequests, providing a natural way to program LLM applications. Exposing\nSemantic Variables to the public LLM service allows it to perform conventional\ndata flow analysis to uncover the correlation across multiple LLM requests.\nThis correlation opens a brand-new optimization space for the end-to-end\nperformance of LLM-based applications. Extensive evaluations demonstrate that\nParrot can achieve up to an order-of-magnitude improvement for popular and\npractical use cases of LLM applications."}, "hash": "9e07a69475878b78b374425df72d68abc0ea3b749d87c9ea002fb336c7824d2d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "de0ac9e9-a8e5-456a-8363-0b4dd995a53c", "node_type": "1", "metadata": {"page_label": "11", "file_name": "2405_19888v1.pdf", "Title of this paper": "Parrot: Efficient Serving of LLM-based Applications with Semantic Variable", "Authors": "Chaofan Lin, Zhenhua Han, Chengruidong Zhang, Yuqing Yang, Fan Yang, Chen Chen, Lili Qiu", "Date published": "05/30/2024", "URL": "http://arxiv.org/abs/2405.19888v1", "summary": "The rise of large language models (LLMs) has enabled LLM-based applications\n(a.k.a. AI agents or co-pilots), a new software paradigm that combines the\nstrength of LLM and conventional software. Diverse LLM applications from\ndifferent tenants could design complex workflows using multiple LLM requests to\naccomplish one task. However, they have to use the over-simplified\nrequest-level API provided by today's public LLM services, losing essential\napplication-level information. Public LLM services have to blindly optimize\nindividual LLM requests, leading to sub-optimal end-to-end performance of LLM\napplications.\n  This paper introduces Parrot, an LLM service system that focuses on the\nend-to-end experience of LLM-based applications. Parrot proposes Semantic\nVariable, a unified abstraction to expose application-level knowledge to public\nLLM services. A Semantic Variable annotates an input/output variable in the\nprompt of a request, and creates the data pipeline when connecting multiple LLM\nrequests, providing a natural way to program LLM applications. Exposing\nSemantic Variables to the public LLM service allows it to perform conventional\ndata flow analysis to uncover the correlation across multiple LLM requests.\nThis correlation opens a brand-new optimization space for the end-to-end\nperformance of LLM-based applications. Extensive evaluations demonstrate that\nParrot can achieve up to an order-of-magnitude improvement for popular and\npractical use cases of LLM applications."}, "hash": "d2feeb91ea9bf61c6a4f831012aac7af9431d30252d731f7711daacb83bb56dc", "class_name": "RelatedNodeInfo"}}, "text": "Parrot and vLLM use the paged memory management [25],\nthus both systems can hold the same number of tokens in\nan LLM engine (A100, LLaMA 7B). Parrot further achieves\n1.1\u00d7 \u223c 1.7\u00d7speedup over vLLM because of the better GPU\nkernel. Although vLLM can save extra memory usage of the\nshared prompt, its GPU kernel still has to reload the tokens\nrepeatedly. Given that the token generation of LLMs is bound\nby memory bandwidth, such redundant memory loading slows\ndown the end-to-end inference. By combining FlashAtten-\ntion and PagedAttention, Parrot only needs to load the tokens\nof the shared prompt once, when computing the attention\nfrom the diverged tokens of different users. Parrot\u2019s speedup\nof shared prompt mainly comes from the token generation,\nthus the longer output length leads to higher improvement.\nFigure 16 shows Parrot achieves 1.58\u00d7and1.84\u00d7speedup\ncompared to vLLM using paged attention, showing 40ms\nper-output-token latency at a batch size of 32.\nIn Figure 17, we further evaluated the serving of multiple\nGPTs applications [42], each of which has multiple users, in\na multi-GPU cluster. Four A6000 (48GB) GPUs are deployed\nwith four LLM engines (LLaMA 7B). We select four GPTs\napplications in four popular categories including productivity,\nprogramming, image generation, and data analysis. The LLM\nrequests are randomly generated from the four categories with\nequal probability. LLM requests arrive at fixed rates following\nPoisson distribution. Parrot can sustain 12\u00d7higher request\nrates compared to the baseline without sharing. Because the\nbaseline\u2019s scheduling policy is not aware of the shared prompt\nwithin each LLM application, the requests are mixed in all\nLLM engines making it impossible to reuse the common\nprompt prefix. Parrot\u2019s scheduling policy co-locates LLM\nrequests of the same applications to maximize the sharing op-", "mimetype": "text/plain", "start_char_idx": 3216, "end_char_idx": 5070, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c6583cdd-284e-4c86-a9ab-abb12025c826": {"__data__": {"id_": "c6583cdd-284e-4c86-a9ab-abb12025c826", "embedding": null, "metadata": {"page_label": "12", "file_name": "2405_19888v1.pdf", "Title of this paper": "Parrot: Efficient Serving of LLM-based Applications with Semantic Variable", "Authors": "Chaofan Lin, Zhenhua Han, Chengruidong Zhang, Yuqing Yang, Fan Yang, Chen Chen, Lili Qiu", "Date published": "05/30/2024", "URL": "http://arxiv.org/abs/2405.19888v1", "summary": "The rise of large language models (LLMs) has enabled LLM-based applications\n(a.k.a. AI agents or co-pilots), a new software paradigm that combines the\nstrength of LLM and conventional software. Diverse LLM applications from\ndifferent tenants could design complex workflows using multiple LLM requests to\naccomplish one task. However, they have to use the over-simplified\nrequest-level API provided by today's public LLM services, losing essential\napplication-level information. Public LLM services have to blindly optimize\nindividual LLM requests, leading to sub-optimal end-to-end performance of LLM\napplications.\n  This paper introduces Parrot, an LLM service system that focuses on the\nend-to-end experience of LLM-based applications. Parrot proposes Semantic\nVariable, a unified abstraction to expose application-level knowledge to public\nLLM services. A Semantic Variable annotates an input/output variable in the\nprompt of a request, and creates the data pipeline when connecting multiple LLM\nrequests, providing a natural way to program LLM applications. Exposing\nSemantic Variables to the public LLM service allows it to perform conventional\ndata flow analysis to uncover the correlation across multiple LLM requests.\nThis correlation opens a brand-new optimization space for the end-to-end\nperformance of LLM-based applications. Extensive evaluations demonstrate that\nParrot can achieve up to an order-of-magnitude improvement for popular and\npractical use cases of LLM applications."}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c71416dd-8a88-43c4-b5c7-eb0e11238506", "node_type": "4", "metadata": {"page_label": "12", "file_name": "2405_19888v1.pdf", "Title of this paper": "Parrot: Efficient Serving of LLM-based Applications with Semantic Variable", "Authors": "Chaofan Lin, Zhenhua Han, Chengruidong Zhang, Yuqing Yang, Fan Yang, Chen Chen, Lili Qiu", "Date published": "05/30/2024", "URL": "http://arxiv.org/abs/2405.19888v1", "summary": "The rise of large language models (LLMs) has enabled LLM-based applications\n(a.k.a. AI agents or co-pilots), a new software paradigm that combines the\nstrength of LLM and conventional software. Diverse LLM applications from\ndifferent tenants could design complex workflows using multiple LLM requests to\naccomplish one task. However, they have to use the over-simplified\nrequest-level API provided by today's public LLM services, losing essential\napplication-level information. Public LLM services have to blindly optimize\nindividual LLM requests, leading to sub-optimal end-to-end performance of LLM\napplications.\n  This paper introduces Parrot, an LLM service system that focuses on the\nend-to-end experience of LLM-based applications. Parrot proposes Semantic\nVariable, a unified abstraction to expose application-level knowledge to public\nLLM services. A Semantic Variable annotates an input/output variable in the\nprompt of a request, and creates the data pipeline when connecting multiple LLM\nrequests, providing a natural way to program LLM applications. Exposing\nSemantic Variables to the public LLM service allows it to perform conventional\ndata flow analysis to uncover the correlation across multiple LLM requests.\nThis correlation opens a brand-new optimization space for the end-to-end\nperformance of LLM-based applications. Extensive evaluations demonstrate that\nParrot can achieve up to an order-of-magnitude improvement for popular and\npractical use cases of LLM applications."}, "hash": "66ab3f524cbc6266d40b05bfed58083ed18bd3a6891f11baf8eee4515f608fd6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b64a0b99-ffb2-4020-b46e-5ff110f1124f", "node_type": "1", "metadata": {}, "hash": "e40dd4dbb5eee6b7787114bbb49900ac0d6d8047a1b66a6ef9d8f0edaebba360", "class_name": "RelatedNodeInfo"}}, "text": "012345678910111213141516\nRequest rate (req/s)0100200300Normalized latency\n (ms/token)\nParrot\nParrot w/ PagedAttention\nParrot w/o Scheduling\nBaseline (vLLM)Figure 17: Serving multiple GPTs applications.\nportunity, achieving both lower inference latency and higher\ncluster throughput. After turning off such affinity scheduling\npolicy, Parrot only exhibits 3\u00d7higher request rates compared\nto the baseline, because the requests with shared prefix are\noften dispatched to different engines thus reduced the sharing\nopportunities. Moreover, Parrot\u2019s attention kernel helps Parrot\nto achieve 2.4\u00d7higher rate compared to Parrot using vLLM\u2019s\nPagedAttention, by avoiding the redundant memory loading\nfor attention of shared prompts.\n8.4 Multi-agent Applications\nWe assess the performance of multi-agent systems utiliz-\ning MetaGPT [22] within Parrot. A workflow is constructed\nwith three distinct roles. Initially, the Architect outlines the\nproject\u2019s file structures and specifies APIs within each file\nfor a given task. Subsequently, multiple Coders undertake the\nproject implementation, with each focusing on a specific file.\nFollowing the integration of the code from all files, several\nReviewers engage in the process, each examining and com-\nmenting on a single file. The Coders then revise their code\nbased on these comments. This review-and-revision cycle\nis iterated three times to produce the final code. Figure 18\nillustrates the latency and memory consumption of Parrot\ncompared to baseline systems on one A100 running LLaMA\n13B. Parrot achieves a speedup of up to 11.7\u00d7compared\nwith the latency-centric baseline. The primary improvement\nis attributed to Parrot\u2019s capability to deduct the performance\nobjectives for LLM requests based on the end-to-end perfor-\nmance criteria. For this specific multi-agent scenario, the goal\nis to minimize the time taken to deliver the final code. Parrot\nidentifies multiple task groups within the parallel processes of\ncoding, reviewing, and revising, facilitating larger batch sizes\nto enhance throughput and reduce the completion time of task\ngroups. We also contrast Parrot with an throughput-centric\nbaseline that uses larger batch on purpose to optimize cluster\nthroughput, which also shows higher concurrency and better\ncompletion time than the latency-centric baseline.\nEven when compared to the throughput-centric baseline,\nParrot demonstrates superiority, being faster by up to 2.45\u00d7.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2432, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b64a0b99-ffb2-4020-b46e-5ff110f1124f": {"__data__": {"id_": "b64a0b99-ffb2-4020-b46e-5ff110f1124f", "embedding": null, "metadata": {"page_label": "12", "file_name": "2405_19888v1.pdf", "Title of this paper": "Parrot: Efficient Serving of LLM-based Applications with Semantic Variable", "Authors": "Chaofan Lin, Zhenhua Han, Chengruidong Zhang, Yuqing Yang, Fan Yang, Chen Chen, Lili Qiu", "Date published": "05/30/2024", "URL": "http://arxiv.org/abs/2405.19888v1", "summary": "The rise of large language models (LLMs) has enabled LLM-based applications\n(a.k.a. AI agents or co-pilots), a new software paradigm that combines the\nstrength of LLM and conventional software. Diverse LLM applications from\ndifferent tenants could design complex workflows using multiple LLM requests to\naccomplish one task. However, they have to use the over-simplified\nrequest-level API provided by today's public LLM services, losing essential\napplication-level information. Public LLM services have to blindly optimize\nindividual LLM requests, leading to sub-optimal end-to-end performance of LLM\napplications.\n  This paper introduces Parrot, an LLM service system that focuses on the\nend-to-end experience of LLM-based applications. Parrot proposes Semantic\nVariable, a unified abstraction to expose application-level knowledge to public\nLLM services. A Semantic Variable annotates an input/output variable in the\nprompt of a request, and creates the data pipeline when connecting multiple LLM\nrequests, providing a natural way to program LLM applications. Exposing\nSemantic Variables to the public LLM service allows it to perform conventional\ndata flow analysis to uncover the correlation across multiple LLM requests.\nThis correlation opens a brand-new optimization space for the end-to-end\nperformance of LLM-based applications. Extensive evaluations demonstrate that\nParrot can achieve up to an order-of-magnitude improvement for popular and\npractical use cases of LLM applications."}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c71416dd-8a88-43c4-b5c7-eb0e11238506", "node_type": "4", "metadata": {"page_label": "12", "file_name": "2405_19888v1.pdf", "Title of this paper": "Parrot: Efficient Serving of LLM-based Applications with Semantic Variable", "Authors": "Chaofan Lin, Zhenhua Han, Chengruidong Zhang, Yuqing Yang, Fan Yang, Chen Chen, Lili Qiu", "Date published": "05/30/2024", "URL": "http://arxiv.org/abs/2405.19888v1", "summary": "The rise of large language models (LLMs) has enabled LLM-based applications\n(a.k.a. AI agents or co-pilots), a new software paradigm that combines the\nstrength of LLM and conventional software. Diverse LLM applications from\ndifferent tenants could design complex workflows using multiple LLM requests to\naccomplish one task. However, they have to use the over-simplified\nrequest-level API provided by today's public LLM services, losing essential\napplication-level information. Public LLM services have to blindly optimize\nindividual LLM requests, leading to sub-optimal end-to-end performance of LLM\napplications.\n  This paper introduces Parrot, an LLM service system that focuses on the\nend-to-end experience of LLM-based applications. Parrot proposes Semantic\nVariable, a unified abstraction to expose application-level knowledge to public\nLLM services. A Semantic Variable annotates an input/output variable in the\nprompt of a request, and creates the data pipeline when connecting multiple LLM\nrequests, providing a natural way to program LLM applications. Exposing\nSemantic Variables to the public LLM service allows it to perform conventional\ndata flow analysis to uncover the correlation across multiple LLM requests.\nThis correlation opens a brand-new optimization space for the end-to-end\nperformance of LLM-based applications. Extensive evaluations demonstrate that\nParrot can achieve up to an order-of-magnitude improvement for popular and\npractical use cases of LLM applications."}, "hash": "66ab3f524cbc6266d40b05bfed58083ed18bd3a6891f11baf8eee4515f608fd6", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c6583cdd-284e-4c86-a9ab-abb12025c826", "node_type": "1", "metadata": {"page_label": "12", "file_name": "2405_19888v1.pdf", "Title of this paper": "Parrot: Efficient Serving of LLM-based Applications with Semantic Variable", "Authors": "Chaofan Lin, Zhenhua Han, Chengruidong Zhang, Yuqing Yang, Fan Yang, Chen Chen, Lili Qiu", "Date published": "05/30/2024", "URL": "http://arxiv.org/abs/2405.19888v1", "summary": "The rise of large language models (LLMs) has enabled LLM-based applications\n(a.k.a. AI agents or co-pilots), a new software paradigm that combines the\nstrength of LLM and conventional software. Diverse LLM applications from\ndifferent tenants could design complex workflows using multiple LLM requests to\naccomplish one task. However, they have to use the over-simplified\nrequest-level API provided by today's public LLM services, losing essential\napplication-level information. Public LLM services have to blindly optimize\nindividual LLM requests, leading to sub-optimal end-to-end performance of LLM\napplications.\n  This paper introduces Parrot, an LLM service system that focuses on the\nend-to-end experience of LLM-based applications. Parrot proposes Semantic\nVariable, a unified abstraction to expose application-level knowledge to public\nLLM services. A Semantic Variable annotates an input/output variable in the\nprompt of a request, and creates the data pipeline when connecting multiple LLM\nrequests, providing a natural way to program LLM applications. Exposing\nSemantic Variables to the public LLM service allows it to perform conventional\ndata flow analysis to uncover the correlation across multiple LLM requests.\nThis correlation opens a brand-new optimization space for the end-to-end\nperformance of LLM-based applications. Extensive evaluations demonstrate that\nParrot can achieve up to an order-of-magnitude improvement for popular and\npractical use cases of LLM applications."}, "hash": "916752eb6ef9caada57fc6494a59c9e70c48be1660f8f8b240c7c99c56399cef", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8895c478-587d-472e-be81-35370b307761", "node_type": "1", "metadata": {}, "hash": "b7c6a1122df6b3365174edd1e237379260ee3a2f199b7c7da5f26c76863203b6", "class_name": "RelatedNodeInfo"}}, "text": "Parrot achieves a speedup of up to 11.7\u00d7compared\nwith the latency-centric baseline. The primary improvement\nis attributed to Parrot\u2019s capability to deduct the performance\nobjectives for LLM requests based on the end-to-end perfor-\nmance criteria. For this specific multi-agent scenario, the goal\nis to minimize the time taken to deliver the final code. Parrot\nidentifies multiple task groups within the parallel processes of\ncoding, reviewing, and revising, facilitating larger batch sizes\nto enhance throughput and reduce the completion time of task\ngroups. We also contrast Parrot with an throughput-centric\nbaseline that uses larger batch on purpose to optimize cluster\nthroughput, which also shows higher concurrency and better\ncompletion time than the latency-centric baseline.\nEven when compared to the throughput-centric baseline,\nParrot demonstrates superiority, being faster by up to 2.45\u00d7.\nThis enhancement mainly stems from Parrot\u2019s ability to\n24002600 11.7x\n4 8 12 16\nNumber of Files050010001500Average Latency (s)1.00x 1.04x 1.14x 1.16x 1.22x1.61x1.88x2.35x\n1.27x1.58x2.03x2.45x7.19x\n4.9x\n3.0xParrot\nParrot w/ PagedAttention\nParrot w/o Sharing\nBaseline (vLLM, Throughput)\nBaseline (vLLM, Latency)(a) End-to-end Latency\n4 8 12 16\nNumber of Files01020304050GPU Memory of\nKV Cache (GB)GPU Memory Capacity\nParrot\nParrot w/o Sharing\n(b) GPU Memory of KV Cache\nFigure 18: The latency and memory usage for multi-agent\nprogramming, with varying number of files to program.\ndecrease redundancy through its prompt structure analysis,\nwhich contributes a 2.35\u00d7acceleration. Given the interactive\nnature of the roles in MetaGPT, there is considerable overlap\nin the context among different roles, which Parrot capitalizes\non by sharing this common context as a prompt prefix. The\nstatic prefix sharing mechanism from vLLM does not work\nin this dynamic scenario. Without a grasp of the prompt\u2019s\nstructure, it cannot identify dynamically generated Semantic\nVariables that could also be shared during runtime. As de-\npicted in Figure 18b, Parrot without this sharing capability\nwould hit the GPU memory ceiling. Additionally, Parrot\u2019s spe-\ncialized GPU kernel for processing the shared prefix achieves\na further 1.2\u00d7speedup when there are 16 files, compared to\nusing vLLM\u2019s PagedAttention, due to the reduced memory\ntransactions.", "mimetype": "text/plain", "start_char_idx": 1533, "end_char_idx": 3860, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8895c478-587d-472e-be81-35370b307761": {"__data__": {"id_": "8895c478-587d-472e-be81-35370b307761", "embedding": null, "metadata": {"page_label": "12", "file_name": "2405_19888v1.pdf", "Title of this paper": "Parrot: Efficient Serving of LLM-based Applications with Semantic Variable", "Authors": "Chaofan Lin, Zhenhua Han, Chengruidong Zhang, Yuqing Yang, Fan Yang, Chen Chen, Lili Qiu", "Date published": "05/30/2024", "URL": "http://arxiv.org/abs/2405.19888v1", "summary": "The rise of large language models (LLMs) has enabled LLM-based applications\n(a.k.a. AI agents or co-pilots), a new software paradigm that combines the\nstrength of LLM and conventional software. Diverse LLM applications from\ndifferent tenants could design complex workflows using multiple LLM requests to\naccomplish one task. However, they have to use the over-simplified\nrequest-level API provided by today's public LLM services, losing essential\napplication-level information. Public LLM services have to blindly optimize\nindividual LLM requests, leading to sub-optimal end-to-end performance of LLM\napplications.\n  This paper introduces Parrot, an LLM service system that focuses on the\nend-to-end experience of LLM-based applications. Parrot proposes Semantic\nVariable, a unified abstraction to expose application-level knowledge to public\nLLM services. A Semantic Variable annotates an input/output variable in the\nprompt of a request, and creates the data pipeline when connecting multiple LLM\nrequests, providing a natural way to program LLM applications. Exposing\nSemantic Variables to the public LLM service allows it to perform conventional\ndata flow analysis to uncover the correlation across multiple LLM requests.\nThis correlation opens a brand-new optimization space for the end-to-end\nperformance of LLM-based applications. Extensive evaluations demonstrate that\nParrot can achieve up to an order-of-magnitude improvement for popular and\npractical use cases of LLM applications."}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c71416dd-8a88-43c4-b5c7-eb0e11238506", "node_type": "4", "metadata": {"page_label": "12", "file_name": "2405_19888v1.pdf", "Title of this paper": "Parrot: Efficient Serving of LLM-based Applications with Semantic Variable", "Authors": "Chaofan Lin, Zhenhua Han, Chengruidong Zhang, Yuqing Yang, Fan Yang, Chen Chen, Lili Qiu", "Date published": "05/30/2024", "URL": "http://arxiv.org/abs/2405.19888v1", "summary": "The rise of large language models (LLMs) has enabled LLM-based applications\n(a.k.a. AI agents or co-pilots), a new software paradigm that combines the\nstrength of LLM and conventional software. Diverse LLM applications from\ndifferent tenants could design complex workflows using multiple LLM requests to\naccomplish one task. However, they have to use the over-simplified\nrequest-level API provided by today's public LLM services, losing essential\napplication-level information. Public LLM services have to blindly optimize\nindividual LLM requests, leading to sub-optimal end-to-end performance of LLM\napplications.\n  This paper introduces Parrot, an LLM service system that focuses on the\nend-to-end experience of LLM-based applications. Parrot proposes Semantic\nVariable, a unified abstraction to expose application-level knowledge to public\nLLM services. A Semantic Variable annotates an input/output variable in the\nprompt of a request, and creates the data pipeline when connecting multiple LLM\nrequests, providing a natural way to program LLM applications. Exposing\nSemantic Variables to the public LLM service allows it to perform conventional\ndata flow analysis to uncover the correlation across multiple LLM requests.\nThis correlation opens a brand-new optimization space for the end-to-end\nperformance of LLM-based applications. Extensive evaluations demonstrate that\nParrot can achieve up to an order-of-magnitude improvement for popular and\npractical use cases of LLM applications."}, "hash": "66ab3f524cbc6266d40b05bfed58083ed18bd3a6891f11baf8eee4515f608fd6", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b64a0b99-ffb2-4020-b46e-5ff110f1124f", "node_type": "1", "metadata": {"page_label": "12", "file_name": "2405_19888v1.pdf", "Title of this paper": "Parrot: Efficient Serving of LLM-based Applications with Semantic Variable", "Authors": "Chaofan Lin, Zhenhua Han, Chengruidong Zhang, Yuqing Yang, Fan Yang, Chen Chen, Lili Qiu", "Date published": "05/30/2024", "URL": "http://arxiv.org/abs/2405.19888v1", "summary": "The rise of large language models (LLMs) has enabled LLM-based applications\n(a.k.a. AI agents or co-pilots), a new software paradigm that combines the\nstrength of LLM and conventional software. Diverse LLM applications from\ndifferent tenants could design complex workflows using multiple LLM requests to\naccomplish one task. However, they have to use the over-simplified\nrequest-level API provided by today's public LLM services, losing essential\napplication-level information. Public LLM services have to blindly optimize\nindividual LLM requests, leading to sub-optimal end-to-end performance of LLM\napplications.\n  This paper introduces Parrot, an LLM service system that focuses on the\nend-to-end experience of LLM-based applications. Parrot proposes Semantic\nVariable, a unified abstraction to expose application-level knowledge to public\nLLM services. A Semantic Variable annotates an input/output variable in the\nprompt of a request, and creates the data pipeline when connecting multiple LLM\nrequests, providing a natural way to program LLM applications. Exposing\nSemantic Variables to the public LLM service allows it to perform conventional\ndata flow analysis to uncover the correlation across multiple LLM requests.\nThis correlation opens a brand-new optimization space for the end-to-end\nperformance of LLM-based applications. Extensive evaluations demonstrate that\nParrot can achieve up to an order-of-magnitude improvement for popular and\npractical use cases of LLM applications."}, "hash": "02e23e26b47f346804f5b46c0bec889d946208b6dd3662908e92ace68f68d8d0", "class_name": "RelatedNodeInfo"}}, "text": "decrease redundancy through its prompt structure analysis,\nwhich contributes a 2.35\u00d7acceleration. Given the interactive\nnature of the roles in MetaGPT, there is considerable overlap\nin the context among different roles, which Parrot capitalizes\non by sharing this common context as a prompt prefix. The\nstatic prefix sharing mechanism from vLLM does not work\nin this dynamic scenario. Without a grasp of the prompt\u2019s\nstructure, it cannot identify dynamically generated Semantic\nVariables that could also be shared during runtime. As de-\npicted in Figure 18b, Parrot without this sharing capability\nwould hit the GPU memory ceiling. Additionally, Parrot\u2019s spe-\ncialized GPU kernel for processing the shared prefix achieves\na further 1.2\u00d7speedup when there are 16 files, compared to\nusing vLLM\u2019s PagedAttention, due to the reduced memory\ntransactions.\n8.5 Scheduling of Mixed Workloads\nTo assess the performance of Parrot on a multi-GPU setup, we\nconfigure a cluster with four A6000 (48GB) GPUs, each host-\ning a separate LLM engine (LLaMA 7B), resulting in a total\nof four LLM engines. We emulate a real-world scenario where\nLLM services encounter a variety of demands by injecting a\nmix of requests from chat applications at a rate of 1 req/s and\nfrom data analytic tasks (i.e., map-reduce applications) previ-\nously analyzed in \u00a78.2. Requests from the chat applications\nare characterized by their need for low latency, whereas the\nmap-reduce applications prioritize high throughput, creating a\nchallenge when they are concurrently processed by the same\nLLM engine. We benchmark Parrot against two reference\nimplementations: one tailored for latency, limiting engine ca-\npacity to reduce decoding time, and another for throughput,", "mimetype": "text/plain", "start_char_idx": 3011, "end_char_idx": 4741, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b0b924dc-5101-40ca-9efc-334e6b57913a": {"__data__": {"id_": "b0b924dc-5101-40ca-9efc-334e6b57913a", "embedding": null, "metadata": {"page_label": "13", "file_name": "2405_19888v1.pdf", "Title of this paper": "Parrot: Efficient Serving of LLM-based Applications with Semantic Variable", "Authors": "Chaofan Lin, Zhenhua Han, Chengruidong Zhang, Yuqing Yang, Fan Yang, Chen Chen, Lili Qiu", "Date published": "05/30/2024", "URL": "http://arxiv.org/abs/2405.19888v1", "summary": "The rise of large language models (LLMs) has enabled LLM-based applications\n(a.k.a. AI agents or co-pilots), a new software paradigm that combines the\nstrength of LLM and conventional software. Diverse LLM applications from\ndifferent tenants could design complex workflows using multiple LLM requests to\naccomplish one task. However, they have to use the over-simplified\nrequest-level API provided by today's public LLM services, losing essential\napplication-level information. Public LLM services have to blindly optimize\nindividual LLM requests, leading to sub-optimal end-to-end performance of LLM\napplications.\n  This paper introduces Parrot, an LLM service system that focuses on the\nend-to-end experience of LLM-based applications. Parrot proposes Semantic\nVariable, a unified abstraction to expose application-level knowledge to public\nLLM services. A Semantic Variable annotates an input/output variable in the\nprompt of a request, and creates the data pipeline when connecting multiple LLM\nrequests, providing a natural way to program LLM applications. Exposing\nSemantic Variables to the public LLM service allows it to perform conventional\ndata flow analysis to uncover the correlation across multiple LLM requests.\nThis correlation opens a brand-new optimization space for the end-to-end\nperformance of LLM-based applications. Extensive evaluations demonstrate that\nParrot can achieve up to an order-of-magnitude improvement for popular and\npractical use cases of LLM applications."}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8931491b-6711-4bc1-b824-c863efd9bd88", "node_type": "4", "metadata": {"page_label": "13", "file_name": "2405_19888v1.pdf", "Title of this paper": "Parrot: Efficient Serving of LLM-based Applications with Semantic Variable", "Authors": "Chaofan Lin, Zhenhua Han, Chengruidong Zhang, Yuqing Yang, Fan Yang, Chen Chen, Lili Qiu", "Date published": "05/30/2024", "URL": "http://arxiv.org/abs/2405.19888v1", "summary": "The rise of large language models (LLMs) has enabled LLM-based applications\n(a.k.a. AI agents or co-pilots), a new software paradigm that combines the\nstrength of LLM and conventional software. Diverse LLM applications from\ndifferent tenants could design complex workflows using multiple LLM requests to\naccomplish one task. However, they have to use the over-simplified\nrequest-level API provided by today's public LLM services, losing essential\napplication-level information. Public LLM services have to blindly optimize\nindividual LLM requests, leading to sub-optimal end-to-end performance of LLM\napplications.\n  This paper introduces Parrot, an LLM service system that focuses on the\nend-to-end experience of LLM-based applications. Parrot proposes Semantic\nVariable, a unified abstraction to expose application-level knowledge to public\nLLM services. A Semantic Variable annotates an input/output variable in the\nprompt of a request, and creates the data pipeline when connecting multiple LLM\nrequests, providing a natural way to program LLM applications. Exposing\nSemantic Variables to the public LLM service allows it to perform conventional\ndata flow analysis to uncover the correlation across multiple LLM requests.\nThis correlation opens a brand-new optimization space for the end-to-end\nperformance of LLM-based applications. Extensive evaluations demonstrate that\nParrot can achieve up to an order-of-magnitude improvement for popular and\npractical use cases of LLM applications."}, "hash": "fcec32f3abc0db889cc76d5cf7041fa3d591b172fdfe368977f1bc4c8bdfecdd", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "774e005a-de09-4136-a514-dea6cac22cc2", "node_type": "1", "metadata": {}, "hash": "2da0e47e730481fedbc439fcd4f7d4572c202ad653ed3a424ff82daa5646f63d", "class_name": "RelatedNodeInfo"}}, "text": "0200400600800\n149.1184.6827.6Average Chat\n Normalized Latency (ms)\n020406080\n45.177.8\n41.4Average Chat\nDecode Time (ms)\n020406080100\n23.2 24.586.4Average\nMap-Reduce JCT (s)\nParrot Baseline (Throughput) Baseline (Latency)Figure 19: The mixture of chat and map-reduce applications.\nutilizing full engine capacity to maximize GPU utilization.\nThe results depicted in Figure 19 demonstrate that Par-\nrot attains a 5.5\u00d7and1.23\u00d7improvement in normalized\nlatency (measured as request latency per number of output\ntokens) [25, 56] for chat applications in comparison to the\nlatency-focused and throughput-focused baselines, respec-\ntively. In terms of token generation speed for chat applications,\nParrot delivers performance on par with the latency-centric\nbaseline and outperforms the throughput-centric baseline by\n1.72\u00d7. For map-reduce applications, Parrot reaches a 3.7\u00d7\nspeedup over the latency-centric baseline and is 1.05\u00d7more\nefficient than the throughput-centric baseline. Parrot excels\nby providing both low latency for chat applications and high\nthroughput for map-reduce applications. It mitigates the con-\ntention between chat and map-reduce workloads by intelli-\ngently scheduling them on separate engines. These findings\nunderscore the significance of specialized handling for diverse\nrequests to enhance the overall performance of LLM services.\n9 Related Works\nDeep Learning Serving Systems. The field of model serv-\ning has seen a surge of research activity in recent years,\nwith many systems developed to address the different chal-\nlenges of deep learning model deployment. The systems in-\nclude Clipper [10], TensorFlow Serving [39], Clockwork [19],\nREEF [20], AlpaServe [28], which have explored many as-\npects including batching, caching, placement, scheduling,\nmodel parallelism for the serving of single or multiple models.\nThese systems were proposed for serving general deep learn-\ning models, which have less consideration about the unique\nrequirements of large language models, e.g., autoregressive\ndecoding. Orca [56] proposed a fine-grained scheduling mech-\nanism that can batch multiple LLM requests at the iteration\nlevel, which is also known as continuous batching. vLLM\nproposes PagedAttention [25] allows the batching of LLM\nrequests with different lengths using non-contiguous memory,\nincreasing memory utilization. These systems for LLM serv-\ning still treat LLM requests separately, missing the opportuni-\nties to understand the interconnections within an application\nand exploit the commonality of different requests. Parrot isorthogonal to them.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2578, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "774e005a-de09-4136-a514-dea6cac22cc2": {"__data__": {"id_": "774e005a-de09-4136-a514-dea6cac22cc2", "embedding": null, "metadata": {"page_label": "13", "file_name": "2405_19888v1.pdf", "Title of this paper": "Parrot: Efficient Serving of LLM-based Applications with Semantic Variable", "Authors": "Chaofan Lin, Zhenhua Han, Chengruidong Zhang, Yuqing Yang, Fan Yang, Chen Chen, Lili Qiu", "Date published": "05/30/2024", "URL": "http://arxiv.org/abs/2405.19888v1", "summary": "The rise of large language models (LLMs) has enabled LLM-based applications\n(a.k.a. AI agents or co-pilots), a new software paradigm that combines the\nstrength of LLM and conventional software. Diverse LLM applications from\ndifferent tenants could design complex workflows using multiple LLM requests to\naccomplish one task. However, they have to use the over-simplified\nrequest-level API provided by today's public LLM services, losing essential\napplication-level information. Public LLM services have to blindly optimize\nindividual LLM requests, leading to sub-optimal end-to-end performance of LLM\napplications.\n  This paper introduces Parrot, an LLM service system that focuses on the\nend-to-end experience of LLM-based applications. Parrot proposes Semantic\nVariable, a unified abstraction to expose application-level knowledge to public\nLLM services. A Semantic Variable annotates an input/output variable in the\nprompt of a request, and creates the data pipeline when connecting multiple LLM\nrequests, providing a natural way to program LLM applications. Exposing\nSemantic Variables to the public LLM service allows it to perform conventional\ndata flow analysis to uncover the correlation across multiple LLM requests.\nThis correlation opens a brand-new optimization space for the end-to-end\nperformance of LLM-based applications. Extensive evaluations demonstrate that\nParrot can achieve up to an order-of-magnitude improvement for popular and\npractical use cases of LLM applications."}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8931491b-6711-4bc1-b824-c863efd9bd88", "node_type": "4", "metadata": {"page_label": "13", "file_name": "2405_19888v1.pdf", "Title of this paper": "Parrot: Efficient Serving of LLM-based Applications with Semantic Variable", "Authors": "Chaofan Lin, Zhenhua Han, Chengruidong Zhang, Yuqing Yang, Fan Yang, Chen Chen, Lili Qiu", "Date published": "05/30/2024", "URL": "http://arxiv.org/abs/2405.19888v1", "summary": "The rise of large language models (LLMs) has enabled LLM-based applications\n(a.k.a. AI agents or co-pilots), a new software paradigm that combines the\nstrength of LLM and conventional software. Diverse LLM applications from\ndifferent tenants could design complex workflows using multiple LLM requests to\naccomplish one task. However, they have to use the over-simplified\nrequest-level API provided by today's public LLM services, losing essential\napplication-level information. Public LLM services have to blindly optimize\nindividual LLM requests, leading to sub-optimal end-to-end performance of LLM\napplications.\n  This paper introduces Parrot, an LLM service system that focuses on the\nend-to-end experience of LLM-based applications. Parrot proposes Semantic\nVariable, a unified abstraction to expose application-level knowledge to public\nLLM services. A Semantic Variable annotates an input/output variable in the\nprompt of a request, and creates the data pipeline when connecting multiple LLM\nrequests, providing a natural way to program LLM applications. Exposing\nSemantic Variables to the public LLM service allows it to perform conventional\ndata flow analysis to uncover the correlation across multiple LLM requests.\nThis correlation opens a brand-new optimization space for the end-to-end\nperformance of LLM-based applications. Extensive evaluations demonstrate that\nParrot can achieve up to an order-of-magnitude improvement for popular and\npractical use cases of LLM applications."}, "hash": "fcec32f3abc0db889cc76d5cf7041fa3d591b172fdfe368977f1bc4c8bdfecdd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b0b924dc-5101-40ca-9efc-334e6b57913a", "node_type": "1", "metadata": {"page_label": "13", "file_name": "2405_19888v1.pdf", "Title of this paper": "Parrot: Efficient Serving of LLM-based Applications with Semantic Variable", "Authors": "Chaofan Lin, Zhenhua Han, Chengruidong Zhang, Yuqing Yang, Fan Yang, Chen Chen, Lili Qiu", "Date published": "05/30/2024", "URL": "http://arxiv.org/abs/2405.19888v1", "summary": "The rise of large language models (LLMs) has enabled LLM-based applications\n(a.k.a. AI agents or co-pilots), a new software paradigm that combines the\nstrength of LLM and conventional software. Diverse LLM applications from\ndifferent tenants could design complex workflows using multiple LLM requests to\naccomplish one task. However, they have to use the over-simplified\nrequest-level API provided by today's public LLM services, losing essential\napplication-level information. Public LLM services have to blindly optimize\nindividual LLM requests, leading to sub-optimal end-to-end performance of LLM\napplications.\n  This paper introduces Parrot, an LLM service system that focuses on the\nend-to-end experience of LLM-based applications. Parrot proposes Semantic\nVariable, a unified abstraction to expose application-level knowledge to public\nLLM services. A Semantic Variable annotates an input/output variable in the\nprompt of a request, and creates the data pipeline when connecting multiple LLM\nrequests, providing a natural way to program LLM applications. Exposing\nSemantic Variables to the public LLM service allows it to perform conventional\ndata flow analysis to uncover the correlation across multiple LLM requests.\nThis correlation opens a brand-new optimization space for the end-to-end\nperformance of LLM-based applications. Extensive evaluations demonstrate that\nParrot can achieve up to an order-of-magnitude improvement for popular and\npractical use cases of LLM applications."}, "hash": "29e01464bdd0f255671a10f6819ccb318646cc21c6891202e92d5e3ab8822ed2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9e437f86-3703-43f5-87d3-7bf016b69d2c", "node_type": "1", "metadata": {}, "hash": "577f8e1d59fc10894756d9bedd684a81952a753eb7e0ba519376770941bbd6a6", "class_name": "RelatedNodeInfo"}}, "text": "These systems were proposed for serving general deep learn-\ning models, which have less consideration about the unique\nrequirements of large language models, e.g., autoregressive\ndecoding. Orca [56] proposed a fine-grained scheduling mech-\nanism that can batch multiple LLM requests at the iteration\nlevel, which is also known as continuous batching. vLLM\nproposes PagedAttention [25] allows the batching of LLM\nrequests with different lengths using non-contiguous memory,\nincreasing memory utilization. These systems for LLM serv-\ning still treat LLM requests separately, missing the opportuni-\nties to understand the interconnections within an application\nand exploit the commonality of different requests. Parrot isorthogonal to them. With more application-level knowledge\nexposed by Semantic Variables, Parrot can do data flow analy-\nsis on LLM requests, which enables a brand new optimization\nspace with the final goal of optimizing the end-to-end perfor-\nmance of applications, rather than individual requests.\nLLM Orchestrator Frameworks. LLM orchestration\nframeworks help developers create and manage applications\npowered by LLMs. They simplify the process of prompt de-\nsign, and orchestration of multiple LLM requests, which en-\nable developers to interact with LLMs easily. LangChain [8]\nis a Python framework that provides many workflow patterns,\ne.g., chain, map-reduce so that developers can easily cus-\ntomize their own LLM applications. Semantic Kernel [36]\nintroduces Planners are semantic agents that can automati-\ncally generate plans based on the needs of the users. Prompt-\nFlow [35] supports chains of native and semantic functions\nand visualizes them as a graph. LlamaIndex [29] allows de-\nvelopers to use natural language queries to retrieve relevant\ndocuments. Parrot is orthogonal to these frameworks and can\nbe easily integrated with these frameworks to support Parrot\u2019s\nAPIs with Semantic Variable abstraction, as discussed in \u00a76.\nDAG-aware System Optimizations. Dependency graphs\nor DAGs (Directed Acyclic Graphs) widely exist in many\nkinds of systems, and many optimizations have been proposed\nto optimize the systems by exploiting the DAG information.\nTez [4], Dryad [23], and Graphene [16] use the task depen-\ndency to optimize the scheduling and packing of parallel data\nanalytic workloads. SONIC [30], Caerus [59], and Orion [31]\noptimize serverless functions from the aspects of communica-\ntion, latency, and cost. Parrot learns from the previous system\nworks and realizes the importance of correlations of LLM\nrequests to optimize the end-to-end performance of LLM ap-\nplications. This motivates Parrot to build APIs for exposing\nsuch dependency information.", "mimetype": "text/plain", "start_char_idx": 1841, "end_char_idx": 4535, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9e437f86-3703-43f5-87d3-7bf016b69d2c": {"__data__": {"id_": "9e437f86-3703-43f5-87d3-7bf016b69d2c", "embedding": null, "metadata": {"page_label": "13", "file_name": "2405_19888v1.pdf", "Title of this paper": "Parrot: Efficient Serving of LLM-based Applications with Semantic Variable", "Authors": "Chaofan Lin, Zhenhua Han, Chengruidong Zhang, Yuqing Yang, Fan Yang, Chen Chen, Lili Qiu", "Date published": "05/30/2024", "URL": "http://arxiv.org/abs/2405.19888v1", "summary": "The rise of large language models (LLMs) has enabled LLM-based applications\n(a.k.a. AI agents or co-pilots), a new software paradigm that combines the\nstrength of LLM and conventional software. Diverse LLM applications from\ndifferent tenants could design complex workflows using multiple LLM requests to\naccomplish one task. However, they have to use the over-simplified\nrequest-level API provided by today's public LLM services, losing essential\napplication-level information. Public LLM services have to blindly optimize\nindividual LLM requests, leading to sub-optimal end-to-end performance of LLM\napplications.\n  This paper introduces Parrot, an LLM service system that focuses on the\nend-to-end experience of LLM-based applications. Parrot proposes Semantic\nVariable, a unified abstraction to expose application-level knowledge to public\nLLM services. A Semantic Variable annotates an input/output variable in the\nprompt of a request, and creates the data pipeline when connecting multiple LLM\nrequests, providing a natural way to program LLM applications. Exposing\nSemantic Variables to the public LLM service allows it to perform conventional\ndata flow analysis to uncover the correlation across multiple LLM requests.\nThis correlation opens a brand-new optimization space for the end-to-end\nperformance of LLM-based applications. Extensive evaluations demonstrate that\nParrot can achieve up to an order-of-magnitude improvement for popular and\npractical use cases of LLM applications."}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8931491b-6711-4bc1-b824-c863efd9bd88", "node_type": "4", "metadata": {"page_label": "13", "file_name": "2405_19888v1.pdf", "Title of this paper": "Parrot: Efficient Serving of LLM-based Applications with Semantic Variable", "Authors": "Chaofan Lin, Zhenhua Han, Chengruidong Zhang, Yuqing Yang, Fan Yang, Chen Chen, Lili Qiu", "Date published": "05/30/2024", "URL": "http://arxiv.org/abs/2405.19888v1", "summary": "The rise of large language models (LLMs) has enabled LLM-based applications\n(a.k.a. AI agents or co-pilots), a new software paradigm that combines the\nstrength of LLM and conventional software. Diverse LLM applications from\ndifferent tenants could design complex workflows using multiple LLM requests to\naccomplish one task. However, they have to use the over-simplified\nrequest-level API provided by today's public LLM services, losing essential\napplication-level information. Public LLM services have to blindly optimize\nindividual LLM requests, leading to sub-optimal end-to-end performance of LLM\napplications.\n  This paper introduces Parrot, an LLM service system that focuses on the\nend-to-end experience of LLM-based applications. Parrot proposes Semantic\nVariable, a unified abstraction to expose application-level knowledge to public\nLLM services. A Semantic Variable annotates an input/output variable in the\nprompt of a request, and creates the data pipeline when connecting multiple LLM\nrequests, providing a natural way to program LLM applications. Exposing\nSemantic Variables to the public LLM service allows it to perform conventional\ndata flow analysis to uncover the correlation across multiple LLM requests.\nThis correlation opens a brand-new optimization space for the end-to-end\nperformance of LLM-based applications. Extensive evaluations demonstrate that\nParrot can achieve up to an order-of-magnitude improvement for popular and\npractical use cases of LLM applications."}, "hash": "fcec32f3abc0db889cc76d5cf7041fa3d591b172fdfe368977f1bc4c8bdfecdd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "774e005a-de09-4136-a514-dea6cac22cc2", "node_type": "1", "metadata": {"page_label": "13", "file_name": "2405_19888v1.pdf", "Title of this paper": "Parrot: Efficient Serving of LLM-based Applications with Semantic Variable", "Authors": "Chaofan Lin, Zhenhua Han, Chengruidong Zhang, Yuqing Yang, Fan Yang, Chen Chen, Lili Qiu", "Date published": "05/30/2024", "URL": "http://arxiv.org/abs/2405.19888v1", "summary": "The rise of large language models (LLMs) has enabled LLM-based applications\n(a.k.a. AI agents or co-pilots), a new software paradigm that combines the\nstrength of LLM and conventional software. Diverse LLM applications from\ndifferent tenants could design complex workflows using multiple LLM requests to\naccomplish one task. However, they have to use the over-simplified\nrequest-level API provided by today's public LLM services, losing essential\napplication-level information. Public LLM services have to blindly optimize\nindividual LLM requests, leading to sub-optimal end-to-end performance of LLM\napplications.\n  This paper introduces Parrot, an LLM service system that focuses on the\nend-to-end experience of LLM-based applications. Parrot proposes Semantic\nVariable, a unified abstraction to expose application-level knowledge to public\nLLM services. A Semantic Variable annotates an input/output variable in the\nprompt of a request, and creates the data pipeline when connecting multiple LLM\nrequests, providing a natural way to program LLM applications. Exposing\nSemantic Variables to the public LLM service allows it to perform conventional\ndata flow analysis to uncover the correlation across multiple LLM requests.\nThis correlation opens a brand-new optimization space for the end-to-end\nperformance of LLM-based applications. Extensive evaluations demonstrate that\nParrot can achieve up to an order-of-magnitude improvement for popular and\npractical use cases of LLM applications."}, "hash": "4864bf246437f0caefdfa099fb30e2c383042e613d837b36b5e5806bbbdac6c2", "class_name": "RelatedNodeInfo"}}, "text": "DAG-aware System Optimizations. Dependency graphs\nor DAGs (Directed Acyclic Graphs) widely exist in many\nkinds of systems, and many optimizations have been proposed\nto optimize the systems by exploiting the DAG information.\nTez [4], Dryad [23], and Graphene [16] use the task depen-\ndency to optimize the scheduling and packing of parallel data\nanalytic workloads. SONIC [30], Caerus [59], and Orion [31]\noptimize serverless functions from the aspects of communica-\ntion, latency, and cost. Parrot learns from the previous system\nworks and realizes the importance of correlations of LLM\nrequests to optimize the end-to-end performance of LLM ap-\nplications. This motivates Parrot to build APIs for exposing\nsuch dependency information. Moreover, it is unique to LLM\napplications to understand the prompt structure in addition to\nrequest-level dependency, which is necessary for communica-\ntion and identifying commonality across LLM requests. This\nmotivates us to propose the Semantic Variable abstraction,\ninstead of just using a DAG of requests.\n10 Conclusion\nThis paper proposes Parrot that treats LLM applications as\nfirst-class citizens and targets to optimize the end-to-end per-\nformance of LLM applications, instead of only optimizing\nindividual LLM requests. We propose Semantic Variable as\nthe key abstraction that exposes the dependency and common-\nality of LLM requests, enabling a new optimization space.\nOur evaluation shows Parrot can optimize LLM-based ap-\nplications by up to 11.7\u00d7. We envision this new angle of\nefficiency improvement of LLM applications brings a broad", "mimetype": "text/plain", "start_char_idx": 3800, "end_char_idx": 5387, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "92bd5626-be87-4331-937b-b9aa0fd9930d": {"__data__": {"id_": "92bd5626-be87-4331-937b-b9aa0fd9930d", "embedding": null, "metadata": {"page_label": "14", "file_name": "2405_19888v1.pdf", "Title of this paper": "Parrot: Efficient Serving of LLM-based Applications with Semantic Variable", "Authors": "Chaofan Lin, Zhenhua Han, Chengruidong Zhang, Yuqing Yang, Fan Yang, Chen Chen, Lili Qiu", "Date published": "05/30/2024", "URL": "http://arxiv.org/abs/2405.19888v1", "summary": "The rise of large language models (LLMs) has enabled LLM-based applications\n(a.k.a. AI agents or co-pilots), a new software paradigm that combines the\nstrength of LLM and conventional software. Diverse LLM applications from\ndifferent tenants could design complex workflows using multiple LLM requests to\naccomplish one task. However, they have to use the over-simplified\nrequest-level API provided by today's public LLM services, losing essential\napplication-level information. Public LLM services have to blindly optimize\nindividual LLM requests, leading to sub-optimal end-to-end performance of LLM\napplications.\n  This paper introduces Parrot, an LLM service system that focuses on the\nend-to-end experience of LLM-based applications. Parrot proposes Semantic\nVariable, a unified abstraction to expose application-level knowledge to public\nLLM services. A Semantic Variable annotates an input/output variable in the\nprompt of a request, and creates the data pipeline when connecting multiple LLM\nrequests, providing a natural way to program LLM applications. Exposing\nSemantic Variables to the public LLM service allows it to perform conventional\ndata flow analysis to uncover the correlation across multiple LLM requests.\nThis correlation opens a brand-new optimization space for the end-to-end\nperformance of LLM-based applications. Extensive evaluations demonstrate that\nParrot can achieve up to an order-of-magnitude improvement for popular and\npractical use cases of LLM applications."}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ffb52c6f-dc64-4f9b-be30-05c3786cac37", "node_type": "4", "metadata": {"page_label": "14", "file_name": "2405_19888v1.pdf", "Title of this paper": "Parrot: Efficient Serving of LLM-based Applications with Semantic Variable", "Authors": "Chaofan Lin, Zhenhua Han, Chengruidong Zhang, Yuqing Yang, Fan Yang, Chen Chen, Lili Qiu", "Date published": "05/30/2024", "URL": "http://arxiv.org/abs/2405.19888v1", "summary": "The rise of large language models (LLMs) has enabled LLM-based applications\n(a.k.a. AI agents or co-pilots), a new software paradigm that combines the\nstrength of LLM and conventional software. Diverse LLM applications from\ndifferent tenants could design complex workflows using multiple LLM requests to\naccomplish one task. However, they have to use the over-simplified\nrequest-level API provided by today's public LLM services, losing essential\napplication-level information. Public LLM services have to blindly optimize\nindividual LLM requests, leading to sub-optimal end-to-end performance of LLM\napplications.\n  This paper introduces Parrot, an LLM service system that focuses on the\nend-to-end experience of LLM-based applications. Parrot proposes Semantic\nVariable, a unified abstraction to expose application-level knowledge to public\nLLM services. A Semantic Variable annotates an input/output variable in the\nprompt of a request, and creates the data pipeline when connecting multiple LLM\nrequests, providing a natural way to program LLM applications. Exposing\nSemantic Variables to the public LLM service allows it to perform conventional\ndata flow analysis to uncover the correlation across multiple LLM requests.\nThis correlation opens a brand-new optimization space for the end-to-end\nperformance of LLM-based applications. Extensive evaluations demonstrate that\nParrot can achieve up to an order-of-magnitude improvement for popular and\npractical use cases of LLM applications."}, "hash": "5194841e39445758b5da6d6b60f209dff74ea8e9543b8fb68f997ae6d093bf18", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6e8ee0bc-90cb-4283-8400-cdba598f6e4a", "node_type": "1", "metadata": {}, "hash": "c049ecbc38db82d24740307b23a325dabf19c3c2c764767c696a956403d7d1eb", "class_name": "RelatedNodeInfo"}}, "text": "future direction to study other scheduling features like the\nfairness of end-to-end performance of LLM applications.\nAcknowledgments\nWe thank the anonymous reviewers and the shepherd for their\nconstructive feedback and suggestions. Zhenhua Han, Yuqing\nYang and Chen Chen are the corresponding authors.\nReferences\n[1]Mart\u00edn Abadi, Paul Barham, Jianmin Chen, Zhifeng\nChen, Andy Davis, Jeffrey Dean, Matthieu Devin, San-\njay Ghemawat, Geoffrey Irving, Michael Isard, Man-\njunath Kudlur, Josh Levenberg, Rajat Monga, Sherry\nMoore, Derek G. Murray, Benoit Steiner, Paul Tucker,\nVijay Vasudevan, Pete Warden, Martin Wicke, Yuan Yu,\nand Xiaoqiang Zheng. TensorFlow: A system for Large-\nScale machine learning. In 12th USENIX Symposium on\nOperating Systems Design and Implementation (OSDI\n16), pages 265\u2013283, Savannah, GA, November 2016.\nUSENIX Association.\n[2]Amey Agrawal, Nitin Kedia, Ashish Panwar, Jayashree\nMohan, Nipun Kwatra, Bhargav S Gulavani, Alexey Tu-\nmanov, and Ramachandran Ramjee. Taming throughput-\nlatency tradeoff in llm inference with sarathi-serve.\narXiv preprint arXiv:2403.02310 , 2024.\n[3]Ganesh Ananthanarayanan, Srikanth Kandula, Albert\nGreenberg, Ion Stoica, Yi Lu, Bikas Saha, and Edward\nHarris. Reining in the outliers in Map-Reduce clusters\nusing mantri. In 9th USENIX Symposium on Operating\nSystems Design and Implementation (OSDI 10) , Van-\ncouver, BC, October 2010. USENIX Association.\n[4]Apache. Tez. https://tez.apache.org/ , November\n2019.\n[5]Apache. Kafka. https://kafka.apache.org/ , Octo-\nber 2023.\n[6]Zhengda Bian, Hongxin Liu, Boxiang Wang, Haichen\nHuang, Yongbin Li, Chuanrui Wang, Fan Cui, and Yang\nYou. Colossal-ai: A unified deep learning system for\nlarge-scale parallel training. CoRR , abs/2110.14883,\n2021.\n[7]S\u00e9bastien Bubeck, Varun Chandrasekaran, Ronen Eldan,\nJohannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee,\nYin Tat Lee, Yuanzhi Li, Scott Lundberg, Harsha Nori,\nHamid Palangi, Marco Tulio Ribeiro, and Yi Zhang.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1959, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6e8ee0bc-90cb-4283-8400-cdba598f6e4a": {"__data__": {"id_": "6e8ee0bc-90cb-4283-8400-cdba598f6e4a", "embedding": null, "metadata": {"page_label": "14", "file_name": "2405_19888v1.pdf", "Title of this paper": "Parrot: Efficient Serving of LLM-based Applications with Semantic Variable", "Authors": "Chaofan Lin, Zhenhua Han, Chengruidong Zhang, Yuqing Yang, Fan Yang, Chen Chen, Lili Qiu", "Date published": "05/30/2024", "URL": "http://arxiv.org/abs/2405.19888v1", "summary": "The rise of large language models (LLMs) has enabled LLM-based applications\n(a.k.a. AI agents or co-pilots), a new software paradigm that combines the\nstrength of LLM and conventional software. Diverse LLM applications from\ndifferent tenants could design complex workflows using multiple LLM requests to\naccomplish one task. However, they have to use the over-simplified\nrequest-level API provided by today's public LLM services, losing essential\napplication-level information. Public LLM services have to blindly optimize\nindividual LLM requests, leading to sub-optimal end-to-end performance of LLM\napplications.\n  This paper introduces Parrot, an LLM service system that focuses on the\nend-to-end experience of LLM-based applications. Parrot proposes Semantic\nVariable, a unified abstraction to expose application-level knowledge to public\nLLM services. A Semantic Variable annotates an input/output variable in the\nprompt of a request, and creates the data pipeline when connecting multiple LLM\nrequests, providing a natural way to program LLM applications. Exposing\nSemantic Variables to the public LLM service allows it to perform conventional\ndata flow analysis to uncover the correlation across multiple LLM requests.\nThis correlation opens a brand-new optimization space for the end-to-end\nperformance of LLM-based applications. Extensive evaluations demonstrate that\nParrot can achieve up to an order-of-magnitude improvement for popular and\npractical use cases of LLM applications."}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ffb52c6f-dc64-4f9b-be30-05c3786cac37", "node_type": "4", "metadata": {"page_label": "14", "file_name": "2405_19888v1.pdf", "Title of this paper": "Parrot: Efficient Serving of LLM-based Applications with Semantic Variable", "Authors": "Chaofan Lin, Zhenhua Han, Chengruidong Zhang, Yuqing Yang, Fan Yang, Chen Chen, Lili Qiu", "Date published": "05/30/2024", "URL": "http://arxiv.org/abs/2405.19888v1", "summary": "The rise of large language models (LLMs) has enabled LLM-based applications\n(a.k.a. AI agents or co-pilots), a new software paradigm that combines the\nstrength of LLM and conventional software. Diverse LLM applications from\ndifferent tenants could design complex workflows using multiple LLM requests to\naccomplish one task. However, they have to use the over-simplified\nrequest-level API provided by today's public LLM services, losing essential\napplication-level information. Public LLM services have to blindly optimize\nindividual LLM requests, leading to sub-optimal end-to-end performance of LLM\napplications.\n  This paper introduces Parrot, an LLM service system that focuses on the\nend-to-end experience of LLM-based applications. Parrot proposes Semantic\nVariable, a unified abstraction to expose application-level knowledge to public\nLLM services. A Semantic Variable annotates an input/output variable in the\nprompt of a request, and creates the data pipeline when connecting multiple LLM\nrequests, providing a natural way to program LLM applications. Exposing\nSemantic Variables to the public LLM service allows it to perform conventional\ndata flow analysis to uncover the correlation across multiple LLM requests.\nThis correlation opens a brand-new optimization space for the end-to-end\nperformance of LLM-based applications. Extensive evaluations demonstrate that\nParrot can achieve up to an order-of-magnitude improvement for popular and\npractical use cases of LLM applications."}, "hash": "5194841e39445758b5da6d6b60f209dff74ea8e9543b8fb68f997ae6d093bf18", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "92bd5626-be87-4331-937b-b9aa0fd9930d", "node_type": "1", "metadata": {"page_label": "14", "file_name": "2405_19888v1.pdf", "Title of this paper": "Parrot: Efficient Serving of LLM-based Applications with Semantic Variable", "Authors": "Chaofan Lin, Zhenhua Han, Chengruidong Zhang, Yuqing Yang, Fan Yang, Chen Chen, Lili Qiu", "Date published": "05/30/2024", "URL": "http://arxiv.org/abs/2405.19888v1", "summary": "The rise of large language models (LLMs) has enabled LLM-based applications\n(a.k.a. AI agents or co-pilots), a new software paradigm that combines the\nstrength of LLM and conventional software. Diverse LLM applications from\ndifferent tenants could design complex workflows using multiple LLM requests to\naccomplish one task. However, they have to use the over-simplified\nrequest-level API provided by today's public LLM services, losing essential\napplication-level information. Public LLM services have to blindly optimize\nindividual LLM requests, leading to sub-optimal end-to-end performance of LLM\napplications.\n  This paper introduces Parrot, an LLM service system that focuses on the\nend-to-end experience of LLM-based applications. Parrot proposes Semantic\nVariable, a unified abstraction to expose application-level knowledge to public\nLLM services. A Semantic Variable annotates an input/output variable in the\nprompt of a request, and creates the data pipeline when connecting multiple LLM\nrequests, providing a natural way to program LLM applications. Exposing\nSemantic Variables to the public LLM service allows it to perform conventional\ndata flow analysis to uncover the correlation across multiple LLM requests.\nThis correlation opens a brand-new optimization space for the end-to-end\nperformance of LLM-based applications. Extensive evaluations demonstrate that\nParrot can achieve up to an order-of-magnitude improvement for popular and\npractical use cases of LLM applications."}, "hash": "e53fe7f4125842d83f3dfc0887843a8718b4bbcd2cc0e2754b5508e280916079", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c9882fdf-97aa-4574-aa6d-bae79f6dd5c8", "node_type": "1", "metadata": {}, "hash": "9a0657f3a5295b2412f3441cf38a05d188b77723d40a9087f8d4dee53b11b5b1", "class_name": "RelatedNodeInfo"}}, "text": "Tez. https://tez.apache.org/ , November\n2019.\n[5]Apache. Kafka. https://kafka.apache.org/ , Octo-\nber 2023.\n[6]Zhengda Bian, Hongxin Liu, Boxiang Wang, Haichen\nHuang, Yongbin Li, Chuanrui Wang, Fan Cui, and Yang\nYou. Colossal-ai: A unified deep learning system for\nlarge-scale parallel training. CoRR , abs/2110.14883,\n2021.\n[7]S\u00e9bastien Bubeck, Varun Chandrasekaran, Ronen Eldan,\nJohannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee,\nYin Tat Lee, Yuanzhi Li, Scott Lundberg, Harsha Nori,\nHamid Palangi, Marco Tulio Ribeiro, and Yi Zhang.\nSparks of artificial general intelligence: Early experi-\nments with gpt-4, 2023.[8]Harrison Chase. LangChain. https://github.com/\nlangchain-ai/langchain , October 2022.\n[9]Lequn Chen. Dissecting batching effects in gpt infer-\nence. https://le.qun.ch/en/blog/2023/05/13/\ntransformer-batching/ , May 2023.\n[10] Daniel Crankshaw, Xin Wang, Guilio Zhou, Michael J.\nFranklin, Joseph E. Gonzalez, and Ion Stoica. Clipper:\nA Low-Latency online prediction serving system. In\n14th USENIX Symposium on Networked Systems Design\nand Implementation (NSDI 17) , pages 613\u2013627, Boston,\nMA, March 2017. USENIX Association.\n[11] Tri Dao. Flashattention-2: Faster attention with bet-\nter parallelism and work partitioning. arXiv preprint\narXiv:2307.08691 , 2023.\n[12] Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and\nChristopher R\u00e9. Flashattention: Fast and memory-\nefficient exact attention with io-awareness. In S. Koyejo,\nS. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and\nA. Oh, editors, Advances in Neural Information Process-\ning Systems , volume 35, pages 16344\u201316359. Curran\nAssociates, Inc., 2022.\n[13] Bill Gates. Ai is about to completely change how you\nuse computers and upend the software industry. https:\n//www.gatesnotes.com/AI-agents , Nov 2023.\n[14] Google. Google bard. https://bard.google.com/ ,\nNov 2023.", "mimetype": "text/plain", "start_char_idx": 1422, "end_char_idx": 3263, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c9882fdf-97aa-4574-aa6d-bae79f6dd5c8": {"__data__": {"id_": "c9882fdf-97aa-4574-aa6d-bae79f6dd5c8", "embedding": null, "metadata": {"page_label": "14", "file_name": "2405_19888v1.pdf", "Title of this paper": "Parrot: Efficient Serving of LLM-based Applications with Semantic Variable", "Authors": "Chaofan Lin, Zhenhua Han, Chengruidong Zhang, Yuqing Yang, Fan Yang, Chen Chen, Lili Qiu", "Date published": "05/30/2024", "URL": "http://arxiv.org/abs/2405.19888v1", "summary": "The rise of large language models (LLMs) has enabled LLM-based applications\n(a.k.a. AI agents or co-pilots), a new software paradigm that combines the\nstrength of LLM and conventional software. Diverse LLM applications from\ndifferent tenants could design complex workflows using multiple LLM requests to\naccomplish one task. However, they have to use the over-simplified\nrequest-level API provided by today's public LLM services, losing essential\napplication-level information. Public LLM services have to blindly optimize\nindividual LLM requests, leading to sub-optimal end-to-end performance of LLM\napplications.\n  This paper introduces Parrot, an LLM service system that focuses on the\nend-to-end experience of LLM-based applications. Parrot proposes Semantic\nVariable, a unified abstraction to expose application-level knowledge to public\nLLM services. A Semantic Variable annotates an input/output variable in the\nprompt of a request, and creates the data pipeline when connecting multiple LLM\nrequests, providing a natural way to program LLM applications. Exposing\nSemantic Variables to the public LLM service allows it to perform conventional\ndata flow analysis to uncover the correlation across multiple LLM requests.\nThis correlation opens a brand-new optimization space for the end-to-end\nperformance of LLM-based applications. Extensive evaluations demonstrate that\nParrot can achieve up to an order-of-magnitude improvement for popular and\npractical use cases of LLM applications."}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ffb52c6f-dc64-4f9b-be30-05c3786cac37", "node_type": "4", "metadata": {"page_label": "14", "file_name": "2405_19888v1.pdf", "Title of this paper": "Parrot: Efficient Serving of LLM-based Applications with Semantic Variable", "Authors": "Chaofan Lin, Zhenhua Han, Chengruidong Zhang, Yuqing Yang, Fan Yang, Chen Chen, Lili Qiu", "Date published": "05/30/2024", "URL": "http://arxiv.org/abs/2405.19888v1", "summary": "The rise of large language models (LLMs) has enabled LLM-based applications\n(a.k.a. AI agents or co-pilots), a new software paradigm that combines the\nstrength of LLM and conventional software. Diverse LLM applications from\ndifferent tenants could design complex workflows using multiple LLM requests to\naccomplish one task. However, they have to use the over-simplified\nrequest-level API provided by today's public LLM services, losing essential\napplication-level information. Public LLM services have to blindly optimize\nindividual LLM requests, leading to sub-optimal end-to-end performance of LLM\napplications.\n  This paper introduces Parrot, an LLM service system that focuses on the\nend-to-end experience of LLM-based applications. Parrot proposes Semantic\nVariable, a unified abstraction to expose application-level knowledge to public\nLLM services. A Semantic Variable annotates an input/output variable in the\nprompt of a request, and creates the data pipeline when connecting multiple LLM\nrequests, providing a natural way to program LLM applications. Exposing\nSemantic Variables to the public LLM service allows it to perform conventional\ndata flow analysis to uncover the correlation across multiple LLM requests.\nThis correlation opens a brand-new optimization space for the end-to-end\nperformance of LLM-based applications. Extensive evaluations demonstrate that\nParrot can achieve up to an order-of-magnitude improvement for popular and\npractical use cases of LLM applications."}, "hash": "5194841e39445758b5da6d6b60f209dff74ea8e9543b8fb68f997ae6d093bf18", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6e8ee0bc-90cb-4283-8400-cdba598f6e4a", "node_type": "1", "metadata": {"page_label": "14", "file_name": "2405_19888v1.pdf", "Title of this paper": "Parrot: Efficient Serving of LLM-based Applications with Semantic Variable", "Authors": "Chaofan Lin, Zhenhua Han, Chengruidong Zhang, Yuqing Yang, Fan Yang, Chen Chen, Lili Qiu", "Date published": "05/30/2024", "URL": "http://arxiv.org/abs/2405.19888v1", "summary": "The rise of large language models (LLMs) has enabled LLM-based applications\n(a.k.a. AI agents or co-pilots), a new software paradigm that combines the\nstrength of LLM and conventional software. Diverse LLM applications from\ndifferent tenants could design complex workflows using multiple LLM requests to\naccomplish one task. However, they have to use the over-simplified\nrequest-level API provided by today's public LLM services, losing essential\napplication-level information. Public LLM services have to blindly optimize\nindividual LLM requests, leading to sub-optimal end-to-end performance of LLM\napplications.\n  This paper introduces Parrot, an LLM service system that focuses on the\nend-to-end experience of LLM-based applications. Parrot proposes Semantic\nVariable, a unified abstraction to expose application-level knowledge to public\nLLM services. A Semantic Variable annotates an input/output variable in the\nprompt of a request, and creates the data pipeline when connecting multiple LLM\nrequests, providing a natural way to program LLM applications. Exposing\nSemantic Variables to the public LLM service allows it to perform conventional\ndata flow analysis to uncover the correlation across multiple LLM requests.\nThis correlation opens a brand-new optimization space for the end-to-end\nperformance of LLM-based applications. Extensive evaluations demonstrate that\nParrot can achieve up to an order-of-magnitude improvement for popular and\npractical use cases of LLM applications."}, "hash": "df8984115ee1b69c2ded5dc7ff3586a7ce6ba662265b7e5cadf147fb6763cb34", "class_name": "RelatedNodeInfo"}}, "text": "arXiv preprint\narXiv:2307.08691 , 2023.\n[12] Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and\nChristopher R\u00e9. Flashattention: Fast and memory-\nefficient exact attention with io-awareness. In S. Koyejo,\nS. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and\nA. Oh, editors, Advances in Neural Information Process-\ning Systems , volume 35, pages 16344\u201316359. Curran\nAssociates, Inc., 2022.\n[13] Bill Gates. Ai is about to completely change how you\nuse computers and upend the software industry. https:\n//www.gatesnotes.com/AI-agents , Nov 2023.\n[14] Google. Google bard. https://bard.google.com/ ,\nNov 2023.\n[15] Robert Grandl, Mosharaf Chowdhury, Aditya Akella,\nand Ganesh Ananthanarayanan. Altruistic scheduling\nin Multi-Resource clusters. In 12th USENIX Sympo-\nsium on Operating Systems Design and Implementa-\ntion (OSDI 16) , pages 65\u201380, Savannah, GA, November\n2016. USENIX Association.\n[16] Robert Grandl, Srikanth Kandula, Sriram Rao, Aditya\nAkella, and Janardhan Kulkarni. GRAPHENE: Packing\nand Dependency-Aware scheduling for Data-Parallel\nclusters. In 12th USENIX Symposium on Operating\nSystems Design and Implementation (OSDI 16) , pages\n81\u201397, Savannah, GA, November 2016. USENIX Asso-\nciation.\n[17] Juncheng Gu, Mosharaf Chowdhury, Kang G. Shin,\nYibo Zhu, Myeongjae Jeon, Junjie Qian, Hongqiang Liu,\nand Chuanxiong Guo. Tiresias: A GPU cluster manager\nfor distributed deep learning. In 16th USENIX Sympo-\nsium on Networked Systems Design and Implementation\n(NSDI 19) , pages 485\u2013500, Boston, MA, February 2019.\nUSENIX Association.\n[18] guidance ai. Guidance. https://github.com/\nguidance-ai/guidance , November 2023.", "mimetype": "text/plain", "start_char_idx": 2665, "end_char_idx": 4285, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "aaf8e015-6662-455f-afb8-57d94a3cba72": {"__data__": {"id_": "aaf8e015-6662-455f-afb8-57d94a3cba72", "embedding": null, "metadata": {"page_label": "15", "file_name": "2405_19888v1.pdf", "Title of this paper": "Parrot: Efficient Serving of LLM-based Applications with Semantic Variable", "Authors": "Chaofan Lin, Zhenhua Han, Chengruidong Zhang, Yuqing Yang, Fan Yang, Chen Chen, Lili Qiu", "Date published": "05/30/2024", "URL": "http://arxiv.org/abs/2405.19888v1", "summary": "The rise of large language models (LLMs) has enabled LLM-based applications\n(a.k.a. AI agents or co-pilots), a new software paradigm that combines the\nstrength of LLM and conventional software. Diverse LLM applications from\ndifferent tenants could design complex workflows using multiple LLM requests to\naccomplish one task. However, they have to use the over-simplified\nrequest-level API provided by today's public LLM services, losing essential\napplication-level information. Public LLM services have to blindly optimize\nindividual LLM requests, leading to sub-optimal end-to-end performance of LLM\napplications.\n  This paper introduces Parrot, an LLM service system that focuses on the\nend-to-end experience of LLM-based applications. Parrot proposes Semantic\nVariable, a unified abstraction to expose application-level knowledge to public\nLLM services. A Semantic Variable annotates an input/output variable in the\nprompt of a request, and creates the data pipeline when connecting multiple LLM\nrequests, providing a natural way to program LLM applications. Exposing\nSemantic Variables to the public LLM service allows it to perform conventional\ndata flow analysis to uncover the correlation across multiple LLM requests.\nThis correlation opens a brand-new optimization space for the end-to-end\nperformance of LLM-based applications. Extensive evaluations demonstrate that\nParrot can achieve up to an order-of-magnitude improvement for popular and\npractical use cases of LLM applications."}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8ca79964-0014-454e-873a-34fd27a7d9ac", "node_type": "4", "metadata": {"page_label": "15", "file_name": "2405_19888v1.pdf", "Title of this paper": "Parrot: Efficient Serving of LLM-based Applications with Semantic Variable", "Authors": "Chaofan Lin, Zhenhua Han, Chengruidong Zhang, Yuqing Yang, Fan Yang, Chen Chen, Lili Qiu", "Date published": "05/30/2024", "URL": "http://arxiv.org/abs/2405.19888v1", "summary": "The rise of large language models (LLMs) has enabled LLM-based applications\n(a.k.a. AI agents or co-pilots), a new software paradigm that combines the\nstrength of LLM and conventional software. Diverse LLM applications from\ndifferent tenants could design complex workflows using multiple LLM requests to\naccomplish one task. However, they have to use the over-simplified\nrequest-level API provided by today's public LLM services, losing essential\napplication-level information. Public LLM services have to blindly optimize\nindividual LLM requests, leading to sub-optimal end-to-end performance of LLM\napplications.\n  This paper introduces Parrot, an LLM service system that focuses on the\nend-to-end experience of LLM-based applications. Parrot proposes Semantic\nVariable, a unified abstraction to expose application-level knowledge to public\nLLM services. A Semantic Variable annotates an input/output variable in the\nprompt of a request, and creates the data pipeline when connecting multiple LLM\nrequests, providing a natural way to program LLM applications. Exposing\nSemantic Variables to the public LLM service allows it to perform conventional\ndata flow analysis to uncover the correlation across multiple LLM requests.\nThis correlation opens a brand-new optimization space for the end-to-end\nperformance of LLM-based applications. Extensive evaluations demonstrate that\nParrot can achieve up to an order-of-magnitude improvement for popular and\npractical use cases of LLM applications."}, "hash": "48785ccca23fcedf44fad59a551e79db39f0c89350bfca43bada6bc2148b5e81", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "58284db1-384a-412b-8fe3-cc6b58a32c92", "node_type": "1", "metadata": {}, "hash": "7520cc31273a1ce575576420d62fe11edae40d530b2d0eb5702970e6f7c1932f", "class_name": "RelatedNodeInfo"}}, "text": "[19] Arpan Gujarati, Reza Karimi, Safya Alzayat, Wei Hao,\nAntoine Kaufmann, Ymir Vigfusson, and Jonathan\nMace. Serving DNNs like clockwork: Performance\npredictability from the bottom up. In 14th USENIX Sym-\nposium on Operating Systems Design and Implementa-\ntion (OSDI 20) , pages 443\u2013462. USENIX Association,\nNovember 2020.\n[20] Mingcong Han, Hanze Zhang, Rong Chen, and Haibo\nChen. Microsecond-scale preemption for concurrent\nGPU-accelerated DNN inferences. In 16th USENIX\nSymposium on Operating Systems Design and Imple-\nmentation (OSDI 22) , pages 539\u2013558, Carlsbad, CA,\nJuly 2022. USENIX Association.\n[21] Connor Holmes, Masahiro Tanaka, Michael Wyatt, Am-\nmar Ahmad Awan, Jeff Rasley, Samyam Rajbhan-\ndari, Reza Yazdani Aminabadi, Heyang Qin, Arash\nBakhtiari, Lev Kurilenko, et al. Deepspeed-fastgen:\nHigh-throughput text generation for llms via mii and\ndeepspeed-inference. arXiv preprint arXiv:2401.08671 ,\n2024.\n[22] Sirui Hong, Xiawu Zheng, Jonathan Chen, Yuheng\nCheng, Ceyao Zhang, Zili Wang, Steven Ka Shing Yau,\nZijuan Lin, Liyang Zhou, Chenyu Ran, et al. Metagpt:\nMeta programming for multi-agent collaborative frame-\nwork. arXiv preprint arXiv:2308.00352 , 2023.\n[23] Michael Isard, Mihai Budiu, Yuan Yu, Andrew Birrell,\nand Dennis Fetterly. Dryad: Distributed data-parallel\nprograms from sequential building blocks. In Proceed-\nings of the 2nd ACM SIGOPS/EuroSys European Con-\nference on Computer Systems 2007 , EuroSys \u201907, page\n59\u201372, New York, NY , USA, 2007. Association for Com-\nputing Machinery.\n[24] Suhas Jayaram Subramanya, Daiyaan Arfeen, Shouxu\nLin, Aurick Qiao, Zhihao Jia, and Gregory R. Ganger.\nSia: Heterogeneity-aware, goodput-optimized ml-cluster\nscheduling. In Proceedings of the 29th Symposium on\nOperating Systems Principles , SOSP \u201923, page 642\u2013657,\nNew York, NY , USA, 2023. Association for Computing\nMachinery.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1849, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "58284db1-384a-412b-8fe3-cc6b58a32c92": {"__data__": {"id_": "58284db1-384a-412b-8fe3-cc6b58a32c92", "embedding": null, "metadata": {"page_label": "15", "file_name": "2405_19888v1.pdf", "Title of this paper": "Parrot: Efficient Serving of LLM-based Applications with Semantic Variable", "Authors": "Chaofan Lin, Zhenhua Han, Chengruidong Zhang, Yuqing Yang, Fan Yang, Chen Chen, Lili Qiu", "Date published": "05/30/2024", "URL": "http://arxiv.org/abs/2405.19888v1", "summary": "The rise of large language models (LLMs) has enabled LLM-based applications\n(a.k.a. AI agents or co-pilots), a new software paradigm that combines the\nstrength of LLM and conventional software. Diverse LLM applications from\ndifferent tenants could design complex workflows using multiple LLM requests to\naccomplish one task. However, they have to use the over-simplified\nrequest-level API provided by today's public LLM services, losing essential\napplication-level information. Public LLM services have to blindly optimize\nindividual LLM requests, leading to sub-optimal end-to-end performance of LLM\napplications.\n  This paper introduces Parrot, an LLM service system that focuses on the\nend-to-end experience of LLM-based applications. Parrot proposes Semantic\nVariable, a unified abstraction to expose application-level knowledge to public\nLLM services. A Semantic Variable annotates an input/output variable in the\nprompt of a request, and creates the data pipeline when connecting multiple LLM\nrequests, providing a natural way to program LLM applications. Exposing\nSemantic Variables to the public LLM service allows it to perform conventional\ndata flow analysis to uncover the correlation across multiple LLM requests.\nThis correlation opens a brand-new optimization space for the end-to-end\nperformance of LLM-based applications. Extensive evaluations demonstrate that\nParrot can achieve up to an order-of-magnitude improvement for popular and\npractical use cases of LLM applications."}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8ca79964-0014-454e-873a-34fd27a7d9ac", "node_type": "4", "metadata": {"page_label": "15", "file_name": "2405_19888v1.pdf", "Title of this paper": "Parrot: Efficient Serving of LLM-based Applications with Semantic Variable", "Authors": "Chaofan Lin, Zhenhua Han, Chengruidong Zhang, Yuqing Yang, Fan Yang, Chen Chen, Lili Qiu", "Date published": "05/30/2024", "URL": "http://arxiv.org/abs/2405.19888v1", "summary": "The rise of large language models (LLMs) has enabled LLM-based applications\n(a.k.a. AI agents or co-pilots), a new software paradigm that combines the\nstrength of LLM and conventional software. Diverse LLM applications from\ndifferent tenants could design complex workflows using multiple LLM requests to\naccomplish one task. However, they have to use the over-simplified\nrequest-level API provided by today's public LLM services, losing essential\napplication-level information. Public LLM services have to blindly optimize\nindividual LLM requests, leading to sub-optimal end-to-end performance of LLM\napplications.\n  This paper introduces Parrot, an LLM service system that focuses on the\nend-to-end experience of LLM-based applications. Parrot proposes Semantic\nVariable, a unified abstraction to expose application-level knowledge to public\nLLM services. A Semantic Variable annotates an input/output variable in the\nprompt of a request, and creates the data pipeline when connecting multiple LLM\nrequests, providing a natural way to program LLM applications. Exposing\nSemantic Variables to the public LLM service allows it to perform conventional\ndata flow analysis to uncover the correlation across multiple LLM requests.\nThis correlation opens a brand-new optimization space for the end-to-end\nperformance of LLM-based applications. Extensive evaluations demonstrate that\nParrot can achieve up to an order-of-magnitude improvement for popular and\npractical use cases of LLM applications."}, "hash": "48785ccca23fcedf44fad59a551e79db39f0c89350bfca43bada6bc2148b5e81", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "aaf8e015-6662-455f-afb8-57d94a3cba72", "node_type": "1", "metadata": {"page_label": "15", "file_name": "2405_19888v1.pdf", "Title of this paper": "Parrot: Efficient Serving of LLM-based Applications with Semantic Variable", "Authors": "Chaofan Lin, Zhenhua Han, Chengruidong Zhang, Yuqing Yang, Fan Yang, Chen Chen, Lili Qiu", "Date published": "05/30/2024", "URL": "http://arxiv.org/abs/2405.19888v1", "summary": "The rise of large language models (LLMs) has enabled LLM-based applications\n(a.k.a. AI agents or co-pilots), a new software paradigm that combines the\nstrength of LLM and conventional software. Diverse LLM applications from\ndifferent tenants could design complex workflows using multiple LLM requests to\naccomplish one task. However, they have to use the over-simplified\nrequest-level API provided by today's public LLM services, losing essential\napplication-level information. Public LLM services have to blindly optimize\nindividual LLM requests, leading to sub-optimal end-to-end performance of LLM\napplications.\n  This paper introduces Parrot, an LLM service system that focuses on the\nend-to-end experience of LLM-based applications. Parrot proposes Semantic\nVariable, a unified abstraction to expose application-level knowledge to public\nLLM services. A Semantic Variable annotates an input/output variable in the\nprompt of a request, and creates the data pipeline when connecting multiple LLM\nrequests, providing a natural way to program LLM applications. Exposing\nSemantic Variables to the public LLM service allows it to perform conventional\ndata flow analysis to uncover the correlation across multiple LLM requests.\nThis correlation opens a brand-new optimization space for the end-to-end\nperformance of LLM-based applications. Extensive evaluations demonstrate that\nParrot can achieve up to an order-of-magnitude improvement for popular and\npractical use cases of LLM applications."}, "hash": "9b0f425e32db208d32faa7364e91bc0f38135855ac1965b4c1573375837c67af", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "15c41549-39b3-4891-8ebe-8e9261a82581", "node_type": "1", "metadata": {}, "hash": "b67e1403de8379eaa2f7c04b2c16dadf4f3768a914bd07f872f75ef1e635adfb", "class_name": "RelatedNodeInfo"}}, "text": "Dryad: Distributed data-parallel\nprograms from sequential building blocks. In Proceed-\nings of the 2nd ACM SIGOPS/EuroSys European Con-\nference on Computer Systems 2007 , EuroSys \u201907, page\n59\u201372, New York, NY , USA, 2007. Association for Com-\nputing Machinery.\n[24] Suhas Jayaram Subramanya, Daiyaan Arfeen, Shouxu\nLin, Aurick Qiao, Zhihao Jia, and Gregory R. Ganger.\nSia: Heterogeneity-aware, goodput-optimized ml-cluster\nscheduling. In Proceedings of the 29th Symposium on\nOperating Systems Principles , SOSP \u201923, page 642\u2013657,\nNew York, NY , USA, 2023. Association for Computing\nMachinery.\n[25] Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying\nSheng, Lianmin Zheng, Cody Hao Yu, Joseph Gonzalez,\nHao Zhang, and Ion Stoica. Efficient memory man-\nagement for large language model serving with page-\ndattention. In Proceedings of the 29th Symposium on\nOperating Systems Principles , SOSP \u201923, page 611\u2013626,\nNew York, NY , USA, 2023. Association for Computing\nMachinery.\n[26] Benjamin Lefaudeux, Francisco Massa, Diana\nLiskovich, Wenhan Xiong, Vittorio Caggiano, Sean\nNaren, Min Xu, Jieru Hu, Marta Tintore, Susan Zhang,Patrick Labatut, and Daniel Haziza. xformers: A modu-\nlar and hackable transformer modelling library. https:\n//github.com/facebookresearch/xformers ,\n2022.\n[27] Yucheng Li. Unlocking context constraints of llms: En-\nhancing context efficiency of llms with self-information-\nbased content filtering, 2023.\n[28] Zhuohan Li, Lianmin Zheng, Yinmin Zhong, Vincent\nLiu, Ying Sheng, Xin Jin, Yanping Huang, Zhifeng Chen,\nHao Zhang, Joseph E. Gonzalez, and Ion Stoica. Al-\npaServe: Statistical multiplexing with model parallelism\nfor deep learning serving. In 17th USENIX Sympo-\nsium on Operating Systems Design and Implementa-\ntion (OSDI 23) , pages 663\u2013679, Boston, MA, July 2023.\nUSENIX Association.\n[29] Jerry Liu. LlamaIndex, November 2022.\n[30] Ashraf Mahgoub, Karthick Shankar, Subrata Mitra,\nAna Klimovic, Somali Chaterji, and Saurabh Bagchi.\nSONIC: Application-aware data passing for chained\nserverless applications.", "mimetype": "text/plain", "start_char_idx": 1257, "end_char_idx": 3289, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "15c41549-39b3-4891-8ebe-8e9261a82581": {"__data__": {"id_": "15c41549-39b3-4891-8ebe-8e9261a82581", "embedding": null, "metadata": {"page_label": "15", "file_name": "2405_19888v1.pdf", "Title of this paper": "Parrot: Efficient Serving of LLM-based Applications with Semantic Variable", "Authors": "Chaofan Lin, Zhenhua Han, Chengruidong Zhang, Yuqing Yang, Fan Yang, Chen Chen, Lili Qiu", "Date published": "05/30/2024", "URL": "http://arxiv.org/abs/2405.19888v1", "summary": "The rise of large language models (LLMs) has enabled LLM-based applications\n(a.k.a. AI agents or co-pilots), a new software paradigm that combines the\nstrength of LLM and conventional software. Diverse LLM applications from\ndifferent tenants could design complex workflows using multiple LLM requests to\naccomplish one task. However, they have to use the over-simplified\nrequest-level API provided by today's public LLM services, losing essential\napplication-level information. Public LLM services have to blindly optimize\nindividual LLM requests, leading to sub-optimal end-to-end performance of LLM\napplications.\n  This paper introduces Parrot, an LLM service system that focuses on the\nend-to-end experience of LLM-based applications. Parrot proposes Semantic\nVariable, a unified abstraction to expose application-level knowledge to public\nLLM services. A Semantic Variable annotates an input/output variable in the\nprompt of a request, and creates the data pipeline when connecting multiple LLM\nrequests, providing a natural way to program LLM applications. Exposing\nSemantic Variables to the public LLM service allows it to perform conventional\ndata flow analysis to uncover the correlation across multiple LLM requests.\nThis correlation opens a brand-new optimization space for the end-to-end\nperformance of LLM-based applications. Extensive evaluations demonstrate that\nParrot can achieve up to an order-of-magnitude improvement for popular and\npractical use cases of LLM applications."}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8ca79964-0014-454e-873a-34fd27a7d9ac", "node_type": "4", "metadata": {"page_label": "15", "file_name": "2405_19888v1.pdf", "Title of this paper": "Parrot: Efficient Serving of LLM-based Applications with Semantic Variable", "Authors": "Chaofan Lin, Zhenhua Han, Chengruidong Zhang, Yuqing Yang, Fan Yang, Chen Chen, Lili Qiu", "Date published": "05/30/2024", "URL": "http://arxiv.org/abs/2405.19888v1", "summary": "The rise of large language models (LLMs) has enabled LLM-based applications\n(a.k.a. AI agents or co-pilots), a new software paradigm that combines the\nstrength of LLM and conventional software. Diverse LLM applications from\ndifferent tenants could design complex workflows using multiple LLM requests to\naccomplish one task. However, they have to use the over-simplified\nrequest-level API provided by today's public LLM services, losing essential\napplication-level information. Public LLM services have to blindly optimize\nindividual LLM requests, leading to sub-optimal end-to-end performance of LLM\napplications.\n  This paper introduces Parrot, an LLM service system that focuses on the\nend-to-end experience of LLM-based applications. Parrot proposes Semantic\nVariable, a unified abstraction to expose application-level knowledge to public\nLLM services. A Semantic Variable annotates an input/output variable in the\nprompt of a request, and creates the data pipeline when connecting multiple LLM\nrequests, providing a natural way to program LLM applications. Exposing\nSemantic Variables to the public LLM service allows it to perform conventional\ndata flow analysis to uncover the correlation across multiple LLM requests.\nThis correlation opens a brand-new optimization space for the end-to-end\nperformance of LLM-based applications. Extensive evaluations demonstrate that\nParrot can achieve up to an order-of-magnitude improvement for popular and\npractical use cases of LLM applications."}, "hash": "48785ccca23fcedf44fad59a551e79db39f0c89350bfca43bada6bc2148b5e81", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "58284db1-384a-412b-8fe3-cc6b58a32c92", "node_type": "1", "metadata": {"page_label": "15", "file_name": "2405_19888v1.pdf", "Title of this paper": "Parrot: Efficient Serving of LLM-based Applications with Semantic Variable", "Authors": "Chaofan Lin, Zhenhua Han, Chengruidong Zhang, Yuqing Yang, Fan Yang, Chen Chen, Lili Qiu", "Date published": "05/30/2024", "URL": "http://arxiv.org/abs/2405.19888v1", "summary": "The rise of large language models (LLMs) has enabled LLM-based applications\n(a.k.a. AI agents or co-pilots), a new software paradigm that combines the\nstrength of LLM and conventional software. Diverse LLM applications from\ndifferent tenants could design complex workflows using multiple LLM requests to\naccomplish one task. However, they have to use the over-simplified\nrequest-level API provided by today's public LLM services, losing essential\napplication-level information. Public LLM services have to blindly optimize\nindividual LLM requests, leading to sub-optimal end-to-end performance of LLM\napplications.\n  This paper introduces Parrot, an LLM service system that focuses on the\nend-to-end experience of LLM-based applications. Parrot proposes Semantic\nVariable, a unified abstraction to expose application-level knowledge to public\nLLM services. A Semantic Variable annotates an input/output variable in the\nprompt of a request, and creates the data pipeline when connecting multiple LLM\nrequests, providing a natural way to program LLM applications. Exposing\nSemantic Variables to the public LLM service allows it to perform conventional\ndata flow analysis to uncover the correlation across multiple LLM requests.\nThis correlation opens a brand-new optimization space for the end-to-end\nperformance of LLM-based applications. Extensive evaluations demonstrate that\nParrot can achieve up to an order-of-magnitude improvement for popular and\npractical use cases of LLM applications."}, "hash": "9bb62a0d002b70a9e770d77c47b80e5c79a0d225f2e9229502ebfa96c1bb3be1", "class_name": "RelatedNodeInfo"}}, "text": "[28] Zhuohan Li, Lianmin Zheng, Yinmin Zhong, Vincent\nLiu, Ying Sheng, Xin Jin, Yanping Huang, Zhifeng Chen,\nHao Zhang, Joseph E. Gonzalez, and Ion Stoica. Al-\npaServe: Statistical multiplexing with model parallelism\nfor deep learning serving. In 17th USENIX Sympo-\nsium on Operating Systems Design and Implementa-\ntion (OSDI 23) , pages 663\u2013679, Boston, MA, July 2023.\nUSENIX Association.\n[29] Jerry Liu. LlamaIndex, November 2022.\n[30] Ashraf Mahgoub, Karthick Shankar, Subrata Mitra,\nAna Klimovic, Somali Chaterji, and Saurabh Bagchi.\nSONIC: Application-aware data passing for chained\nserverless applications. In 2021 USENIX Annual Tech-\nnical Conference (USENIX ATC 21) , pages 285\u2013301.\nUSENIX Association, July 2021.\n[31] Ashraf Mahgoub, Edgardo Barsallo Yi, Karthick\nShankar, Sameh Elnikety, Somali Chaterji, and Saurabh\nBagchi. ORION and the three rights: Sizing, bundling,\nand prewarming for serverless DAGs. In 16th USENIX\nSymposium on Operating Systems Design and Imple-\nmentation (OSDI 22) , pages 303\u2013320, Carlsbad, CA,\nJuly 2022. USENIX Association.\n[32] Microsoft. Bing chat. https://www.bing.com/chat ,\nNov 2023.\n[33] Microsoft. Meeting recap in microsoft\nteams. https://www.microsoft.com/en-us/\nmicrosoft-teams/premium , May 2023.\n[34] Microsoft. Microsoft 365 copilot. https:\n//www.microsoft.com/en-us/microsoft-365/\nenterprise/microsoft-365-copilot , Mar 2023.\n[35] Microsoft. PromptFlow. https://github.com/\nmicrosoft/promptflow , November 2023.\n[36] Microsoft. Semantic Kernel. https://github.com/\nmicrosoft/semantic-kernel , November 2023.\n[37] Deepak Narayanan, Keshav Santhanam, Fiodar\nKazhamiaka, Amar Phanishayee, and Matei Zaharia.\nHeterogeneity-Aware cluster scheduling policies for\ndeep learning workloads. In 14th USENIX Symposium\non Operating Systems Design and Implementation\n(OSDI 20) , pages 481\u2013498. USENIX Association,\nNovember 2020.", "mimetype": "text/plain", "start_char_idx": 2677, "end_char_idx": 4545, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "05a3945c-aec3-4538-8d36-daf50c57cf03": {"__data__": {"id_": "05a3945c-aec3-4538-8d36-daf50c57cf03", "embedding": null, "metadata": {"page_label": "16", "file_name": "2405_19888v1.pdf", "Title of this paper": "Parrot: Efficient Serving of LLM-based Applications with Semantic Variable", "Authors": "Chaofan Lin, Zhenhua Han, Chengruidong Zhang, Yuqing Yang, Fan Yang, Chen Chen, Lili Qiu", "Date published": "05/30/2024", "URL": "http://arxiv.org/abs/2405.19888v1", "summary": "The rise of large language models (LLMs) has enabled LLM-based applications\n(a.k.a. AI agents or co-pilots), a new software paradigm that combines the\nstrength of LLM and conventional software. Diverse LLM applications from\ndifferent tenants could design complex workflows using multiple LLM requests to\naccomplish one task. However, they have to use the over-simplified\nrequest-level API provided by today's public LLM services, losing essential\napplication-level information. Public LLM services have to blindly optimize\nindividual LLM requests, leading to sub-optimal end-to-end performance of LLM\napplications.\n  This paper introduces Parrot, an LLM service system that focuses on the\nend-to-end experience of LLM-based applications. Parrot proposes Semantic\nVariable, a unified abstraction to expose application-level knowledge to public\nLLM services. A Semantic Variable annotates an input/output variable in the\nprompt of a request, and creates the data pipeline when connecting multiple LLM\nrequests, providing a natural way to program LLM applications. Exposing\nSemantic Variables to the public LLM service allows it to perform conventional\ndata flow analysis to uncover the correlation across multiple LLM requests.\nThis correlation opens a brand-new optimization space for the end-to-end\nperformance of LLM-based applications. Extensive evaluations demonstrate that\nParrot can achieve up to an order-of-magnitude improvement for popular and\npractical use cases of LLM applications."}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2ce9e853-5564-4047-a410-0efaffd3add7", "node_type": "4", "metadata": {"page_label": "16", "file_name": "2405_19888v1.pdf", "Title of this paper": "Parrot: Efficient Serving of LLM-based Applications with Semantic Variable", "Authors": "Chaofan Lin, Zhenhua Han, Chengruidong Zhang, Yuqing Yang, Fan Yang, Chen Chen, Lili Qiu", "Date published": "05/30/2024", "URL": "http://arxiv.org/abs/2405.19888v1", "summary": "The rise of large language models (LLMs) has enabled LLM-based applications\n(a.k.a. AI agents or co-pilots), a new software paradigm that combines the\nstrength of LLM and conventional software. Diverse LLM applications from\ndifferent tenants could design complex workflows using multiple LLM requests to\naccomplish one task. However, they have to use the over-simplified\nrequest-level API provided by today's public LLM services, losing essential\napplication-level information. Public LLM services have to blindly optimize\nindividual LLM requests, leading to sub-optimal end-to-end performance of LLM\napplications.\n  This paper introduces Parrot, an LLM service system that focuses on the\nend-to-end experience of LLM-based applications. Parrot proposes Semantic\nVariable, a unified abstraction to expose application-level knowledge to public\nLLM services. A Semantic Variable annotates an input/output variable in the\nprompt of a request, and creates the data pipeline when connecting multiple LLM\nrequests, providing a natural way to program LLM applications. Exposing\nSemantic Variables to the public LLM service allows it to perform conventional\ndata flow analysis to uncover the correlation across multiple LLM requests.\nThis correlation opens a brand-new optimization space for the end-to-end\nperformance of LLM-based applications. Extensive evaluations demonstrate that\nParrot can achieve up to an order-of-magnitude improvement for popular and\npractical use cases of LLM applications."}, "hash": "e408ce3eade71a2b8e9f753f322717118349c3e0f9711e8dd12a1b0db893c578", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9b468e9f-c44a-47d1-b48c-968837de705e", "node_type": "1", "metadata": {}, "hash": "bf26719c71068d5c39fc436d2df5acc9ae83f13b2ee7d4b1ffcb270fa292c4d8", "class_name": "RelatedNodeInfo"}}, "text": "[38] Flemming Nielson, Hanne R Nielson, and Chris Hankin.\nPrinciples of program analysis . Springer, 2015.\n[39] Christopher Olston, Fangwei Li, Jeremiah Harmsen, Jor-\ndan Soyke, Kiril Gorovoy, Li Lao, Noah Fiedel, Sukriti\nRamesh, and Vinu Rajashekhar. Tensorflow-serving:\nFlexible, high-performance ml serving. In Workshop on\nML Systems at NIPS 2017 , 2017.\n[40] OpenAI. Chatgpt. https://chat.openai.com/ , Nov\n2023.\n[41] OpenAI. Gpt-4 technical report, 2023.\n[42] OpenAI. Introducing gpts. https://openai.com/\nblog/introducing-gpts , Nov 2023.\n[43] OpenAI. OpenAI Triton. https://github.com/\nopenai/triton , November 2023.\n[44] OpenAI. Production best practices - ope-\nnai api. https://platform.openai.com/\ndocs/guides/production-best-practices/\nimproving-latencies , Nov 2023.\n[45] Adam Paszke, Sam Gross, Soumith Chintala, Gregory\nChanan, Edward Yang, Zachary DeVito, Zeming Lin,\nAlban Desmaison, Luca Antiga, and Adam Lerer. Auto-\nmatic differentiation in pytorch. 2017.\n[46] Pratyush Patel, Esha Choukse, Chaojie Zhang, \u00cd\u00f1igo\nGoiri, Aashaka Shah, Saeed Maleki, and Ricardo Bian-\nchini. Splitwise: Efficient generative llm inference using\nphase splitting. arXiv preprint arXiv:2311.18677 , 2023.\n[47] Chen Qian, Xin Cong, Cheng Yang, Weize Chen,\nYusheng Su, Juyuan Xu, Zhiyuan Liu, and Maosong\nSun. Communicative agents for software development.\narXiv preprint arXiv:2307.07924 , 2023.\n[48] Sebasti\u00e1n Ram\u00edrez. FastAPI. https://github.com/\ntiangolo/fastapi .\n[49] Mohammad Shoeybi, Mostofa Patwary, Raul Puri,\nPatrick LeGresley, Jared Casper, and Bryan Catanzaro.\nMegatron-lm: Training multi-billion parameter language\nmodels using model parallelism. CoRR , abs/1909.08053,\n2019.\n[50] ShareGPT Team. Sharegpt dataset. https://\nsharegpt.com/ , Nov 2023.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1755, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9b468e9f-c44a-47d1-b48c-968837de705e": {"__data__": {"id_": "9b468e9f-c44a-47d1-b48c-968837de705e", "embedding": null, "metadata": {"page_label": "16", "file_name": "2405_19888v1.pdf", "Title of this paper": "Parrot: Efficient Serving of LLM-based Applications with Semantic Variable", "Authors": "Chaofan Lin, Zhenhua Han, Chengruidong Zhang, Yuqing Yang, Fan Yang, Chen Chen, Lili Qiu", "Date published": "05/30/2024", "URL": "http://arxiv.org/abs/2405.19888v1", "summary": "The rise of large language models (LLMs) has enabled LLM-based applications\n(a.k.a. AI agents or co-pilots), a new software paradigm that combines the\nstrength of LLM and conventional software. Diverse LLM applications from\ndifferent tenants could design complex workflows using multiple LLM requests to\naccomplish one task. However, they have to use the over-simplified\nrequest-level API provided by today's public LLM services, losing essential\napplication-level information. Public LLM services have to blindly optimize\nindividual LLM requests, leading to sub-optimal end-to-end performance of LLM\napplications.\n  This paper introduces Parrot, an LLM service system that focuses on the\nend-to-end experience of LLM-based applications. Parrot proposes Semantic\nVariable, a unified abstraction to expose application-level knowledge to public\nLLM services. A Semantic Variable annotates an input/output variable in the\nprompt of a request, and creates the data pipeline when connecting multiple LLM\nrequests, providing a natural way to program LLM applications. Exposing\nSemantic Variables to the public LLM service allows it to perform conventional\ndata flow analysis to uncover the correlation across multiple LLM requests.\nThis correlation opens a brand-new optimization space for the end-to-end\nperformance of LLM-based applications. Extensive evaluations demonstrate that\nParrot can achieve up to an order-of-magnitude improvement for popular and\npractical use cases of LLM applications."}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2ce9e853-5564-4047-a410-0efaffd3add7", "node_type": "4", "metadata": {"page_label": "16", "file_name": "2405_19888v1.pdf", "Title of this paper": "Parrot: Efficient Serving of LLM-based Applications with Semantic Variable", "Authors": "Chaofan Lin, Zhenhua Han, Chengruidong Zhang, Yuqing Yang, Fan Yang, Chen Chen, Lili Qiu", "Date published": "05/30/2024", "URL": "http://arxiv.org/abs/2405.19888v1", "summary": "The rise of large language models (LLMs) has enabled LLM-based applications\n(a.k.a. AI agents or co-pilots), a new software paradigm that combines the\nstrength of LLM and conventional software. Diverse LLM applications from\ndifferent tenants could design complex workflows using multiple LLM requests to\naccomplish one task. However, they have to use the over-simplified\nrequest-level API provided by today's public LLM services, losing essential\napplication-level information. Public LLM services have to blindly optimize\nindividual LLM requests, leading to sub-optimal end-to-end performance of LLM\napplications.\n  This paper introduces Parrot, an LLM service system that focuses on the\nend-to-end experience of LLM-based applications. Parrot proposes Semantic\nVariable, a unified abstraction to expose application-level knowledge to public\nLLM services. A Semantic Variable annotates an input/output variable in the\nprompt of a request, and creates the data pipeline when connecting multiple LLM\nrequests, providing a natural way to program LLM applications. Exposing\nSemantic Variables to the public LLM service allows it to perform conventional\ndata flow analysis to uncover the correlation across multiple LLM requests.\nThis correlation opens a brand-new optimization space for the end-to-end\nperformance of LLM-based applications. Extensive evaluations demonstrate that\nParrot can achieve up to an order-of-magnitude improvement for popular and\npractical use cases of LLM applications."}, "hash": "e408ce3eade71a2b8e9f753f322717118349c3e0f9711e8dd12a1b0db893c578", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "05a3945c-aec3-4538-8d36-daf50c57cf03", "node_type": "1", "metadata": {"page_label": "16", "file_name": "2405_19888v1.pdf", "Title of this paper": "Parrot: Efficient Serving of LLM-based Applications with Semantic Variable", "Authors": "Chaofan Lin, Zhenhua Han, Chengruidong Zhang, Yuqing Yang, Fan Yang, Chen Chen, Lili Qiu", "Date published": "05/30/2024", "URL": "http://arxiv.org/abs/2405.19888v1", "summary": "The rise of large language models (LLMs) has enabled LLM-based applications\n(a.k.a. AI agents or co-pilots), a new software paradigm that combines the\nstrength of LLM and conventional software. Diverse LLM applications from\ndifferent tenants could design complex workflows using multiple LLM requests to\naccomplish one task. However, they have to use the over-simplified\nrequest-level API provided by today's public LLM services, losing essential\napplication-level information. Public LLM services have to blindly optimize\nindividual LLM requests, leading to sub-optimal end-to-end performance of LLM\napplications.\n  This paper introduces Parrot, an LLM service system that focuses on the\nend-to-end experience of LLM-based applications. Parrot proposes Semantic\nVariable, a unified abstraction to expose application-level knowledge to public\nLLM services. A Semantic Variable annotates an input/output variable in the\nprompt of a request, and creates the data pipeline when connecting multiple LLM\nrequests, providing a natural way to program LLM applications. Exposing\nSemantic Variables to the public LLM service allows it to perform conventional\ndata flow analysis to uncover the correlation across multiple LLM requests.\nThis correlation opens a brand-new optimization space for the end-to-end\nperformance of LLM-based applications. Extensive evaluations demonstrate that\nParrot can achieve up to an order-of-magnitude improvement for popular and\npractical use cases of LLM applications."}, "hash": "d20f8f798bb00ea8a53bae8fc828d4690ad6f392e5ca90c81146f40210c29853", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a2d3c32c-304c-4504-ab39-d87bad96aa5e", "node_type": "1", "metadata": {}, "hash": "d12cde67e589ae9a555bceebb8065ca27f686bfa42ef311332905629d7547d15", "class_name": "RelatedNodeInfo"}}, "text": "[47] Chen Qian, Xin Cong, Cheng Yang, Weize Chen,\nYusheng Su, Juyuan Xu, Zhiyuan Liu, and Maosong\nSun. Communicative agents for software development.\narXiv preprint arXiv:2307.07924 , 2023.\n[48] Sebasti\u00e1n Ram\u00edrez. FastAPI. https://github.com/\ntiangolo/fastapi .\n[49] Mohammad Shoeybi, Mostofa Patwary, Raul Puri,\nPatrick LeGresley, Jared Casper, and Bryan Catanzaro.\nMegatron-lm: Training multi-billion parameter language\nmodels using model parallelism. CoRR , abs/1909.08053,\n2019.\n[50] ShareGPT Team. Sharegpt dataset. https://\nsharegpt.com/ , Nov 2023.\n[51] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix, Bap-\ntiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar,\nAurelien Rodriguez, Armand Joulin, Edouard Grave,\nand Guillaume Lample. Llama: Open and efficient foun-\ndation language models, 2023.[52] Unknown. Prompt of bing chat. https:\n//www.make-safe-ai.com/is-bing-chat-safe/\nPrompts_Conversations.txt , Nov 2023.\n[53] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Perric\nCistac, Clara Ma, Yacine Jernite, Julien Plu, Canwen\nXu, Teven Le Scao, Sylvain Gugger, Mariama Drame,\nQuentin Lhoest, and Alexander M. Rush. Transformers:\nState-of-the-Art Natural Language Processing. pages\n38\u201345. Association for Computational Linguistics, Oc-\ntober 2020.\n[54] Qingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu,\nShaokun Zhang, Erkang Zhu, Beibin Li, Li Jiang, Xi-\naoyun Zhang, and Chi Wang. Autogen: Enabling next-\ngen llm applications via multi-agent conversation frame-\nwork. arXiv preprint arXiv:2308.08155 , 2023.\n[55] Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song\nHan, and Mike Lewis. Efficient streaming language\nmodels with attention sinks. arXiv , 2023.\n[56] Gyeong-In Yu, Joo Seong Jeong, Geon-Woo Kim, Soo-\njeong Kim, and Byung-Gon Chun.", "mimetype": "text/plain", "start_char_idx": 1200, "end_char_idx": 3045, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a2d3c32c-304c-4504-ab39-d87bad96aa5e": {"__data__": {"id_": "a2d3c32c-304c-4504-ab39-d87bad96aa5e", "embedding": null, "metadata": {"page_label": "16", "file_name": "2405_19888v1.pdf", "Title of this paper": "Parrot: Efficient Serving of LLM-based Applications with Semantic Variable", "Authors": "Chaofan Lin, Zhenhua Han, Chengruidong Zhang, Yuqing Yang, Fan Yang, Chen Chen, Lili Qiu", "Date published": "05/30/2024", "URL": "http://arxiv.org/abs/2405.19888v1", "summary": "The rise of large language models (LLMs) has enabled LLM-based applications\n(a.k.a. AI agents or co-pilots), a new software paradigm that combines the\nstrength of LLM and conventional software. Diverse LLM applications from\ndifferent tenants could design complex workflows using multiple LLM requests to\naccomplish one task. However, they have to use the over-simplified\nrequest-level API provided by today's public LLM services, losing essential\napplication-level information. Public LLM services have to blindly optimize\nindividual LLM requests, leading to sub-optimal end-to-end performance of LLM\napplications.\n  This paper introduces Parrot, an LLM service system that focuses on the\nend-to-end experience of LLM-based applications. Parrot proposes Semantic\nVariable, a unified abstraction to expose application-level knowledge to public\nLLM services. A Semantic Variable annotates an input/output variable in the\nprompt of a request, and creates the data pipeline when connecting multiple LLM\nrequests, providing a natural way to program LLM applications. Exposing\nSemantic Variables to the public LLM service allows it to perform conventional\ndata flow analysis to uncover the correlation across multiple LLM requests.\nThis correlation opens a brand-new optimization space for the end-to-end\nperformance of LLM-based applications. Extensive evaluations demonstrate that\nParrot can achieve up to an order-of-magnitude improvement for popular and\npractical use cases of LLM applications."}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2ce9e853-5564-4047-a410-0efaffd3add7", "node_type": "4", "metadata": {"page_label": "16", "file_name": "2405_19888v1.pdf", "Title of this paper": "Parrot: Efficient Serving of LLM-based Applications with Semantic Variable", "Authors": "Chaofan Lin, Zhenhua Han, Chengruidong Zhang, Yuqing Yang, Fan Yang, Chen Chen, Lili Qiu", "Date published": "05/30/2024", "URL": "http://arxiv.org/abs/2405.19888v1", "summary": "The rise of large language models (LLMs) has enabled LLM-based applications\n(a.k.a. AI agents or co-pilots), a new software paradigm that combines the\nstrength of LLM and conventional software. Diverse LLM applications from\ndifferent tenants could design complex workflows using multiple LLM requests to\naccomplish one task. However, they have to use the over-simplified\nrequest-level API provided by today's public LLM services, losing essential\napplication-level information. Public LLM services have to blindly optimize\nindividual LLM requests, leading to sub-optimal end-to-end performance of LLM\napplications.\n  This paper introduces Parrot, an LLM service system that focuses on the\nend-to-end experience of LLM-based applications. Parrot proposes Semantic\nVariable, a unified abstraction to expose application-level knowledge to public\nLLM services. A Semantic Variable annotates an input/output variable in the\nprompt of a request, and creates the data pipeline when connecting multiple LLM\nrequests, providing a natural way to program LLM applications. Exposing\nSemantic Variables to the public LLM service allows it to perform conventional\ndata flow analysis to uncover the correlation across multiple LLM requests.\nThis correlation opens a brand-new optimization space for the end-to-end\nperformance of LLM-based applications. Extensive evaluations demonstrate that\nParrot can achieve up to an order-of-magnitude improvement for popular and\npractical use cases of LLM applications."}, "hash": "e408ce3eade71a2b8e9f753f322717118349c3e0f9711e8dd12a1b0db893c578", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9b468e9f-c44a-47d1-b48c-968837de705e", "node_type": "1", "metadata": {"page_label": "16", "file_name": "2405_19888v1.pdf", "Title of this paper": "Parrot: Efficient Serving of LLM-based Applications with Semantic Variable", "Authors": "Chaofan Lin, Zhenhua Han, Chengruidong Zhang, Yuqing Yang, Fan Yang, Chen Chen, Lili Qiu", "Date published": "05/30/2024", "URL": "http://arxiv.org/abs/2405.19888v1", "summary": "The rise of large language models (LLMs) has enabled LLM-based applications\n(a.k.a. AI agents or co-pilots), a new software paradigm that combines the\nstrength of LLM and conventional software. Diverse LLM applications from\ndifferent tenants could design complex workflows using multiple LLM requests to\naccomplish one task. However, they have to use the over-simplified\nrequest-level API provided by today's public LLM services, losing essential\napplication-level information. Public LLM services have to blindly optimize\nindividual LLM requests, leading to sub-optimal end-to-end performance of LLM\napplications.\n  This paper introduces Parrot, an LLM service system that focuses on the\nend-to-end experience of LLM-based applications. Parrot proposes Semantic\nVariable, a unified abstraction to expose application-level knowledge to public\nLLM services. A Semantic Variable annotates an input/output variable in the\nprompt of a request, and creates the data pipeline when connecting multiple LLM\nrequests, providing a natural way to program LLM applications. Exposing\nSemantic Variables to the public LLM service allows it to perform conventional\ndata flow analysis to uncover the correlation across multiple LLM requests.\nThis correlation opens a brand-new optimization space for the end-to-end\nperformance of LLM-based applications. Extensive evaluations demonstrate that\nParrot can achieve up to an order-of-magnitude improvement for popular and\npractical use cases of LLM applications."}, "hash": "7b09c11a8538f9ea079b47637219e8616bb353d3c7ba6d217d1aebd33bb35d13", "class_name": "RelatedNodeInfo"}}, "text": "pages\n38\u201345. Association for Computational Linguistics, Oc-\ntober 2020.\n[54] Qingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu,\nShaokun Zhang, Erkang Zhu, Beibin Li, Li Jiang, Xi-\naoyun Zhang, and Chi Wang. Autogen: Enabling next-\ngen llm applications via multi-agent conversation frame-\nwork. arXiv preprint arXiv:2308.08155 , 2023.\n[55] Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song\nHan, and Mike Lewis. Efficient streaming language\nmodels with attention sinks. arXiv , 2023.\n[56] Gyeong-In Yu, Joo Seong Jeong, Geon-Woo Kim, Soo-\njeong Kim, and Byung-Gon Chun. Orca: A distributed\nserving system for Transformer-Based generative mod-\nels. In 16th USENIX Symposium on Operating Systems\nDesign and Implementation (OSDI 22) , pages 521\u2013538,\nCarlsbad, CA, July 2022. USENIX Association.\n[57] Matei Zaharia, Dhruba Borthakur, Joydeep Sen Sarma,\nKhaled Elmeleegy, Scott Shenker, and Ion Stoica. Delay\nscheduling: A simple technique for achieving locality\nand fairness in cluster scheduling. In Proceedings of\nthe 5th European Conference on Computer Systems ,\nEuroSys \u201910, page 265\u2013278, New York, NY , USA, 2010.\nAssociation for Computing Machinery.\n[58] Matei Zaharia, Andy Konwinski, Anthony D Joseph,\nRandy H Katz, and Ion Stoica. Improving mapreduce\nperformance in heterogeneous environments. In 8th\nUSENIX Symposium on Operating Systems Design and\nImplementation (OSDI 08) , San Diego, CA, 2008.\n[59] Hong Zhang, Yupeng Tang, Anurag Khandelwal, Jin-\ngrong Chen, and Ion Stoica. Caerus: NIMBLE task\nscheduling for serverless analytics. In 18th USENIX\nSymposium on Networked Systems Design and Imple-\nmentation (NSDI 21) , pages 653\u2013669. USENIX Associ-\nation, April 2021.\n[60] Susan Zhang, Stephen Roller, Naman Goyal, Mikel\nArtetxe, Moya Chen, Shuohui Chen, Christopher De-\nwan, Mona Diab, Xian Li, Xi Victoria Lin, Todor Mi-\nhaylov, Myle Ott, Sam Shleifer, Kurt Shuster, Daniel\nSimig, Punit Singh Koura, Anjali Sridhar, Tianlu Wang,\nand Luke Zettlemoyer. Opt: Open pre-trained trans-\nformer language models, 2022.", "mimetype": "text/plain", "start_char_idx": 2482, "end_char_idx": 4496, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e5729fb3-cfed-4db2-b25f-c46d56b197da": {"__data__": {"id_": "e5729fb3-cfed-4db2-b25f-c46d56b197da", "embedding": null, "metadata": {"page_label": "17", "file_name": "2405_19888v1.pdf", "Title of this paper": "Parrot: Efficient Serving of LLM-based Applications with Semantic Variable", "Authors": "Chaofan Lin, Zhenhua Han, Chengruidong Zhang, Yuqing Yang, Fan Yang, Chen Chen, Lili Qiu", "Date published": "05/30/2024", "URL": "http://arxiv.org/abs/2405.19888v1", "summary": "The rise of large language models (LLMs) has enabled LLM-based applications\n(a.k.a. AI agents or co-pilots), a new software paradigm that combines the\nstrength of LLM and conventional software. Diverse LLM applications from\ndifferent tenants could design complex workflows using multiple LLM requests to\naccomplish one task. However, they have to use the over-simplified\nrequest-level API provided by today's public LLM services, losing essential\napplication-level information. Public LLM services have to blindly optimize\nindividual LLM requests, leading to sub-optimal end-to-end performance of LLM\napplications.\n  This paper introduces Parrot, an LLM service system that focuses on the\nend-to-end experience of LLM-based applications. Parrot proposes Semantic\nVariable, a unified abstraction to expose application-level knowledge to public\nLLM services. A Semantic Variable annotates an input/output variable in the\nprompt of a request, and creates the data pipeline when connecting multiple LLM\nrequests, providing a natural way to program LLM applications. Exposing\nSemantic Variables to the public LLM service allows it to perform conventional\ndata flow analysis to uncover the correlation across multiple LLM requests.\nThis correlation opens a brand-new optimization space for the end-to-end\nperformance of LLM-based applications. Extensive evaluations demonstrate that\nParrot can achieve up to an order-of-magnitude improvement for popular and\npractical use cases of LLM applications."}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "9aab5d51-fac2-4486-8bb8-5e193179b662", "node_type": "4", "metadata": {"page_label": "17", "file_name": "2405_19888v1.pdf", "Title of this paper": "Parrot: Efficient Serving of LLM-based Applications with Semantic Variable", "Authors": "Chaofan Lin, Zhenhua Han, Chengruidong Zhang, Yuqing Yang, Fan Yang, Chen Chen, Lili Qiu", "Date published": "05/30/2024", "URL": "http://arxiv.org/abs/2405.19888v1", "summary": "The rise of large language models (LLMs) has enabled LLM-based applications\n(a.k.a. AI agents or co-pilots), a new software paradigm that combines the\nstrength of LLM and conventional software. Diverse LLM applications from\ndifferent tenants could design complex workflows using multiple LLM requests to\naccomplish one task. However, they have to use the over-simplified\nrequest-level API provided by today's public LLM services, losing essential\napplication-level information. Public LLM services have to blindly optimize\nindividual LLM requests, leading to sub-optimal end-to-end performance of LLM\napplications.\n  This paper introduces Parrot, an LLM service system that focuses on the\nend-to-end experience of LLM-based applications. Parrot proposes Semantic\nVariable, a unified abstraction to expose application-level knowledge to public\nLLM services. A Semantic Variable annotates an input/output variable in the\nprompt of a request, and creates the data pipeline when connecting multiple LLM\nrequests, providing a natural way to program LLM applications. Exposing\nSemantic Variables to the public LLM service allows it to perform conventional\ndata flow analysis to uncover the correlation across multiple LLM requests.\nThis correlation opens a brand-new optimization space for the end-to-end\nperformance of LLM-based applications. Extensive evaluations demonstrate that\nParrot can achieve up to an order-of-magnitude improvement for popular and\npractical use cases of LLM applications."}, "hash": "fb87594b5a6ec567674555bfecf7d62b5e4f21cd57a1898f75f7f3898abb2d2d", "class_name": "RelatedNodeInfo"}}, "text": "[61] Hanyu Zhao, Zhenhua Han, Zhi Yang, Quanlu Zhang,\nFan Yang, Lidong Zhou, Mao Yang, Francis C.M. Lau,\nYuqi Wang, Yifan Xiong, and Bin Wang. HiveD: Sharing\na GPU cluster for deep learning with guarantees. In 14th\nUSENIX Symposium on Operating Systems Design and\nImplementation (OSDI 20) , pages 515\u2013532. USENIX\nAssociation, November 2020.\n[62] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan\nZhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuo-\nhan Li, Dacheng Li, Eric. P Xing, Hao Zhang, Joseph E.\nGonzalez, and Ion Stoica. Judging llm-as-a-judge with\nmt-bench and chatbot arena, 2023.\n[63] Lianmin Zheng, Liangsheng Yin, Zhiqiang Xie, Jeff\nHuang, Chuyue Sun, Cody Hao Yu, Shiyi Cao, Chris-\ntos Kozyrakis, Ion Stoica, Joseph E. Gonzalez, Clark\nBarrett, and Ying Sheng. Efficiently programming large\nlanguage models using sglang, 2023.\n[64] Yinmin Zhong, Shengyu Liu, Junda Chen, Jianbo Hu,\nYibo Zhu, Xuanzhe Liu, Xin Jin, and Hao Zhang. Dist-\nserve: Disaggregating prefill and decoding for goodput-\noptimized large language model serving. arXiv preprint\narXiv:2401.09670 , 2024.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1082, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}}, "docstore/metadata": {"d7ebcbf1-5d35-4044-a578-e8bb3ab36f46": {"doc_hash": "259940c26088d614fee7c935257dc0208e733911429abae6028cd114a021bea7", "ref_doc_id": "fa5c2731-1ef9-497c-bc3b-557fd958eeb4"}, "52d41ace-99fb-46fc-a8e8-383e1b54f674": {"doc_hash": "62f97548ad831f4f1f46db6239dfee6c410141e40e751faa24dc78492dbe6528", "ref_doc_id": "fa5c2731-1ef9-497c-bc3b-557fd958eeb4"}, "f8a2dc6c-72eb-4c22-858c-ebde310dbc4b": {"doc_hash": "86bb8d547757d9756bed1181feb66044c8dd2a83e7f8706fc3319a3bdd9d0560", "ref_doc_id": "fa5c2731-1ef9-497c-bc3b-557fd958eeb4"}, "260ccb22-9083-4c8e-8236-5de334684fcc": {"doc_hash": "23289684e1e2aa6e0de4ca64928336be908b3ad4ad177d28cd03874fecf8cb64", "ref_doc_id": "e80fb7f0-ffbb-4071-b1ea-28d614ada6b6"}, "0abda68a-949c-4163-bf0d-c764f0361133": {"doc_hash": "542152b35e77329042ea236e96a1a403f267be78834cc7628314ec4932a8bee1", "ref_doc_id": "e80fb7f0-ffbb-4071-b1ea-28d614ada6b6"}, "9bb3ca12-d623-42c5-9f41-dda20e1aba71": {"doc_hash": "a744f6787516d7b65405520faf747bdb28e5bdedc6f10325d1620062397e7687", "ref_doc_id": "e80fb7f0-ffbb-4071-b1ea-28d614ada6b6"}, "9e2b072e-c6f8-4cd0-89c3-934f10c1c63a": {"doc_hash": "a3affb1f27b0d2fef1e0c4c5799443d31325d7438d0cbae64e7bab7ea1828bb6", "ref_doc_id": "d9baf175-0877-4dac-997b-5c89b78c24fb"}, "1667c3c7-4008-4211-a2cd-352fd219db94": {"doc_hash": "b8fc8a837f50aa9dd258f666746c403cf25581bb6fe20ba3ec292507aae75a87", "ref_doc_id": "d9baf175-0877-4dac-997b-5c89b78c24fb"}, "668a598d-f2b4-4b2f-b0f3-d0c148fed797": {"doc_hash": "3f4fb98cca1de01f910a01205ca8136a7044bd1dfb301449c83793ab71431bce", "ref_doc_id": "d9baf175-0877-4dac-997b-5c89b78c24fb"}, "a94d38c7-a0e0-4e6a-a35e-273cf47c8a05": {"doc_hash": "c55d150a9ee47d377ab1ff6a2aa2b12d2874fe4f5bb25b916b163794d312f14d", "ref_doc_id": "9e9820f0-efe5-4358-9ddd-fea783d89323"}, "1539fe43-e4a4-4320-9019-560ebb9ac7a1": {"doc_hash": "fb1dfaf4c7eaf046435294b8ff1f98cd9a95f28f5e794255cdfc357c4ed5ba04", "ref_doc_id": "9e9820f0-efe5-4358-9ddd-fea783d89323"}, "5891bbdf-ac44-4840-8d3c-a86747da6898": {"doc_hash": "44ba25e294b1f15b87b21e3d9b1877b38271c584fd21744c3b53c98bbc4de404", "ref_doc_id": "9e9820f0-efe5-4358-9ddd-fea783d89323"}, "0651355d-edd1-4c2f-8294-ac7261b54d4f": {"doc_hash": "34d9204c444f905d338334cdc90803a979c02469329d559ed5c54e60ca25ce89", "ref_doc_id": "6946f5c6-9cfa-4c68-b1ce-2af59fcaebd7"}, "b2681130-7644-43d4-80a1-42a4d02dc1e3": {"doc_hash": "71e3b26f72463773a0b744e2c380627e32541c505f397f554b03262e284e4301", "ref_doc_id": "6946f5c6-9cfa-4c68-b1ce-2af59fcaebd7"}, "7b146d9e-6512-4fd1-aa7c-37b24cc6ba70": {"doc_hash": "c0c26ac65d3db4d5e199a28824bb23832f82485edde4005cc7bdf91587412a55", "ref_doc_id": "6946f5c6-9cfa-4c68-b1ce-2af59fcaebd7"}, "11ff1a7a-2f8f-4af4-af4a-92947c724046": {"doc_hash": "b4d07648d1bc3ab15f2b2790e6957a892835c3710d5ee2d15ebf3bdc9d5c680f", "ref_doc_id": "e7f2314e-8bed-47c9-b43a-309408d62cad"}, "a22fdf36-4f4d-4d92-91d1-80b6a4f818d6": {"doc_hash": "c2303b09e459e38e891b0e067804366a6e661330fd76921347a8c3d6dbd74c79", "ref_doc_id": "e7f2314e-8bed-47c9-b43a-309408d62cad"}, "cd0287d5-964d-4ed6-bdc5-0006d217a25f": {"doc_hash": "e4f54541873764b0086ae3948cad750a7db352abe1e6de47dd2b5f2502226008", "ref_doc_id": "e7f2314e-8bed-47c9-b43a-309408d62cad"}, "05d8b490-dad4-4b6a-add5-486b4533d430": {"doc_hash": "bf510e02b91575770c530d2d3fc942725ce8ad84b254766aedb147115a64ab2b", "ref_doc_id": "ffd77891-69dc-49ff-9527-db9920cd7dbb"}, "23e26e03-e0fd-4f8c-b75c-feecc2600d94": {"doc_hash": "a1eb4732a5aa567cadcdda5d5b2d1f238aa4250bb8b41673c91c59f16be0769d", "ref_doc_id": "ffd77891-69dc-49ff-9527-db9920cd7dbb"}, "b0d5731c-5c33-438e-84f0-c2ca69792b65": {"doc_hash": "d60c70a170bf62a64b627dc3dc11552899fd3a311694bf74a05283ee6b79cf5c", "ref_doc_id": "ffd77891-69dc-49ff-9527-db9920cd7dbb"}, "f5e52809-757b-454a-85d9-1762efc43e23": {"doc_hash": "8f641c0aca1d5a6b9f020d8699d74c167789495c7fd72fda38c9635e760526b7", "ref_doc_id": "62a5c817-91b9-4641-a461-ee893700467a"}, "eb4678d4-44a9-4c66-a402-6bd90b65394d": {"doc_hash": "3d6c43ff647234e08ada4b204dc5887fa1f272b1a02f689e2a308ed1bda08de4", "ref_doc_id": "62a5c817-91b9-4641-a461-ee893700467a"}, "5420325b-29ea-4cc3-b18b-f111ac9069e2": {"doc_hash": "1778f4f741b9e8a821f28b20a60f1c6d3b417b1dccf2623e7b118831d27d2616", "ref_doc_id": "62a5c817-91b9-4641-a461-ee893700467a"}, "2ef43774-b1dc-4346-bf8a-c26ca1b57ed3": {"doc_hash": "91df90bcda0bb8922b5fb86474f0f1df7325ed3ed643cea213e4ec799d26950a", "ref_doc_id": "f242335c-3d9e-4a1c-aa50-287c6dd1ce98"}, "755a9e00-39bd-4a57-8c6f-24ab452a0f7a": {"doc_hash": "7ceed37e38e1eee03d34a7ed1c48ee978bf7bc61966b0a976b32c069b0f715e0", "ref_doc_id": "f242335c-3d9e-4a1c-aa50-287c6dd1ce98"}, "940d3026-179d-405b-9650-6033dcd18dee": {"doc_hash": "c54cd56531a0f823f9967344250beb168447386d856e1ba900fef61b90001479", "ref_doc_id": "f242335c-3d9e-4a1c-aa50-287c6dd1ce98"}, "372d39f1-c991-4a83-9902-7961458c59b8": {"doc_hash": "6592ab5948277bd3dac9a8594fd008c50df8f86432d14afcdd1346ef6fce422d", "ref_doc_id": "27c11532-acdd-45ca-8762-8191c0f96067"}, "767437ef-ae0e-46fb-a914-52e3a8852009": {"doc_hash": "ec00bb1a48a3cec1fe56857ad4416769c3d0ebe3deb5f6659ab107caa3d6ccbc", "ref_doc_id": "27c11532-acdd-45ca-8762-8191c0f96067"}, "0bef3e4c-6a41-4526-ba13-b8d846b954b9": {"doc_hash": "038943718ae7bb7b0b20620a002a66e4509191be0982691aae873f0423d0906b", "ref_doc_id": "27c11532-acdd-45ca-8762-8191c0f96067"}, "5329a245-1753-4c4e-b58c-d9a84b28bd20": {"doc_hash": "1f9accb5da2091debe93c53627ddd2fd4813342f79e0823ec89f5e67c897eba1", "ref_doc_id": "f799a209-ea73-44e2-8183-2da4ea8ee984"}, "de0ac9e9-a8e5-456a-8363-0b4dd995a53c": {"doc_hash": "d2feeb91ea9bf61c6a4f831012aac7af9431d30252d731f7711daacb83bb56dc", "ref_doc_id": "f799a209-ea73-44e2-8183-2da4ea8ee984"}, "ebb694d8-b24b-4e15-a1fb-9cb36ee41f2c": {"doc_hash": "06dd57e5db1b91ba8efd6c8cb87be1d12d2750b3d4e6f9b04bb8347bb6f03bfe", "ref_doc_id": "f799a209-ea73-44e2-8183-2da4ea8ee984"}, "c6583cdd-284e-4c86-a9ab-abb12025c826": {"doc_hash": "916752eb6ef9caada57fc6494a59c9e70c48be1660f8f8b240c7c99c56399cef", "ref_doc_id": "c71416dd-8a88-43c4-b5c7-eb0e11238506"}, "b64a0b99-ffb2-4020-b46e-5ff110f1124f": {"doc_hash": "02e23e26b47f346804f5b46c0bec889d946208b6dd3662908e92ace68f68d8d0", "ref_doc_id": "c71416dd-8a88-43c4-b5c7-eb0e11238506"}, "8895c478-587d-472e-be81-35370b307761": {"doc_hash": "379602a7cd331fad0ba7dd69506088bf737762c66004e6ba56f0b63ffa4a00bf", "ref_doc_id": "c71416dd-8a88-43c4-b5c7-eb0e11238506"}, "b0b924dc-5101-40ca-9efc-334e6b57913a": {"doc_hash": "29e01464bdd0f255671a10f6819ccb318646cc21c6891202e92d5e3ab8822ed2", "ref_doc_id": "8931491b-6711-4bc1-b824-c863efd9bd88"}, "774e005a-de09-4136-a514-dea6cac22cc2": {"doc_hash": "4864bf246437f0caefdfa099fb30e2c383042e613d837b36b5e5806bbbdac6c2", "ref_doc_id": "8931491b-6711-4bc1-b824-c863efd9bd88"}, "9e437f86-3703-43f5-87d3-7bf016b69d2c": {"doc_hash": "23e6a5ca7d6e932528a45e2bb74c46e8737d4b79315af03e289298ce1c6724c1", "ref_doc_id": "8931491b-6711-4bc1-b824-c863efd9bd88"}, "92bd5626-be87-4331-937b-b9aa0fd9930d": {"doc_hash": "e53fe7f4125842d83f3dfc0887843a8718b4bbcd2cc0e2754b5508e280916079", "ref_doc_id": "ffb52c6f-dc64-4f9b-be30-05c3786cac37"}, "6e8ee0bc-90cb-4283-8400-cdba598f6e4a": {"doc_hash": "df8984115ee1b69c2ded5dc7ff3586a7ce6ba662265b7e5cadf147fb6763cb34", "ref_doc_id": "ffb52c6f-dc64-4f9b-be30-05c3786cac37"}, "c9882fdf-97aa-4574-aa6d-bae79f6dd5c8": {"doc_hash": "efe7f7e4839710287611f04f773fa5f00f09de561a205b2d1fd66ad255153ea0", "ref_doc_id": "ffb52c6f-dc64-4f9b-be30-05c3786cac37"}, "aaf8e015-6662-455f-afb8-57d94a3cba72": {"doc_hash": "9b0f425e32db208d32faa7364e91bc0f38135855ac1965b4c1573375837c67af", "ref_doc_id": "8ca79964-0014-454e-873a-34fd27a7d9ac"}, "58284db1-384a-412b-8fe3-cc6b58a32c92": {"doc_hash": "9bb62a0d002b70a9e770d77c47b80e5c79a0d225f2e9229502ebfa96c1bb3be1", "ref_doc_id": "8ca79964-0014-454e-873a-34fd27a7d9ac"}, "15c41549-39b3-4891-8ebe-8e9261a82581": {"doc_hash": "df80aa021e981046c9ee66eb33d73b5f136056448ccb16041128aafd8ab3649a", "ref_doc_id": "8ca79964-0014-454e-873a-34fd27a7d9ac"}, "05a3945c-aec3-4538-8d36-daf50c57cf03": {"doc_hash": "d20f8f798bb00ea8a53bae8fc828d4690ad6f392e5ca90c81146f40210c29853", "ref_doc_id": "2ce9e853-5564-4047-a410-0efaffd3add7"}, "9b468e9f-c44a-47d1-b48c-968837de705e": {"doc_hash": "7b09c11a8538f9ea079b47637219e8616bb353d3c7ba6d217d1aebd33bb35d13", "ref_doc_id": "2ce9e853-5564-4047-a410-0efaffd3add7"}, "a2d3c32c-304c-4504-ab39-d87bad96aa5e": {"doc_hash": "db62242a68da0585c2d4e3126c175b397eb6d307c79b09baae6e986088590506", "ref_doc_id": "2ce9e853-5564-4047-a410-0efaffd3add7"}, "e5729fb3-cfed-4db2-b25f-c46d56b197da": {"doc_hash": "fb87594b5a6ec567674555bfecf7d62b5e4f21cd57a1898f75f7f3898abb2d2d", "ref_doc_id": "9aab5d51-fac2-4486-8bb8-5e193179b662"}}, "docstore/ref_doc_info": {"fa5c2731-1ef9-497c-bc3b-557fd958eeb4": {"node_ids": ["d7ebcbf1-5d35-4044-a578-e8bb3ab36f46", "52d41ace-99fb-46fc-a8e8-383e1b54f674", "f8a2dc6c-72eb-4c22-858c-ebde310dbc4b"], "metadata": {"page_label": "1", "file_name": "2405_19888v1.pdf", "Title of this paper": "Parrot: Efficient Serving of LLM-based Applications with Semantic Variable", "Authors": "Chaofan Lin, Zhenhua Han, Chengruidong Zhang, Yuqing Yang, Fan Yang, Chen Chen, Lili Qiu", "Date published": "05/30/2024", "URL": "http://arxiv.org/abs/2405.19888v1", "summary": "The rise of large language models (LLMs) has enabled LLM-based applications\n(a.k.a. AI agents or co-pilots), a new software paradigm that combines the\nstrength of LLM and conventional software. Diverse LLM applications from\ndifferent tenants could design complex workflows using multiple LLM requests to\naccomplish one task. However, they have to use the over-simplified\nrequest-level API provided by today's public LLM services, losing essential\napplication-level information. Public LLM services have to blindly optimize\nindividual LLM requests, leading to sub-optimal end-to-end performance of LLM\napplications.\n  This paper introduces Parrot, an LLM service system that focuses on the\nend-to-end experience of LLM-based applications. Parrot proposes Semantic\nVariable, a unified abstraction to expose application-level knowledge to public\nLLM services. A Semantic Variable annotates an input/output variable in the\nprompt of a request, and creates the data pipeline when connecting multiple LLM\nrequests, providing a natural way to program LLM applications. Exposing\nSemantic Variables to the public LLM service allows it to perform conventional\ndata flow analysis to uncover the correlation across multiple LLM requests.\nThis correlation opens a brand-new optimization space for the end-to-end\nperformance of LLM-based applications. Extensive evaluations demonstrate that\nParrot can achieve up to an order-of-magnitude improvement for popular and\npractical use cases of LLM applications."}}, "e80fb7f0-ffbb-4071-b1ea-28d614ada6b6": {"node_ids": ["260ccb22-9083-4c8e-8236-5de334684fcc", "0abda68a-949c-4163-bf0d-c764f0361133", "9bb3ca12-d623-42c5-9f41-dda20e1aba71"], "metadata": {"page_label": "2", "file_name": "2405_19888v1.pdf", "Title of this paper": "Parrot: Efficient Serving of LLM-based Applications with Semantic Variable", "Authors": "Chaofan Lin, Zhenhua Han, Chengruidong Zhang, Yuqing Yang, Fan Yang, Chen Chen, Lili Qiu", "Date published": "05/30/2024", "URL": "http://arxiv.org/abs/2405.19888v1", "summary": "The rise of large language models (LLMs) has enabled LLM-based applications\n(a.k.a. AI agents or co-pilots), a new software paradigm that combines the\nstrength of LLM and conventional software. Diverse LLM applications from\ndifferent tenants could design complex workflows using multiple LLM requests to\naccomplish one task. However, they have to use the over-simplified\nrequest-level API provided by today's public LLM services, losing essential\napplication-level information. Public LLM services have to blindly optimize\nindividual LLM requests, leading to sub-optimal end-to-end performance of LLM\napplications.\n  This paper introduces Parrot, an LLM service system that focuses on the\nend-to-end experience of LLM-based applications. Parrot proposes Semantic\nVariable, a unified abstraction to expose application-level knowledge to public\nLLM services. A Semantic Variable annotates an input/output variable in the\nprompt of a request, and creates the data pipeline when connecting multiple LLM\nrequests, providing a natural way to program LLM applications. Exposing\nSemantic Variables to the public LLM service allows it to perform conventional\ndata flow analysis to uncover the correlation across multiple LLM requests.\nThis correlation opens a brand-new optimization space for the end-to-end\nperformance of LLM-based applications. Extensive evaluations demonstrate that\nParrot can achieve up to an order-of-magnitude improvement for popular and\npractical use cases of LLM applications."}}, "d9baf175-0877-4dac-997b-5c89b78c24fb": {"node_ids": ["9e2b072e-c6f8-4cd0-89c3-934f10c1c63a", "1667c3c7-4008-4211-a2cd-352fd219db94", "668a598d-f2b4-4b2f-b0f3-d0c148fed797"], "metadata": {"page_label": "3", "file_name": "2405_19888v1.pdf", "Title of this paper": "Parrot: Efficient Serving of LLM-based Applications with Semantic Variable", "Authors": "Chaofan Lin, Zhenhua Han, Chengruidong Zhang, Yuqing Yang, Fan Yang, Chen Chen, Lili Qiu", "Date published": "05/30/2024", "URL": "http://arxiv.org/abs/2405.19888v1", "summary": "The rise of large language models (LLMs) has enabled LLM-based applications\n(a.k.a. AI agents or co-pilots), a new software paradigm that combines the\nstrength of LLM and conventional software. Diverse LLM applications from\ndifferent tenants could design complex workflows using multiple LLM requests to\naccomplish one task. However, they have to use the over-simplified\nrequest-level API provided by today's public LLM services, losing essential\napplication-level information. Public LLM services have to blindly optimize\nindividual LLM requests, leading to sub-optimal end-to-end performance of LLM\napplications.\n  This paper introduces Parrot, an LLM service system that focuses on the\nend-to-end experience of LLM-based applications. Parrot proposes Semantic\nVariable, a unified abstraction to expose application-level knowledge to public\nLLM services. A Semantic Variable annotates an input/output variable in the\nprompt of a request, and creates the data pipeline when connecting multiple LLM\nrequests, providing a natural way to program LLM applications. Exposing\nSemantic Variables to the public LLM service allows it to perform conventional\ndata flow analysis to uncover the correlation across multiple LLM requests.\nThis correlation opens a brand-new optimization space for the end-to-end\nperformance of LLM-based applications. Extensive evaluations demonstrate that\nParrot can achieve up to an order-of-magnitude improvement for popular and\npractical use cases of LLM applications."}}, "9e9820f0-efe5-4358-9ddd-fea783d89323": {"node_ids": ["a94d38c7-a0e0-4e6a-a35e-273cf47c8a05", "1539fe43-e4a4-4320-9019-560ebb9ac7a1", "5891bbdf-ac44-4840-8d3c-a86747da6898"], "metadata": {"page_label": "4", "file_name": "2405_19888v1.pdf", "Title of this paper": "Parrot: Efficient Serving of LLM-based Applications with Semantic Variable", "Authors": "Chaofan Lin, Zhenhua Han, Chengruidong Zhang, Yuqing Yang, Fan Yang, Chen Chen, Lili Qiu", "Date published": "05/30/2024", "URL": "http://arxiv.org/abs/2405.19888v1", "summary": "The rise of large language models (LLMs) has enabled LLM-based applications\n(a.k.a. AI agents or co-pilots), a new software paradigm that combines the\nstrength of LLM and conventional software. Diverse LLM applications from\ndifferent tenants could design complex workflows using multiple LLM requests to\naccomplish one task. However, they have to use the over-simplified\nrequest-level API provided by today's public LLM services, losing essential\napplication-level information. Public LLM services have to blindly optimize\nindividual LLM requests, leading to sub-optimal end-to-end performance of LLM\napplications.\n  This paper introduces Parrot, an LLM service system that focuses on the\nend-to-end experience of LLM-based applications. Parrot proposes Semantic\nVariable, a unified abstraction to expose application-level knowledge to public\nLLM services. A Semantic Variable annotates an input/output variable in the\nprompt of a request, and creates the data pipeline when connecting multiple LLM\nrequests, providing a natural way to program LLM applications. Exposing\nSemantic Variables to the public LLM service allows it to perform conventional\ndata flow analysis to uncover the correlation across multiple LLM requests.\nThis correlation opens a brand-new optimization space for the end-to-end\nperformance of LLM-based applications. Extensive evaluations demonstrate that\nParrot can achieve up to an order-of-magnitude improvement for popular and\npractical use cases of LLM applications."}}, "6946f5c6-9cfa-4c68-b1ce-2af59fcaebd7": {"node_ids": ["0651355d-edd1-4c2f-8294-ac7261b54d4f", "b2681130-7644-43d4-80a1-42a4d02dc1e3", "7b146d9e-6512-4fd1-aa7c-37b24cc6ba70"], "metadata": {"page_label": "5", "file_name": "2405_19888v1.pdf", "Title of this paper": "Parrot: Efficient Serving of LLM-based Applications with Semantic Variable", "Authors": "Chaofan Lin, Zhenhua Han, Chengruidong Zhang, Yuqing Yang, Fan Yang, Chen Chen, Lili Qiu", "Date published": "05/30/2024", "URL": "http://arxiv.org/abs/2405.19888v1", "summary": "The rise of large language models (LLMs) has enabled LLM-based applications\n(a.k.a. AI agents or co-pilots), a new software paradigm that combines the\nstrength of LLM and conventional software. Diverse LLM applications from\ndifferent tenants could design complex workflows using multiple LLM requests to\naccomplish one task. However, they have to use the over-simplified\nrequest-level API provided by today's public LLM services, losing essential\napplication-level information. Public LLM services have to blindly optimize\nindividual LLM requests, leading to sub-optimal end-to-end performance of LLM\napplications.\n  This paper introduces Parrot, an LLM service system that focuses on the\nend-to-end experience of LLM-based applications. Parrot proposes Semantic\nVariable, a unified abstraction to expose application-level knowledge to public\nLLM services. A Semantic Variable annotates an input/output variable in the\nprompt of a request, and creates the data pipeline when connecting multiple LLM\nrequests, providing a natural way to program LLM applications. Exposing\nSemantic Variables to the public LLM service allows it to perform conventional\ndata flow analysis to uncover the correlation across multiple LLM requests.\nThis correlation opens a brand-new optimization space for the end-to-end\nperformance of LLM-based applications. Extensive evaluations demonstrate that\nParrot can achieve up to an order-of-magnitude improvement for popular and\npractical use cases of LLM applications."}}, "e7f2314e-8bed-47c9-b43a-309408d62cad": {"node_ids": ["11ff1a7a-2f8f-4af4-af4a-92947c724046", "a22fdf36-4f4d-4d92-91d1-80b6a4f818d6", "cd0287d5-964d-4ed6-bdc5-0006d217a25f"], "metadata": {"page_label": "6", "file_name": "2405_19888v1.pdf", "Title of this paper": "Parrot: Efficient Serving of LLM-based Applications with Semantic Variable", "Authors": "Chaofan Lin, Zhenhua Han, Chengruidong Zhang, Yuqing Yang, Fan Yang, Chen Chen, Lili Qiu", "Date published": "05/30/2024", "URL": "http://arxiv.org/abs/2405.19888v1", "summary": "The rise of large language models (LLMs) has enabled LLM-based applications\n(a.k.a. AI agents or co-pilots), a new software paradigm that combines the\nstrength of LLM and conventional software. Diverse LLM applications from\ndifferent tenants could design complex workflows using multiple LLM requests to\naccomplish one task. However, they have to use the over-simplified\nrequest-level API provided by today's public LLM services, losing essential\napplication-level information. Public LLM services have to blindly optimize\nindividual LLM requests, leading to sub-optimal end-to-end performance of LLM\napplications.\n  This paper introduces Parrot, an LLM service system that focuses on the\nend-to-end experience of LLM-based applications. Parrot proposes Semantic\nVariable, a unified abstraction to expose application-level knowledge to public\nLLM services. A Semantic Variable annotates an input/output variable in the\nprompt of a request, and creates the data pipeline when connecting multiple LLM\nrequests, providing a natural way to program LLM applications. Exposing\nSemantic Variables to the public LLM service allows it to perform conventional\ndata flow analysis to uncover the correlation across multiple LLM requests.\nThis correlation opens a brand-new optimization space for the end-to-end\nperformance of LLM-based applications. Extensive evaluations demonstrate that\nParrot can achieve up to an order-of-magnitude improvement for popular and\npractical use cases of LLM applications."}}, "ffd77891-69dc-49ff-9527-db9920cd7dbb": {"node_ids": ["05d8b490-dad4-4b6a-add5-486b4533d430", "23e26e03-e0fd-4f8c-b75c-feecc2600d94", "b0d5731c-5c33-438e-84f0-c2ca69792b65"], "metadata": {"page_label": "7", "file_name": "2405_19888v1.pdf", "Title of this paper": "Parrot: Efficient Serving of LLM-based Applications with Semantic Variable", "Authors": "Chaofan Lin, Zhenhua Han, Chengruidong Zhang, Yuqing Yang, Fan Yang, Chen Chen, Lili Qiu", "Date published": "05/30/2024", "URL": "http://arxiv.org/abs/2405.19888v1", "summary": "The rise of large language models (LLMs) has enabled LLM-based applications\n(a.k.a. AI agents or co-pilots), a new software paradigm that combines the\nstrength of LLM and conventional software. Diverse LLM applications from\ndifferent tenants could design complex workflows using multiple LLM requests to\naccomplish one task. However, they have to use the over-simplified\nrequest-level API provided by today's public LLM services, losing essential\napplication-level information. Public LLM services have to blindly optimize\nindividual LLM requests, leading to sub-optimal end-to-end performance of LLM\napplications.\n  This paper introduces Parrot, an LLM service system that focuses on the\nend-to-end experience of LLM-based applications. Parrot proposes Semantic\nVariable, a unified abstraction to expose application-level knowledge to public\nLLM services. A Semantic Variable annotates an input/output variable in the\nprompt of a request, and creates the data pipeline when connecting multiple LLM\nrequests, providing a natural way to program LLM applications. Exposing\nSemantic Variables to the public LLM service allows it to perform conventional\ndata flow analysis to uncover the correlation across multiple LLM requests.\nThis correlation opens a brand-new optimization space for the end-to-end\nperformance of LLM-based applications. Extensive evaluations demonstrate that\nParrot can achieve up to an order-of-magnitude improvement for popular and\npractical use cases of LLM applications."}}, "62a5c817-91b9-4641-a461-ee893700467a": {"node_ids": ["f5e52809-757b-454a-85d9-1762efc43e23", "eb4678d4-44a9-4c66-a402-6bd90b65394d", "5420325b-29ea-4cc3-b18b-f111ac9069e2"], "metadata": {"page_label": "8", "file_name": "2405_19888v1.pdf", "Title of this paper": "Parrot: Efficient Serving of LLM-based Applications with Semantic Variable", "Authors": "Chaofan Lin, Zhenhua Han, Chengruidong Zhang, Yuqing Yang, Fan Yang, Chen Chen, Lili Qiu", "Date published": "05/30/2024", "URL": "http://arxiv.org/abs/2405.19888v1", "summary": "The rise of large language models (LLMs) has enabled LLM-based applications\n(a.k.a. AI agents or co-pilots), a new software paradigm that combines the\nstrength of LLM and conventional software. Diverse LLM applications from\ndifferent tenants could design complex workflows using multiple LLM requests to\naccomplish one task. However, they have to use the over-simplified\nrequest-level API provided by today's public LLM services, losing essential\napplication-level information. Public LLM services have to blindly optimize\nindividual LLM requests, leading to sub-optimal end-to-end performance of LLM\napplications.\n  This paper introduces Parrot, an LLM service system that focuses on the\nend-to-end experience of LLM-based applications. Parrot proposes Semantic\nVariable, a unified abstraction to expose application-level knowledge to public\nLLM services. A Semantic Variable annotates an input/output variable in the\nprompt of a request, and creates the data pipeline when connecting multiple LLM\nrequests, providing a natural way to program LLM applications. Exposing\nSemantic Variables to the public LLM service allows it to perform conventional\ndata flow analysis to uncover the correlation across multiple LLM requests.\nThis correlation opens a brand-new optimization space for the end-to-end\nperformance of LLM-based applications. Extensive evaluations demonstrate that\nParrot can achieve up to an order-of-magnitude improvement for popular and\npractical use cases of LLM applications."}}, "f242335c-3d9e-4a1c-aa50-287c6dd1ce98": {"node_ids": ["2ef43774-b1dc-4346-bf8a-c26ca1b57ed3", "755a9e00-39bd-4a57-8c6f-24ab452a0f7a", "940d3026-179d-405b-9650-6033dcd18dee"], "metadata": {"page_label": "9", "file_name": "2405_19888v1.pdf", "Title of this paper": "Parrot: Efficient Serving of LLM-based Applications with Semantic Variable", "Authors": "Chaofan Lin, Zhenhua Han, Chengruidong Zhang, Yuqing Yang, Fan Yang, Chen Chen, Lili Qiu", "Date published": "05/30/2024", "URL": "http://arxiv.org/abs/2405.19888v1", "summary": "The rise of large language models (LLMs) has enabled LLM-based applications\n(a.k.a. AI agents or co-pilots), a new software paradigm that combines the\nstrength of LLM and conventional software. Diverse LLM applications from\ndifferent tenants could design complex workflows using multiple LLM requests to\naccomplish one task. However, they have to use the over-simplified\nrequest-level API provided by today's public LLM services, losing essential\napplication-level information. Public LLM services have to blindly optimize\nindividual LLM requests, leading to sub-optimal end-to-end performance of LLM\napplications.\n  This paper introduces Parrot, an LLM service system that focuses on the\nend-to-end experience of LLM-based applications. Parrot proposes Semantic\nVariable, a unified abstraction to expose application-level knowledge to public\nLLM services. A Semantic Variable annotates an input/output variable in the\nprompt of a request, and creates the data pipeline when connecting multiple LLM\nrequests, providing a natural way to program LLM applications. Exposing\nSemantic Variables to the public LLM service allows it to perform conventional\ndata flow analysis to uncover the correlation across multiple LLM requests.\nThis correlation opens a brand-new optimization space for the end-to-end\nperformance of LLM-based applications. Extensive evaluations demonstrate that\nParrot can achieve up to an order-of-magnitude improvement for popular and\npractical use cases of LLM applications."}}, "27c11532-acdd-45ca-8762-8191c0f96067": {"node_ids": ["372d39f1-c991-4a83-9902-7961458c59b8", "767437ef-ae0e-46fb-a914-52e3a8852009", "0bef3e4c-6a41-4526-ba13-b8d846b954b9"], "metadata": {"page_label": "10", "file_name": "2405_19888v1.pdf", "Title of this paper": "Parrot: Efficient Serving of LLM-based Applications with Semantic Variable", "Authors": "Chaofan Lin, Zhenhua Han, Chengruidong Zhang, Yuqing Yang, Fan Yang, Chen Chen, Lili Qiu", "Date published": "05/30/2024", "URL": "http://arxiv.org/abs/2405.19888v1", "summary": "The rise of large language models (LLMs) has enabled LLM-based applications\n(a.k.a. AI agents or co-pilots), a new software paradigm that combines the\nstrength of LLM and conventional software. Diverse LLM applications from\ndifferent tenants could design complex workflows using multiple LLM requests to\naccomplish one task. However, they have to use the over-simplified\nrequest-level API provided by today's public LLM services, losing essential\napplication-level information. Public LLM services have to blindly optimize\nindividual LLM requests, leading to sub-optimal end-to-end performance of LLM\napplications.\n  This paper introduces Parrot, an LLM service system that focuses on the\nend-to-end experience of LLM-based applications. Parrot proposes Semantic\nVariable, a unified abstraction to expose application-level knowledge to public\nLLM services. A Semantic Variable annotates an input/output variable in the\nprompt of a request, and creates the data pipeline when connecting multiple LLM\nrequests, providing a natural way to program LLM applications. Exposing\nSemantic Variables to the public LLM service allows it to perform conventional\ndata flow analysis to uncover the correlation across multiple LLM requests.\nThis correlation opens a brand-new optimization space for the end-to-end\nperformance of LLM-based applications. Extensive evaluations demonstrate that\nParrot can achieve up to an order-of-magnitude improvement for popular and\npractical use cases of LLM applications."}}, "f799a209-ea73-44e2-8183-2da4ea8ee984": {"node_ids": ["5329a245-1753-4c4e-b58c-d9a84b28bd20", "de0ac9e9-a8e5-456a-8363-0b4dd995a53c", "ebb694d8-b24b-4e15-a1fb-9cb36ee41f2c"], "metadata": {"page_label": "11", "file_name": "2405_19888v1.pdf", "Title of this paper": "Parrot: Efficient Serving of LLM-based Applications with Semantic Variable", "Authors": "Chaofan Lin, Zhenhua Han, Chengruidong Zhang, Yuqing Yang, Fan Yang, Chen Chen, Lili Qiu", "Date published": "05/30/2024", "URL": "http://arxiv.org/abs/2405.19888v1", "summary": "The rise of large language models (LLMs) has enabled LLM-based applications\n(a.k.a. AI agents or co-pilots), a new software paradigm that combines the\nstrength of LLM and conventional software. Diverse LLM applications from\ndifferent tenants could design complex workflows using multiple LLM requests to\naccomplish one task. However, they have to use the over-simplified\nrequest-level API provided by today's public LLM services, losing essential\napplication-level information. Public LLM services have to blindly optimize\nindividual LLM requests, leading to sub-optimal end-to-end performance of LLM\napplications.\n  This paper introduces Parrot, an LLM service system that focuses on the\nend-to-end experience of LLM-based applications. Parrot proposes Semantic\nVariable, a unified abstraction to expose application-level knowledge to public\nLLM services. A Semantic Variable annotates an input/output variable in the\nprompt of a request, and creates the data pipeline when connecting multiple LLM\nrequests, providing a natural way to program LLM applications. Exposing\nSemantic Variables to the public LLM service allows it to perform conventional\ndata flow analysis to uncover the correlation across multiple LLM requests.\nThis correlation opens a brand-new optimization space for the end-to-end\nperformance of LLM-based applications. Extensive evaluations demonstrate that\nParrot can achieve up to an order-of-magnitude improvement for popular and\npractical use cases of LLM applications."}}, "c71416dd-8a88-43c4-b5c7-eb0e11238506": {"node_ids": ["c6583cdd-284e-4c86-a9ab-abb12025c826", "b64a0b99-ffb2-4020-b46e-5ff110f1124f", "8895c478-587d-472e-be81-35370b307761"], "metadata": {"page_label": "12", "file_name": "2405_19888v1.pdf", "Title of this paper": "Parrot: Efficient Serving of LLM-based Applications with Semantic Variable", "Authors": "Chaofan Lin, Zhenhua Han, Chengruidong Zhang, Yuqing Yang, Fan Yang, Chen Chen, Lili Qiu", "Date published": "05/30/2024", "URL": "http://arxiv.org/abs/2405.19888v1", "summary": "The rise of large language models (LLMs) has enabled LLM-based applications\n(a.k.a. AI agents or co-pilots), a new software paradigm that combines the\nstrength of LLM and conventional software. Diverse LLM applications from\ndifferent tenants could design complex workflows using multiple LLM requests to\naccomplish one task. However, they have to use the over-simplified\nrequest-level API provided by today's public LLM services, losing essential\napplication-level information. Public LLM services have to blindly optimize\nindividual LLM requests, leading to sub-optimal end-to-end performance of LLM\napplications.\n  This paper introduces Parrot, an LLM service system that focuses on the\nend-to-end experience of LLM-based applications. Parrot proposes Semantic\nVariable, a unified abstraction to expose application-level knowledge to public\nLLM services. A Semantic Variable annotates an input/output variable in the\nprompt of a request, and creates the data pipeline when connecting multiple LLM\nrequests, providing a natural way to program LLM applications. Exposing\nSemantic Variables to the public LLM service allows it to perform conventional\ndata flow analysis to uncover the correlation across multiple LLM requests.\nThis correlation opens a brand-new optimization space for the end-to-end\nperformance of LLM-based applications. Extensive evaluations demonstrate that\nParrot can achieve up to an order-of-magnitude improvement for popular and\npractical use cases of LLM applications."}}, "8931491b-6711-4bc1-b824-c863efd9bd88": {"node_ids": ["b0b924dc-5101-40ca-9efc-334e6b57913a", "774e005a-de09-4136-a514-dea6cac22cc2", "9e437f86-3703-43f5-87d3-7bf016b69d2c"], "metadata": {"page_label": "13", "file_name": "2405_19888v1.pdf", "Title of this paper": "Parrot: Efficient Serving of LLM-based Applications with Semantic Variable", "Authors": "Chaofan Lin, Zhenhua Han, Chengruidong Zhang, Yuqing Yang, Fan Yang, Chen Chen, Lili Qiu", "Date published": "05/30/2024", "URL": "http://arxiv.org/abs/2405.19888v1", "summary": "The rise of large language models (LLMs) has enabled LLM-based applications\n(a.k.a. AI agents or co-pilots), a new software paradigm that combines the\nstrength of LLM and conventional software. Diverse LLM applications from\ndifferent tenants could design complex workflows using multiple LLM requests to\naccomplish one task. However, they have to use the over-simplified\nrequest-level API provided by today's public LLM services, losing essential\napplication-level information. Public LLM services have to blindly optimize\nindividual LLM requests, leading to sub-optimal end-to-end performance of LLM\napplications.\n  This paper introduces Parrot, an LLM service system that focuses on the\nend-to-end experience of LLM-based applications. Parrot proposes Semantic\nVariable, a unified abstraction to expose application-level knowledge to public\nLLM services. A Semantic Variable annotates an input/output variable in the\nprompt of a request, and creates the data pipeline when connecting multiple LLM\nrequests, providing a natural way to program LLM applications. Exposing\nSemantic Variables to the public LLM service allows it to perform conventional\ndata flow analysis to uncover the correlation across multiple LLM requests.\nThis correlation opens a brand-new optimization space for the end-to-end\nperformance of LLM-based applications. Extensive evaluations demonstrate that\nParrot can achieve up to an order-of-magnitude improvement for popular and\npractical use cases of LLM applications."}}, "ffb52c6f-dc64-4f9b-be30-05c3786cac37": {"node_ids": ["92bd5626-be87-4331-937b-b9aa0fd9930d", "6e8ee0bc-90cb-4283-8400-cdba598f6e4a", "c9882fdf-97aa-4574-aa6d-bae79f6dd5c8"], "metadata": {"page_label": "14", "file_name": "2405_19888v1.pdf", "Title of this paper": "Parrot: Efficient Serving of LLM-based Applications with Semantic Variable", "Authors": "Chaofan Lin, Zhenhua Han, Chengruidong Zhang, Yuqing Yang, Fan Yang, Chen Chen, Lili Qiu", "Date published": "05/30/2024", "URL": "http://arxiv.org/abs/2405.19888v1", "summary": "The rise of large language models (LLMs) has enabled LLM-based applications\n(a.k.a. AI agents or co-pilots), a new software paradigm that combines the\nstrength of LLM and conventional software. Diverse LLM applications from\ndifferent tenants could design complex workflows using multiple LLM requests to\naccomplish one task. However, they have to use the over-simplified\nrequest-level API provided by today's public LLM services, losing essential\napplication-level information. Public LLM services have to blindly optimize\nindividual LLM requests, leading to sub-optimal end-to-end performance of LLM\napplications.\n  This paper introduces Parrot, an LLM service system that focuses on the\nend-to-end experience of LLM-based applications. Parrot proposes Semantic\nVariable, a unified abstraction to expose application-level knowledge to public\nLLM services. A Semantic Variable annotates an input/output variable in the\nprompt of a request, and creates the data pipeline when connecting multiple LLM\nrequests, providing a natural way to program LLM applications. Exposing\nSemantic Variables to the public LLM service allows it to perform conventional\ndata flow analysis to uncover the correlation across multiple LLM requests.\nThis correlation opens a brand-new optimization space for the end-to-end\nperformance of LLM-based applications. Extensive evaluations demonstrate that\nParrot can achieve up to an order-of-magnitude improvement for popular and\npractical use cases of LLM applications."}}, "8ca79964-0014-454e-873a-34fd27a7d9ac": {"node_ids": ["aaf8e015-6662-455f-afb8-57d94a3cba72", "58284db1-384a-412b-8fe3-cc6b58a32c92", "15c41549-39b3-4891-8ebe-8e9261a82581"], "metadata": {"page_label": "15", "file_name": "2405_19888v1.pdf", "Title of this paper": "Parrot: Efficient Serving of LLM-based Applications with Semantic Variable", "Authors": "Chaofan Lin, Zhenhua Han, Chengruidong Zhang, Yuqing Yang, Fan Yang, Chen Chen, Lili Qiu", "Date published": "05/30/2024", "URL": "http://arxiv.org/abs/2405.19888v1", "summary": "The rise of large language models (LLMs) has enabled LLM-based applications\n(a.k.a. AI agents or co-pilots), a new software paradigm that combines the\nstrength of LLM and conventional software. Diverse LLM applications from\ndifferent tenants could design complex workflows using multiple LLM requests to\naccomplish one task. However, they have to use the over-simplified\nrequest-level API provided by today's public LLM services, losing essential\napplication-level information. Public LLM services have to blindly optimize\nindividual LLM requests, leading to sub-optimal end-to-end performance of LLM\napplications.\n  This paper introduces Parrot, an LLM service system that focuses on the\nend-to-end experience of LLM-based applications. Parrot proposes Semantic\nVariable, a unified abstraction to expose application-level knowledge to public\nLLM services. A Semantic Variable annotates an input/output variable in the\nprompt of a request, and creates the data pipeline when connecting multiple LLM\nrequests, providing a natural way to program LLM applications. Exposing\nSemantic Variables to the public LLM service allows it to perform conventional\ndata flow analysis to uncover the correlation across multiple LLM requests.\nThis correlation opens a brand-new optimization space for the end-to-end\nperformance of LLM-based applications. Extensive evaluations demonstrate that\nParrot can achieve up to an order-of-magnitude improvement for popular and\npractical use cases of LLM applications."}}, "2ce9e853-5564-4047-a410-0efaffd3add7": {"node_ids": ["05a3945c-aec3-4538-8d36-daf50c57cf03", "9b468e9f-c44a-47d1-b48c-968837de705e", "a2d3c32c-304c-4504-ab39-d87bad96aa5e"], "metadata": {"page_label": "16", "file_name": "2405_19888v1.pdf", "Title of this paper": "Parrot: Efficient Serving of LLM-based Applications with Semantic Variable", "Authors": "Chaofan Lin, Zhenhua Han, Chengruidong Zhang, Yuqing Yang, Fan Yang, Chen Chen, Lili Qiu", "Date published": "05/30/2024", "URL": "http://arxiv.org/abs/2405.19888v1", "summary": "The rise of large language models (LLMs) has enabled LLM-based applications\n(a.k.a. AI agents or co-pilots), a new software paradigm that combines the\nstrength of LLM and conventional software. Diverse LLM applications from\ndifferent tenants could design complex workflows using multiple LLM requests to\naccomplish one task. However, they have to use the over-simplified\nrequest-level API provided by today's public LLM services, losing essential\napplication-level information. Public LLM services have to blindly optimize\nindividual LLM requests, leading to sub-optimal end-to-end performance of LLM\napplications.\n  This paper introduces Parrot, an LLM service system that focuses on the\nend-to-end experience of LLM-based applications. Parrot proposes Semantic\nVariable, a unified abstraction to expose application-level knowledge to public\nLLM services. A Semantic Variable annotates an input/output variable in the\nprompt of a request, and creates the data pipeline when connecting multiple LLM\nrequests, providing a natural way to program LLM applications. Exposing\nSemantic Variables to the public LLM service allows it to perform conventional\ndata flow analysis to uncover the correlation across multiple LLM requests.\nThis correlation opens a brand-new optimization space for the end-to-end\nperformance of LLM-based applications. Extensive evaluations demonstrate that\nParrot can achieve up to an order-of-magnitude improvement for popular and\npractical use cases of LLM applications."}}, "9aab5d51-fac2-4486-8bb8-5e193179b662": {"node_ids": ["e5729fb3-cfed-4db2-b25f-c46d56b197da"], "metadata": {"page_label": "17", "file_name": "2405_19888v1.pdf", "Title of this paper": "Parrot: Efficient Serving of LLM-based Applications with Semantic Variable", "Authors": "Chaofan Lin, Zhenhua Han, Chengruidong Zhang, Yuqing Yang, Fan Yang, Chen Chen, Lili Qiu", "Date published": "05/30/2024", "URL": "http://arxiv.org/abs/2405.19888v1", "summary": "The rise of large language models (LLMs) has enabled LLM-based applications\n(a.k.a. AI agents or co-pilots), a new software paradigm that combines the\nstrength of LLM and conventional software. Diverse LLM applications from\ndifferent tenants could design complex workflows using multiple LLM requests to\naccomplish one task. However, they have to use the over-simplified\nrequest-level API provided by today's public LLM services, losing essential\napplication-level information. Public LLM services have to blindly optimize\nindividual LLM requests, leading to sub-optimal end-to-end performance of LLM\napplications.\n  This paper introduces Parrot, an LLM service system that focuses on the\nend-to-end experience of LLM-based applications. Parrot proposes Semantic\nVariable, a unified abstraction to expose application-level knowledge to public\nLLM services. A Semantic Variable annotates an input/output variable in the\nprompt of a request, and creates the data pipeline when connecting multiple LLM\nrequests, providing a natural way to program LLM applications. Exposing\nSemantic Variables to the public LLM service allows it to perform conventional\ndata flow analysis to uncover the correlation across multiple LLM requests.\nThis correlation opens a brand-new optimization space for the end-to-end\nperformance of LLM-based applications. Extensive evaluations demonstrate that\nParrot can achieve up to an order-of-magnitude improvement for popular and\npractical use cases of LLM applications."}}}}