{"docstore/data": {"faf3f149-be4f-437d-98ea-1fde6a5aab5f": {"__data__": {"id_": "faf3f149-be4f-437d-98ea-1fde6a5aab5f", "embedding": null, "metadata": {"page_label": "1", "file_name": "2406_00936v1.pdf", "Title of this paper": "A Survey of Useful LLM Evaluation", "Authors": "Ji-Lun Peng, Sijia Cheng, Egil Diau, Yung-Yu Shih, Po-Heng Chen, Yen-Ting Lin, Yun-Nung Chen", "Date published": "06/03/2024", "URL": "http://arxiv.org/abs/2406.00936v1", "summary": "LLMs have gotten attention across various research domains due to their\nexceptional performance on a wide range of complex tasks. Therefore, refined\nmethods to evaluate the capabilities of LLMs are needed to determine the tasks\nand responsibility they should undertake. Our study mainly discussed how LLMs,\nas useful tools, should be effectively assessed. We proposed the two-stage\nframework: from ``core ability'' to ``agent'', clearly explaining how LLMs can\nbe applied based on their specific capabilities, along with the evaluation\nmethods in each stage. Core ability refers to the capabilities that LLMs need\nin order to generate high-quality natural language texts. After confirming LLMs\npossess core ability, they can solve real-world and complex tasks as agent. In\nthe \"core ability\" stage, we discussed the reasoning ability, societal impact,\nand domain knowledge of LLMs. In the ``agent'' stage, we demonstrated embodied\naction, planning, and tool learning of LLMs agent applications. Finally, we\nexamined the challenges currently confronting the evaluation methods for LLMs,\nas well as the directions for future development."}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "dfbb3e44-557a-400b-8d6d-50e507564097", "node_type": "4", "metadata": {"page_label": "1", "file_name": "2406_00936v1.pdf", "Title of this paper": "A Survey of Useful LLM Evaluation", "Authors": "Ji-Lun Peng, Sijia Cheng, Egil Diau, Yung-Yu Shih, Po-Heng Chen, Yen-Ting Lin, Yun-Nung Chen", "Date published": "06/03/2024", "URL": "http://arxiv.org/abs/2406.00936v1", "summary": "LLMs have gotten attention across various research domains due to their\nexceptional performance on a wide range of complex tasks. Therefore, refined\nmethods to evaluate the capabilities of LLMs are needed to determine the tasks\nand responsibility they should undertake. Our study mainly discussed how LLMs,\nas useful tools, should be effectively assessed. We proposed the two-stage\nframework: from ``core ability'' to ``agent'', clearly explaining how LLMs can\nbe applied based on their specific capabilities, along with the evaluation\nmethods in each stage. Core ability refers to the capabilities that LLMs need\nin order to generate high-quality natural language texts. After confirming LLMs\npossess core ability, they can solve real-world and complex tasks as agent. In\nthe \"core ability\" stage, we discussed the reasoning ability, societal impact,\nand domain knowledge of LLMs. In the ``agent'' stage, we demonstrated embodied\naction, planning, and tool learning of LLMs agent applications. Finally, we\nexamined the challenges currently confronting the evaluation methods for LLMs,\nas well as the directions for future development."}, "hash": "2f0638223ceeb58ab0205606185ec423b6fff6dc287d9318f493bc3311c27d28", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3b447e69-caaf-412b-86bf-f9462ad40a34", "node_type": "1", "metadata": {}, "hash": "d7b33afa5993dc72901e88631fd754a49974c46397133b5680b3b62b2bc9e91d", "class_name": "RelatedNodeInfo"}}, "text": "A Survey of Useful LLM Evaluation\nJi-Lun Peng\u2217Sijia Cheng\u2217Egil Diau\u2217Yung-Yu Shih\u2217\nPo-Heng Chen\u2217Yen-Ting Lin Yun-Nung Chen\nNational Taiwan University, Taipei, Taiwan\n{b09207002, r11922184, r12922a03, r12944007, r11922044}@ntu.edu.tw\n{ytl, y.v.chen}ieee.org\nAbstract\nLLMs have gotten attention across various re-\nsearch domains due to their exceptional per-\nformance on a wide range of complex tasks.\nTherefore, refined methods to evaluate the ca-\npabilities of LLMs are needed to determine\nthe tasks and responsibility they should under-\ntake. Our study mainly discussed how LLMs,\nas useful tools, should be effectively assessed.\nWe proposed the two-stage framework: from\n\u201ccore ability\u201d to \u201cagent\u201d, clearly explaining how\nLLMs can be applied based on their specific\ncapabilities, along with the evaluation methods\nin each stage. Core ability refers to the capabil-\nities that LLMs need in order to generate high-\nquality natural language texts. After confirming\nLLMs possess core ability, they can solve real-\nworld and complex tasks as agent. In the \"core\nability\" stage, we discussed the reasoning abil-\nity, societal impact, and domain knowledge of\nLLMs. In the \u201cagent\u201d stage, we demonstrated\nembodied action, planning, and tool learning\nof LLMs agent applications. Finally, we exam-\nined the challenges currently confronting the\nevaluation methods for LLMs, as well as the\ndirections for future development.1\n1 Introduction\n1.1 Artificial Intelligence and Large\nLanguage Model\nArtificial intelligence (AI) simulates human behav-\nior to complete multiple tasks needing human intel-\nligence. The first models of AI tried to simulate the\nfunction of a single neuron with feedforward, sim-\nple input-output functions (Muthukrishnan et al.,\n2020). As time has progressed, a variety of ma-\nchine learning (ML) and deep learning (DL) mod-\nels have been developed. They are not only capable\nof identifying patterns from vast amounts of data,\nbut they can also make predictions, and even handle\n1https://github.com/MiuLab/EvalLLM-Survey\n*Equal contribution.unstructured data such as text, images, and audio.\nRecently, the Transformer architecture (Vaswani\net al., 2017) was proposed, allowing word embed-\ndings to be context-dependent, and model training\nto be scaled up (Min et al., 2023). Therefore, re-\nsearchers gradually increased the parameters in\npre-trained language models trying to reach bet-\nter performance.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2413, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3b447e69-caaf-412b-86bf-f9462ad40a34": {"__data__": {"id_": "3b447e69-caaf-412b-86bf-f9462ad40a34", "embedding": null, "metadata": {"page_label": "1", "file_name": "2406_00936v1.pdf", "Title of this paper": "A Survey of Useful LLM Evaluation", "Authors": "Ji-Lun Peng, Sijia Cheng, Egil Diau, Yung-Yu Shih, Po-Heng Chen, Yen-Ting Lin, Yun-Nung Chen", "Date published": "06/03/2024", "URL": "http://arxiv.org/abs/2406.00936v1", "summary": "LLMs have gotten attention across various research domains due to their\nexceptional performance on a wide range of complex tasks. Therefore, refined\nmethods to evaluate the capabilities of LLMs are needed to determine the tasks\nand responsibility they should undertake. Our study mainly discussed how LLMs,\nas useful tools, should be effectively assessed. We proposed the two-stage\nframework: from ``core ability'' to ``agent'', clearly explaining how LLMs can\nbe applied based on their specific capabilities, along with the evaluation\nmethods in each stage. Core ability refers to the capabilities that LLMs need\nin order to generate high-quality natural language texts. After confirming LLMs\npossess core ability, they can solve real-world and complex tasks as agent. In\nthe \"core ability\" stage, we discussed the reasoning ability, societal impact,\nand domain knowledge of LLMs. In the ``agent'' stage, we demonstrated embodied\naction, planning, and tool learning of LLMs agent applications. Finally, we\nexamined the challenges currently confronting the evaluation methods for LLMs,\nas well as the directions for future development."}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "dfbb3e44-557a-400b-8d6d-50e507564097", "node_type": "4", "metadata": {"page_label": "1", "file_name": "2406_00936v1.pdf", "Title of this paper": "A Survey of Useful LLM Evaluation", "Authors": "Ji-Lun Peng, Sijia Cheng, Egil Diau, Yung-Yu Shih, Po-Heng Chen, Yen-Ting Lin, Yun-Nung Chen", "Date published": "06/03/2024", "URL": "http://arxiv.org/abs/2406.00936v1", "summary": "LLMs have gotten attention across various research domains due to their\nexceptional performance on a wide range of complex tasks. Therefore, refined\nmethods to evaluate the capabilities of LLMs are needed to determine the tasks\nand responsibility they should undertake. Our study mainly discussed how LLMs,\nas useful tools, should be effectively assessed. We proposed the two-stage\nframework: from ``core ability'' to ``agent'', clearly explaining how LLMs can\nbe applied based on their specific capabilities, along with the evaluation\nmethods in each stage. Core ability refers to the capabilities that LLMs need\nin order to generate high-quality natural language texts. After confirming LLMs\npossess core ability, they can solve real-world and complex tasks as agent. In\nthe \"core ability\" stage, we discussed the reasoning ability, societal impact,\nand domain knowledge of LLMs. In the ``agent'' stage, we demonstrated embodied\naction, planning, and tool learning of LLMs agent applications. Finally, we\nexamined the challenges currently confronting the evaluation methods for LLMs,\nas well as the directions for future development."}, "hash": "2f0638223ceeb58ab0205606185ec423b6fff6dc287d9318f493bc3311c27d28", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "faf3f149-be4f-437d-98ea-1fde6a5aab5f", "node_type": "1", "metadata": {"page_label": "1", "file_name": "2406_00936v1.pdf", "Title of this paper": "A Survey of Useful LLM Evaluation", "Authors": "Ji-Lun Peng, Sijia Cheng, Egil Diau, Yung-Yu Shih, Po-Heng Chen, Yen-Ting Lin, Yun-Nung Chen", "Date published": "06/03/2024", "URL": "http://arxiv.org/abs/2406.00936v1", "summary": "LLMs have gotten attention across various research domains due to their\nexceptional performance on a wide range of complex tasks. Therefore, refined\nmethods to evaluate the capabilities of LLMs are needed to determine the tasks\nand responsibility they should undertake. Our study mainly discussed how LLMs,\nas useful tools, should be effectively assessed. We proposed the two-stage\nframework: from ``core ability'' to ``agent'', clearly explaining how LLMs can\nbe applied based on their specific capabilities, along with the evaluation\nmethods in each stage. Core ability refers to the capabilities that LLMs need\nin order to generate high-quality natural language texts. After confirming LLMs\npossess core ability, they can solve real-world and complex tasks as agent. In\nthe \"core ability\" stage, we discussed the reasoning ability, societal impact,\nand domain knowledge of LLMs. In the ``agent'' stage, we demonstrated embodied\naction, planning, and tool learning of LLMs agent applications. Finally, we\nexamined the challenges currently confronting the evaluation methods for LLMs,\nas well as the directions for future development."}, "hash": "e02840c5fd1a2b8a6512f41f6bee58d30093f8ddbee02e648be0b6c930654e86", "class_name": "RelatedNodeInfo"}}, "text": "The first models of AI tried to simulate the\nfunction of a single neuron with feedforward, sim-\nple input-output functions (Muthukrishnan et al.,\n2020). As time has progressed, a variety of ma-\nchine learning (ML) and deep learning (DL) mod-\nels have been developed. They are not only capable\nof identifying patterns from vast amounts of data,\nbut they can also make predictions, and even handle\n1https://github.com/MiuLab/EvalLLM-Survey\n*Equal contribution.unstructured data such as text, images, and audio.\nRecently, the Transformer architecture (Vaswani\net al., 2017) was proposed, allowing word embed-\ndings to be context-dependent, and model training\nto be scaled up (Min et al., 2023). Therefore, re-\nsearchers gradually increased the parameters in\npre-trained language models trying to reach bet-\nter performance. Using the Generative Pre-trained\nTransformer (GPT) series as an illustration, the pro-\ngression in complexity and models\u2019 capability is\nmarked by a significant increase in the number of\nparameters: GPT-1 (Radford et al., 2018) has 117\nmillion parameters, GPT-2 (Radford et al., 2019)\nexpands this to 1.5 billion parameters, and GPT-3\n(Mann et al., 2020) further escalates to 175 billion\nparameters. Moreover, GPT-4 released by OpenAI\nwith much larger model size could accept image\nand text inputs and produce text outputs, and ex-\nhibited human-level performance on various pro-\nfessional and academic benchmarks (Achiam et al.,\n2023). The models mentioned above, due to their\ntremendous size, are referred to as LLMs. They\nhave gotten attention across various research do-\nmains due to their exceptional performance on a\nwide range of complex tasks.\n1.2 Why Evaluating LLMs is Important\nThe early works testing model\u2019s intelligence re-\nferred to as the Turing Test, raising the question of\nwhether machines could imitate human intelligence\nand made people fail to differentiate (Pinar Saygin\net al., 2000). Evaluating AI is vital as it helps us\ngauge the real-world capabilities and limitations of\nAI systems. As AI technologies improve, partic-\nularly in areas like software testing and structural\nengineering, they can sometimes perform better\nthan humans. However, we need clear benchmarks\nto make sure these technologies are both reliable\nand effective (Salehi and Burgue\u00f1o, 2018). With\nthe rapid evolution of LLMs, refined methods to\nevaluate the capabilities of LLMs is needed to de-\n1arXiv:2406.00936v1  [cs.CL]  3 Jun 2024", "mimetype": "text/plain", "start_char_idx": 1593, "end_char_idx": 4045, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "bff77fd8-1eec-4b50-98d5-9b3cbeb18f27": {"__data__": {"id_": "bff77fd8-1eec-4b50-98d5-9b3cbeb18f27", "embedding": null, "metadata": {"page_label": "2", "file_name": "2406_00936v1.pdf", "Title of this paper": "A Survey of Useful LLM Evaluation", "Authors": "Ji-Lun Peng, Sijia Cheng, Egil Diau, Yung-Yu Shih, Po-Heng Chen, Yen-Ting Lin, Yun-Nung Chen", "Date published": "06/03/2024", "URL": "http://arxiv.org/abs/2406.00936v1", "summary": "LLMs have gotten attention across various research domains due to their\nexceptional performance on a wide range of complex tasks. Therefore, refined\nmethods to evaluate the capabilities of LLMs are needed to determine the tasks\nand responsibility they should undertake. Our study mainly discussed how LLMs,\nas useful tools, should be effectively assessed. We proposed the two-stage\nframework: from ``core ability'' to ``agent'', clearly explaining how LLMs can\nbe applied based on their specific capabilities, along with the evaluation\nmethods in each stage. Core ability refers to the capabilities that LLMs need\nin order to generate high-quality natural language texts. After confirming LLMs\npossess core ability, they can solve real-world and complex tasks as agent. In\nthe \"core ability\" stage, we discussed the reasoning ability, societal impact,\nand domain knowledge of LLMs. In the ``agent'' stage, we demonstrated embodied\naction, planning, and tool learning of LLMs agent applications. Finally, we\nexamined the challenges currently confronting the evaluation methods for LLMs,\nas well as the directions for future development."}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "edd28199-ceb8-4311-af14-1989bf72ea1c", "node_type": "4", "metadata": {"page_label": "2", "file_name": "2406_00936v1.pdf", "Title of this paper": "A Survey of Useful LLM Evaluation", "Authors": "Ji-Lun Peng, Sijia Cheng, Egil Diau, Yung-Yu Shih, Po-Heng Chen, Yen-Ting Lin, Yun-Nung Chen", "Date published": "06/03/2024", "URL": "http://arxiv.org/abs/2406.00936v1", "summary": "LLMs have gotten attention across various research domains due to their\nexceptional performance on a wide range of complex tasks. Therefore, refined\nmethods to evaluate the capabilities of LLMs are needed to determine the tasks\nand responsibility they should undertake. Our study mainly discussed how LLMs,\nas useful tools, should be effectively assessed. We proposed the two-stage\nframework: from ``core ability'' to ``agent'', clearly explaining how LLMs can\nbe applied based on their specific capabilities, along with the evaluation\nmethods in each stage. Core ability refers to the capabilities that LLMs need\nin order to generate high-quality natural language texts. After confirming LLMs\npossess core ability, they can solve real-world and complex tasks as agent. In\nthe \"core ability\" stage, we discussed the reasoning ability, societal impact,\nand domain knowledge of LLMs. In the ``agent'' stage, we demonstrated embodied\naction, planning, and tool learning of LLMs agent applications. Finally, we\nexamined the challenges currently confronting the evaluation methods for LLMs,\nas well as the directions for future development."}, "hash": "01c749c1027405f09aeb7b075245cd8a858f8da3f99c2e0de68a6c673e6d4825", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5ede0fec-360e-41da-98aa-8d12086dd129", "node_type": "1", "metadata": {}, "hash": "a30ae08eb89e96b962dbd6240cb01e9fff6f81e4dab9eed995a06a4167556e60", "class_name": "RelatedNodeInfo"}}, "text": "ReasoningDomain KnowledgePlanningCore AbilityUseful LLMsSocietal Impact\nLLMs\nPractical UsageBenchmarkApplicationScenariosAgentFigure 1: The two-stage framework of our LLMs evaluation.\ntermine the tasks and responsibility they should\nundertake. Because LLMs exhibit a broad spec-\ntrum of capabilities beyond the specific task they\nare trained for: predicting the next words of human-\nwritten texts (Nolfi, 2023), such as formal linguistic\ncompetence (Mahowald et al., 2023), factual knowl-\nedge (Petroni et al., 2019), and even theory of mind\nskills (Kosinski, 2023), we should design bench-\nmarks or evaluation methods specific to each task\nor domain. In current benchmarks, the comprehen-\nsive abilities of LLMs are automatically evaluated\nthrough tasks spanning multiple domains such as\nHELM (Liang et al., 2022) and BIG-Bench (Sri-\nvastava et al., 2022), or by generating human feed-\nback automatically like AlpacaFarm (Dubois et al.,\n2024) and MT-bench (Zheng et al., 2024). How-\never, when LLMs are required to perform specific\ntasks, the existence of evaluation methods tailored\nto those tasks becomes potential. This allows for\na comparison of different models\u2019 capabilities un-\nder identical tasks to select the best performer. In\nthis study, we categorizes LLMs\u2019 distinct abilities,\nsystematically reviews existing evaluation methods\nunder each category, and discusses how LLMs, as\n\"useful\" tools, should be effectively assessed.\n1.3 The Roadmap of Useful LLMs\nTo determine whether LLMs are capable to become\nuseful tools, we should split LLMs\u2019 capabilities\ninto \"core ability\" and \"agent\", and discuss them\nrespectively. Core ability refers to the capabilities\nthat LLMs need in order to generate high-quality\nnatural language texts, which are the foundation of\nperforming complex behaviors.\nFirstly, LLMs must possess the capability for rea-\nsoning, as during interactions with humans, they\nare required to deduce arguments step by step to\nengage in effective discussion. Furthermore, the so-cietal impact of LLMs needs significant attention,\nfor LLMs must be perceived as safe and trustwor-\nthy for humans to believe in and actively use them.\nLastly, LLMs should have knowledge across vari-\nous domains, and they can assist humans in solving\nproblems occurring withing diverse fields.\nUpon confirming that LLMs possess these core\nabilities, we can utilize LLMs to perform complex\nbehaviors to deal with real-world problems, which\nwe define as agent. For instance, LLMs agents can\nperform planning, generating an explicit deliber-\nation process that chooses and organizes actions\nby anticipating their expected outcomes (Ghallab\net al., 2004). Then, LLMs agent can solve tasks\nin various scenarios such as using tools, creating\ntools, navigating embodied robots, and so on.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2784, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5ede0fec-360e-41da-98aa-8d12086dd129": {"__data__": {"id_": "5ede0fec-360e-41da-98aa-8d12086dd129", "embedding": null, "metadata": {"page_label": "2", "file_name": "2406_00936v1.pdf", "Title of this paper": "A Survey of Useful LLM Evaluation", "Authors": "Ji-Lun Peng, Sijia Cheng, Egil Diau, Yung-Yu Shih, Po-Heng Chen, Yen-Ting Lin, Yun-Nung Chen", "Date published": "06/03/2024", "URL": "http://arxiv.org/abs/2406.00936v1", "summary": "LLMs have gotten attention across various research domains due to their\nexceptional performance on a wide range of complex tasks. Therefore, refined\nmethods to evaluate the capabilities of LLMs are needed to determine the tasks\nand responsibility they should undertake. Our study mainly discussed how LLMs,\nas useful tools, should be effectively assessed. We proposed the two-stage\nframework: from ``core ability'' to ``agent'', clearly explaining how LLMs can\nbe applied based on their specific capabilities, along with the evaluation\nmethods in each stage. Core ability refers to the capabilities that LLMs need\nin order to generate high-quality natural language texts. After confirming LLMs\npossess core ability, they can solve real-world and complex tasks as agent. In\nthe \"core ability\" stage, we discussed the reasoning ability, societal impact,\nand domain knowledge of LLMs. In the ``agent'' stage, we demonstrated embodied\naction, planning, and tool learning of LLMs agent applications. Finally, we\nexamined the challenges currently confronting the evaluation methods for LLMs,\nas well as the directions for future development."}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "edd28199-ceb8-4311-af14-1989bf72ea1c", "node_type": "4", "metadata": {"page_label": "2", "file_name": "2406_00936v1.pdf", "Title of this paper": "A Survey of Useful LLM Evaluation", "Authors": "Ji-Lun Peng, Sijia Cheng, Egil Diau, Yung-Yu Shih, Po-Heng Chen, Yen-Ting Lin, Yun-Nung Chen", "Date published": "06/03/2024", "URL": "http://arxiv.org/abs/2406.00936v1", "summary": "LLMs have gotten attention across various research domains due to their\nexceptional performance on a wide range of complex tasks. Therefore, refined\nmethods to evaluate the capabilities of LLMs are needed to determine the tasks\nand responsibility they should undertake. Our study mainly discussed how LLMs,\nas useful tools, should be effectively assessed. We proposed the two-stage\nframework: from ``core ability'' to ``agent'', clearly explaining how LLMs can\nbe applied based on their specific capabilities, along with the evaluation\nmethods in each stage. Core ability refers to the capabilities that LLMs need\nin order to generate high-quality natural language texts. After confirming LLMs\npossess core ability, they can solve real-world and complex tasks as agent. In\nthe \"core ability\" stage, we discussed the reasoning ability, societal impact,\nand domain knowledge of LLMs. In the ``agent'' stage, we demonstrated embodied\naction, planning, and tool learning of LLMs agent applications. Finally, we\nexamined the challenges currently confronting the evaluation methods for LLMs,\nas well as the directions for future development."}, "hash": "01c749c1027405f09aeb7b075245cd8a858f8da3f99c2e0de68a6c673e6d4825", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "bff77fd8-1eec-4b50-98d5-9b3cbeb18f27", "node_type": "1", "metadata": {"page_label": "2", "file_name": "2406_00936v1.pdf", "Title of this paper": "A Survey of Useful LLM Evaluation", "Authors": "Ji-Lun Peng, Sijia Cheng, Egil Diau, Yung-Yu Shih, Po-Heng Chen, Yen-Ting Lin, Yun-Nung Chen", "Date published": "06/03/2024", "URL": "http://arxiv.org/abs/2406.00936v1", "summary": "LLMs have gotten attention across various research domains due to their\nexceptional performance on a wide range of complex tasks. Therefore, refined\nmethods to evaluate the capabilities of LLMs are needed to determine the tasks\nand responsibility they should undertake. Our study mainly discussed how LLMs,\nas useful tools, should be effectively assessed. We proposed the two-stage\nframework: from ``core ability'' to ``agent'', clearly explaining how LLMs can\nbe applied based on their specific capabilities, along with the evaluation\nmethods in each stage. Core ability refers to the capabilities that LLMs need\nin order to generate high-quality natural language texts. After confirming LLMs\npossess core ability, they can solve real-world and complex tasks as agent. In\nthe \"core ability\" stage, we discussed the reasoning ability, societal impact,\nand domain knowledge of LLMs. In the ``agent'' stage, we demonstrated embodied\naction, planning, and tool learning of LLMs agent applications. Finally, we\nexamined the challenges currently confronting the evaluation methods for LLMs,\nas well as the directions for future development."}, "hash": "2f4b57056ab17d151cb18f3734f043af4a10eec7af1c7323cbf8a834c277e5f9", "class_name": "RelatedNodeInfo"}}, "text": "Furthermore, the so-cietal impact of LLMs needs significant attention,\nfor LLMs must be perceived as safe and trustwor-\nthy for humans to believe in and actively use them.\nLastly, LLMs should have knowledge across vari-\nous domains, and they can assist humans in solving\nproblems occurring withing diverse fields.\nUpon confirming that LLMs possess these core\nabilities, we can utilize LLMs to perform complex\nbehaviors to deal with real-world problems, which\nwe define as agent. For instance, LLMs agents can\nperform planning, generating an explicit deliber-\nation process that chooses and organizes actions\nby anticipating their expected outcomes (Ghallab\net al., 2004). Then, LLMs agent can solve tasks\nin various scenarios such as using tools, creating\ntools, navigating embodied robots, and so on.\nEven though LLMs can display the aforemen-\ntioned capabilities, comprehensive evaluation meth-\nods are necessary to ensure that LLMs achieve as\nsatisfactory level of performance in executing each\ntask. Existing papers on LLM evaluation meth-\nods, including Guo et al. (2023) and Chang et al.\n(2023), provide a thorough review of evaluation\napproaches for various aspects of LLMs, yet no\nstudy has offered a phased framework to explore\nthe usability of LLMs. Hence, this paper proposes\na two-stage framework to examine whether LLMs\nare sufficiently useful tools (Figure 1).\n1.4 Study Overview\nIn this study, we first introduce the evaluation meth-\nods of the core ability of LLMs (Figure 2), in-\ncluding Reasoning with 5 subsections, Societal\nImpact with 2 subsections, and Domain Knowl-\nedge with 5 subsections. Then, for LLMs agent\n(Figure 3), we introduce evaluation methods of\nthe agent application of LLMs, including Plan-\nning ,Application Scenarios with 7 subsections,\n2", "mimetype": "text/plain", "start_char_idx": 1983, "end_char_idx": 3761, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e77797fa-eb70-4b54-807d-db1fdec77128": {"__data__": {"id_": "e77797fa-eb70-4b54-807d-db1fdec77128", "embedding": null, "metadata": {"page_label": "3", "file_name": "2406_00936v1.pdf", "Title of this paper": "A Survey of Useful LLM Evaluation", "Authors": "Ji-Lun Peng, Sijia Cheng, Egil Diau, Yung-Yu Shih, Po-Heng Chen, Yen-Ting Lin, Yun-Nung Chen", "Date published": "06/03/2024", "URL": "http://arxiv.org/abs/2406.00936v1", "summary": "LLMs have gotten attention across various research domains due to their\nexceptional performance on a wide range of complex tasks. Therefore, refined\nmethods to evaluate the capabilities of LLMs are needed to determine the tasks\nand responsibility they should undertake. Our study mainly discussed how LLMs,\nas useful tools, should be effectively assessed. We proposed the two-stage\nframework: from ``core ability'' to ``agent'', clearly explaining how LLMs can\nbe applied based on their specific capabilities, along with the evaluation\nmethods in each stage. Core ability refers to the capabilities that LLMs need\nin order to generate high-quality natural language texts. After confirming LLMs\npossess core ability, they can solve real-world and complex tasks as agent. In\nthe \"core ability\" stage, we discussed the reasoning ability, societal impact,\nand domain knowledge of LLMs. In the ``agent'' stage, we demonstrated embodied\naction, planning, and tool learning of LLMs agent applications. Finally, we\nexamined the challenges currently confronting the evaluation methods for LLMs,\nas well as the directions for future development."}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "90bdc65e-3c51-4860-9904-ba44dedf5c3d", "node_type": "4", "metadata": {"page_label": "3", "file_name": "2406_00936v1.pdf", "Title of this paper": "A Survey of Useful LLM Evaluation", "Authors": "Ji-Lun Peng, Sijia Cheng, Egil Diau, Yung-Yu Shih, Po-Heng Chen, Yen-Ting Lin, Yun-Nung Chen", "Date published": "06/03/2024", "URL": "http://arxiv.org/abs/2406.00936v1", "summary": "LLMs have gotten attention across various research domains due to their\nexceptional performance on a wide range of complex tasks. Therefore, refined\nmethods to evaluate the capabilities of LLMs are needed to determine the tasks\nand responsibility they should undertake. Our study mainly discussed how LLMs,\nas useful tools, should be effectively assessed. We proposed the two-stage\nframework: from ``core ability'' to ``agent'', clearly explaining how LLMs can\nbe applied based on their specific capabilities, along with the evaluation\nmethods in each stage. Core ability refers to the capabilities that LLMs need\nin order to generate high-quality natural language texts. After confirming LLMs\npossess core ability, they can solve real-world and complex tasks as agent. In\nthe \"core ability\" stage, we discussed the reasoning ability, societal impact,\nand domain knowledge of LLMs. In the ``agent'' stage, we demonstrated embodied\naction, planning, and tool learning of LLMs agent applications. Finally, we\nexamined the challenges currently confronting the evaluation methods for LLMs,\nas well as the directions for future development."}, "hash": "f0fd69f0723e3eb7326c930f2d3e945c0f919b1ea51d7a04b3c53cd730aacce6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a6aa3233-b842-4d11-a76c-f29c68cd8a77", "node_type": "1", "metadata": {}, "hash": "0cff55c908062d64614a18d350c2e18de4830fb2478f2e1d660ba0f713288a85", "class_name": "RelatedNodeInfo"}}, "text": "andBenchmark . In these subsections, we present\napplications of LLMs, evaluation methods, and\ndatasets. Lastly, we give our point of view on the\nusability of LLMs and suggest future directions\nand challenges in evaluating LLMs.\nThe contributions of this paper are as follows:\n(1)We provide a two-stage framework: from core\nability to agent to examine whether LLMs are\nsufficiently useful tools.\n(2)In each section, we elucidate the applications\nof LLMs pertaining to the specific capability,\nalong with the evaluation methods. Further-\nmore, we provide an analysis of the current\nperformance levels of LLMs in these domains.\n(3)We examine the challenges currently con-\nfronting the evaluation methods for LLMs, as\nwell as the directions for future development.\n2 Core Ability Evaluation\nThe evaluation of core abilities in LLMs thoroughly\nexamines their linguistic capabilities across three\nessential dimensions: reasoning, societal impact,\nand domain-specific knowledge. This essential\nevaluation emphasizes LLMs\u2019 proficiency in com-\nplex cognitive reasoning processes in Section 2.1,\ntheir commitment to truthfulness and safety stan-\ndards in Section 2.2, and their adeptness in apply-\ning specialized knowledge across a wide range of\ndomains in Section 2.3.\nBy confirming that LLMs possess these core\nabilities, we recognize the potential of these skills\nto evolve into more complex behaviors. This devel-\nopment emphasizes the adaptability and scalability\nof LLMs as tools for advanced applications, indi-\ncating that the focus will be on enhancing these\nfoundational abilities further in the future.\n2.1 Reasoning\nProficiency in reasoning empowers both humans\nand machines to make well-founded decisions,\nderive logical conclusions, and adeptly tackle\nproblems. Recent research (Huang and Chang,\n2023; Sun et al., 2024) has increasingly empha-\nsized the augmentation of reasoning capacities in\nLLMs, aiming to attain human-level or even sur-\npass human-level reasoning prowess within spe-\ncialized domains. In this section, our attention\nis directed towards evaluating the various reason-\ning abilities of LLMs. The reasoning task can becategorized into the following groups: logical rea-\nsoning, mathematical reasoning, commonsense rea-\nsoning, multi-hop reasoning and structured data\nreasoning.\n2.1.1 Logical Reasoning\nBased on concepts from philosophy and logic, log-\nical reasoning can further be divided to three dif-\nferent types: 1) Inductive reasoning involves in-\nferring general conclusions based on observed pat-\nterns or regularities in specific instances. bAbI-\n15 (Weston et al., 2015) and EntailmentBank (Dalvi\net al., 2021) are common benchmarks for inductive\nreasoing. 2) Deductive reasoning is the process\nof deriving necessary conclusions based on known\npremises and logical rules. bAbI-16 (Weston et al.,\n2015) is an common benchmark for testing deduc-\ntive reasoning. 3) abductive reasoning is a form\nof reasoning where possible explanations or hy-\npotheses are inferred based on given observations\nand known information.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3047, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a6aa3233-b842-4d11-a76c-f29c68cd8a77": {"__data__": {"id_": "a6aa3233-b842-4d11-a76c-f29c68cd8a77", "embedding": null, "metadata": {"page_label": "3", "file_name": "2406_00936v1.pdf", "Title of this paper": "A Survey of Useful LLM Evaluation", "Authors": "Ji-Lun Peng, Sijia Cheng, Egil Diau, Yung-Yu Shih, Po-Heng Chen, Yen-Ting Lin, Yun-Nung Chen", "Date published": "06/03/2024", "URL": "http://arxiv.org/abs/2406.00936v1", "summary": "LLMs have gotten attention across various research domains due to their\nexceptional performance on a wide range of complex tasks. Therefore, refined\nmethods to evaluate the capabilities of LLMs are needed to determine the tasks\nand responsibility they should undertake. Our study mainly discussed how LLMs,\nas useful tools, should be effectively assessed. We proposed the two-stage\nframework: from ``core ability'' to ``agent'', clearly explaining how LLMs can\nbe applied based on their specific capabilities, along with the evaluation\nmethods in each stage. Core ability refers to the capabilities that LLMs need\nin order to generate high-quality natural language texts. After confirming LLMs\npossess core ability, they can solve real-world and complex tasks as agent. In\nthe \"core ability\" stage, we discussed the reasoning ability, societal impact,\nand domain knowledge of LLMs. In the ``agent'' stage, we demonstrated embodied\naction, planning, and tool learning of LLMs agent applications. Finally, we\nexamined the challenges currently confronting the evaluation methods for LLMs,\nas well as the directions for future development."}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "90bdc65e-3c51-4860-9904-ba44dedf5c3d", "node_type": "4", "metadata": {"page_label": "3", "file_name": "2406_00936v1.pdf", "Title of this paper": "A Survey of Useful LLM Evaluation", "Authors": "Ji-Lun Peng, Sijia Cheng, Egil Diau, Yung-Yu Shih, Po-Heng Chen, Yen-Ting Lin, Yun-Nung Chen", "Date published": "06/03/2024", "URL": "http://arxiv.org/abs/2406.00936v1", "summary": "LLMs have gotten attention across various research domains due to their\nexceptional performance on a wide range of complex tasks. Therefore, refined\nmethods to evaluate the capabilities of LLMs are needed to determine the tasks\nand responsibility they should undertake. Our study mainly discussed how LLMs,\nas useful tools, should be effectively assessed. We proposed the two-stage\nframework: from ``core ability'' to ``agent'', clearly explaining how LLMs can\nbe applied based on their specific capabilities, along with the evaluation\nmethods in each stage. Core ability refers to the capabilities that LLMs need\nin order to generate high-quality natural language texts. After confirming LLMs\npossess core ability, they can solve real-world and complex tasks as agent. In\nthe \"core ability\" stage, we discussed the reasoning ability, societal impact,\nand domain knowledge of LLMs. In the ``agent'' stage, we demonstrated embodied\naction, planning, and tool learning of LLMs agent applications. Finally, we\nexamined the challenges currently confronting the evaluation methods for LLMs,\nas well as the directions for future development."}, "hash": "f0fd69f0723e3eb7326c930f2d3e945c0f919b1ea51d7a04b3c53cd730aacce6", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e77797fa-eb70-4b54-807d-db1fdec77128", "node_type": "1", "metadata": {"page_label": "3", "file_name": "2406_00936v1.pdf", "Title of this paper": "A Survey of Useful LLM Evaluation", "Authors": "Ji-Lun Peng, Sijia Cheng, Egil Diau, Yung-Yu Shih, Po-Heng Chen, Yen-Ting Lin, Yun-Nung Chen", "Date published": "06/03/2024", "URL": "http://arxiv.org/abs/2406.00936v1", "summary": "LLMs have gotten attention across various research domains due to their\nexceptional performance on a wide range of complex tasks. Therefore, refined\nmethods to evaluate the capabilities of LLMs are needed to determine the tasks\nand responsibility they should undertake. Our study mainly discussed how LLMs,\nas useful tools, should be effectively assessed. We proposed the two-stage\nframework: from ``core ability'' to ``agent'', clearly explaining how LLMs can\nbe applied based on their specific capabilities, along with the evaluation\nmethods in each stage. Core ability refers to the capabilities that LLMs need\nin order to generate high-quality natural language texts. After confirming LLMs\npossess core ability, they can solve real-world and complex tasks as agent. In\nthe \"core ability\" stage, we discussed the reasoning ability, societal impact,\nand domain knowledge of LLMs. In the ``agent'' stage, we demonstrated embodied\naction, planning, and tool learning of LLMs agent applications. Finally, we\nexamined the challenges currently confronting the evaluation methods for LLMs,\nas well as the directions for future development."}, "hash": "02f1274e47dbc9251bfd21c7a326aa3c0f00ac303cf9b8e4d599cd6804f00fd6", "class_name": "RelatedNodeInfo"}}, "text": "2.1.1 Logical Reasoning\nBased on concepts from philosophy and logic, log-\nical reasoning can further be divided to three dif-\nferent types: 1) Inductive reasoning involves in-\nferring general conclusions based on observed pat-\nterns or regularities in specific instances. bAbI-\n15 (Weston et al., 2015) and EntailmentBank (Dalvi\net al., 2021) are common benchmarks for inductive\nreasoing. 2) Deductive reasoning is the process\nof deriving necessary conclusions based on known\npremises and logical rules. bAbI-16 (Weston et al.,\n2015) is an common benchmark for testing deduc-\ntive reasoning. 3) abductive reasoning is a form\nof reasoning where possible explanations or hy-\npotheses are inferred based on given observations\nand known information. \u03b1-NLI, \u03b1-NLG (Bhaga-\nvatula et al., 2019) and AbductiveRules (Young\net al., 2022) are several benchmarks for abductive\nreasoning. Table 1 show several examples of each\ntype of logical reasoning task.\nXu et al. (2023a) is a comprehensive study on\nlogical reasoning in several LLMs including text-\ndavinci-003, ChatGPT and BARD. They found that\nBARD perform best generally among three models\nand ChatGPT performs worse in deductive and\ninductive settings. Besides, they also show that\nChatGPT falls short in generation tasks since it is\ntailored for chatting. Han et al. (2023) and Liu et al.\n(2023) include GPT-4 in their evaluation and found\nthat its performance qualitatively matches that of\nhumans in some scenarios.\n2.1.2 Mathematical Reasoning\nMathematical reasoning necessitates models to\ngrasp and manipulate mathematical concepts across\ndiverse scenarios. For example, the problem\nmay request model to perform arithmetic opera-\ntions and manipulating abstract symbols to attain\nan accurate numerical outcome. Notable exam-\nples include GSM8K (Cobbe et al., 2021) and\nMATH (Hendrycks et al., 2021).\nStolfo et al. (2023) found that instruction-tuned\nLLM have a remarkable improvement in both sen-\nsitivity and robustness on mathematical problem\ncompared to non-instruction-tuned models. Yuan\net al. (2023) compare the arithemtic capability of\n13 models on each operation types and found that\n3", "mimetype": "text/plain", "start_char_idx": 2302, "end_char_idx": 4446, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c78be11e-f963-4dcd-b6e9-62453327ab2f": {"__data__": {"id_": "c78be11e-f963-4dcd-b6e9-62453327ab2f", "embedding": null, "metadata": {"page_label": "4", "file_name": "2406_00936v1.pdf", "Title of this paper": "A Survey of Useful LLM Evaluation", "Authors": "Ji-Lun Peng, Sijia Cheng, Egil Diau, Yung-Yu Shih, Po-Heng Chen, Yen-Ting Lin, Yun-Nung Chen", "Date published": "06/03/2024", "URL": "http://arxiv.org/abs/2406.00936v1", "summary": "LLMs have gotten attention across various research domains due to their\nexceptional performance on a wide range of complex tasks. Therefore, refined\nmethods to evaluate the capabilities of LLMs are needed to determine the tasks\nand responsibility they should undertake. Our study mainly discussed how LLMs,\nas useful tools, should be effectively assessed. We proposed the two-stage\nframework: from ``core ability'' to ``agent'', clearly explaining how LLMs can\nbe applied based on their specific capabilities, along with the evaluation\nmethods in each stage. Core ability refers to the capabilities that LLMs need\nin order to generate high-quality natural language texts. After confirming LLMs\npossess core ability, they can solve real-world and complex tasks as agent. In\nthe \"core ability\" stage, we discussed the reasoning ability, societal impact,\nand domain knowledge of LLMs. In the ``agent'' stage, we demonstrated embodied\naction, planning, and tool learning of LLMs agent applications. Finally, we\nexamined the challenges currently confronting the evaluation methods for LLMs,\nas well as the directions for future development."}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ef3b04be-211f-422b-8af0-ff52fe993fd8", "node_type": "4", "metadata": {"page_label": "4", "file_name": "2406_00936v1.pdf", "Title of this paper": "A Survey of Useful LLM Evaluation", "Authors": "Ji-Lun Peng, Sijia Cheng, Egil Diau, Yung-Yu Shih, Po-Heng Chen, Yen-Ting Lin, Yun-Nung Chen", "Date published": "06/03/2024", "URL": "http://arxiv.org/abs/2406.00936v1", "summary": "LLMs have gotten attention across various research domains due to their\nexceptional performance on a wide range of complex tasks. Therefore, refined\nmethods to evaluate the capabilities of LLMs are needed to determine the tasks\nand responsibility they should undertake. Our study mainly discussed how LLMs,\nas useful tools, should be effectively assessed. We proposed the two-stage\nframework: from ``core ability'' to ``agent'', clearly explaining how LLMs can\nbe applied based on their specific capabilities, along with the evaluation\nmethods in each stage. Core ability refers to the capabilities that LLMs need\nin order to generate high-quality natural language texts. After confirming LLMs\npossess core ability, they can solve real-world and complex tasks as agent. In\nthe \"core ability\" stage, we discussed the reasoning ability, societal impact,\nand domain knowledge of LLMs. In the ``agent'' stage, we demonstrated embodied\naction, planning, and tool learning of LLMs agent applications. Finally, we\nexamined the challenges currently confronting the evaluation methods for LLMs,\nas well as the directions for future development."}, "hash": "7e9ab8e12e58db329e26c6f0bdb4e32c3ce9a0dcac5124909ded26e1e6309982", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "990643d2-e51f-4773-89d7-6460837d2efe", "node_type": "1", "metadata": {}, "hash": "f3b61e2494b48972a0763f341a5ec43eec40a32055f1bdbc4bb56ad4d82a325a", "class_name": "RelatedNodeInfo"}}, "text": "Core Ability Evaluation\n(Sec. 2)Reasoning\n(Sec. 2.1)Logical Reasoning Weston et al. (2015), Bhagavatula et al. (2019)\nMathematical Reasoning Cobbe et al. (2021), Hendrycks et al. (2021)\nCommonsense Reasoning Talmor et al. (2018), Mihaylov et al. (2018)\nMulti-hop Reasoning Geva et al. (2021), Yang et al. (2018)\nStructured Data Reasoning Chen et al. (2020), Zhang et al. (2018)\nSocietal Impact\n(Sec. 2.2)Safety Lin et al. (2023), Kim et al. (2024b), Yuan et al. (2024), Scherrer et al. (2023)\nTruthfulness Jiang et al. (2024), Zhang et al. (2024b), Hort et al. (2021), Zhang et al. (2023)\nDomain Knowledge\n(Sec. 2.3)Finance Wu et al. (2023), Xie et al. (2023), Li et al. (2023b)\nLegislation Blair-Stanek et al. (2023), Engel and Mcadams (2024), Liga and Robaldo (2023), Deroy et al. (2023)\nPsychology Lu et al. (2024), Demszky et al. (2023), Demszky et al. (2023)\nMedicine Agrawal et al. (2022), Sharma and Thakur (2023), Benoit (2023), Kumar (2023), Thirunavukarasu et al. (2023)\nEducation Abdelghani et al. (2023), Jia et al. (2021), Menick et al. (2022), Dijkstra et al. (2022), Kasneci et al. (2023)\nFigure 2: The overview of core ability evaluation.\nType Example Source Input answer\nInductive\nReasoningbAbI-15 (Weston et al., 2015) Sheep are afraid of wolves. Cats are afraid of dogs. Mice\nare afraid of cats. Gertrude is a sheep. What is Gertrude\nafraid of?wolves\nDeductive\nReasoningbAbI-16 (Weston et al., 2015) Lily is a swan. Lily is white. Bernhard is green. Greg is a\nswan. What color is Greg?white\nAbductive\nReasoning\u03b1-NLI (Bhagavatula et al., 2019) obs1: I walked into my math class. obs2: I ended up failing.\nhyp1: I saw the string by the door. hyp2: I didn\u2019t study for\nthe test.hyp2\nTable 1: Examples for different types of logical reasoning.\nGPT-4 is the only model that have excellent perfor-\nmance in every of them.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1833, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "990643d2-e51f-4773-89d7-6460837d2efe": {"__data__": {"id_": "990643d2-e51f-4773-89d7-6460837d2efe", "embedding": null, "metadata": {"page_label": "4", "file_name": "2406_00936v1.pdf", "Title of this paper": "A Survey of Useful LLM Evaluation", "Authors": "Ji-Lun Peng, Sijia Cheng, Egil Diau, Yung-Yu Shih, Po-Heng Chen, Yen-Ting Lin, Yun-Nung Chen", "Date published": "06/03/2024", "URL": "http://arxiv.org/abs/2406.00936v1", "summary": "LLMs have gotten attention across various research domains due to their\nexceptional performance on a wide range of complex tasks. Therefore, refined\nmethods to evaluate the capabilities of LLMs are needed to determine the tasks\nand responsibility they should undertake. Our study mainly discussed how LLMs,\nas useful tools, should be effectively assessed. We proposed the two-stage\nframework: from ``core ability'' to ``agent'', clearly explaining how LLMs can\nbe applied based on their specific capabilities, along with the evaluation\nmethods in each stage. Core ability refers to the capabilities that LLMs need\nin order to generate high-quality natural language texts. After confirming LLMs\npossess core ability, they can solve real-world and complex tasks as agent. In\nthe \"core ability\" stage, we discussed the reasoning ability, societal impact,\nand domain knowledge of LLMs. In the ``agent'' stage, we demonstrated embodied\naction, planning, and tool learning of LLMs agent applications. Finally, we\nexamined the challenges currently confronting the evaluation methods for LLMs,\nas well as the directions for future development."}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ef3b04be-211f-422b-8af0-ff52fe993fd8", "node_type": "4", "metadata": {"page_label": "4", "file_name": "2406_00936v1.pdf", "Title of this paper": "A Survey of Useful LLM Evaluation", "Authors": "Ji-Lun Peng, Sijia Cheng, Egil Diau, Yung-Yu Shih, Po-Heng Chen, Yen-Ting Lin, Yun-Nung Chen", "Date published": "06/03/2024", "URL": "http://arxiv.org/abs/2406.00936v1", "summary": "LLMs have gotten attention across various research domains due to their\nexceptional performance on a wide range of complex tasks. Therefore, refined\nmethods to evaluate the capabilities of LLMs are needed to determine the tasks\nand responsibility they should undertake. Our study mainly discussed how LLMs,\nas useful tools, should be effectively assessed. We proposed the two-stage\nframework: from ``core ability'' to ``agent'', clearly explaining how LLMs can\nbe applied based on their specific capabilities, along with the evaluation\nmethods in each stage. Core ability refers to the capabilities that LLMs need\nin order to generate high-quality natural language texts. After confirming LLMs\npossess core ability, they can solve real-world and complex tasks as agent. In\nthe \"core ability\" stage, we discussed the reasoning ability, societal impact,\nand domain knowledge of LLMs. In the ``agent'' stage, we demonstrated embodied\naction, planning, and tool learning of LLMs agent applications. Finally, we\nexamined the challenges currently confronting the evaluation methods for LLMs,\nas well as the directions for future development."}, "hash": "7e9ab8e12e58db329e26c6f0bdb4e32c3ce9a0dcac5124909ded26e1e6309982", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c78be11e-f963-4dcd-b6e9-62453327ab2f", "node_type": "1", "metadata": {"page_label": "4", "file_name": "2406_00936v1.pdf", "Title of this paper": "A Survey of Useful LLM Evaluation", "Authors": "Ji-Lun Peng, Sijia Cheng, Egil Diau, Yung-Yu Shih, Po-Heng Chen, Yen-Ting Lin, Yun-Nung Chen", "Date published": "06/03/2024", "URL": "http://arxiv.org/abs/2406.00936v1", "summary": "LLMs have gotten attention across various research domains due to their\nexceptional performance on a wide range of complex tasks. Therefore, refined\nmethods to evaluate the capabilities of LLMs are needed to determine the tasks\nand responsibility they should undertake. Our study mainly discussed how LLMs,\nas useful tools, should be effectively assessed. We proposed the two-stage\nframework: from ``core ability'' to ``agent'', clearly explaining how LLMs can\nbe applied based on their specific capabilities, along with the evaluation\nmethods in each stage. Core ability refers to the capabilities that LLMs need\nin order to generate high-quality natural language texts. After confirming LLMs\npossess core ability, they can solve real-world and complex tasks as agent. In\nthe \"core ability\" stage, we discussed the reasoning ability, societal impact,\nand domain knowledge of LLMs. In the ``agent'' stage, we demonstrated embodied\naction, planning, and tool learning of LLMs agent applications. Finally, we\nexamined the challenges currently confronting the evaluation methods for LLMs,\nas well as the directions for future development."}, "hash": "67749fa6d9edaa0cfa231d9788a18e33deaaf4482f9e0f83bf00a2f7e50c9994", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "098bc478-c3ba-4f37-a3cb-41e532ba7bbc", "node_type": "1", "metadata": {}, "hash": "85cbcf672313692078b4a6c3a22e4ace1a28435ad0e8f5b9f945904ca472bed0", "class_name": "RelatedNodeInfo"}}, "text": "Cats are afraid of dogs. Mice\nare afraid of cats. Gertrude is a sheep. What is Gertrude\nafraid of?wolves\nDeductive\nReasoningbAbI-16 (Weston et al., 2015) Lily is a swan. Lily is white. Bernhard is green. Greg is a\nswan. What color is Greg?white\nAbductive\nReasoning\u03b1-NLI (Bhagavatula et al., 2019) obs1: I walked into my math class. obs2: I ended up failing.\nhyp1: I saw the string by the door. hyp2: I didn\u2019t study for\nthe test.hyp2\nTable 1: Examples for different types of logical reasoning.\nGPT-4 is the only model that have excellent perfor-\nmance in every of them.\n2.1.3 Commonsense Reasoning\nCommonsense reasoning entails the capacity to\ngrasp and apply fundamental knowledge about the\nworld. It\u2019s essential for machines to reach a level\nof comprehension and interaction comparable to\nhuman cognition. Moreover, commonsense cogni-\ntion is pivotal in various reasoning processes such\nas causal detection, spatial and temporal under-\nstanding, among others. Typically, commonsense\nreasoning tasks are structured as multiple-choice\nor true/false problem, which contain questions that\nrequire model to apply commonsense knowledge\nto answer. For instance, the problem may ask\n\"Where do you put your grapes just before check-\ning out?\", and the model should select the correct\nanswer, which is \"grocery cart.\" The Common-\nsenseQA (Talmor et al., 2018) consist questions\nwith complex semantics that require prior knowl-\nedge to answer. Similarly, OpenBookQA (Mi-\nhaylov et al., 2018) contains elementary-level ques-\ntions designed to assess understanding of basic\nscientific facts and their application in novel sce-narios.\nBang et al. (2023) shows that ChatGPT has com-\nmonsense reasoning capability over several com-\nmonsence benchmark over general knowledge (Tal-\nmor et al., 2018) and physical concepts (Bisk et al.,\n2020; Wang et al., 2018). Bian et al. (2024) shows\nthat instruction tuning models have superior perfor-\nmance on several commonsense QA dataset includ-\ning CommonsenseQA (Talmor et al., 2018) and\nOpenBookQA (Mihaylov et al., 2018), which illus-\ntrates that commonsense ability can be improved\nby with human alignment.\n2.1.4 Multi-hop Reasoning\nThe multi-hop reasoning tasks necessitate models\nto engage in sequential reasoning steps to derive\nanswers. It serves as a prominent assessment for\nLLMs, evaluating their capability to analyze ques-\ntions and solve them through a step-by-step de-\ncomposition process akin to human-level ability.\nThe process can be viewed as an amalgamation of\ndiverse reasoning ability, as each step may necessi-\ntate the application of one or more of the reasoning\ntasks discussed earlier.", "mimetype": "text/plain", "start_char_idx": 1265, "end_char_idx": 3902, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "098bc478-c3ba-4f37-a3cb-41e532ba7bbc": {"__data__": {"id_": "098bc478-c3ba-4f37-a3cb-41e532ba7bbc", "embedding": null, "metadata": {"page_label": "4", "file_name": "2406_00936v1.pdf", "Title of this paper": "A Survey of Useful LLM Evaluation", "Authors": "Ji-Lun Peng, Sijia Cheng, Egil Diau, Yung-Yu Shih, Po-Heng Chen, Yen-Ting Lin, Yun-Nung Chen", "Date published": "06/03/2024", "URL": "http://arxiv.org/abs/2406.00936v1", "summary": "LLMs have gotten attention across various research domains due to their\nexceptional performance on a wide range of complex tasks. Therefore, refined\nmethods to evaluate the capabilities of LLMs are needed to determine the tasks\nand responsibility they should undertake. Our study mainly discussed how LLMs,\nas useful tools, should be effectively assessed. We proposed the two-stage\nframework: from ``core ability'' to ``agent'', clearly explaining how LLMs can\nbe applied based on their specific capabilities, along with the evaluation\nmethods in each stage. Core ability refers to the capabilities that LLMs need\nin order to generate high-quality natural language texts. After confirming LLMs\npossess core ability, they can solve real-world and complex tasks as agent. In\nthe \"core ability\" stage, we discussed the reasoning ability, societal impact,\nand domain knowledge of LLMs. In the ``agent'' stage, we demonstrated embodied\naction, planning, and tool learning of LLMs agent applications. Finally, we\nexamined the challenges currently confronting the evaluation methods for LLMs,\nas well as the directions for future development."}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ef3b04be-211f-422b-8af0-ff52fe993fd8", "node_type": "4", "metadata": {"page_label": "4", "file_name": "2406_00936v1.pdf", "Title of this paper": "A Survey of Useful LLM Evaluation", "Authors": "Ji-Lun Peng, Sijia Cheng, Egil Diau, Yung-Yu Shih, Po-Heng Chen, Yen-Ting Lin, Yun-Nung Chen", "Date published": "06/03/2024", "URL": "http://arxiv.org/abs/2406.00936v1", "summary": "LLMs have gotten attention across various research domains due to their\nexceptional performance on a wide range of complex tasks. Therefore, refined\nmethods to evaluate the capabilities of LLMs are needed to determine the tasks\nand responsibility they should undertake. Our study mainly discussed how LLMs,\nas useful tools, should be effectively assessed. We proposed the two-stage\nframework: from ``core ability'' to ``agent'', clearly explaining how LLMs can\nbe applied based on their specific capabilities, along with the evaluation\nmethods in each stage. Core ability refers to the capabilities that LLMs need\nin order to generate high-quality natural language texts. After confirming LLMs\npossess core ability, they can solve real-world and complex tasks as agent. In\nthe \"core ability\" stage, we discussed the reasoning ability, societal impact,\nand domain knowledge of LLMs. In the ``agent'' stage, we demonstrated embodied\naction, planning, and tool learning of LLMs agent applications. Finally, we\nexamined the challenges currently confronting the evaluation methods for LLMs,\nas well as the directions for future development."}, "hash": "7e9ab8e12e58db329e26c6f0bdb4e32c3ce9a0dcac5124909ded26e1e6309982", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "990643d2-e51f-4773-89d7-6460837d2efe", "node_type": "1", "metadata": {"page_label": "4", "file_name": "2406_00936v1.pdf", "Title of this paper": "A Survey of Useful LLM Evaluation", "Authors": "Ji-Lun Peng, Sijia Cheng, Egil Diau, Yung-Yu Shih, Po-Heng Chen, Yen-Ting Lin, Yun-Nung Chen", "Date published": "06/03/2024", "URL": "http://arxiv.org/abs/2406.00936v1", "summary": "LLMs have gotten attention across various research domains due to their\nexceptional performance on a wide range of complex tasks. Therefore, refined\nmethods to evaluate the capabilities of LLMs are needed to determine the tasks\nand responsibility they should undertake. Our study mainly discussed how LLMs,\nas useful tools, should be effectively assessed. We proposed the two-stage\nframework: from ``core ability'' to ``agent'', clearly explaining how LLMs can\nbe applied based on their specific capabilities, along with the evaluation\nmethods in each stage. Core ability refers to the capabilities that LLMs need\nin order to generate high-quality natural language texts. After confirming LLMs\npossess core ability, they can solve real-world and complex tasks as agent. In\nthe \"core ability\" stage, we discussed the reasoning ability, societal impact,\nand domain knowledge of LLMs. In the ``agent'' stage, we demonstrated embodied\naction, planning, and tool learning of LLMs agent applications. Finally, we\nexamined the challenges currently confronting the evaluation methods for LLMs,\nas well as the directions for future development."}, "hash": "430d966d8caae0a1b8099d082234faf3c8cb39f63d996bd27b4ed1eb8c6f3b9d", "class_name": "RelatedNodeInfo"}}, "text": "Bian et al. (2024) shows\nthat instruction tuning models have superior perfor-\nmance on several commonsense QA dataset includ-\ning CommonsenseQA (Talmor et al., 2018) and\nOpenBookQA (Mihaylov et al., 2018), which illus-\ntrates that commonsense ability can be improved\nby with human alignment.\n2.1.4 Multi-hop Reasoning\nThe multi-hop reasoning tasks necessitate models\nto engage in sequential reasoning steps to derive\nanswers. It serves as a prominent assessment for\nLLMs, evaluating their capability to analyze ques-\ntions and solve them through a step-by-step de-\ncomposition process akin to human-level ability.\nThe process can be viewed as an amalgamation of\ndiverse reasoning ability, as each step may necessi-\ntate the application of one or more of the reasoning\ntasks discussed earlier. For instance, the question\nmight be, \u2019Was the director of \u2019Interstellar\u2019 born\n4", "mimetype": "text/plain", "start_char_idx": 3110, "end_char_idx": 3982, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "388ad8ee-add3-43f4-b285-5761813e73e4": {"__data__": {"id_": "388ad8ee-add3-43f4-b285-5761813e73e4", "embedding": null, "metadata": {"page_label": "5", "file_name": "2406_00936v1.pdf", "Title of this paper": "A Survey of Useful LLM Evaluation", "Authors": "Ji-Lun Peng, Sijia Cheng, Egil Diau, Yung-Yu Shih, Po-Heng Chen, Yen-Ting Lin, Yun-Nung Chen", "Date published": "06/03/2024", "URL": "http://arxiv.org/abs/2406.00936v1", "summary": "LLMs have gotten attention across various research domains due to their\nexceptional performance on a wide range of complex tasks. Therefore, refined\nmethods to evaluate the capabilities of LLMs are needed to determine the tasks\nand responsibility they should undertake. Our study mainly discussed how LLMs,\nas useful tools, should be effectively assessed. We proposed the two-stage\nframework: from ``core ability'' to ``agent'', clearly explaining how LLMs can\nbe applied based on their specific capabilities, along with the evaluation\nmethods in each stage. Core ability refers to the capabilities that LLMs need\nin order to generate high-quality natural language texts. After confirming LLMs\npossess core ability, they can solve real-world and complex tasks as agent. In\nthe \"core ability\" stage, we discussed the reasoning ability, societal impact,\nand domain knowledge of LLMs. In the ``agent'' stage, we demonstrated embodied\naction, planning, and tool learning of LLMs agent applications. Finally, we\nexamined the challenges currently confronting the evaluation methods for LLMs,\nas well as the directions for future development."}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "020929b8-9749-415c-93be-f98fdd7e0106", "node_type": "4", "metadata": {"page_label": "5", "file_name": "2406_00936v1.pdf", "Title of this paper": "A Survey of Useful LLM Evaluation", "Authors": "Ji-Lun Peng, Sijia Cheng, Egil Diau, Yung-Yu Shih, Po-Heng Chen, Yen-Ting Lin, Yun-Nung Chen", "Date published": "06/03/2024", "URL": "http://arxiv.org/abs/2406.00936v1", "summary": "LLMs have gotten attention across various research domains due to their\nexceptional performance on a wide range of complex tasks. Therefore, refined\nmethods to evaluate the capabilities of LLMs are needed to determine the tasks\nand responsibility they should undertake. Our study mainly discussed how LLMs,\nas useful tools, should be effectively assessed. We proposed the two-stage\nframework: from ``core ability'' to ``agent'', clearly explaining how LLMs can\nbe applied based on their specific capabilities, along with the evaluation\nmethods in each stage. Core ability refers to the capabilities that LLMs need\nin order to generate high-quality natural language texts. After confirming LLMs\npossess core ability, they can solve real-world and complex tasks as agent. In\nthe \"core ability\" stage, we discussed the reasoning ability, societal impact,\nand domain knowledge of LLMs. In the ``agent'' stage, we demonstrated embodied\naction, planning, and tool learning of LLMs agent applications. Finally, we\nexamined the challenges currently confronting the evaluation methods for LLMs,\nas well as the directions for future development."}, "hash": "d9f06365b32f65cd2ee6ed5bb8596c036e598840df30705061499b6f40225c56", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "cbb04b97-c8c3-48c6-9a6b-d237d998829e", "node_type": "1", "metadata": {}, "hash": "836af057d8d6dbe4aacad3a04b89b8b9eb83de4cd667fb77ba52abdd81da1377", "class_name": "RelatedNodeInfo"}}, "text": "in Paris?\u2019 In this case, the models must first iden-\ntify the director of the movie and then ascertain\ntheir birthplace. StrategyQA (Geva et al., 2021)\nrequires models to generate several implicit rea-\nsoning steps to devise a strategy leading to a final\ndecision for the question. HotpotQA (Yang et al.,\n2018) is requires finding and reasoning over multi-\nple supporting documents to formulate responses.\nIts questions are diverse and not confined by any\npre-existing knowledge bases. HoVer (Jiang et al.,\n2020) requires models to gather facts from multiple\nWikipedia articles which are related to a claim and\ndetermine if these facts substantiate the claim.\nZheng et al. (2023b) discovered that ChatGPT\nfails to deliver reliable and accurate answers on\nHotpotQA. Their further analysis indicates that this\nfailure can stem from various factors, with factual\ncorrectness being the most critical. Addressing this\nissue, they underscore the significance of knowl-\nedge memorization and recall for LLMs.\n2.1.5 Structured Data Reasoning\nThe aforementioned reasoning tasks have primarily\nconcentrated on scenarios involving purely plain\ntext data. In contrast, structured data, characterized\nby specific formats like tables, knowledge graphs,\nand databases, presents greater challenges for ma-\nchine comprehension and reasoning. To perform\nstructured data reasoning, models must be able\nto understand the format of the data, analyze the\ninformation it contains, and generate answers to\nquestions related to that data.\nHybridQA (Chen et al., 2020) integrates ques-\ntions aligned with Wikipedia tables and multiple\nfree-form corpora linked with entities from the ta-\nble. The model is required to aggregate both tab-\nular and textual information to generate answers.\nMetaQA (Zhang et al., 2018) comprises question-\nanswer pairs within the movie domain and offers\na knowledge graph (KG) to facilitate information\nretrieval. The models are tasked with conducting\nmulti-hop reasoning on the KG and accommodat-\ning potential mismatches between KG entities and\nthe question in order to derive answers. Spider Re-\nalistic (Deng et al., 2020) presents a SQL-based\nQA dataset, necessitating models to engage in text-\nto-SQL generation. Specifically, models must ac-\ncurately identify textual references to columns and\nvalues and map them to the provided database\nschema.\nGao et al., 2023 conducted a comprehensive in-\nvestigation into the text-to-SQL task across multi-ple LLMs, employing various prompt engineering\nmethods. Furthermore, they performed fine-tuning\nexperiments on open-source models. However,\ntheir findings revealed that even after fine-tuning,\nthe performance of these models still lags behind\nproprietary models with zero-shot evaluation.\n2.2 Societal Impact\nLLMs have become crucial elements in modern so-\nciety, significantly influencing various fields. With\ntheir remarkable abilities in text generation and\ncomprehension, LLMs are reshaping our interac-\ntions with information. Therefore, it is essential to\nunderstand the implications of LLMs.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3053, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "cbb04b97-c8c3-48c6-9a6b-d237d998829e": {"__data__": {"id_": "cbb04b97-c8c3-48c6-9a6b-d237d998829e", "embedding": null, "metadata": {"page_label": "5", "file_name": "2406_00936v1.pdf", "Title of this paper": "A Survey of Useful LLM Evaluation", "Authors": "Ji-Lun Peng, Sijia Cheng, Egil Diau, Yung-Yu Shih, Po-Heng Chen, Yen-Ting Lin, Yun-Nung Chen", "Date published": "06/03/2024", "URL": "http://arxiv.org/abs/2406.00936v1", "summary": "LLMs have gotten attention across various research domains due to their\nexceptional performance on a wide range of complex tasks. Therefore, refined\nmethods to evaluate the capabilities of LLMs are needed to determine the tasks\nand responsibility they should undertake. Our study mainly discussed how LLMs,\nas useful tools, should be effectively assessed. We proposed the two-stage\nframework: from ``core ability'' to ``agent'', clearly explaining how LLMs can\nbe applied based on their specific capabilities, along with the evaluation\nmethods in each stage. Core ability refers to the capabilities that LLMs need\nin order to generate high-quality natural language texts. After confirming LLMs\npossess core ability, they can solve real-world and complex tasks as agent. In\nthe \"core ability\" stage, we discussed the reasoning ability, societal impact,\nand domain knowledge of LLMs. In the ``agent'' stage, we demonstrated embodied\naction, planning, and tool learning of LLMs agent applications. Finally, we\nexamined the challenges currently confronting the evaluation methods for LLMs,\nas well as the directions for future development."}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "020929b8-9749-415c-93be-f98fdd7e0106", "node_type": "4", "metadata": {"page_label": "5", "file_name": "2406_00936v1.pdf", "Title of this paper": "A Survey of Useful LLM Evaluation", "Authors": "Ji-Lun Peng, Sijia Cheng, Egil Diau, Yung-Yu Shih, Po-Heng Chen, Yen-Ting Lin, Yun-Nung Chen", "Date published": "06/03/2024", "URL": "http://arxiv.org/abs/2406.00936v1", "summary": "LLMs have gotten attention across various research domains due to their\nexceptional performance on a wide range of complex tasks. Therefore, refined\nmethods to evaluate the capabilities of LLMs are needed to determine the tasks\nand responsibility they should undertake. Our study mainly discussed how LLMs,\nas useful tools, should be effectively assessed. We proposed the two-stage\nframework: from ``core ability'' to ``agent'', clearly explaining how LLMs can\nbe applied based on their specific capabilities, along with the evaluation\nmethods in each stage. Core ability refers to the capabilities that LLMs need\nin order to generate high-quality natural language texts. After confirming LLMs\npossess core ability, they can solve real-world and complex tasks as agent. In\nthe \"core ability\" stage, we discussed the reasoning ability, societal impact,\nand domain knowledge of LLMs. In the ``agent'' stage, we demonstrated embodied\naction, planning, and tool learning of LLMs agent applications. Finally, we\nexamined the challenges currently confronting the evaluation methods for LLMs,\nas well as the directions for future development."}, "hash": "d9f06365b32f65cd2ee6ed5bb8596c036e598840df30705061499b6f40225c56", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "388ad8ee-add3-43f4-b285-5761813e73e4", "node_type": "1", "metadata": {"page_label": "5", "file_name": "2406_00936v1.pdf", "Title of this paper": "A Survey of Useful LLM Evaluation", "Authors": "Ji-Lun Peng, Sijia Cheng, Egil Diau, Yung-Yu Shih, Po-Heng Chen, Yen-Ting Lin, Yun-Nung Chen", "Date published": "06/03/2024", "URL": "http://arxiv.org/abs/2406.00936v1", "summary": "LLMs have gotten attention across various research domains due to their\nexceptional performance on a wide range of complex tasks. Therefore, refined\nmethods to evaluate the capabilities of LLMs are needed to determine the tasks\nand responsibility they should undertake. Our study mainly discussed how LLMs,\nas useful tools, should be effectively assessed. We proposed the two-stage\nframework: from ``core ability'' to ``agent'', clearly explaining how LLMs can\nbe applied based on their specific capabilities, along with the evaluation\nmethods in each stage. Core ability refers to the capabilities that LLMs need\nin order to generate high-quality natural language texts. After confirming LLMs\npossess core ability, they can solve real-world and complex tasks as agent. In\nthe \"core ability\" stage, we discussed the reasoning ability, societal impact,\nand domain knowledge of LLMs. In the ``agent'' stage, we demonstrated embodied\naction, planning, and tool learning of LLMs agent applications. Finally, we\nexamined the challenges currently confronting the evaluation methods for LLMs,\nas well as the directions for future development."}, "hash": "fbe371ab6c2e9e016bf64edc6e3f6d9331708c2d6cc7f6322f625731ccf0ceea", "class_name": "RelatedNodeInfo"}}, "text": "Specifically, models must ac-\ncurately identify textual references to columns and\nvalues and map them to the provided database\nschema.\nGao et al., 2023 conducted a comprehensive in-\nvestigation into the text-to-SQL task across multi-ple LLMs, employing various prompt engineering\nmethods. Furthermore, they performed fine-tuning\nexperiments on open-source models. However,\ntheir findings revealed that even after fine-tuning,\nthe performance of these models still lags behind\nproprietary models with zero-shot evaluation.\n2.2 Societal Impact\nLLMs have become crucial elements in modern so-\nciety, significantly influencing various fields. With\ntheir remarkable abilities in text generation and\ncomprehension, LLMs are reshaping our interac-\ntions with information. Therefore, it is essential to\nunderstand the implications of LLMs. By explor-\ning these dimensions, we aim to comprehend the\nbroader societal impacts of LLMs. Our goal is to\nsimplify complex concepts into accessible insights,\nimproving our ability to evaluate LLMs effectively.\nThis discussion explores the societal impacts of\nLLMs, focusing on two critical aspects: Safety and\nTrustworthiness. Through exploring these dimen-\nsions, we aim to understand the broader societal\nimplications of LLMs.\n2.2.1 Safety\nIn this section, we explore essential safety mech-\nanisms required to protect users when interacting\nwith LLMs. Ensuring that these models generate\nonly safe content is crucial, Oviedo-Trespalacios\net al. (2023) found that ChatGPT sometimes made\nincorrect or harmful statements, emphasizing the\nneed for expert verification. We address safety con-\ncerns by categorizing them into three main areas:\nThis section explores essential concerns related to\nthe safety of LLMs, including Content Safety, Se-\ncurity, and Ethical Consideration .\nContent Safety As LLMs and generative AI be-\ncome more prevalent, the associated content safety\nrisks also escalate. Benchmarks offer critical in-\nsights into these risks. ToxicChat (Lin et al., 2023),\nbased on real user queries from an open-source\nchatbot, emphasizes the unique challenges of de-\ntecting toxicity in user-AI conversations. The Open\nAI Moderation Dataset (Markov et al., 2023) pro-\nvides a comprehensive approach to identifying un-\ndesired content in real-world applications.\nAEGISSAFETYDATASET (Ghosh et al., 2024),\nwith around 26,000 human-LLM interaction in-\nstances annotated by humans, deepens the under-\nstanding of content safety issues. The AI Safety\nBenchmark v0.5 (Vidgen et al., 2024), created by\n5", "mimetype": "text/plain", "start_char_idx": 2222, "end_char_idx": 4759, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b0adc3f3-eb87-4348-9280-92d4b18c017c": {"__data__": {"id_": "b0adc3f3-eb87-4348-9280-92d4b18c017c", "embedding": null, "metadata": {"page_label": "6", "file_name": "2406_00936v1.pdf", "Title of this paper": "A Survey of Useful LLM Evaluation", "Authors": "Ji-Lun Peng, Sijia Cheng, Egil Diau, Yung-Yu Shih, Po-Heng Chen, Yen-Ting Lin, Yun-Nung Chen", "Date published": "06/03/2024", "URL": "http://arxiv.org/abs/2406.00936v1", "summary": "LLMs have gotten attention across various research domains due to their\nexceptional performance on a wide range of complex tasks. Therefore, refined\nmethods to evaluate the capabilities of LLMs are needed to determine the tasks\nand responsibility they should undertake. Our study mainly discussed how LLMs,\nas useful tools, should be effectively assessed. We proposed the two-stage\nframework: from ``core ability'' to ``agent'', clearly explaining how LLMs can\nbe applied based on their specific capabilities, along with the evaluation\nmethods in each stage. Core ability refers to the capabilities that LLMs need\nin order to generate high-quality natural language texts. After confirming LLMs\npossess core ability, they can solve real-world and complex tasks as agent. In\nthe \"core ability\" stage, we discussed the reasoning ability, societal impact,\nand domain knowledge of LLMs. In the ``agent'' stage, we demonstrated embodied\naction, planning, and tool learning of LLMs agent applications. Finally, we\nexamined the challenges currently confronting the evaluation methods for LLMs,\nas well as the directions for future development."}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "140c55bd-2358-4977-8253-be2cd1bd7bd1", "node_type": "4", "metadata": {"page_label": "6", "file_name": "2406_00936v1.pdf", "Title of this paper": "A Survey of Useful LLM Evaluation", "Authors": "Ji-Lun Peng, Sijia Cheng, Egil Diau, Yung-Yu Shih, Po-Heng Chen, Yen-Ting Lin, Yun-Nung Chen", "Date published": "06/03/2024", "URL": "http://arxiv.org/abs/2406.00936v1", "summary": "LLMs have gotten attention across various research domains due to their\nexceptional performance on a wide range of complex tasks. Therefore, refined\nmethods to evaluate the capabilities of LLMs are needed to determine the tasks\nand responsibility they should undertake. Our study mainly discussed how LLMs,\nas useful tools, should be effectively assessed. We proposed the two-stage\nframework: from ``core ability'' to ``agent'', clearly explaining how LLMs can\nbe applied based on their specific capabilities, along with the evaluation\nmethods in each stage. Core ability refers to the capabilities that LLMs need\nin order to generate high-quality natural language texts. After confirming LLMs\npossess core ability, they can solve real-world and complex tasks as agent. In\nthe \"core ability\" stage, we discussed the reasoning ability, societal impact,\nand domain knowledge of LLMs. In the ``agent'' stage, we demonstrated embodied\naction, planning, and tool learning of LLMs agent applications. Finally, we\nexamined the challenges currently confronting the evaluation methods for LLMs,\nas well as the directions for future development."}, "hash": "4e1c81d64a2370ad7db91b2e5b96d144788bca8432cdb5c2c668ffb47ceec25e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f895595b-da20-4b6e-a13d-413989804509", "node_type": "1", "metadata": {}, "hash": "d5524509774ddf00fd08c1b89abb3498b4def1783b1828a4654f6008f91a3698", "class_name": "RelatedNodeInfo"}}, "text": "the MLCommons AI Safety Working Group, fo-\ncuses on evaluating LLM safety. SALAD-Bench\n(Li et al., 2024a), designed to estimate LLMs, in-\ncludes evaluations of attack and defense methods.\nSafetyBench (Scherrer et al., 2023), a comprehen-\nsive benchmark for evaluating the safety of LLMs,\nwhich comprises 11,435 diverse multiple-choice\nquestions spanning seven distinct categories of\nsafety concerns. CValues (Xu et al., 2023b), the\nfirst Chinese human values evaluation benchmark\nto measure the alignment ability of LLMs in terms\nof both safety and responsibility criteria. KCDD\n(Kim et al., 2024a) contains 22,249 dialogues gen-\nerated by crowd workers, designed to simulate of-\nfline scenarios. This dataset categorizes dialogues\ninto four criminal classes that align with interna-\ntional legal standards. BeaverTails (Ji et al., 2023)\nintroduces a novel \"QA moderation\" strategy to test\nmodels\u2019 safety alignment, offering a fresh perspec-\ntive distinct from conventional content moderation\napproaches.\nAdditionally, it is crucial to ensure that LLMs\ndo not produce adult content accessible by minors\n(Cifuentes et al., 2022; Karamizadeh et al., 2023),\nmitigate any harmful content that could affect chil-\ndren, guarantee that outputs do not encourage ille-\ngal activities (Nayerifard et al., 2023; Casino et al.,\n2022), and avoid the generation of content that\ncould incite violence. In this section, benchmarks\nand datasets play a vital role in evaluating the safety\nalignment of LLMs. By providing annotated data\nthat highlights harmful or inappropriate content,\nthese resources enable researchers to develop and\nrefine algorithms for content moderation and safety\nenforcement.\nSecurity This section reviews a collection of pa-\npers that focus on the dual aspects of enhancing\ndata privacy practices and strengthening the re-\nsilience of LLMs against adversarial threats. Staab\net al. (2023) discusses the ability of LLMs to infer\npersonal attributes such as location, income, and\ngender from seemingly innocuous text inputs, us-\ning a dataset derived from actual Reddit profiles to\ndemonstrate significant privacy risks. The discus-\nsion extends with Kim et al. (2024b) introducing\nProPILE, a probing tool that enables data subjects\nto detect potential PII leakage in services based on\nLLMs. Das et al. (2024) examines these vulnera-\nbilities in depth, highlighting the urgent need for\nimproved security protocols and the exploration of\neffective defenses, while Yan et al. (2024a) focuseson clarifying the data privacy concerns associated\nwith LLMs. Moreover, Carlini et al. (2023) and\nYao et al. (2024) emphasize the significant privacy\nrisks posed by LLMs, particularly through their\ntendency to memorize and reproduce parts of their\ntraining data verbatim.\nOn the resilience against adversarial attacks, Yip\net al.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2824, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f895595b-da20-4b6e-a13d-413989804509": {"__data__": {"id_": "f895595b-da20-4b6e-a13d-413989804509", "embedding": null, "metadata": {"page_label": "6", "file_name": "2406_00936v1.pdf", "Title of this paper": "A Survey of Useful LLM Evaluation", "Authors": "Ji-Lun Peng, Sijia Cheng, Egil Diau, Yung-Yu Shih, Po-Heng Chen, Yen-Ting Lin, Yun-Nung Chen", "Date published": "06/03/2024", "URL": "http://arxiv.org/abs/2406.00936v1", "summary": "LLMs have gotten attention across various research domains due to their\nexceptional performance on a wide range of complex tasks. Therefore, refined\nmethods to evaluate the capabilities of LLMs are needed to determine the tasks\nand responsibility they should undertake. Our study mainly discussed how LLMs,\nas useful tools, should be effectively assessed. We proposed the two-stage\nframework: from ``core ability'' to ``agent'', clearly explaining how LLMs can\nbe applied based on their specific capabilities, along with the evaluation\nmethods in each stage. Core ability refers to the capabilities that LLMs need\nin order to generate high-quality natural language texts. After confirming LLMs\npossess core ability, they can solve real-world and complex tasks as agent. In\nthe \"core ability\" stage, we discussed the reasoning ability, societal impact,\nand domain knowledge of LLMs. In the ``agent'' stage, we demonstrated embodied\naction, planning, and tool learning of LLMs agent applications. Finally, we\nexamined the challenges currently confronting the evaluation methods for LLMs,\nas well as the directions for future development."}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "140c55bd-2358-4977-8253-be2cd1bd7bd1", "node_type": "4", "metadata": {"page_label": "6", "file_name": "2406_00936v1.pdf", "Title of this paper": "A Survey of Useful LLM Evaluation", "Authors": "Ji-Lun Peng, Sijia Cheng, Egil Diau, Yung-Yu Shih, Po-Heng Chen, Yen-Ting Lin, Yun-Nung Chen", "Date published": "06/03/2024", "URL": "http://arxiv.org/abs/2406.00936v1", "summary": "LLMs have gotten attention across various research domains due to their\nexceptional performance on a wide range of complex tasks. Therefore, refined\nmethods to evaluate the capabilities of LLMs are needed to determine the tasks\nand responsibility they should undertake. Our study mainly discussed how LLMs,\nas useful tools, should be effectively assessed. We proposed the two-stage\nframework: from ``core ability'' to ``agent'', clearly explaining how LLMs can\nbe applied based on their specific capabilities, along with the evaluation\nmethods in each stage. Core ability refers to the capabilities that LLMs need\nin order to generate high-quality natural language texts. After confirming LLMs\npossess core ability, they can solve real-world and complex tasks as agent. In\nthe \"core ability\" stage, we discussed the reasoning ability, societal impact,\nand domain knowledge of LLMs. In the ``agent'' stage, we demonstrated embodied\naction, planning, and tool learning of LLMs agent applications. Finally, we\nexamined the challenges currently confronting the evaluation methods for LLMs,\nas well as the directions for future development."}, "hash": "4e1c81d64a2370ad7db91b2e5b96d144788bca8432cdb5c2c668ffb47ceec25e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b0adc3f3-eb87-4348-9280-92d4b18c017c", "node_type": "1", "metadata": {"page_label": "6", "file_name": "2406_00936v1.pdf", "Title of this paper": "A Survey of Useful LLM Evaluation", "Authors": "Ji-Lun Peng, Sijia Cheng, Egil Diau, Yung-Yu Shih, Po-Heng Chen, Yen-Ting Lin, Yun-Nung Chen", "Date published": "06/03/2024", "URL": "http://arxiv.org/abs/2406.00936v1", "summary": "LLMs have gotten attention across various research domains due to their\nexceptional performance on a wide range of complex tasks. Therefore, refined\nmethods to evaluate the capabilities of LLMs are needed to determine the tasks\nand responsibility they should undertake. Our study mainly discussed how LLMs,\nas useful tools, should be effectively assessed. We proposed the two-stage\nframework: from ``core ability'' to ``agent'', clearly explaining how LLMs can\nbe applied based on their specific capabilities, along with the evaluation\nmethods in each stage. Core ability refers to the capabilities that LLMs need\nin order to generate high-quality natural language texts. After confirming LLMs\npossess core ability, they can solve real-world and complex tasks as agent. In\nthe \"core ability\" stage, we discussed the reasoning ability, societal impact,\nand domain knowledge of LLMs. In the ``agent'' stage, we demonstrated embodied\naction, planning, and tool learning of LLMs agent applications. Finally, we\nexamined the challenges currently confronting the evaluation methods for LLMs,\nas well as the directions for future development."}, "hash": "cc8900eed240f542ba3b567a749ecbe59990aea35fc5213611d743f0e2726bb6", "class_name": "RelatedNodeInfo"}}, "text": "The discus-\nsion extends with Kim et al. (2024b) introducing\nProPILE, a probing tool that enables data subjects\nto detect potential PII leakage in services based on\nLLMs. Das et al. (2024) examines these vulnera-\nbilities in depth, highlighting the urgent need for\nimproved security protocols and the exploration of\neffective defenses, while Yan et al. (2024a) focuseson clarifying the data privacy concerns associated\nwith LLMs. Moreover, Carlini et al. (2023) and\nYao et al. (2024) emphasize the significant privacy\nrisks posed by LLMs, particularly through their\ntendency to memorize and reproduce parts of their\ntraining data verbatim.\nOn the resilience against adversarial attacks, Yip\net al. (2024) introduces a framework that quantifies\nthe resilience of applications against prompt inject\nattacks using innovative techniques for robust and\ninteroperable evaluations. Liu et al. (2024b); Jin\net al. (2024) both proposes for the use of gradient-\nbased method to enhance the evaluation of adver-\nsarial resilience in LLM. These methodologies em-\nphasize a critical shift towards more sophisticated\nand reliable assessments of adversarial threat land-\nscapes in LLMs. RigorLLM (Yuan et al., 2024),\na framework employing techniques like energy-\nbased data generation and minimax optimization\nto enhance the moderation of harmful content and\nimprove resilience against complex adversarial at-\ntacks. InjecAgent (Zhan et al., 2024), a benchmark\nspecifically designed to assess the vulnerability of\ntool-integrated LLM agents to indirect prompt in-\njection attacks, demonstrating significant suscepti-\nbility in commonly used LLM agents.\nEthical Consideration Ethical evaluation in sen-\nsitive areas such as medical ethics (Balas et al.,\n2024) and moral decision-making (Scherrer et al.,\n2023) is challenging. These studies are important\nfor enhancing the ethical functioning of LLMs, en-\nsuring their responsible application in real-world\nsettings. In the following sections, we will intro-\nduce some existing studies that address ethical con-\nsiderations in LLM evaluation and their approaches\nto improving ethical standards.\nSorensen et al. (2024) emphasizes value plu-\nralism in decision-making. They introduce Val-\nuePrism, a dataset of 218k values, rights, and duties\nconnected to 31k human-written situations, used\nto build Value Kaleidoscope (Kaleido), a model\nthat explains and assesses human values. Kaleido\u2019s\noutputs are preferred over GPT-4, showing more\naccuracy and broader coverage. Duan et al. (2024)\nexplores LLMs\u2019 ethical values using Moral Founda-\ntion Theory, proposing DeNEVIL, a prompt genera-\ntion algorithm that reveals value inclinations. They\ncreate MoralPrompt, a dataset of 2,397 prompts,\nand benchmark LLMs\u2019 intrinsic values, finding\nmany models misaligned. They develop VILMO,\nan alignment method enhancing value compliance,\n6", "mimetype": "text/plain", "start_char_idx": 2127, "end_char_idx": 4983, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "27030cf0-94e5-46bb-8fb0-67a42eaf9b60": {"__data__": {"id_": "27030cf0-94e5-46bb-8fb0-67a42eaf9b60", "embedding": null, "metadata": {"page_label": "7", "file_name": "2406_00936v1.pdf", "Title of this paper": "A Survey of Useful LLM Evaluation", "Authors": "Ji-Lun Peng, Sijia Cheng, Egil Diau, Yung-Yu Shih, Po-Heng Chen, Yen-Ting Lin, Yun-Nung Chen", "Date published": "06/03/2024", "URL": "http://arxiv.org/abs/2406.00936v1", "summary": "LLMs have gotten attention across various research domains due to their\nexceptional performance on a wide range of complex tasks. Therefore, refined\nmethods to evaluate the capabilities of LLMs are needed to determine the tasks\nand responsibility they should undertake. Our study mainly discussed how LLMs,\nas useful tools, should be effectively assessed. We proposed the two-stage\nframework: from ``core ability'' to ``agent'', clearly explaining how LLMs can\nbe applied based on their specific capabilities, along with the evaluation\nmethods in each stage. Core ability refers to the capabilities that LLMs need\nin order to generate high-quality natural language texts. After confirming LLMs\npossess core ability, they can solve real-world and complex tasks as agent. In\nthe \"core ability\" stage, we discussed the reasoning ability, societal impact,\nand domain knowledge of LLMs. In the ``agent'' stage, we demonstrated embodied\naction, planning, and tool learning of LLMs agent applications. Finally, we\nexamined the challenges currently confronting the evaluation methods for LLMs,\nas well as the directions for future development."}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ad91309c-b7e2-48ea-9862-10a6b3fecb85", "node_type": "4", "metadata": {"page_label": "7", "file_name": "2406_00936v1.pdf", "Title of this paper": "A Survey of Useful LLM Evaluation", "Authors": "Ji-Lun Peng, Sijia Cheng, Egil Diau, Yung-Yu Shih, Po-Heng Chen, Yen-Ting Lin, Yun-Nung Chen", "Date published": "06/03/2024", "URL": "http://arxiv.org/abs/2406.00936v1", "summary": "LLMs have gotten attention across various research domains due to their\nexceptional performance on a wide range of complex tasks. Therefore, refined\nmethods to evaluate the capabilities of LLMs are needed to determine the tasks\nand responsibility they should undertake. Our study mainly discussed how LLMs,\nas useful tools, should be effectively assessed. We proposed the two-stage\nframework: from ``core ability'' to ``agent'', clearly explaining how LLMs can\nbe applied based on their specific capabilities, along with the evaluation\nmethods in each stage. Core ability refers to the capabilities that LLMs need\nin order to generate high-quality natural language texts. After confirming LLMs\npossess core ability, they can solve real-world and complex tasks as agent. In\nthe \"core ability\" stage, we discussed the reasoning ability, societal impact,\nand domain knowledge of LLMs. In the ``agent'' stage, we demonstrated embodied\naction, planning, and tool learning of LLMs agent applications. Finally, we\nexamined the challenges currently confronting the evaluation methods for LLMs,\nas well as the directions for future development."}, "hash": "c82f626162188e8990006fc15de9140187e99ac8cf65328930a9589b9d38c998", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d3394261-fd6b-4ca7-958e-14236d6ee8ad", "node_type": "1", "metadata": {}, "hash": "c8ead675a8075d8faffc0305faaaef752e8edb0fdc5befbeb7d8c6aa7bfa4424", "class_name": "RelatedNodeInfo"}}, "text": "applicable to both black-box and open-source mod-\nels. In the MoralChoice datasets (Scherrer et al.,\n2023), one example of a moral dilemma involves a\nscenario where the respondent is a driver approach-\ning a pedestrian crossing the street. The possible\nresponses are: (1) Slowing down and waiting for\nthe pedestrian, which reflects a responsible and\nethical decision. (2) Accelerating and hitting the\npedestrian, which represents an irresponsible and\nunethical choice. This dilemma illustrates the types\nof decision-making scenarios used to evaluate ethi-\ncal behavior in realistic settings.\nScherrer et al. (2023) introduces a novel sta-\ntistical method to examine the moral beliefs of\nLLMs and quantifies how likely LLMs are to make\ndecisions in various moral scenarios, analyzing\ntheir responses across 680 high-ambiguity and 687\nlow-ambiguity dilemmas. The findings indicate\nthat LLMs generally align with common sense in\nstraightforward situations but exhibit notable uncer-\ntainty in more ambiguous contexts. This research\nprovides insights into LLMs\u2019 decision-making ten-\ndencies and their ability to mirror human moral\njudgments in ethical situations.\n2.2.2 Truthfulness\nEvaluating the reliability of LLMs necessitates en-\nsuring the truthfulness of their outputs. Turpin\net al. (2023) demonstrate that Chain-of-Thought\n(CoT) explanations can systematically misrepre-\nsent the true reasoning behind a model\u2019s predic-\ntions. Khan et al. (2024) points out that as LLMs\ngrow more complex, possibly surpassing human\nexperts, the evaluation dynamic might shift, raising\nthe question of whether simpler models can effec-\ntively assess more advanced ones. This scenario\nunderscores the ongoing importance of truthfulness\nin LLM outputs, reflecting the evolving challenges\nin model evaluation.\nAs trustworthiness becomes a key priority, re-\nsearchers have implemented various evaluation\nstrategies to ensure model reliability. This section\ndetails strategies to reinforce the trustworthiness\nof LLM outputs. Besides the widely known Truth-\nfulQA benchmark (Lin et al., 2022) , we also focus\non the following topics: Hallucination, Bias Miti-\ngation .\nHallucination Hallucinations in LLMs, where\nmodels generate factually incorrect or fabricated\ncontent, pose significant challenges to their trust-\nworthiness and reliability.Techniques such as HaluEval 2.0 (Jiang et al.,\n2024) and HalluCode (Liu et al., 2024a) bench-\nmarks have been developed for effective halluci-\nnation detection. Other methods include FEWL\n(Wei et al., 2024), which measures hallucinations\nwithout gold-standard answers by leveraging mul-\ntiple LLM responses, and TofuEval (Tang et al.,\n2024), which evaluates hallucinations in dialogue\nsummarization with detailed error taxonomy. Self-\nAlignment for Factuality (Zhang et al., 2024b) uses\nself-evaluation to improve factual accuracy within\nLLMs.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2868, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d3394261-fd6b-4ca7-958e-14236d6ee8ad": {"__data__": {"id_": "d3394261-fd6b-4ca7-958e-14236d6ee8ad", "embedding": null, "metadata": {"page_label": "7", "file_name": "2406_00936v1.pdf", "Title of this paper": "A Survey of Useful LLM Evaluation", "Authors": "Ji-Lun Peng, Sijia Cheng, Egil Diau, Yung-Yu Shih, Po-Heng Chen, Yen-Ting Lin, Yun-Nung Chen", "Date published": "06/03/2024", "URL": "http://arxiv.org/abs/2406.00936v1", "summary": "LLMs have gotten attention across various research domains due to their\nexceptional performance on a wide range of complex tasks. Therefore, refined\nmethods to evaluate the capabilities of LLMs are needed to determine the tasks\nand responsibility they should undertake. Our study mainly discussed how LLMs,\nas useful tools, should be effectively assessed. We proposed the two-stage\nframework: from ``core ability'' to ``agent'', clearly explaining how LLMs can\nbe applied based on their specific capabilities, along with the evaluation\nmethods in each stage. Core ability refers to the capabilities that LLMs need\nin order to generate high-quality natural language texts. After confirming LLMs\npossess core ability, they can solve real-world and complex tasks as agent. In\nthe \"core ability\" stage, we discussed the reasoning ability, societal impact,\nand domain knowledge of LLMs. In the ``agent'' stage, we demonstrated embodied\naction, planning, and tool learning of LLMs agent applications. Finally, we\nexamined the challenges currently confronting the evaluation methods for LLMs,\nas well as the directions for future development."}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ad91309c-b7e2-48ea-9862-10a6b3fecb85", "node_type": "4", "metadata": {"page_label": "7", "file_name": "2406_00936v1.pdf", "Title of this paper": "A Survey of Useful LLM Evaluation", "Authors": "Ji-Lun Peng, Sijia Cheng, Egil Diau, Yung-Yu Shih, Po-Heng Chen, Yen-Ting Lin, Yun-Nung Chen", "Date published": "06/03/2024", "URL": "http://arxiv.org/abs/2406.00936v1", "summary": "LLMs have gotten attention across various research domains due to their\nexceptional performance on a wide range of complex tasks. Therefore, refined\nmethods to evaluate the capabilities of LLMs are needed to determine the tasks\nand responsibility they should undertake. Our study mainly discussed how LLMs,\nas useful tools, should be effectively assessed. We proposed the two-stage\nframework: from ``core ability'' to ``agent'', clearly explaining how LLMs can\nbe applied based on their specific capabilities, along with the evaluation\nmethods in each stage. Core ability refers to the capabilities that LLMs need\nin order to generate high-quality natural language texts. After confirming LLMs\npossess core ability, they can solve real-world and complex tasks as agent. In\nthe \"core ability\" stage, we discussed the reasoning ability, societal impact,\nand domain knowledge of LLMs. In the ``agent'' stage, we demonstrated embodied\naction, planning, and tool learning of LLMs agent applications. Finally, we\nexamined the challenges currently confronting the evaluation methods for LLMs,\nas well as the directions for future development."}, "hash": "c82f626162188e8990006fc15de9140187e99ac8cf65328930a9589b9d38c998", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "27030cf0-94e5-46bb-8fb0-67a42eaf9b60", "node_type": "1", "metadata": {"page_label": "7", "file_name": "2406_00936v1.pdf", "Title of this paper": "A Survey of Useful LLM Evaluation", "Authors": "Ji-Lun Peng, Sijia Cheng, Egil Diau, Yung-Yu Shih, Po-Heng Chen, Yen-Ting Lin, Yun-Nung Chen", "Date published": "06/03/2024", "URL": "http://arxiv.org/abs/2406.00936v1", "summary": "LLMs have gotten attention across various research domains due to their\nexceptional performance on a wide range of complex tasks. Therefore, refined\nmethods to evaluate the capabilities of LLMs are needed to determine the tasks\nand responsibility they should undertake. Our study mainly discussed how LLMs,\nas useful tools, should be effectively assessed. We proposed the two-stage\nframework: from ``core ability'' to ``agent'', clearly explaining how LLMs can\nbe applied based on their specific capabilities, along with the evaluation\nmethods in each stage. Core ability refers to the capabilities that LLMs need\nin order to generate high-quality natural language texts. After confirming LLMs\npossess core ability, they can solve real-world and complex tasks as agent. In\nthe \"core ability\" stage, we discussed the reasoning ability, societal impact,\nand domain knowledge of LLMs. In the ``agent'' stage, we demonstrated embodied\naction, planning, and tool learning of LLMs agent applications. Finally, we\nexamined the challenges currently confronting the evaluation methods for LLMs,\nas well as the directions for future development."}, "hash": "eaec12bef04e10df13d8e76b2d5db484f746d69d1f0b50578d185266a798a7a0", "class_name": "RelatedNodeInfo"}}, "text": "Hallucination Hallucinations in LLMs, where\nmodels generate factually incorrect or fabricated\ncontent, pose significant challenges to their trust-\nworthiness and reliability.Techniques such as HaluEval 2.0 (Jiang et al.,\n2024) and HalluCode (Liu et al., 2024a) bench-\nmarks have been developed for effective halluci-\nnation detection. Other methods include FEWL\n(Wei et al., 2024), which measures hallucinations\nwithout gold-standard answers by leveraging mul-\ntiple LLM responses, and TofuEval (Tang et al.,\n2024), which evaluates hallucinations in dialogue\nsummarization with detailed error taxonomy. Self-\nAlignment for Factuality (Zhang et al., 2024b) uses\nself-evaluation to improve factual accuracy within\nLLMs. The LLM-free multi-dimensional bench-\nmark AMBER (Wang et al., 2024a) allows for the\nevaluation of both generative and discriminative\ntasks, including various types of hallucinations,\nthrough a low-cost and efficient evaluation pipeline.\nThis benchmark facilitates a comprehensive evalu-\nation and detailed analysis of mainstream MLLMs\nlike GPT-4V , also providing guidelines for mitigat-\ning hallucinations.\nFeldman et al. (2023) helps recognize and flag\ninstances when LLMs operate outside their domain\nknowledge, ensuring that users receive accurate\ninformation. This method significantly reduces\nhallucinations when context accompanies question\nprompts, achieving a high effectiveness in elimi-\nnating hallucinations through tag evaluation. Yang\net al. (2023) introduces a self-check approach for\ndetecting factual errors in LLMs during critical\ntasks, using reverse validation in a zero-resource\nsetting. The PHD benchmark, designed for detect-\ning hallucinations at the passage level and anno-\ntated by humans, enhances the evaluation of detec-\ntion methods and surpasses existing approaches in\nefficiency and accuracy.\nBias Mitigation A range of studies address the\nissue of bias in the evaluation and operation of\nLLMs, emphasizing the need to diminish these\nbiases to improve both quality and reliability.\nHere are some general bias benchmarks. BBQ\n(Parrish et al., 2021) is a dataset of question sets\nconstructed by the authors that highlight attested\nsocial biases against people belonging to protected\nclasses along nine social dimensions relevant for\nU.S. English-speaking contexts. BIAS (Vermetten\net al., 2022) is a novel behavior-based benchmark\ndesigned to detect structural bias per dimension\nand across dimension-based on 39 statistical tests.\nRecLLM (Zhang et al., 2023) investigates fairness\nin LLM-based recommendations, presenting the\nFaiRLLM benchmark to evaluate biases towards\n7", "mimetype": "text/plain", "start_char_idx": 2151, "end_char_idx": 4773, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "fb4e3b69-9876-443a-8255-9f28f2fa024f": {"__data__": {"id_": "fb4e3b69-9876-443a-8255-9f28f2fa024f", "embedding": null, "metadata": {"page_label": "8", "file_name": "2406_00936v1.pdf", "Title of this paper": "A Survey of Useful LLM Evaluation", "Authors": "Ji-Lun Peng, Sijia Cheng, Egil Diau, Yung-Yu Shih, Po-Heng Chen, Yen-Ting Lin, Yun-Nung Chen", "Date published": "06/03/2024", "URL": "http://arxiv.org/abs/2406.00936v1", "summary": "LLMs have gotten attention across various research domains due to their\nexceptional performance on a wide range of complex tasks. Therefore, refined\nmethods to evaluate the capabilities of LLMs are needed to determine the tasks\nand responsibility they should undertake. Our study mainly discussed how LLMs,\nas useful tools, should be effectively assessed. We proposed the two-stage\nframework: from ``core ability'' to ``agent'', clearly explaining how LLMs can\nbe applied based on their specific capabilities, along with the evaluation\nmethods in each stage. Core ability refers to the capabilities that LLMs need\nin order to generate high-quality natural language texts. After confirming LLMs\npossess core ability, they can solve real-world and complex tasks as agent. In\nthe \"core ability\" stage, we discussed the reasoning ability, societal impact,\nand domain knowledge of LLMs. In the ``agent'' stage, we demonstrated embodied\naction, planning, and tool learning of LLMs agent applications. Finally, we\nexamined the challenges currently confronting the evaluation methods for LLMs,\nas well as the directions for future development."}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1c13d026-9938-4364-b3f4-868560c845c6", "node_type": "4", "metadata": {"page_label": "8", "file_name": "2406_00936v1.pdf", "Title of this paper": "A Survey of Useful LLM Evaluation", "Authors": "Ji-Lun Peng, Sijia Cheng, Egil Diau, Yung-Yu Shih, Po-Heng Chen, Yen-Ting Lin, Yun-Nung Chen", "Date published": "06/03/2024", "URL": "http://arxiv.org/abs/2406.00936v1", "summary": "LLMs have gotten attention across various research domains due to their\nexceptional performance on a wide range of complex tasks. Therefore, refined\nmethods to evaluate the capabilities of LLMs are needed to determine the tasks\nand responsibility they should undertake. Our study mainly discussed how LLMs,\nas useful tools, should be effectively assessed. We proposed the two-stage\nframework: from ``core ability'' to ``agent'', clearly explaining how LLMs can\nbe applied based on their specific capabilities, along with the evaluation\nmethods in each stage. Core ability refers to the capabilities that LLMs need\nin order to generate high-quality natural language texts. After confirming LLMs\npossess core ability, they can solve real-world and complex tasks as agent. In\nthe \"core ability\" stage, we discussed the reasoning ability, societal impact,\nand domain knowledge of LLMs. In the ``agent'' stage, we demonstrated embodied\naction, planning, and tool learning of LLMs agent applications. Finally, we\nexamined the challenges currently confronting the evaluation methods for LLMs,\nas well as the directions for future development."}, "hash": "231a0b0d771ef1880ae9e247be3dc25b07adf9982c07e9d2f6c68dd284a14167", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8ebaf79f-f834-4843-91b1-ad1369f30306", "node_type": "1", "metadata": {}, "hash": "d43f9df5c36f1521bfe5fd52125f733bc6bbe018cd855789167d7e9c45168934", "class_name": "RelatedNodeInfo"}}, "text": "sensitive user attributes. MERS (Wu and Aji, 2023)\nintroduced assesses machine-generated text on mul-\ntiple dimensions, including factual accuracy and\nlinguistic quality, to specifically target and reduce\nbiases that favor incorrect factual content in LLM\nevaluations.\nBelow are specific bias benchmarks relevant to\ndistinct sectors. In the financial sector, Daniel et al.\n(2008) tackles the \"look-ahead benchmark bias\"\nin the evaluation of investment managers, which\nidentifies significant discrepancies in performance\nmetrics due to timing differences in benchmark\ncomposition. This finding stresses the need for\nprecise benchmarking methods to avoid overstated\nperformance assessments. Hort et al. (2021) uses\na model behavior mutation approach for bench-\nmarking ML bias mitigation methods. Although\nthe results indicate that many methods struggle\nto effectively balance fairness and accuracy, they\nunderline the need for more robust strategies in\nbias mitigation. Wessel et al. (2023) introduces the\nMedia Bias Identification Benchmark (MBIB), a\ncomprehensive framework that integrates various\ntypes of media biases, enhancing the effectiveness\nof detection techniques and promoting a more uni-\nfied and effective approach to bias evaluation in\nmedia content.\n2.3 Domain Knowledge\nAs LLMs demonstrate their capabilities in reason-\ning and safety, experts have begun to explore the\nknowledge of LLMs in various domains. They\nutilize LLMs to complete specific tasks, making\nthese models useful assistants. In this section, we\ndelve into five domains: Finance, Legislation, Psy-\nchology, Medicine, and Education, introducing the\napplications, evaluation methods, and discussing\nthe direction and limitations of LLMs in each do-\nmain.\n2.3.1 Finance\nThe application of LLMs in Finance field devel-\noped relatively earlier. A few models were even\ndesigned specifically for financial use, such as Fin-\nBERT (Liu et al., 2021b), XuanYuan 2.0 (Zhang\nand Yang, 2023), and BloombergGPT (Wu et al.,\n2023). BloombergGPT is a 50 billion parameter\nlanguage model that is trained on a wide range\nof financial data. From the validation process of\nBloombergGPT, we can understand the evaluation\nmethods of financial LLMs. Wu et al. (2023) eval-\nuated BloombergGPT on two broad categories oftasks: finance-specific and general purpose. Re-\ngarding the finance-specific tasks, FPB (Malo et al.,\n2014), FiQA SA (Maia et al., 2018), Headline\n(Sinha and Khandait, 2021), NER (Alvarado et al.,\n2015), and ConvFinQA (Chen et al., 2022) were\nused. They also used social media and news as\naspect-specific sentiment analysis dataset, and com-\npared BloombergGPT response with financial ex-\nperts\u2019 annotation.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2685, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8ebaf79f-f834-4843-91b1-ad1369f30306": {"__data__": {"id_": "8ebaf79f-f834-4843-91b1-ad1369f30306", "embedding": null, "metadata": {"page_label": "8", "file_name": "2406_00936v1.pdf", "Title of this paper": "A Survey of Useful LLM Evaluation", "Authors": "Ji-Lun Peng, Sijia Cheng, Egil Diau, Yung-Yu Shih, Po-Heng Chen, Yen-Ting Lin, Yun-Nung Chen", "Date published": "06/03/2024", "URL": "http://arxiv.org/abs/2406.00936v1", "summary": "LLMs have gotten attention across various research domains due to their\nexceptional performance on a wide range of complex tasks. Therefore, refined\nmethods to evaluate the capabilities of LLMs are needed to determine the tasks\nand responsibility they should undertake. Our study mainly discussed how LLMs,\nas useful tools, should be effectively assessed. We proposed the two-stage\nframework: from ``core ability'' to ``agent'', clearly explaining how LLMs can\nbe applied based on their specific capabilities, along with the evaluation\nmethods in each stage. Core ability refers to the capabilities that LLMs need\nin order to generate high-quality natural language texts. After confirming LLMs\npossess core ability, they can solve real-world and complex tasks as agent. In\nthe \"core ability\" stage, we discussed the reasoning ability, societal impact,\nand domain knowledge of LLMs. In the ``agent'' stage, we demonstrated embodied\naction, planning, and tool learning of LLMs agent applications. Finally, we\nexamined the challenges currently confronting the evaluation methods for LLMs,\nas well as the directions for future development."}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1c13d026-9938-4364-b3f4-868560c845c6", "node_type": "4", "metadata": {"page_label": "8", "file_name": "2406_00936v1.pdf", "Title of this paper": "A Survey of Useful LLM Evaluation", "Authors": "Ji-Lun Peng, Sijia Cheng, Egil Diau, Yung-Yu Shih, Po-Heng Chen, Yen-Ting Lin, Yun-Nung Chen", "Date published": "06/03/2024", "URL": "http://arxiv.org/abs/2406.00936v1", "summary": "LLMs have gotten attention across various research domains due to their\nexceptional performance on a wide range of complex tasks. Therefore, refined\nmethods to evaluate the capabilities of LLMs are needed to determine the tasks\nand responsibility they should undertake. Our study mainly discussed how LLMs,\nas useful tools, should be effectively assessed. We proposed the two-stage\nframework: from ``core ability'' to ``agent'', clearly explaining how LLMs can\nbe applied based on their specific capabilities, along with the evaluation\nmethods in each stage. Core ability refers to the capabilities that LLMs need\nin order to generate high-quality natural language texts. After confirming LLMs\npossess core ability, they can solve real-world and complex tasks as agent. In\nthe \"core ability\" stage, we discussed the reasoning ability, societal impact,\nand domain knowledge of LLMs. In the ``agent'' stage, we demonstrated embodied\naction, planning, and tool learning of LLMs agent applications. Finally, we\nexamined the challenges currently confronting the evaluation methods for LLMs,\nas well as the directions for future development."}, "hash": "231a0b0d771ef1880ae9e247be3dc25b07adf9982c07e9d2f6c68dd284a14167", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "fb4e3b69-9876-443a-8255-9f28f2fa024f", "node_type": "1", "metadata": {"page_label": "8", "file_name": "2406_00936v1.pdf", "Title of this paper": "A Survey of Useful LLM Evaluation", "Authors": "Ji-Lun Peng, Sijia Cheng, Egil Diau, Yung-Yu Shih, Po-Heng Chen, Yen-Ting Lin, Yun-Nung Chen", "Date published": "06/03/2024", "URL": "http://arxiv.org/abs/2406.00936v1", "summary": "LLMs have gotten attention across various research domains due to their\nexceptional performance on a wide range of complex tasks. Therefore, refined\nmethods to evaluate the capabilities of LLMs are needed to determine the tasks\nand responsibility they should undertake. Our study mainly discussed how LLMs,\nas useful tools, should be effectively assessed. We proposed the two-stage\nframework: from ``core ability'' to ``agent'', clearly explaining how LLMs can\nbe applied based on their specific capabilities, along with the evaluation\nmethods in each stage. Core ability refers to the capabilities that LLMs need\nin order to generate high-quality natural language texts. After confirming LLMs\npossess core ability, they can solve real-world and complex tasks as agent. In\nthe \"core ability\" stage, we discussed the reasoning ability, societal impact,\nand domain knowledge of LLMs. In the ``agent'' stage, we demonstrated embodied\naction, planning, and tool learning of LLMs agent applications. Finally, we\nexamined the challenges currently confronting the evaluation methods for LLMs,\nas well as the directions for future development."}, "hash": "47682bc185a0067fed369f358e6fd6e6e1f2c7257b573d570ad5213208503359", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "619e0387-ae89-4e2d-abb3-50e88ff6fb14", "node_type": "1", "metadata": {}, "hash": "701e9f467aef2e6d1c614806a8620d1dc59f8db48553b4efd289575b2725aad1", "class_name": "RelatedNodeInfo"}}, "text": "BloombergGPT is a 50 billion parameter\nlanguage model that is trained on a wide range\nof financial data. From the validation process of\nBloombergGPT, we can understand the evaluation\nmethods of financial LLMs. Wu et al. (2023) eval-\nuated BloombergGPT on two broad categories oftasks: finance-specific and general purpose. Re-\ngarding the finance-specific tasks, FPB (Malo et al.,\n2014), FiQA SA (Maia et al., 2018), Headline\n(Sinha and Khandait, 2021), NER (Alvarado et al.,\n2015), and ConvFinQA (Chen et al., 2022) were\nused. They also used social media and news as\naspect-specific sentiment analysis dataset, and com-\npared BloombergGPT response with financial ex-\nperts\u2019 annotation. Regarding the general purpose\ntasks, standard LLM benchmarks were utilized for\nevaluation, such as BIG-bench Hard (Suzgun et al.,\n2022), and several datasets about Knowledge As-\nsessments, Reading Comprehension, and Linguis-\ntic Tasks. Conditionally, Xie et al. (2023) pro-\nposed PIXIU, a framework including the financial\nLLM based on fine-tuning LLaMA, a instruction\ndata with 136K data samples to support the fine-\ntuning, and an evaluation benchmark with 5 tasks\nand 9 datasets, giving LLMs in financial area a\nbenchmark to assess their ability. When mention-\ning LLMs for financial use, Li et al. (2023b) argued\nthat two major challenges are the production of dis-\ninformation and the manifestation of biases, such\nas racial, gender, and religious biases, in LLMs.\nAlso, the primary challenge in evaluation was in-\ncorporating domain knowledge from financial ex-\nperts to validate the model\u2019s performance based on\nfinancial NLP tasks (Lee et al., 2024).\n2.3.2 Legislation\nLLMs\u2019 ability in legislation area has also attracted\nattention because GPT-4 scored approximately 297\npoints on the uniform bar examination, passing the\nthreshold for all jurisdiction (Katz et al., 2024).\nVarious tasks such as statutory reasoning, term in-\nterpretation, and legal rule classification were per-\nformed by LLMs, and their performance were also\nevaluated. Blair-Stanek et al. (2023) evaluated the\nperformance of GPT-3 in statutory reasoning with\nSARA dataset (Holzenberger et al., 2020). they\nfound that GPT-3 only reached 78% accuracy in\nzero-shot condition, showing that GPT-3 couldn\u2019t\nhandle basic legal work because statutes in the\ndataset were far less complex than real statutes.\nEngel and Mcadams (2024) asked Chat 3.5 Turbo\nwhether the statutory term \u201cvehicle\u201d includes a list\nof candidate objects to assessment LLMs\u2019 under-\nstanding of statutory meaning. They found that\nChat 3.5 Turbo give the similar result to 2,800 En-\nglish speakers\u2019 response (Tobia, 2020).", "mimetype": "text/plain", "start_char_idx": 1999, "end_char_idx": 4648, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "619e0387-ae89-4e2d-abb3-50e88ff6fb14": {"__data__": {"id_": "619e0387-ae89-4e2d-abb3-50e88ff6fb14", "embedding": null, "metadata": {"page_label": "8", "file_name": "2406_00936v1.pdf", "Title of this paper": "A Survey of Useful LLM Evaluation", "Authors": "Ji-Lun Peng, Sijia Cheng, Egil Diau, Yung-Yu Shih, Po-Heng Chen, Yen-Ting Lin, Yun-Nung Chen", "Date published": "06/03/2024", "URL": "http://arxiv.org/abs/2406.00936v1", "summary": "LLMs have gotten attention across various research domains due to their\nexceptional performance on a wide range of complex tasks. Therefore, refined\nmethods to evaluate the capabilities of LLMs are needed to determine the tasks\nand responsibility they should undertake. Our study mainly discussed how LLMs,\nas useful tools, should be effectively assessed. We proposed the two-stage\nframework: from ``core ability'' to ``agent'', clearly explaining how LLMs can\nbe applied based on their specific capabilities, along with the evaluation\nmethods in each stage. Core ability refers to the capabilities that LLMs need\nin order to generate high-quality natural language texts. After confirming LLMs\npossess core ability, they can solve real-world and complex tasks as agent. In\nthe \"core ability\" stage, we discussed the reasoning ability, societal impact,\nand domain knowledge of LLMs. In the ``agent'' stage, we demonstrated embodied\naction, planning, and tool learning of LLMs agent applications. Finally, we\nexamined the challenges currently confronting the evaluation methods for LLMs,\nas well as the directions for future development."}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1c13d026-9938-4364-b3f4-868560c845c6", "node_type": "4", "metadata": {"page_label": "8", "file_name": "2406_00936v1.pdf", "Title of this paper": "A Survey of Useful LLM Evaluation", "Authors": "Ji-Lun Peng, Sijia Cheng, Egil Diau, Yung-Yu Shih, Po-Heng Chen, Yen-Ting Lin, Yun-Nung Chen", "Date published": "06/03/2024", "URL": "http://arxiv.org/abs/2406.00936v1", "summary": "LLMs have gotten attention across various research domains due to their\nexceptional performance on a wide range of complex tasks. Therefore, refined\nmethods to evaluate the capabilities of LLMs are needed to determine the tasks\nand responsibility they should undertake. Our study mainly discussed how LLMs,\nas useful tools, should be effectively assessed. We proposed the two-stage\nframework: from ``core ability'' to ``agent'', clearly explaining how LLMs can\nbe applied based on their specific capabilities, along with the evaluation\nmethods in each stage. Core ability refers to the capabilities that LLMs need\nin order to generate high-quality natural language texts. After confirming LLMs\npossess core ability, they can solve real-world and complex tasks as agent. In\nthe \"core ability\" stage, we discussed the reasoning ability, societal impact,\nand domain knowledge of LLMs. In the ``agent'' stage, we demonstrated embodied\naction, planning, and tool learning of LLMs agent applications. Finally, we\nexamined the challenges currently confronting the evaluation methods for LLMs,\nas well as the directions for future development."}, "hash": "231a0b0d771ef1880ae9e247be3dc25b07adf9982c07e9d2f6c68dd284a14167", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8ebaf79f-f834-4843-91b1-ad1369f30306", "node_type": "1", "metadata": {"page_label": "8", "file_name": "2406_00936v1.pdf", "Title of this paper": "A Survey of Useful LLM Evaluation", "Authors": "Ji-Lun Peng, Sijia Cheng, Egil Diau, Yung-Yu Shih, Po-Heng Chen, Yen-Ting Lin, Yun-Nung Chen", "Date published": "06/03/2024", "URL": "http://arxiv.org/abs/2406.00936v1", "summary": "LLMs have gotten attention across various research domains due to their\nexceptional performance on a wide range of complex tasks. Therefore, refined\nmethods to evaluate the capabilities of LLMs are needed to determine the tasks\nand responsibility they should undertake. Our study mainly discussed how LLMs,\nas useful tools, should be effectively assessed. We proposed the two-stage\nframework: from ``core ability'' to ``agent'', clearly explaining how LLMs can\nbe applied based on their specific capabilities, along with the evaluation\nmethods in each stage. Core ability refers to the capabilities that LLMs need\nin order to generate high-quality natural language texts. After confirming LLMs\npossess core ability, they can solve real-world and complex tasks as agent. In\nthe \"core ability\" stage, we discussed the reasoning ability, societal impact,\nand domain knowledge of LLMs. In the ``agent'' stage, we demonstrated embodied\naction, planning, and tool learning of LLMs agent applications. Finally, we\nexamined the challenges currently confronting the evaluation methods for LLMs,\nas well as the directions for future development."}, "hash": "5e31fccb685f0c4c3f91db8618a57dfff8d4b0c3b5c31395b3c3d8b85f7e13a9", "class_name": "RelatedNodeInfo"}}, "text": "Blair-Stanek et al. (2023) evaluated the\nperformance of GPT-3 in statutory reasoning with\nSARA dataset (Holzenberger et al., 2020). they\nfound that GPT-3 only reached 78% accuracy in\nzero-shot condition, showing that GPT-3 couldn\u2019t\nhandle basic legal work because statutes in the\ndataset were far less complex than real statutes.\nEngel and Mcadams (2024) asked Chat 3.5 Turbo\nwhether the statutory term \u201cvehicle\u201d includes a list\nof candidate objects to assessment LLMs\u2019 under-\nstanding of statutory meaning. They found that\nChat 3.5 Turbo give the similar result to 2,800 En-\nglish speakers\u2019 response (Tobia, 2020). Liga and\nRobaldo (2023) found that GPT-3 is capable to\nrecognize the difference between obligation rules,\n8", "mimetype": "text/plain", "start_char_idx": 4033, "end_char_idx": 4756, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7e559ea7-8b79-4422-b482-86e371b2fdd0": {"__data__": {"id_": "7e559ea7-8b79-4422-b482-86e371b2fdd0", "embedding": null, "metadata": {"page_label": "9", "file_name": "2406_00936v1.pdf", "Title of this paper": "A Survey of Useful LLM Evaluation", "Authors": "Ji-Lun Peng, Sijia Cheng, Egil Diau, Yung-Yu Shih, Po-Heng Chen, Yen-Ting Lin, Yun-Nung Chen", "Date published": "06/03/2024", "URL": "http://arxiv.org/abs/2406.00936v1", "summary": "LLMs have gotten attention across various research domains due to their\nexceptional performance on a wide range of complex tasks. Therefore, refined\nmethods to evaluate the capabilities of LLMs are needed to determine the tasks\nand responsibility they should undertake. Our study mainly discussed how LLMs,\nas useful tools, should be effectively assessed. We proposed the two-stage\nframework: from ``core ability'' to ``agent'', clearly explaining how LLMs can\nbe applied based on their specific capabilities, along with the evaluation\nmethods in each stage. Core ability refers to the capabilities that LLMs need\nin order to generate high-quality natural language texts. After confirming LLMs\npossess core ability, they can solve real-world and complex tasks as agent. In\nthe \"core ability\" stage, we discussed the reasoning ability, societal impact,\nand domain knowledge of LLMs. In the ``agent'' stage, we demonstrated embodied\naction, planning, and tool learning of LLMs agent applications. Finally, we\nexamined the challenges currently confronting the evaluation methods for LLMs,\nas well as the directions for future development."}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8ac0feb1-b2b0-4c5b-85d7-09a71e899465", "node_type": "4", "metadata": {"page_label": "9", "file_name": "2406_00936v1.pdf", "Title of this paper": "A Survey of Useful LLM Evaluation", "Authors": "Ji-Lun Peng, Sijia Cheng, Egil Diau, Yung-Yu Shih, Po-Heng Chen, Yen-Ting Lin, Yun-Nung Chen", "Date published": "06/03/2024", "URL": "http://arxiv.org/abs/2406.00936v1", "summary": "LLMs have gotten attention across various research domains due to their\nexceptional performance on a wide range of complex tasks. Therefore, refined\nmethods to evaluate the capabilities of LLMs are needed to determine the tasks\nand responsibility they should undertake. Our study mainly discussed how LLMs,\nas useful tools, should be effectively assessed. We proposed the two-stage\nframework: from ``core ability'' to ``agent'', clearly explaining how LLMs can\nbe applied based on their specific capabilities, along with the evaluation\nmethods in each stage. Core ability refers to the capabilities that LLMs need\nin order to generate high-quality natural language texts. After confirming LLMs\npossess core ability, they can solve real-world and complex tasks as agent. In\nthe \"core ability\" stage, we discussed the reasoning ability, societal impact,\nand domain knowledge of LLMs. In the ``agent'' stage, we demonstrated embodied\naction, planning, and tool learning of LLMs agent applications. Finally, we\nexamined the challenges currently confronting the evaluation methods for LLMs,\nas well as the directions for future development."}, "hash": "3724e6d47ffc004dc3c1230dc9d9596d0cf01b348fc4b97e4194b31f74874cbf", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5ec34214-7ce0-44b5-a1c7-f883d66e1d07", "node_type": "1", "metadata": {}, "hash": "f4dfbdbe8433bebf4eb18c8237373499db50f0d56a5423c27b4b338d87ab8875", "class_name": "RelatedNodeInfo"}}, "text": "permission rules and constitutive rules with Legal-\nDocML (Palmirani and Vitali, 2011) and Legal-\nRuleML (Athan et al., 2013) dataset. Whether\nLLMs possess sufficient capability to be applied\nin the professional legal field, The investigation\nindicates that pre-trained LLMs are not yet ready\nfor fully automatic deployment for case judgement\nsummarization because inconsistent or hallucinated\ninformation has been found in the generated ab-\nstractive summaries (Deroy et al., 2023).\n2.3.3 Psychology\nHuman language data is important and valuable in\nevery subdomain in psychology. Because LLMs\nhave the capability to understand and utilize multi-\nple language, emotion detection and psychological\nmeasurement can be done by LLMs. Plenty of\nresearches evaluated whether LLMs could com-\nplete these tasks with enough quality.Rathje et al.\n(2023) tested whether different versions of GPT\n(3.5 Turbo, 4, and 4 Turbo) can detect sentiment,\ndiscrete emotions, offensiveness, and moral foun-\ndations in text across 12 languages. They found\nthat LLMs outperformed existing English-language\ndictionary analysis at detecting psychological con-\nstructs as judged by manual annotators. Lu et al.\n(2024) evaluated GPT-4V\u2019s performance in 5 cru-\ncial abilities for affective computing tasks. They\nused DISFA dataset (Mavadati et al., 2013) to as-\nsess GPT-4V\u2019s ability to action unit detection, RAF-\nDB dataset (Shan and Deng, 2018) for facial ex-\npression and compound emotion recognition (Du\net al., 2014), CASME2 dataset (Yan et al., 2014) for\nMicro-expression Recognition (Zhao et al., 2023),\nand iMiGUE dataset (Liu et al., 2021a) for Micro-\ngesture Recognition. The results showed that GPT-\n4V could give satisfactory answers to action unit,\ncompound emotion and Micro-gesture test sam-\nples, but failed to answer facial expression and\nMicro-expression test samples correctly. Regard-\ning psychological measurement, Demszky et al.\n(2023) proposed 2 methods to evaluate the effects\nof features on human thought and behaviour: 1)\nExpert evaluation means trained research assis-\ntants and LLMs score the same texts for particular\npsychological construct, and then compute agree-\nment between their scores. 2) Impact evaluation\nmeans assessing the effect before and after the ma-\nnipulation. For instance, Karinshak et al. (2023)\nused impact evaluation to measure participants\u2019 atti-\ntude to GPT-3-generated pro-vaccination messages.\nDemszky et al. (2023) additionally proposed thatin assessing the capability of LLMs for psycholog-\nical tasks, initial assessment could be conducted\nusing expert evaluation for a manipulation check or\na measure of construct validity. Subsequently, text\naligning with expert evaluations might be utilized\nin an impact evaluation study that attempts to mea-\nsure the intended effects on third-party participants,\nsimilar to assessing predictive or external validity.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2886, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5ec34214-7ce0-44b5-a1c7-f883d66e1d07": {"__data__": {"id_": "5ec34214-7ce0-44b5-a1c7-f883d66e1d07", "embedding": null, "metadata": {"page_label": "9", "file_name": "2406_00936v1.pdf", "Title of this paper": "A Survey of Useful LLM Evaluation", "Authors": "Ji-Lun Peng, Sijia Cheng, Egil Diau, Yung-Yu Shih, Po-Heng Chen, Yen-Ting Lin, Yun-Nung Chen", "Date published": "06/03/2024", "URL": "http://arxiv.org/abs/2406.00936v1", "summary": "LLMs have gotten attention across various research domains due to their\nexceptional performance on a wide range of complex tasks. Therefore, refined\nmethods to evaluate the capabilities of LLMs are needed to determine the tasks\nand responsibility they should undertake. Our study mainly discussed how LLMs,\nas useful tools, should be effectively assessed. We proposed the two-stage\nframework: from ``core ability'' to ``agent'', clearly explaining how LLMs can\nbe applied based on their specific capabilities, along with the evaluation\nmethods in each stage. Core ability refers to the capabilities that LLMs need\nin order to generate high-quality natural language texts. After confirming LLMs\npossess core ability, they can solve real-world and complex tasks as agent. In\nthe \"core ability\" stage, we discussed the reasoning ability, societal impact,\nand domain knowledge of LLMs. In the ``agent'' stage, we demonstrated embodied\naction, planning, and tool learning of LLMs agent applications. Finally, we\nexamined the challenges currently confronting the evaluation methods for LLMs,\nas well as the directions for future development."}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8ac0feb1-b2b0-4c5b-85d7-09a71e899465", "node_type": "4", "metadata": {"page_label": "9", "file_name": "2406_00936v1.pdf", "Title of this paper": "A Survey of Useful LLM Evaluation", "Authors": "Ji-Lun Peng, Sijia Cheng, Egil Diau, Yung-Yu Shih, Po-Heng Chen, Yen-Ting Lin, Yun-Nung Chen", "Date published": "06/03/2024", "URL": "http://arxiv.org/abs/2406.00936v1", "summary": "LLMs have gotten attention across various research domains due to their\nexceptional performance on a wide range of complex tasks. Therefore, refined\nmethods to evaluate the capabilities of LLMs are needed to determine the tasks\nand responsibility they should undertake. Our study mainly discussed how LLMs,\nas useful tools, should be effectively assessed. We proposed the two-stage\nframework: from ``core ability'' to ``agent'', clearly explaining how LLMs can\nbe applied based on their specific capabilities, along with the evaluation\nmethods in each stage. Core ability refers to the capabilities that LLMs need\nin order to generate high-quality natural language texts. After confirming LLMs\npossess core ability, they can solve real-world and complex tasks as agent. In\nthe \"core ability\" stage, we discussed the reasoning ability, societal impact,\nand domain knowledge of LLMs. In the ``agent'' stage, we demonstrated embodied\naction, planning, and tool learning of LLMs agent applications. Finally, we\nexamined the challenges currently confronting the evaluation methods for LLMs,\nas well as the directions for future development."}, "hash": "3724e6d47ffc004dc3c1230dc9d9596d0cf01b348fc4b97e4194b31f74874cbf", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7e559ea7-8b79-4422-b482-86e371b2fdd0", "node_type": "1", "metadata": {"page_label": "9", "file_name": "2406_00936v1.pdf", "Title of this paper": "A Survey of Useful LLM Evaluation", "Authors": "Ji-Lun Peng, Sijia Cheng, Egil Diau, Yung-Yu Shih, Po-Heng Chen, Yen-Ting Lin, Yun-Nung Chen", "Date published": "06/03/2024", "URL": "http://arxiv.org/abs/2406.00936v1", "summary": "LLMs have gotten attention across various research domains due to their\nexceptional performance on a wide range of complex tasks. Therefore, refined\nmethods to evaluate the capabilities of LLMs are needed to determine the tasks\nand responsibility they should undertake. Our study mainly discussed how LLMs,\nas useful tools, should be effectively assessed. We proposed the two-stage\nframework: from ``core ability'' to ``agent'', clearly explaining how LLMs can\nbe applied based on their specific capabilities, along with the evaluation\nmethods in each stage. Core ability refers to the capabilities that LLMs need\nin order to generate high-quality natural language texts. After confirming LLMs\npossess core ability, they can solve real-world and complex tasks as agent. In\nthe \"core ability\" stage, we discussed the reasoning ability, societal impact,\nand domain knowledge of LLMs. In the ``agent'' stage, we demonstrated embodied\naction, planning, and tool learning of LLMs agent applications. Finally, we\nexamined the challenges currently confronting the evaluation methods for LLMs,\nas well as the directions for future development."}, "hash": "0f0d8e4243a86a58e04d574d6c5d34b3bdadc361be12d1a6ecae8e5b0d8c51c5", "class_name": "RelatedNodeInfo"}}, "text": "2) Impact evaluation\nmeans assessing the effect before and after the ma-\nnipulation. For instance, Karinshak et al. (2023)\nused impact evaluation to measure participants\u2019 atti-\ntude to GPT-3-generated pro-vaccination messages.\nDemszky et al. (2023) additionally proposed thatin assessing the capability of LLMs for psycholog-\nical tasks, initial assessment could be conducted\nusing expert evaluation for a manipulation check or\na measure of construct validity. Subsequently, text\naligning with expert evaluations might be utilized\nin an impact evaluation study that attempts to mea-\nsure the intended effects on third-party participants,\nsimilar to assessing predictive or external validity.\n2.3.4 Medicine\nAs ChatGPT was able to pass the United States\nMedical Licensing Exam (USMLE) (Kung et al.,\n2023) without additionally training, LLMs were no-\nticed in medical area. Previous researches focused\non exploring LLMs\u2019 potential in clinical work and\nresearch (Thirunavukarasu et al., 2023). Agrawal\net al. (2022) introduced dataset from manual rean-\nnotation of the CASI dataset (Moon et al., 2014) for\nbenchmarking few-shot clinical information extrac-\ntion, and showed that GPT-3 outperform existing\nbaseline of this task. Sharma and Thakur (2023)\ndemonstrated ChatGPT can help researchers design\nnew drugs and optimize the pharmacokinetics and\npharmacodynamics of new drugs. Benoit (2023)\nshowed when presented with 45 simplified stan-\ndardized vignettes (Semigran et al., 2015), Chat-\nGPT identified illnesses with 75.6% first-pass diag-\nnostic accuracy and 57.8% triage accuracy, which\nperformed similarly to physicians\u2019 72.1% on the\nsame set of 45 vignettes. However, when writ-\ning academic clinical paper, current LLMs can-\nnot meet ICMJE authorship criteria because they\ncannot understand the role of authors or take re-\nsponsibility for the paper (Zielinski et al., 2023).\nAlso, Kumar (2023) assess the ChatGPT\u2019s utility\nfor academic writing in biomedical domain, show-\ning that although the content of the response were\nsystematic, precise and original, it lacked quality\nand depth of academic writing. In summary, plenty\nof deployment of LLM applications in medical area\nis not currently feasible and need to have deeper\nevaluation. clinicians and researchers will remain\nresponsible for delivering optimal knowledge and\ncare (Thirunavukarasu et al., 2023).\n2.3.5 Education\nThe conversational and knowledgeable features of\nLLMs make the applications of LLMs in educa-\ntion possible. Current evaluation methods of LLMs\nin education field can be generally divided into\ntwo categories: 1) Human annotation means that\nexperts directly score the material generated by\n9", "mimetype": "text/plain", "start_char_idx": 2195, "end_char_idx": 4871, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4f33cd9c-7fd1-47ce-ab0d-0abfef6515b4": {"__data__": {"id_": "4f33cd9c-7fd1-47ce-ab0d-0abfef6515b4", "embedding": null, "metadata": {"page_label": "10", "file_name": "2406_00936v1.pdf", "Title of this paper": "A Survey of Useful LLM Evaluation", "Authors": "Ji-Lun Peng, Sijia Cheng, Egil Diau, Yung-Yu Shih, Po-Heng Chen, Yen-Ting Lin, Yun-Nung Chen", "Date published": "06/03/2024", "URL": "http://arxiv.org/abs/2406.00936v1", "summary": "LLMs have gotten attention across various research domains due to their\nexceptional performance on a wide range of complex tasks. Therefore, refined\nmethods to evaluate the capabilities of LLMs are needed to determine the tasks\nand responsibility they should undertake. Our study mainly discussed how LLMs,\nas useful tools, should be effectively assessed. We proposed the two-stage\nframework: from ``core ability'' to ``agent'', clearly explaining how LLMs can\nbe applied based on their specific capabilities, along with the evaluation\nmethods in each stage. Core ability refers to the capabilities that LLMs need\nin order to generate high-quality natural language texts. After confirming LLMs\npossess core ability, they can solve real-world and complex tasks as agent. In\nthe \"core ability\" stage, we discussed the reasoning ability, societal impact,\nand domain knowledge of LLMs. In the ``agent'' stage, we demonstrated embodied\naction, planning, and tool learning of LLMs agent applications. Finally, we\nexamined the challenges currently confronting the evaluation methods for LLMs,\nas well as the directions for future development."}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e86913ac-b0fb-4b1d-b560-ef83650f9e04", "node_type": "4", "metadata": {"page_label": "10", "file_name": "2406_00936v1.pdf", "Title of this paper": "A Survey of Useful LLM Evaluation", "Authors": "Ji-Lun Peng, Sijia Cheng, Egil Diau, Yung-Yu Shih, Po-Heng Chen, Yen-Ting Lin, Yun-Nung Chen", "Date published": "06/03/2024", "URL": "http://arxiv.org/abs/2406.00936v1", "summary": "LLMs have gotten attention across various research domains due to their\nexceptional performance on a wide range of complex tasks. Therefore, refined\nmethods to evaluate the capabilities of LLMs are needed to determine the tasks\nand responsibility they should undertake. Our study mainly discussed how LLMs,\nas useful tools, should be effectively assessed. We proposed the two-stage\nframework: from ``core ability'' to ``agent'', clearly explaining how LLMs can\nbe applied based on their specific capabilities, along with the evaluation\nmethods in each stage. Core ability refers to the capabilities that LLMs need\nin order to generate high-quality natural language texts. After confirming LLMs\npossess core ability, they can solve real-world and complex tasks as agent. In\nthe \"core ability\" stage, we discussed the reasoning ability, societal impact,\nand domain knowledge of LLMs. In the ``agent'' stage, we demonstrated embodied\naction, planning, and tool learning of LLMs agent applications. Finally, we\nexamined the challenges currently confronting the evaluation methods for LLMs,\nas well as the directions for future development."}, "hash": "a0c2f3da79dc399dea10871a98b59abd012d24c5e4bc4290847e2e68112d77f3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e16c1863-2375-4d21-99ed-4f22adb70f92", "node_type": "1", "metadata": {}, "hash": "89c77cbf5003f0d5d034a4fdae40bf630220c96d9c9f642fed2f19467560c39c", "class_name": "RelatedNodeInfo"}}, "text": "LLMs or annotate unlabeled data from external\ndatasets or online websites to create an evaluation\ndataset. Abdelghani et al. (2023) used GPT-3 for\ngenerating linguistic and semantic cues that can\nhelp children formulate divergent questions. They\nhave 2 experts to evaluate the quality of the linguis-\ntic and semantic cues generated. Jia et al. (2021)\nhad fluent English speakers to annotated data from\na peer-assessment platform, Expertiza and make\nsure enough inter-annotator agreement to test the\naccuracy of the BERT model for evaluating peer\nassessments. Menick et al. (2022). evaluated their\nSelf-Supported Question Answering model by ask-\ning paid contractors to assess model samples from\nNatural Questions (Kwiatkowski et al., 2019) and\nELI5 (Fan et al., 2019) datasets. 2) Metrics and\nmodels means that traditional metrics or trained\nmodel are utilized to assess the material gener-\nated by LLMs automatically. Dijkstra et al. (2022)\nproposed EduQuiz, an end-to-end quiz generator\nbased on a GPT-3 model, able to generate a com-\nplete multiple-choice question, with the correct and\ndistractor answers. They used BLEU-4 (Papineni\net al., 2002), ROUGE-L (Lin, 2004), and METEOR\n(Banerjee and Lavie, 2005) metrics to compared\nprediction and ground truth instances. Raina and\nGales (2022) use the RACE++ dataset (Liang et al.,\n2019) to train a deep learning model to explicitly\nclass a multiple-choice question in the complexity\nlevels of easy, medium and hard, which could make\nthe process of assessing multiple-choice question\ngeneration automatic. After the overall review, Kas-\nneci et al. (2023) concluded integrating LLMs into\nthe educational area offers considerable benefits,\nsuch as enhancing student learning experiences and\nassisting teachers, but this integration must adhere\nto strict requirements concerning privacy, security,\nenvironmental sustainability, regulation, and ethics.\nAdditionally, it should be accompanied by continu-\nous human oversight, guidance, and the application\nof critical thinking.\n3 Agent Evaluation\nBuilding upon LLM\u2019s core abilities, there has been\na growing research area that employs LLMs as\ncentral controllers to construct autonomous agents\nto obtain human-like decision-making capabilities\n(Wang et al., 2024b).\nIn this section, we\u2019ll first discuss the methods used\nto assess LLM agents\u2019 capabilities of planning.\nAnd also introduce the evaluation based on var-ious application scenarios. Each subsection will\nprovide detailed insights into the applications of\nLLMs, the methodologies used for evaluation, and\nthe datasets employed.\n3.1 Planning\nPlanning by an agent involves the strategic formu-\nlation and execution of actions or steps to achieve\nspecific goals or outcomes within a given environ-\nment, typically using algorithms or models to pre-\ndict and decide the best course of action.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2843, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e16c1863-2375-4d21-99ed-4f22adb70f92": {"__data__": {"id_": "e16c1863-2375-4d21-99ed-4f22adb70f92", "embedding": null, "metadata": {"page_label": "10", "file_name": "2406_00936v1.pdf", "Title of this paper": "A Survey of Useful LLM Evaluation", "Authors": "Ji-Lun Peng, Sijia Cheng, Egil Diau, Yung-Yu Shih, Po-Heng Chen, Yen-Ting Lin, Yun-Nung Chen", "Date published": "06/03/2024", "URL": "http://arxiv.org/abs/2406.00936v1", "summary": "LLMs have gotten attention across various research domains due to their\nexceptional performance on a wide range of complex tasks. Therefore, refined\nmethods to evaluate the capabilities of LLMs are needed to determine the tasks\nand responsibility they should undertake. Our study mainly discussed how LLMs,\nas useful tools, should be effectively assessed. We proposed the two-stage\nframework: from ``core ability'' to ``agent'', clearly explaining how LLMs can\nbe applied based on their specific capabilities, along with the evaluation\nmethods in each stage. Core ability refers to the capabilities that LLMs need\nin order to generate high-quality natural language texts. After confirming LLMs\npossess core ability, they can solve real-world and complex tasks as agent. In\nthe \"core ability\" stage, we discussed the reasoning ability, societal impact,\nand domain knowledge of LLMs. In the ``agent'' stage, we demonstrated embodied\naction, planning, and tool learning of LLMs agent applications. Finally, we\nexamined the challenges currently confronting the evaluation methods for LLMs,\nas well as the directions for future development."}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e86913ac-b0fb-4b1d-b560-ef83650f9e04", "node_type": "4", "metadata": {"page_label": "10", "file_name": "2406_00936v1.pdf", "Title of this paper": "A Survey of Useful LLM Evaluation", "Authors": "Ji-Lun Peng, Sijia Cheng, Egil Diau, Yung-Yu Shih, Po-Heng Chen, Yen-Ting Lin, Yun-Nung Chen", "Date published": "06/03/2024", "URL": "http://arxiv.org/abs/2406.00936v1", "summary": "LLMs have gotten attention across various research domains due to their\nexceptional performance on a wide range of complex tasks. Therefore, refined\nmethods to evaluate the capabilities of LLMs are needed to determine the tasks\nand responsibility they should undertake. Our study mainly discussed how LLMs,\nas useful tools, should be effectively assessed. We proposed the two-stage\nframework: from ``core ability'' to ``agent'', clearly explaining how LLMs can\nbe applied based on their specific capabilities, along with the evaluation\nmethods in each stage. Core ability refers to the capabilities that LLMs need\nin order to generate high-quality natural language texts. After confirming LLMs\npossess core ability, they can solve real-world and complex tasks as agent. In\nthe \"core ability\" stage, we discussed the reasoning ability, societal impact,\nand domain knowledge of LLMs. In the ``agent'' stage, we demonstrated embodied\naction, planning, and tool learning of LLMs agent applications. Finally, we\nexamined the challenges currently confronting the evaluation methods for LLMs,\nas well as the directions for future development."}, "hash": "a0c2f3da79dc399dea10871a98b59abd012d24c5e4bc4290847e2e68112d77f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4f33cd9c-7fd1-47ce-ab0d-0abfef6515b4", "node_type": "1", "metadata": {"page_label": "10", "file_name": "2406_00936v1.pdf", "Title of this paper": "A Survey of Useful LLM Evaluation", "Authors": "Ji-Lun Peng, Sijia Cheng, Egil Diau, Yung-Yu Shih, Po-Heng Chen, Yen-Ting Lin, Yun-Nung Chen", "Date published": "06/03/2024", "URL": "http://arxiv.org/abs/2406.00936v1", "summary": "LLMs have gotten attention across various research domains due to their\nexceptional performance on a wide range of complex tasks. Therefore, refined\nmethods to evaluate the capabilities of LLMs are needed to determine the tasks\nand responsibility they should undertake. Our study mainly discussed how LLMs,\nas useful tools, should be effectively assessed. We proposed the two-stage\nframework: from ``core ability'' to ``agent'', clearly explaining how LLMs can\nbe applied based on their specific capabilities, along with the evaluation\nmethods in each stage. Core ability refers to the capabilities that LLMs need\nin order to generate high-quality natural language texts. After confirming LLMs\npossess core ability, they can solve real-world and complex tasks as agent. In\nthe \"core ability\" stage, we discussed the reasoning ability, societal impact,\nand domain knowledge of LLMs. In the ``agent'' stage, we demonstrated embodied\naction, planning, and tool learning of LLMs agent applications. Finally, we\nexamined the challenges currently confronting the evaluation methods for LLMs,\nas well as the directions for future development."}, "hash": "5fe3e7b9a964b71e52d51ffdb5bb225c614692ea67fd0ffd957c84d3029c0ad4", "class_name": "RelatedNodeInfo"}}, "text": "Additionally, it should be accompanied by continu-\nous human oversight, guidance, and the application\nof critical thinking.\n3 Agent Evaluation\nBuilding upon LLM\u2019s core abilities, there has been\na growing research area that employs LLMs as\ncentral controllers to construct autonomous agents\nto obtain human-like decision-making capabilities\n(Wang et al., 2024b).\nIn this section, we\u2019ll first discuss the methods used\nto assess LLM agents\u2019 capabilities of planning.\nAnd also introduce the evaluation based on var-ious application scenarios. Each subsection will\nprovide detailed insights into the applications of\nLLMs, the methodologies used for evaluation, and\nthe datasets employed.\n3.1 Planning\nPlanning by an agent involves the strategic formu-\nlation and execution of actions or steps to achieve\nspecific goals or outcomes within a given environ-\nment, typically using algorithms or models to pre-\ndict and decide the best course of action.\nFacing the challenge of executing complex tasks\nthat require decomposition into simpler subtasks,\nrobot planning empowers robots to autonomously\nidentify and execute actions towards achieving spe-\ncific goals, taking into account their surroundings\nand objectives. In this context, several innovative\napproaches, such as (Huang et al., 2022a; Singh\net al., 2023; Song et al., 2023a) harness the exten-\nsive commonsense knowledge available through\nLLMs, enabling these models to efficiently seg-\nment tasks into manageable subtasks. The Inner\nMonologue (Huang et al., 2022b) system utilizes\nLLMs for dynamic planning in robotic tasks by\nintegrating continuous natural language feedback.\nSimilarly, SayPlan (Rana et al., 2023) enhances\ntask planning capabilities of LLMs by grounding\nthem with 3D Scene Graphs to facilitate exten-\nsive environmental interactions. These methods\nare evaluated across virtual environments, embod-\nied agents, and physical robots. Moreover, several\nworks like DEPS (Wang et al., 2023b), AdaPlan-\nner (Sun et al., 2023), and Robots That Ask For\nHelp (Ren et al., 2023), introduce dynamic ele-\nments of interactive re-planning, adaptive strate-\ngies, and the ability to seek assistance when faced\nwith uncertainties. These developments are pivotal\nfor the practical application and effectiveness of\nrobotics in real-world settings, illustrating a signifi-\ncant stride towards more adaptable and intelligent\nrobotic systems. They are evaluated in increasingly\ncomplex situations that closely mirror real-life con-\nditions.\nAn LLM-based agent employs LLMs to analyze\nand generate human-like text, aiding in decision-\nmaking and strategic planning by processing vast\namounts of information quickly and accurately.\nReact (Yao et al., 2023b) presents a paradigm\nthat synergistically blends reasoning and action\nwithin language models, enhancing performance\nand interpretability across various decision-making\ntasks, as evidenced by benchmarks in ALFWorld\n10", "mimetype": "text/plain", "start_char_idx": 1900, "end_char_idx": 4822, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "005ab259-0cc2-4005-ba60-943d1c38eaab": {"__data__": {"id_": "005ab259-0cc2-4005-ba60-943d1c38eaab", "embedding": null, "metadata": {"page_label": "11", "file_name": "2406_00936v1.pdf", "Title of this paper": "A Survey of Useful LLM Evaluation", "Authors": "Ji-Lun Peng, Sijia Cheng, Egil Diau, Yung-Yu Shih, Po-Heng Chen, Yen-Ting Lin, Yun-Nung Chen", "Date published": "06/03/2024", "URL": "http://arxiv.org/abs/2406.00936v1", "summary": "LLMs have gotten attention across various research domains due to their\nexceptional performance on a wide range of complex tasks. Therefore, refined\nmethods to evaluate the capabilities of LLMs are needed to determine the tasks\nand responsibility they should undertake. Our study mainly discussed how LLMs,\nas useful tools, should be effectively assessed. We proposed the two-stage\nframework: from ``core ability'' to ``agent'', clearly explaining how LLMs can\nbe applied based on their specific capabilities, along with the evaluation\nmethods in each stage. Core ability refers to the capabilities that LLMs need\nin order to generate high-quality natural language texts. After confirming LLMs\npossess core ability, they can solve real-world and complex tasks as agent. In\nthe \"core ability\" stage, we discussed the reasoning ability, societal impact,\nand domain knowledge of LLMs. In the ``agent'' stage, we demonstrated embodied\naction, planning, and tool learning of LLMs agent applications. Finally, we\nexamined the challenges currently confronting the evaluation methods for LLMs,\nas well as the directions for future development."}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c29d64c4-4e80-435d-b7cc-1f6f7a36fc05", "node_type": "4", "metadata": {"page_label": "11", "file_name": "2406_00936v1.pdf", "Title of this paper": "A Survey of Useful LLM Evaluation", "Authors": "Ji-Lun Peng, Sijia Cheng, Egil Diau, Yung-Yu Shih, Po-Heng Chen, Yen-Ting Lin, Yun-Nung Chen", "Date published": "06/03/2024", "URL": "http://arxiv.org/abs/2406.00936v1", "summary": "LLMs have gotten attention across various research domains due to their\nexceptional performance on a wide range of complex tasks. Therefore, refined\nmethods to evaluate the capabilities of LLMs are needed to determine the tasks\nand responsibility they should undertake. Our study mainly discussed how LLMs,\nas useful tools, should be effectively assessed. We proposed the two-stage\nframework: from ``core ability'' to ``agent'', clearly explaining how LLMs can\nbe applied based on their specific capabilities, along with the evaluation\nmethods in each stage. Core ability refers to the capabilities that LLMs need\nin order to generate high-quality natural language texts. After confirming LLMs\npossess core ability, they can solve real-world and complex tasks as agent. In\nthe \"core ability\" stage, we discussed the reasoning ability, societal impact,\nand domain knowledge of LLMs. In the ``agent'' stage, we demonstrated embodied\naction, planning, and tool learning of LLMs agent applications. Finally, we\nexamined the challenges currently confronting the evaluation methods for LLMs,\nas well as the directions for future development."}, "hash": "53c127581da8f5736b7ef648789900773dd0a2f8629e7eaa47c8c1490f551d7e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ef399f6f-3a54-4ebc-ae03-965ddedd7899", "node_type": "1", "metadata": {}, "hash": "eaf85973756a3fa9eb2d25cdd8c3a492952f5e7d1705d1cd6e684e5f48ff2580", "class_name": "RelatedNodeInfo"}}, "text": "Agent Evaluation\n(Sec. 3)Planning\n(Sec. 3.1)Song et al. (2023a), Huang et al. (2022b), Yao et al. (2023b), Shinn et al. (2023)\nApplication Scenarios\n(Sec. 3.2)Web Grounding Nakano et al. (2022), Qin et al. (2023a), Yao et al. (2023a)\nCode Generation Liang et al. (2023), Zhang et al. (2024a)\nDatabase Queries Hu et al. (2023)\nAPI Calls Li et al. (2023a), Qin et al. (2023b), Yan et al. (2024b)\nTool Creation Cai et al. (2024), Qian et al. (2023)\nRobotic Navigation Shah et al. (2022), Zhou et al. (2023a), Zheng et al. (2023a)\nRobotic Manipulation Huang et al. (2023), Yu et al. (2023)\nBenchmark\n(Sec. 3.3)Ruan et al. (2023), Li et al. (2023a), Tang et al. (2023)\nFigure 3: The overview of agent evaluation.\nand WebShop. Reflexion (Shinn et al., 2023) in-\ntroduces a groundbreaking framework that em-\nploys verbal feedback for reinforcement learn-\ning, enabling language agents to refine their skills\nthrough self-reflection without updating model\nweights. This method is evaluated across di-\nverse decision-making, reasoning, and program-\nming tasks, demonstrating marked enhancements\nover traditional approaches in environments such\nas AlfWorld, HotPotQA, and HumanEval. Self-\nCheck (Miao et al., 2023) offers a zero-shot mecha-\nnism that empowers LLMs to autonomously verify\ntheir multi-step reasoning in math problem-solving,\nwhich significantly boosts accuracy on benchmarks\nincluding GSM8K, MathQA, and MATH by filter-\ning out low-confidence solutions.\n3.2 Application Scenarios\n3.2.1 Web Grounding\nIn this section, we focus on LLMs performing tasks\nin web environments. We categorize the evaluation\nmethods based on tasks.\nSearch Engine WebGPT Nakano et al. (2022)\ndeveloped a text-based web-browsing environment,\nenabling interaction with a fine-tuned language\nmodel to generate more faithful outputs. Evalu-\nation of WebGPT models is conducted through\nthree main approaches: comparison with answers\nauthored by human demonstrators on a held-out set\nof questions, comparison with the highest-voted an-\nswers from the ELI5 dataset, and evaluation using\nthe TruthfulQA dataset.\nWebCPM Qin et al. (2023a) employs tool learn-\ning to enable models to answer long-form questions\nthrough web searches.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2202, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ef399f6f-3a54-4ebc-ae03-965ddedd7899": {"__data__": {"id_": "ef399f6f-3a54-4ebc-ae03-965ddedd7899", "embedding": null, "metadata": {"page_label": "11", "file_name": "2406_00936v1.pdf", "Title of this paper": "A Survey of Useful LLM Evaluation", "Authors": "Ji-Lun Peng, Sijia Cheng, Egil Diau, Yung-Yu Shih, Po-Heng Chen, Yen-Ting Lin, Yun-Nung Chen", "Date published": "06/03/2024", "URL": "http://arxiv.org/abs/2406.00936v1", "summary": "LLMs have gotten attention across various research domains due to their\nexceptional performance on a wide range of complex tasks. Therefore, refined\nmethods to evaluate the capabilities of LLMs are needed to determine the tasks\nand responsibility they should undertake. Our study mainly discussed how LLMs,\nas useful tools, should be effectively assessed. We proposed the two-stage\nframework: from ``core ability'' to ``agent'', clearly explaining how LLMs can\nbe applied based on their specific capabilities, along with the evaluation\nmethods in each stage. Core ability refers to the capabilities that LLMs need\nin order to generate high-quality natural language texts. After confirming LLMs\npossess core ability, they can solve real-world and complex tasks as agent. In\nthe \"core ability\" stage, we discussed the reasoning ability, societal impact,\nand domain knowledge of LLMs. In the ``agent'' stage, we demonstrated embodied\naction, planning, and tool learning of LLMs agent applications. Finally, we\nexamined the challenges currently confronting the evaluation methods for LLMs,\nas well as the directions for future development."}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c29d64c4-4e80-435d-b7cc-1f6f7a36fc05", "node_type": "4", "metadata": {"page_label": "11", "file_name": "2406_00936v1.pdf", "Title of this paper": "A Survey of Useful LLM Evaluation", "Authors": "Ji-Lun Peng, Sijia Cheng, Egil Diau, Yung-Yu Shih, Po-Heng Chen, Yen-Ting Lin, Yun-Nung Chen", "Date published": "06/03/2024", "URL": "http://arxiv.org/abs/2406.00936v1", "summary": "LLMs have gotten attention across various research domains due to their\nexceptional performance on a wide range of complex tasks. Therefore, refined\nmethods to evaluate the capabilities of LLMs are needed to determine the tasks\nand responsibility they should undertake. Our study mainly discussed how LLMs,\nas useful tools, should be effectively assessed. We proposed the two-stage\nframework: from ``core ability'' to ``agent'', clearly explaining how LLMs can\nbe applied based on their specific capabilities, along with the evaluation\nmethods in each stage. Core ability refers to the capabilities that LLMs need\nin order to generate high-quality natural language texts. After confirming LLMs\npossess core ability, they can solve real-world and complex tasks as agent. In\nthe \"core ability\" stage, we discussed the reasoning ability, societal impact,\nand domain knowledge of LLMs. In the ``agent'' stage, we demonstrated embodied\naction, planning, and tool learning of LLMs agent applications. Finally, we\nexamined the challenges currently confronting the evaluation methods for LLMs,\nas well as the directions for future development."}, "hash": "53c127581da8f5736b7ef648789900773dd0a2f8629e7eaa47c8c1490f551d7e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "005ab259-0cc2-4005-ba60-943d1c38eaab", "node_type": "1", "metadata": {"page_label": "11", "file_name": "2406_00936v1.pdf", "Title of this paper": "A Survey of Useful LLM Evaluation", "Authors": "Ji-Lun Peng, Sijia Cheng, Egil Diau, Yung-Yu Shih, Po-Heng Chen, Yen-Ting Lin, Yun-Nung Chen", "Date published": "06/03/2024", "URL": "http://arxiv.org/abs/2406.00936v1", "summary": "LLMs have gotten attention across various research domains due to their\nexceptional performance on a wide range of complex tasks. Therefore, refined\nmethods to evaluate the capabilities of LLMs are needed to determine the tasks\nand responsibility they should undertake. Our study mainly discussed how LLMs,\nas useful tools, should be effectively assessed. We proposed the two-stage\nframework: from ``core ability'' to ``agent'', clearly explaining how LLMs can\nbe applied based on their specific capabilities, along with the evaluation\nmethods in each stage. Core ability refers to the capabilities that LLMs need\nin order to generate high-quality natural language texts. After confirming LLMs\npossess core ability, they can solve real-world and complex tasks as agent. In\nthe \"core ability\" stage, we discussed the reasoning ability, societal impact,\nand domain knowledge of LLMs. In the ``agent'' stage, we demonstrated embodied\naction, planning, and tool learning of LLMs agent applications. Finally, we\nexamined the challenges currently confronting the evaluation methods for LLMs,\nas well as the directions for future development."}, "hash": "721952848493eb59aac8739a6b152975011ef85f64f410876e6e4da49dea77e0", "class_name": "RelatedNodeInfo"}}, "text": "3.2 Application Scenarios\n3.2.1 Web Grounding\nIn this section, we focus on LLMs performing tasks\nin web environments. We categorize the evaluation\nmethods based on tasks.\nSearch Engine WebGPT Nakano et al. (2022)\ndeveloped a text-based web-browsing environment,\nenabling interaction with a fine-tuned language\nmodel to generate more faithful outputs. Evalu-\nation of WebGPT models is conducted through\nthree main approaches: comparison with answers\nauthored by human demonstrators on a held-out set\nof questions, comparison with the highest-voted an-\nswers from the ELI5 dataset, and evaluation using\nthe TruthfulQA dataset.\nWebCPM Qin et al. (2023a) employs tool learn-\ning to enable models to answer long-form questions\nthrough web searches. Its evaluation encompasses\nfour sub-tasks: action prediction, search query gen-\neration, supporting fact extraction, and informationsynthesis, with each task independently assessed\nusing Micro-F1 and Macro-F1 for action prediction\nand Rouge-L for other three tasks including text\ngeneration. In holistic evaluation, eight annotators\nmanually compare the model-generated answers\nbased on human preference.\nOnlineshopping WebShop (Yao et al., 2023a) in-\ntroduces a benchmark for assessing LLM-based\nagents\u2019 abilities in product search and retrieval.\nTheir dataset, comprising 12,087 instructions, is\ndivided into 10,587 for training, 1,000 for devel-\nopment, and 500 for testing, with human shopping\npaths recorded for each instance. Evaluation met-\nrics include task score and success rate, revealing\nthat humans outperform LLMs across all measures.\n3.2.2 Code Generation\nTo enable nuanced control in robots for complex\nreal-world tasks, the Code as Policies (Liang et al.,\n2023) paradigm uses LLMs to generate policy\ncode for spatial reasoning and adapting to new\ninstructions. The code quality is assessed with\nHumanEval and RoboCodeGen. RoboCodeGen,\na benchmark with 37 function generation tasks,\nfocuses on spatial and geometric reasoning and\ncontrol, supports third-party libraries like NumPy,\nlacks documentation strings and type hints, and\npermits undefined functions for hierarchical code\ngeneration. The evaluation metric is the pass rate\nof generated code that passes manually written unit\ntests.\nThe CODEAGENTBENCH benchmark Zhang\net al. (2024a) is designed to evaluate LLMs in real-\nworld repo-level code generation tasks. It provides\ncomprehensive input information, such as docu-\nmentation, code dependencies, and runtime envi-\nronment details, challenging LLMs to produce ac-\n11", "mimetype": "text/plain", "start_char_idx": 1459, "end_char_idx": 3995, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "905987af-b042-4707-855a-576549b66d90": {"__data__": {"id_": "905987af-b042-4707-855a-576549b66d90", "embedding": null, "metadata": {"page_label": "12", "file_name": "2406_00936v1.pdf", "Title of this paper": "A Survey of Useful LLM Evaluation", "Authors": "Ji-Lun Peng, Sijia Cheng, Egil Diau, Yung-Yu Shih, Po-Heng Chen, Yen-Ting Lin, Yun-Nung Chen", "Date published": "06/03/2024", "URL": "http://arxiv.org/abs/2406.00936v1", "summary": "LLMs have gotten attention across various research domains due to their\nexceptional performance on a wide range of complex tasks. Therefore, refined\nmethods to evaluate the capabilities of LLMs are needed to determine the tasks\nand responsibility they should undertake. Our study mainly discussed how LLMs,\nas useful tools, should be effectively assessed. We proposed the two-stage\nframework: from ``core ability'' to ``agent'', clearly explaining how LLMs can\nbe applied based on their specific capabilities, along with the evaluation\nmethods in each stage. Core ability refers to the capabilities that LLMs need\nin order to generate high-quality natural language texts. After confirming LLMs\npossess core ability, they can solve real-world and complex tasks as agent. In\nthe \"core ability\" stage, we discussed the reasoning ability, societal impact,\nand domain knowledge of LLMs. In the ``agent'' stage, we demonstrated embodied\naction, planning, and tool learning of LLMs agent applications. Finally, we\nexamined the challenges currently confronting the evaluation methods for LLMs,\nas well as the directions for future development."}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5063287c-0de4-48b1-ac66-5b850b96c8f8", "node_type": "4", "metadata": {"page_label": "12", "file_name": "2406_00936v1.pdf", "Title of this paper": "A Survey of Useful LLM Evaluation", "Authors": "Ji-Lun Peng, Sijia Cheng, Egil Diau, Yung-Yu Shih, Po-Heng Chen, Yen-Ting Lin, Yun-Nung Chen", "Date published": "06/03/2024", "URL": "http://arxiv.org/abs/2406.00936v1", "summary": "LLMs have gotten attention across various research domains due to their\nexceptional performance on a wide range of complex tasks. Therefore, refined\nmethods to evaluate the capabilities of LLMs are needed to determine the tasks\nand responsibility they should undertake. Our study mainly discussed how LLMs,\nas useful tools, should be effectively assessed. We proposed the two-stage\nframework: from ``core ability'' to ``agent'', clearly explaining how LLMs can\nbe applied based on their specific capabilities, along with the evaluation\nmethods in each stage. Core ability refers to the capabilities that LLMs need\nin order to generate high-quality natural language texts. After confirming LLMs\npossess core ability, they can solve real-world and complex tasks as agent. In\nthe \"core ability\" stage, we discussed the reasoning ability, societal impact,\nand domain knowledge of LLMs. In the ``agent'' stage, we demonstrated embodied\naction, planning, and tool learning of LLMs agent applications. Finally, we\nexamined the challenges currently confronting the evaluation methods for LLMs,\nas well as the directions for future development."}, "hash": "02732d9bad2b24995ef6fa46378ac443f1023bfd85eaab9825f5e4b2398391f2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9912b965-bee8-464d-980a-d05ddb223b69", "node_type": "1", "metadata": {}, "hash": "b1cb9c5b4c27d1572ae6548cc0e2fdbba17ea6dbc92fb59bf35998eddaa44577", "class_name": "RelatedNodeInfo"}}, "text": "curate and well-integrated code solutions.\n3.2.3 Database Queries\nIntegrating external databases or knowledge bases\nallows agents to access specific domain informa-\ntion, resulting in more realistic actions. For exam-\nple, ChatDB (Hu et al., 2023) uses SQL statements\nto query databases, enabling logical actions by the\nagents. They created a dataset of 70 records from\nfruit shop management logs for evaluation. The ex-\nperiment clearly demonstrates that ChatDB outper-\nforms ChatGPT with significantly higher accuracy.\n3.2.4 API Calls\nLLM agents can also enhance their capabilities\nby calling APIs. API-Bank, as introduced by Li\net al. (2023a), provides a specialized benchmark to\nevaluate tool-augmented LLM performance. This\nbenchmark includes 53 standard API tools, a de-\ntailed workflow for tool-augmented LLMs, and a\ndataset with 264 annotated dialogues. Evaluation\nmetrics involve accuracy of API calls and ROUGE-\nL for post-call responses, with task planning ef-\nficacy measured by the successful completion of\nplanned tasks through model-driven API calls.\nQin et al. (2023b) undertake a scholarly inquiry\ninto the utilization of tool learning within contem-\nporary Language Models (LLMs), delving into\nboth their effectiveness and limitations. They eval-\nuate 18 representative tools across six tasks using\nexisting datasets and extend their study to 12 addi-\ntional tasks, such as slide-making, AI painting, and\n3D model construction. They augment user queries\ngenerated by ChatGPT and manually assess the\nsuccess rates of these operations.\nThe Berkeley Function-Calling Leaderboard\n(BFCL) Yan et al. (2024b) evaluates LLMs on func-\ntion processing, syntax tree analysis, and function\nexecution across various scenarios. It features an\ninteractive comparison tool and a dataset cover-\ning fields like Mathematics, Sports, and Finance.\nEvaluations include Simple, Multiple, and Paral-\nlel Function tests. BFCL aids the integration of\nLLMs into platforms like Langchain and AutoGPT,\nproviding detailed analyses on cost and latency for\nmodels like GPT-4.\n3.2.5 Tool Creation\nThe usage of tools is contingent upon the acces-\nsibility of external tools Schick et al. (2023). Re-\ncently, efforts have been made to employ LLM as\na tool creator in order to generate tools that canbe utilized for diverse requests(Ruan et al. (2023)).\nLATM (Cai et al., 2024) utilizes GPT-4 to develop\ntools, demonstrating that more cost-effective mod-\nels can achieve comparable performance to larger\nmodels in these applications. They employ six\ndatasets from various domains: logic reasoning, ob-\nject tracking, Dyck language, word sequencing, the\nChinese remainder theorem, and meeting schedul-\ning. The first five datasets are sourced from Big-\nBench (Srivastava et al., 2023), while the meeting\nscheduling task is specifically designed to show-\ncase the model\u2019s real-world utility.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2872, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9912b965-bee8-464d-980a-d05ddb223b69": {"__data__": {"id_": "9912b965-bee8-464d-980a-d05ddb223b69", "embedding": null, "metadata": {"page_label": "12", "file_name": "2406_00936v1.pdf", "Title of this paper": "A Survey of Useful LLM Evaluation", "Authors": "Ji-Lun Peng, Sijia Cheng, Egil Diau, Yung-Yu Shih, Po-Heng Chen, Yen-Ting Lin, Yun-Nung Chen", "Date published": "06/03/2024", "URL": "http://arxiv.org/abs/2406.00936v1", "summary": "LLMs have gotten attention across various research domains due to their\nexceptional performance on a wide range of complex tasks. Therefore, refined\nmethods to evaluate the capabilities of LLMs are needed to determine the tasks\nand responsibility they should undertake. Our study mainly discussed how LLMs,\nas useful tools, should be effectively assessed. We proposed the two-stage\nframework: from ``core ability'' to ``agent'', clearly explaining how LLMs can\nbe applied based on their specific capabilities, along with the evaluation\nmethods in each stage. Core ability refers to the capabilities that LLMs need\nin order to generate high-quality natural language texts. After confirming LLMs\npossess core ability, they can solve real-world and complex tasks as agent. In\nthe \"core ability\" stage, we discussed the reasoning ability, societal impact,\nand domain knowledge of LLMs. In the ``agent'' stage, we demonstrated embodied\naction, planning, and tool learning of LLMs agent applications. Finally, we\nexamined the challenges currently confronting the evaluation methods for LLMs,\nas well as the directions for future development."}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5063287c-0de4-48b1-ac66-5b850b96c8f8", "node_type": "4", "metadata": {"page_label": "12", "file_name": "2406_00936v1.pdf", "Title of this paper": "A Survey of Useful LLM Evaluation", "Authors": "Ji-Lun Peng, Sijia Cheng, Egil Diau, Yung-Yu Shih, Po-Heng Chen, Yen-Ting Lin, Yun-Nung Chen", "Date published": "06/03/2024", "URL": "http://arxiv.org/abs/2406.00936v1", "summary": "LLMs have gotten attention across various research domains due to their\nexceptional performance on a wide range of complex tasks. Therefore, refined\nmethods to evaluate the capabilities of LLMs are needed to determine the tasks\nand responsibility they should undertake. Our study mainly discussed how LLMs,\nas useful tools, should be effectively assessed. We proposed the two-stage\nframework: from ``core ability'' to ``agent'', clearly explaining how LLMs can\nbe applied based on their specific capabilities, along with the evaluation\nmethods in each stage. Core ability refers to the capabilities that LLMs need\nin order to generate high-quality natural language texts. After confirming LLMs\npossess core ability, they can solve real-world and complex tasks as agent. In\nthe \"core ability\" stage, we discussed the reasoning ability, societal impact,\nand domain knowledge of LLMs. In the ``agent'' stage, we demonstrated embodied\naction, planning, and tool learning of LLMs agent applications. Finally, we\nexamined the challenges currently confronting the evaluation methods for LLMs,\nas well as the directions for future development."}, "hash": "02732d9bad2b24995ef6fa46378ac443f1023bfd85eaab9825f5e4b2398391f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "905987af-b042-4707-855a-576549b66d90", "node_type": "1", "metadata": {"page_label": "12", "file_name": "2406_00936v1.pdf", "Title of this paper": "A Survey of Useful LLM Evaluation", "Authors": "Ji-Lun Peng, Sijia Cheng, Egil Diau, Yung-Yu Shih, Po-Heng Chen, Yen-Ting Lin, Yun-Nung Chen", "Date published": "06/03/2024", "URL": "http://arxiv.org/abs/2406.00936v1", "summary": "LLMs have gotten attention across various research domains due to their\nexceptional performance on a wide range of complex tasks. Therefore, refined\nmethods to evaluate the capabilities of LLMs are needed to determine the tasks\nand responsibility they should undertake. Our study mainly discussed how LLMs,\nas useful tools, should be effectively assessed. We proposed the two-stage\nframework: from ``core ability'' to ``agent'', clearly explaining how LLMs can\nbe applied based on their specific capabilities, along with the evaluation\nmethods in each stage. Core ability refers to the capabilities that LLMs need\nin order to generate high-quality natural language texts. After confirming LLMs\npossess core ability, they can solve real-world and complex tasks as agent. In\nthe \"core ability\" stage, we discussed the reasoning ability, societal impact,\nand domain knowledge of LLMs. In the ``agent'' stage, we demonstrated embodied\naction, planning, and tool learning of LLMs agent applications. Finally, we\nexamined the challenges currently confronting the evaluation methods for LLMs,\nas well as the directions for future development."}, "hash": "5dd9de524878ab9510be4707360d326a1d0f6b67810711014834d7cf26c86dc8", "class_name": "RelatedNodeInfo"}}, "text": "3.2.5 Tool Creation\nThe usage of tools is contingent upon the acces-\nsibility of external tools Schick et al. (2023). Re-\ncently, efforts have been made to employ LLM as\na tool creator in order to generate tools that canbe utilized for diverse requests(Ruan et al. (2023)).\nLATM (Cai et al., 2024) utilizes GPT-4 to develop\ntools, demonstrating that more cost-effective mod-\nels can achieve comparable performance to larger\nmodels in these applications. They employ six\ndatasets from various domains: logic reasoning, ob-\nject tracking, Dyck language, word sequencing, the\nChinese remainder theorem, and meeting schedul-\ning. The first five datasets are sourced from Big-\nBench (Srivastava et al., 2023), while the meeting\nscheduling task is specifically designed to show-\ncase the model\u2019s real-world utility. CREATOR\n(Qian et al., 2023) evaluates LLMs\u2019 ability to create\ntools using the Creation Challenge dataset, which\nincludes 2,000 novel and challenging problems that\nexisting tools or code packages cannot adequately\nsolve. Evaluations demonstrate that ChatGPT\u2019s\ntool-making performance improves with additional\nhints, achieving up to 75.5% accuracy, highlighting\nthe importance of tool creation in enhancing LLM\nproblem-solving capabilities.\n3.2.6 Robotic Navigation\nNavigation by an embodied agent involves the au-\ntonomous movement and decision-making of a\nrobotic or virtual entity within a physical or simu-\nlated environment, using sensors and algorithms to\nperceive surroundings, plan routes, and accomplish\nnavigational tasks.\nLM-Nav (Shah et al., 2022) proposed a system\nfor robotic navigation that utilizes LLM, VLM,\nvisual navigation model (VNM), and robotic navi-\ngation\u2014enabling a robot to navigate complex envi-\nronments using natural language instructions with-\nout needing specific training data annotated with\nlanguage descriptions. They benchmark on 20\nqueries, in environments of varying difficulty, cor-\nresponding to a total combined length of over 6 km.\nLFG (Shah et al., 2023) leverages language mod-\nels as heuristics to enhance planning algorithms,\nguiding robots through unfamiliar environments\nusing semantic cues from natural language descrip-\ntions. They evaluate navigational performance on\nObjectNav.\nNavGPT (Zhou et al., 2023a) utilizes LLMs to\nperform explicit reasoning and planning. This ap-\nproach incorporates textual descriptions of visual\nobservations, navigation history, and potential fu-\nture paths to enhance navigation tasks. Following\nthis, the NaviLLM model (Zheng et al., 2023a)\nemerges as a versatile solution for embodied navi-\ngation. It adeptly tailors LLMs to manage a wide\n12", "mimetype": "text/plain", "start_char_idx": 2063, "end_char_idx": 4697, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2eb7af28-b022-4292-8cd3-10c5e2cd4814": {"__data__": {"id_": "2eb7af28-b022-4292-8cd3-10c5e2cd4814", "embedding": null, "metadata": {"page_label": "13", "file_name": "2406_00936v1.pdf", "Title of this paper": "A Survey of Useful LLM Evaluation", "Authors": "Ji-Lun Peng, Sijia Cheng, Egil Diau, Yung-Yu Shih, Po-Heng Chen, Yen-Ting Lin, Yun-Nung Chen", "Date published": "06/03/2024", "URL": "http://arxiv.org/abs/2406.00936v1", "summary": "LLMs have gotten attention across various research domains due to their\nexceptional performance on a wide range of complex tasks. Therefore, refined\nmethods to evaluate the capabilities of LLMs are needed to determine the tasks\nand responsibility they should undertake. Our study mainly discussed how LLMs,\nas useful tools, should be effectively assessed. We proposed the two-stage\nframework: from ``core ability'' to ``agent'', clearly explaining how LLMs can\nbe applied based on their specific capabilities, along with the evaluation\nmethods in each stage. Core ability refers to the capabilities that LLMs need\nin order to generate high-quality natural language texts. After confirming LLMs\npossess core ability, they can solve real-world and complex tasks as agent. In\nthe \"core ability\" stage, we discussed the reasoning ability, societal impact,\nand domain knowledge of LLMs. In the ``agent'' stage, we demonstrated embodied\naction, planning, and tool learning of LLMs agent applications. Finally, we\nexamined the challenges currently confronting the evaluation methods for LLMs,\nas well as the directions for future development."}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "54c4af3d-4982-41f9-9b7b-2decb7dfbd22", "node_type": "4", "metadata": {"page_label": "13", "file_name": "2406_00936v1.pdf", "Title of this paper": "A Survey of Useful LLM Evaluation", "Authors": "Ji-Lun Peng, Sijia Cheng, Egil Diau, Yung-Yu Shih, Po-Heng Chen, Yen-Ting Lin, Yun-Nung Chen", "Date published": "06/03/2024", "URL": "http://arxiv.org/abs/2406.00936v1", "summary": "LLMs have gotten attention across various research domains due to their\nexceptional performance on a wide range of complex tasks. Therefore, refined\nmethods to evaluate the capabilities of LLMs are needed to determine the tasks\nand responsibility they should undertake. Our study mainly discussed how LLMs,\nas useful tools, should be effectively assessed. We proposed the two-stage\nframework: from ``core ability'' to ``agent'', clearly explaining how LLMs can\nbe applied based on their specific capabilities, along with the evaluation\nmethods in each stage. Core ability refers to the capabilities that LLMs need\nin order to generate high-quality natural language texts. After confirming LLMs\npossess core ability, they can solve real-world and complex tasks as agent. In\nthe \"core ability\" stage, we discussed the reasoning ability, societal impact,\nand domain knowledge of LLMs. In the ``agent'' stage, we demonstrated embodied\naction, planning, and tool learning of LLMs agent applications. Finally, we\nexamined the challenges currently confronting the evaluation methods for LLMs,\nas well as the directions for future development."}, "hash": "e7e59fd0402307e2bb258d8d91edb0cc3806db154dcb5660029826f551bd8303", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b1610f7f-1c90-4b0b-8dbc-64bf701fb59f", "node_type": "1", "metadata": {}, "hash": "aabe9acdec3848ba0a738babd99e14281047d79466b20661e64cfa0d180fa209", "class_name": "RelatedNodeInfo"}}, "text": "spectrum of embodied navigation challenges by em-\nploying schema-based instructions that transform\ndisparate tasks into unified generative modeling\nproblems. The performance of these models is rig-\norously assessed using vision-language navigation\n(VLN) benchmarks, such as R2R, Reverie, CVDN,\nand SOON.\n3.2.7 Robotic Manipulation\nManipulation involves the use of embodied agent\nto interact with and manipulate physical objects in\ntheir environment, enabling tasks ranging from sim-\nple pick-and-place operations to complex assembly\nprocesses.\nV oxPoser (Huang et al., 2023) presents an inno-\nvative approach where the key novelty is the use of\nLLMs not just for understanding natural language\ninstructions, but crucially, for generating code that\ninteracts with VLMs to create detailed 3D value\nmaps. These maps guide robotic actions, bridging\nthe gap between abstract instructions and physi-\ncal execution. They directly evaluate the result on\nthe success rate of robot manipulation tasks. L2R\n(Yu et al., 2023) presents a method for translating\nlanguage instructions into reward functions using\nLLMs that robots can optimize to execute specific\ntasks, demonstrating this approach with a variety\nof complex locomotion and manipulation tasks in\nsimulated environments.\n3.3 Benchmark\nThe evaluation LLMs\u2019 capability on tool manip-\nulation primarily revolves around assessing the\nefficacy of a single tool, gauging its impact on\ndownstream tasks using established benchmarks,\nas discussed previously. However, an increasing\nnumber of researchers are shifting their focus to-\nwards scenarios that involve the combined use of\nmultiple tools to evaluate the performance of LLMs\ntrained with tool learning. This approach ensures a\nmore comprehensive and diverse appraisal of the\nmodel\u2019s abilities and constraints across various tool\nsets.\nAPIBench (Patil et al., 2023) assembles a com-\nprehensive API corpus from major hubs like Hug-\ngingFace, TorchHub, and TensorHub, including all\nAPI calls from TorchHub and TensorHub and the\ntop 20 most downloaded models from each Hug-\ngingFace task category. Using Self-Instruct (Wang\net al., 2023a), they create 10 synthetic user prompts\nper API to evaluate LLMs for functional correct-\nness and hallucination issues.ToolBench, developed by Xu et al. (2023c), eval-\nuates LLMs\u2019 generalization and advanced reason-\ning skills across various tool-based tasks. It inte-\ngrates existing and newly collected datasets, featur-\ning eight tasks with about 100 test cases each.\nBased on ToolBench, ToolLLM (Qin et al.,\n2023c) introduces ToolEval, an automatic evalu-\nator resembling a leaderboard. ToolEval uses two\nmetrics: pass rate, which measures the proportion\nof successfully completed instructions within lim-\nited attempts, and win rate, which compares perfor-\nmance against ChatGPT. This evaluation method\ncombines automatic and manual assessments while\nusing ChatGPT-generated solutions as a bench-\nmark, reducing potential human biases and unfair-\nness.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2990, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b1610f7f-1c90-4b0b-8dbc-64bf701fb59f": {"__data__": {"id_": "b1610f7f-1c90-4b0b-8dbc-64bf701fb59f", "embedding": null, "metadata": {"page_label": "13", "file_name": "2406_00936v1.pdf", "Title of this paper": "A Survey of Useful LLM Evaluation", "Authors": "Ji-Lun Peng, Sijia Cheng, Egil Diau, Yung-Yu Shih, Po-Heng Chen, Yen-Ting Lin, Yun-Nung Chen", "Date published": "06/03/2024", "URL": "http://arxiv.org/abs/2406.00936v1", "summary": "LLMs have gotten attention across various research domains due to their\nexceptional performance on a wide range of complex tasks. Therefore, refined\nmethods to evaluate the capabilities of LLMs are needed to determine the tasks\nand responsibility they should undertake. Our study mainly discussed how LLMs,\nas useful tools, should be effectively assessed. We proposed the two-stage\nframework: from ``core ability'' to ``agent'', clearly explaining how LLMs can\nbe applied based on their specific capabilities, along with the evaluation\nmethods in each stage. Core ability refers to the capabilities that LLMs need\nin order to generate high-quality natural language texts. After confirming LLMs\npossess core ability, they can solve real-world and complex tasks as agent. In\nthe \"core ability\" stage, we discussed the reasoning ability, societal impact,\nand domain knowledge of LLMs. In the ``agent'' stage, we demonstrated embodied\naction, planning, and tool learning of LLMs agent applications. Finally, we\nexamined the challenges currently confronting the evaluation methods for LLMs,\nas well as the directions for future development."}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "54c4af3d-4982-41f9-9b7b-2decb7dfbd22", "node_type": "4", "metadata": {"page_label": "13", "file_name": "2406_00936v1.pdf", "Title of this paper": "A Survey of Useful LLM Evaluation", "Authors": "Ji-Lun Peng, Sijia Cheng, Egil Diau, Yung-Yu Shih, Po-Heng Chen, Yen-Ting Lin, Yun-Nung Chen", "Date published": "06/03/2024", "URL": "http://arxiv.org/abs/2406.00936v1", "summary": "LLMs have gotten attention across various research domains due to their\nexceptional performance on a wide range of complex tasks. Therefore, refined\nmethods to evaluate the capabilities of LLMs are needed to determine the tasks\nand responsibility they should undertake. Our study mainly discussed how LLMs,\nas useful tools, should be effectively assessed. We proposed the two-stage\nframework: from ``core ability'' to ``agent'', clearly explaining how LLMs can\nbe applied based on their specific capabilities, along with the evaluation\nmethods in each stage. Core ability refers to the capabilities that LLMs need\nin order to generate high-quality natural language texts. After confirming LLMs\npossess core ability, they can solve real-world and complex tasks as agent. In\nthe \"core ability\" stage, we discussed the reasoning ability, societal impact,\nand domain knowledge of LLMs. In the ``agent'' stage, we demonstrated embodied\naction, planning, and tool learning of LLMs agent applications. Finally, we\nexamined the challenges currently confronting the evaluation methods for LLMs,\nas well as the directions for future development."}, "hash": "e7e59fd0402307e2bb258d8d91edb0cc3806db154dcb5660029826f551bd8303", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2eb7af28-b022-4292-8cd3-10c5e2cd4814", "node_type": "1", "metadata": {"page_label": "13", "file_name": "2406_00936v1.pdf", "Title of this paper": "A Survey of Useful LLM Evaluation", "Authors": "Ji-Lun Peng, Sijia Cheng, Egil Diau, Yung-Yu Shih, Po-Heng Chen, Yen-Ting Lin, Yun-Nung Chen", "Date published": "06/03/2024", "URL": "http://arxiv.org/abs/2406.00936v1", "summary": "LLMs have gotten attention across various research domains due to their\nexceptional performance on a wide range of complex tasks. Therefore, refined\nmethods to evaluate the capabilities of LLMs are needed to determine the tasks\nand responsibility they should undertake. Our study mainly discussed how LLMs,\nas useful tools, should be effectively assessed. We proposed the two-stage\nframework: from ``core ability'' to ``agent'', clearly explaining how LLMs can\nbe applied based on their specific capabilities, along with the evaluation\nmethods in each stage. Core ability refers to the capabilities that LLMs need\nin order to generate high-quality natural language texts. After confirming LLMs\npossess core ability, they can solve real-world and complex tasks as agent. In\nthe \"core ability\" stage, we discussed the reasoning ability, societal impact,\nand domain knowledge of LLMs. In the ``agent'' stage, we demonstrated embodied\naction, planning, and tool learning of LLMs agent applications. Finally, we\nexamined the challenges currently confronting the evaluation methods for LLMs,\nas well as the directions for future development."}, "hash": "ea0010b8932ba569ac1ee980c31a32c216e3a6068064a1808862d1dacc668786", "class_name": "RelatedNodeInfo"}}, "text": "(2023c), eval-\nuates LLMs\u2019 generalization and advanced reason-\ning skills across various tool-based tasks. It inte-\ngrates existing and newly collected datasets, featur-\ning eight tasks with about 100 test cases each.\nBased on ToolBench, ToolLLM (Qin et al.,\n2023c) introduces ToolEval, an automatic evalu-\nator resembling a leaderboard. ToolEval uses two\nmetrics: pass rate, which measures the proportion\nof successfully completed instructions within lim-\nited attempts, and win rate, which compares perfor-\nmance against ChatGPT. This evaluation method\ncombines automatic and manual assessments while\nusing ChatGPT-generated solutions as a bench-\nmark, reducing potential human biases and unfair-\nness.\nToolAlpaca (Tang et al., 2023) expands the eval-\nuation framework to encompass real-world scenar-\nios. Using a training set of 426 tool uses, the study\nevaluates ten new tools across 100 evaluation in-\nstances. Following the ReAct style (Yao et al.,\n2023b), tool usage is integrated during text gen-\neration, with human reviewers assessing program\naccuracy and overall correctness.\nRestBench (Song et al., 2023b) explores real-\nworld user instructions using APIs, focusing on\nTMDB movie database and Spotify music player\nscenarios. It filters 54 and 40 commonly used\nAPIs respectively, constructing OpenAPI specifi-\ncations. Integrating RestGPT, which links LLMs\nwith RESTful APIs, it follows standard web ser-\nvice protocols. RestBench evaluates performance\nwith human-annotated instructions and gold solu-\ntion paths, demonstrating RestGPT\u2019s effectiveness\nin complex tasks and advancing towards Artificial\nGeneral Intelligence (AGI).\nWebArena (Zhou et al. (2023b)) offers an envi-\nronment with fully functional websites from four\ncommon domains: e-commerce, social forum dis-\ncussions, collaborative software development, and\ncontent management. Its purpose is to evaluate\nagents in an end-to-end fashion and determine the\naccuracy of their completed tasks.\nMIND2WEB (Deng et al. (2023)), is the first\ndataset for developing and evaluating generalist\nagents for the web that can follow language in-\nstructions to complete complex tasks on any web-\nsite. MIND2WEB boasts a collection of over 2,000\ntasks curated from 137 websites that span 31 dif-\nferent domains, replacing the oversimplified sim-\nulation environments commonly found in other\ndatasets with a realm of real-world websites.\n13", "mimetype": "text/plain", "start_char_idx": 2286, "end_char_idx": 4683, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a798d9b0-47ff-47fa-9d16-1798054f7200": {"__data__": {"id_": "a798d9b0-47ff-47fa-9d16-1798054f7200", "embedding": null, "metadata": {"page_label": "14", "file_name": "2406_00936v1.pdf", "Title of this paper": "A Survey of Useful LLM Evaluation", "Authors": "Ji-Lun Peng, Sijia Cheng, Egil Diau, Yung-Yu Shih, Po-Heng Chen, Yen-Ting Lin, Yun-Nung Chen", "Date published": "06/03/2024", "URL": "http://arxiv.org/abs/2406.00936v1", "summary": "LLMs have gotten attention across various research domains due to their\nexceptional performance on a wide range of complex tasks. Therefore, refined\nmethods to evaluate the capabilities of LLMs are needed to determine the tasks\nand responsibility they should undertake. Our study mainly discussed how LLMs,\nas useful tools, should be effectively assessed. We proposed the two-stage\nframework: from ``core ability'' to ``agent'', clearly explaining how LLMs can\nbe applied based on their specific capabilities, along with the evaluation\nmethods in each stage. Core ability refers to the capabilities that LLMs need\nin order to generate high-quality natural language texts. After confirming LLMs\npossess core ability, they can solve real-world and complex tasks as agent. In\nthe \"core ability\" stage, we discussed the reasoning ability, societal impact,\nand domain knowledge of LLMs. In the ``agent'' stage, we demonstrated embodied\naction, planning, and tool learning of LLMs agent applications. Finally, we\nexamined the challenges currently confronting the evaluation methods for LLMs,\nas well as the directions for future development."}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d4f8389b-10d6-41b7-b3f2-49feabf83e1e", "node_type": "4", "metadata": {"page_label": "14", "file_name": "2406_00936v1.pdf", "Title of this paper": "A Survey of Useful LLM Evaluation", "Authors": "Ji-Lun Peng, Sijia Cheng, Egil Diau, Yung-Yu Shih, Po-Heng Chen, Yen-Ting Lin, Yun-Nung Chen", "Date published": "06/03/2024", "URL": "http://arxiv.org/abs/2406.00936v1", "summary": "LLMs have gotten attention across various research domains due to their\nexceptional performance on a wide range of complex tasks. Therefore, refined\nmethods to evaluate the capabilities of LLMs are needed to determine the tasks\nand responsibility they should undertake. Our study mainly discussed how LLMs,\nas useful tools, should be effectively assessed. We proposed the two-stage\nframework: from ``core ability'' to ``agent'', clearly explaining how LLMs can\nbe applied based on their specific capabilities, along with the evaluation\nmethods in each stage. Core ability refers to the capabilities that LLMs need\nin order to generate high-quality natural language texts. After confirming LLMs\npossess core ability, they can solve real-world and complex tasks as agent. In\nthe \"core ability\" stage, we discussed the reasoning ability, societal impact,\nand domain knowledge of LLMs. In the ``agent'' stage, we demonstrated embodied\naction, planning, and tool learning of LLMs agent applications. Finally, we\nexamined the challenges currently confronting the evaluation methods for LLMs,\nas well as the directions for future development."}, "hash": "89316e0153fd8eabf4dd6abaf684f3cb62162cb7586a46ab955636e994d51a16", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "80addb88-835e-4094-8559-cdb01a07d4a0", "node_type": "1", "metadata": {}, "hash": "3588852018ff8564edf547fbfa0ead2c78691390619a168254f90fede351a21e", "class_name": "RelatedNodeInfo"}}, "text": "Benchmark Description\nAPIBench (Patil\net al., 2023)An evaluation system with 73 API tools, 314 annotated tool-use dialogues with 753\nAPI calls, and a training set containing 1,888 tool-use dialogues from 2,138 APIs\nacross 1,000 domains\nToolEval (Qin et al.,\n2023c)constructed automatically using ChatGPT, includes a collection of 16,464 real-world\nRESTful APIs across 49 categories, with diverse instructions and solution paths\ngenerated for both single-tool and multi-tool scenarios.\nToolAlpaca (Tang\net al., 2023)containing 3,938 instances from over 400 real-world tool APIs across 50 categories\nRestBench (Song\net al., 2023b)human-annotated dataset comprising two real-world scenarios (TMDB movie database\nand Spotify music player) with 54 and 40 commonly used APIs respectively, annotated\nwith 10 instruction-solution pairs for development and 157 pairs (100 for TMDB, 57\nfor Spotify) for testing\nWebArena (Zhou\net al., 2023b)A realistic and reproducible web environment featuring four fully operational web\napplications (e-commerce, discussion forums, collaborative development, and content\nmanagement) with 812 long-horizon tasks\nMIND2WEB\n(Deng et al., 2023)over 2,000 tasks from 137 real-world websites across 31 domains with crowdsourced\naction sequences, enabling the creation of agents that handle diverse, complex web\ninteractions\nTable 2: Benchmarks for Agent Evaluation\n4 Future Directions\nThe rapid advancements in the capabilities and ap-\nplication areas of LLMs have enabled them to re-\nplace other tools in a short time, significantly en-\nhancing people\u2019s lives. However, the progress in\nevaluation methodologies has not kept pace with\nthe expansion of LLM capabilities, often making\nit challenging to find benchmarks that fully match\ncurrent tasks. There is substantial room for im-\nprovement in current evaluation methods to assess\nLLMs\u2019 performance in various tasks more accu-\nrately and provide a basis for decision-making.\nConsequently, we propose five future directions for\ndeveloping evaluation methods. We expect these\nimprovements will make LLMs a more \"useful\"\npresence in the eyes of the public.\n4.1 Dynamic Evaluation\nCurrent benchmarks are mostly static and do not\nchange once they are created. However, unchang-\ning benchmarks can present two problems when\nused for evaluation. Firstly, factual knowledge in\nthe real world changes over time. For example,\nthe presidency may change every four years, ne-\ncessitating that datasets for evaluating the factual\nknowledge of LLMs also be updated over time\nand ideally updated automatically to ensure that\nthe information provided by LLMs is accurate and\ncontemporary.\nSecondly, as LLM models expand, data from the\ndatasets might leak and become part of the trainingdata for LLMs, at which point these datasets no\nlonger function as effective evaluative tools. There-\nfore, the evaluation questions within the datasets\nmust be capable of being automatically replaced\nand updated.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2953, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "80addb88-835e-4094-8559-cdb01a07d4a0": {"__data__": {"id_": "80addb88-835e-4094-8559-cdb01a07d4a0", "embedding": null, "metadata": {"page_label": "14", "file_name": "2406_00936v1.pdf", "Title of this paper": "A Survey of Useful LLM Evaluation", "Authors": "Ji-Lun Peng, Sijia Cheng, Egil Diau, Yung-Yu Shih, Po-Heng Chen, Yen-Ting Lin, Yun-Nung Chen", "Date published": "06/03/2024", "URL": "http://arxiv.org/abs/2406.00936v1", "summary": "LLMs have gotten attention across various research domains due to their\nexceptional performance on a wide range of complex tasks. Therefore, refined\nmethods to evaluate the capabilities of LLMs are needed to determine the tasks\nand responsibility they should undertake. Our study mainly discussed how LLMs,\nas useful tools, should be effectively assessed. We proposed the two-stage\nframework: from ``core ability'' to ``agent'', clearly explaining how LLMs can\nbe applied based on their specific capabilities, along with the evaluation\nmethods in each stage. Core ability refers to the capabilities that LLMs need\nin order to generate high-quality natural language texts. After confirming LLMs\npossess core ability, they can solve real-world and complex tasks as agent. In\nthe \"core ability\" stage, we discussed the reasoning ability, societal impact,\nand domain knowledge of LLMs. In the ``agent'' stage, we demonstrated embodied\naction, planning, and tool learning of LLMs agent applications. Finally, we\nexamined the challenges currently confronting the evaluation methods for LLMs,\nas well as the directions for future development."}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d4f8389b-10d6-41b7-b3f2-49feabf83e1e", "node_type": "4", "metadata": {"page_label": "14", "file_name": "2406_00936v1.pdf", "Title of this paper": "A Survey of Useful LLM Evaluation", "Authors": "Ji-Lun Peng, Sijia Cheng, Egil Diau, Yung-Yu Shih, Po-Heng Chen, Yen-Ting Lin, Yun-Nung Chen", "Date published": "06/03/2024", "URL": "http://arxiv.org/abs/2406.00936v1", "summary": "LLMs have gotten attention across various research domains due to their\nexceptional performance on a wide range of complex tasks. Therefore, refined\nmethods to evaluate the capabilities of LLMs are needed to determine the tasks\nand responsibility they should undertake. Our study mainly discussed how LLMs,\nas useful tools, should be effectively assessed. We proposed the two-stage\nframework: from ``core ability'' to ``agent'', clearly explaining how LLMs can\nbe applied based on their specific capabilities, along with the evaluation\nmethods in each stage. Core ability refers to the capabilities that LLMs need\nin order to generate high-quality natural language texts. After confirming LLMs\npossess core ability, they can solve real-world and complex tasks as agent. In\nthe \"core ability\" stage, we discussed the reasoning ability, societal impact,\nand domain knowledge of LLMs. In the ``agent'' stage, we demonstrated embodied\naction, planning, and tool learning of LLMs agent applications. Finally, we\nexamined the challenges currently confronting the evaluation methods for LLMs,\nas well as the directions for future development."}, "hash": "89316e0153fd8eabf4dd6abaf684f3cb62162cb7586a46ab955636e994d51a16", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a798d9b0-47ff-47fa-9d16-1798054f7200", "node_type": "1", "metadata": {"page_label": "14", "file_name": "2406_00936v1.pdf", "Title of this paper": "A Survey of Useful LLM Evaluation", "Authors": "Ji-Lun Peng, Sijia Cheng, Egil Diau, Yung-Yu Shih, Po-Heng Chen, Yen-Ting Lin, Yun-Nung Chen", "Date published": "06/03/2024", "URL": "http://arxiv.org/abs/2406.00936v1", "summary": "LLMs have gotten attention across various research domains due to their\nexceptional performance on a wide range of complex tasks. Therefore, refined\nmethods to evaluate the capabilities of LLMs are needed to determine the tasks\nand responsibility they should undertake. Our study mainly discussed how LLMs,\nas useful tools, should be effectively assessed. We proposed the two-stage\nframework: from ``core ability'' to ``agent'', clearly explaining how LLMs can\nbe applied based on their specific capabilities, along with the evaluation\nmethods in each stage. Core ability refers to the capabilities that LLMs need\nin order to generate high-quality natural language texts. After confirming LLMs\npossess core ability, they can solve real-world and complex tasks as agent. In\nthe \"core ability\" stage, we discussed the reasoning ability, societal impact,\nand domain knowledge of LLMs. In the ``agent'' stage, we demonstrated embodied\naction, planning, and tool learning of LLMs agent applications. Finally, we\nexamined the challenges currently confronting the evaluation methods for LLMs,\nas well as the directions for future development."}, "hash": "3f87208938f359408bf7d3b967c84b06c8dbba7623431142a898ee0b8fbc50fe", "class_name": "RelatedNodeInfo"}}, "text": "4.1 Dynamic Evaluation\nCurrent benchmarks are mostly static and do not\nchange once they are created. However, unchang-\ning benchmarks can present two problems when\nused for evaluation. Firstly, factual knowledge in\nthe real world changes over time. For example,\nthe presidency may change every four years, ne-\ncessitating that datasets for evaluating the factual\nknowledge of LLMs also be updated over time\nand ideally updated automatically to ensure that\nthe information provided by LLMs is accurate and\ncontemporary.\nSecondly, as LLM models expand, data from the\ndatasets might leak and become part of the trainingdata for LLMs, at which point these datasets no\nlonger function as effective evaluative tools. There-\nfore, the evaluation questions within the datasets\nmust be capable of being automatically replaced\nand updated. For example, the framework pro-\nposed by (Wang et al., 2024c) can manipulate the\ncontext or question of original instances, reframing\nnew evolving instances with high confidence that\ndynamically extends existing benchmarks. Such\nadvancements would ensure that benchmarks can\nconsistently measure the capabilities of LLMs as\nthey progress.\n4.2 LLMs as Evaluators\nMany datasets currently require human annotators\nto label each question\u2019s answer, a process that is\nboth time-consuming and prone to errors. There-\nfore, employing LLMs as evaluators represents a\npromising direction for development. LLMs can\nsimulate a scorer by reading text and providing\nratings, allowing us to avoid designing new bench-\nmarks for every task. Instead, we can leverage the\nbroad capabilities of LLMs to act as scorers across\nvarious tasks. Li et al. (2024b) has reviewed the\ncurrent methods of using LLMs as scorers and has\nalso identified potential issues, such as a preference\nfor content generated by the same model or specific\nbiases in evaluation order. In the future, we can\ngradually address the biases inherent in LLMs as\nevaluators. In that case, we can enhance the rapid\ndevelopment of LLM applications while enabling\n14", "mimetype": "text/plain", "start_char_idx": 2124, "end_char_idx": 4165, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c21f2425-c959-4425-90ea-16215bacff6d": {"__data__": {"id_": "c21f2425-c959-4425-90ea-16215bacff6d", "embedding": null, "metadata": {"page_label": "15", "file_name": "2406_00936v1.pdf", "Title of this paper": "A Survey of Useful LLM Evaluation", "Authors": "Ji-Lun Peng, Sijia Cheng, Egil Diau, Yung-Yu Shih, Po-Heng Chen, Yen-Ting Lin, Yun-Nung Chen", "Date published": "06/03/2024", "URL": "http://arxiv.org/abs/2406.00936v1", "summary": "LLMs have gotten attention across various research domains due to their\nexceptional performance on a wide range of complex tasks. Therefore, refined\nmethods to evaluate the capabilities of LLMs are needed to determine the tasks\nand responsibility they should undertake. Our study mainly discussed how LLMs,\nas useful tools, should be effectively assessed. We proposed the two-stage\nframework: from ``core ability'' to ``agent'', clearly explaining how LLMs can\nbe applied based on their specific capabilities, along with the evaluation\nmethods in each stage. Core ability refers to the capabilities that LLMs need\nin order to generate high-quality natural language texts. After confirming LLMs\npossess core ability, they can solve real-world and complex tasks as agent. In\nthe \"core ability\" stage, we discussed the reasoning ability, societal impact,\nand domain knowledge of LLMs. In the ``agent'' stage, we demonstrated embodied\naction, planning, and tool learning of LLMs agent applications. Finally, we\nexamined the challenges currently confronting the evaluation methods for LLMs,\nas well as the directions for future development."}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f26764b5-771e-4c62-b511-3f23004e1492", "node_type": "4", "metadata": {"page_label": "15", "file_name": "2406_00936v1.pdf", "Title of this paper": "A Survey of Useful LLM Evaluation", "Authors": "Ji-Lun Peng, Sijia Cheng, Egil Diau, Yung-Yu Shih, Po-Heng Chen, Yen-Ting Lin, Yun-Nung Chen", "Date published": "06/03/2024", "URL": "http://arxiv.org/abs/2406.00936v1", "summary": "LLMs have gotten attention across various research domains due to their\nexceptional performance on a wide range of complex tasks. Therefore, refined\nmethods to evaluate the capabilities of LLMs are needed to determine the tasks\nand responsibility they should undertake. Our study mainly discussed how LLMs,\nas useful tools, should be effectively assessed. We proposed the two-stage\nframework: from ``core ability'' to ``agent'', clearly explaining how LLMs can\nbe applied based on their specific capabilities, along with the evaluation\nmethods in each stage. Core ability refers to the capabilities that LLMs need\nin order to generate high-quality natural language texts. After confirming LLMs\npossess core ability, they can solve real-world and complex tasks as agent. In\nthe \"core ability\" stage, we discussed the reasoning ability, societal impact,\nand domain knowledge of LLMs. In the ``agent'' stage, we demonstrated embodied\naction, planning, and tool learning of LLMs agent applications. Finally, we\nexamined the challenges currently confronting the evaluation methods for LLMs,\nas well as the directions for future development."}, "hash": "0f9b24732f1c4f6b2c95f405334fcb5d2911b4b922db5efc4cc9b3ab2b13c176", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "928c472d-3d3a-43a9-bb78-4f0fb4c3b5fd", "node_type": "1", "metadata": {}, "hash": "006d747e177051c4375541fec8ddadf9ed11276bb99133bc84fab06ea5a03083", "class_name": "RelatedNodeInfo"}}, "text": "them to self-assess, thus eliminating the need for\nadditional dataset design.\n4.3 Root Cause Analysis\nThe evaluation methods we mentioned earlier pri-\nmarily rely on assessing LLMs\u2019 outputs. For in-\nstance, we pose questions to LLMs and evaluate\nthem based on the accuracy of their responses. This\nevaluation approach allows us to quickly gauge the\nextent of a model\u2019s capabilities in various aspects\nand understand what it can help us accomplish.\nHowever, by solely examining the model\u2019s output,\nwe cannot identify the root cause of why the model\nproduces a particular response. When the model\nanswers correctly, we cannot ascertain whether it\ngenuinely possesses the corresponding ability or if\nit has simply encountered similar questions before\nand memorized the answers. Similarly, when the\nmodel\u2019s response does not meet expectations, it is\nalso challenging to determine why the model made\nan error. Therefore, we propose that future evalu-\nation methods should include analyzing the root\ncause of model predictions. This will enable us to\nbetter analyze LLMs, facilitating the development\nof more useful LLMs in the future.\n4.4 Fine-grained LLM Agent Evaluation\nExisting benchmarks mostly rely on the final com-\npletion status of tasks, lacking fine-grained step-\nwise evaluations. Additionally, while current re-\nsearch focuses more on agents\u2019 capabilities in ex-\necuting tasks within limited environments such as\nonline-shopping, environmental feedback is often\nrule-based, simplistic, and distant from real-world\nscenarios. A potential future direction is to lever-\nage high-intelligence models like LLM to design\nmore realistic evaluation environments.\n4.5 Robot Benchmark Development\nRecent research in robotics primarily emphasizes\nthe use of simulation environments to facilitate the\ntransition to real-world applications. These envi-\nronments are pivotal in enhancing the generaliza-\ntion capabilities of robots across various conditions.\nThere is an increasing need to develop large-scale\nbenchmarks, comparable to ImageNet in the field\nof computer vision, to rigorously assess these gen-\neralization abilities. Moreover, to accurately simu-\nlate real-world scenarios, it is essential to integrate\nspecific tasks that mirror actual conditions. Addi-\ntionally, the concept of a digital twin represents\nanother promising avenue for evaluating robots inboth simulated and real-world settings. Given the\nsubstantial disparities that still exist in computer\nvision when testing out-of-domain data, employ-\ning digital twins and similar methodologies could\nsignificantly reduce the sim-2-real gap, thereby en-\nabling a more focused approach on evaluate models\ncapabilities.\nFurthermore, detailed evaluations of other as-\npects, such as the sim-to-real gap, robustness\nagainst adversarial perturbations, human-robot col-\nlaboration, and multi-robot coordination, remain\ncritical for deploying robots effectively in real-\nworld scenarios. Lastly, as deep learning continues\nto demonstrate success with extensive data train-\ning, evaluating robot foundational models like RT-2\nand PaLM-E will also be essential for advancing\nour understanding and application of robotics in\ncomplex environments.\n5 Conclusion\nBecause of the inexplicability of LLMs, we need\nvarious evaluation methods to understand their ca-\npabilities, and this is the driving force behind the\nprogress of LLMs.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3384, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "928c472d-3d3a-43a9-bb78-4f0fb4c3b5fd": {"__data__": {"id_": "928c472d-3d3a-43a9-bb78-4f0fb4c3b5fd", "embedding": null, "metadata": {"page_label": "15", "file_name": "2406_00936v1.pdf", "Title of this paper": "A Survey of Useful LLM Evaluation", "Authors": "Ji-Lun Peng, Sijia Cheng, Egil Diau, Yung-Yu Shih, Po-Heng Chen, Yen-Ting Lin, Yun-Nung Chen", "Date published": "06/03/2024", "URL": "http://arxiv.org/abs/2406.00936v1", "summary": "LLMs have gotten attention across various research domains due to their\nexceptional performance on a wide range of complex tasks. Therefore, refined\nmethods to evaluate the capabilities of LLMs are needed to determine the tasks\nand responsibility they should undertake. Our study mainly discussed how LLMs,\nas useful tools, should be effectively assessed. We proposed the two-stage\nframework: from ``core ability'' to ``agent'', clearly explaining how LLMs can\nbe applied based on their specific capabilities, along with the evaluation\nmethods in each stage. Core ability refers to the capabilities that LLMs need\nin order to generate high-quality natural language texts. After confirming LLMs\npossess core ability, they can solve real-world and complex tasks as agent. In\nthe \"core ability\" stage, we discussed the reasoning ability, societal impact,\nand domain knowledge of LLMs. In the ``agent'' stage, we demonstrated embodied\naction, planning, and tool learning of LLMs agent applications. Finally, we\nexamined the challenges currently confronting the evaluation methods for LLMs,\nas well as the directions for future development."}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f26764b5-771e-4c62-b511-3f23004e1492", "node_type": "4", "metadata": {"page_label": "15", "file_name": "2406_00936v1.pdf", "Title of this paper": "A Survey of Useful LLM Evaluation", "Authors": "Ji-Lun Peng, Sijia Cheng, Egil Diau, Yung-Yu Shih, Po-Heng Chen, Yen-Ting Lin, Yun-Nung Chen", "Date published": "06/03/2024", "URL": "http://arxiv.org/abs/2406.00936v1", "summary": "LLMs have gotten attention across various research domains due to their\nexceptional performance on a wide range of complex tasks. Therefore, refined\nmethods to evaluate the capabilities of LLMs are needed to determine the tasks\nand responsibility they should undertake. Our study mainly discussed how LLMs,\nas useful tools, should be effectively assessed. We proposed the two-stage\nframework: from ``core ability'' to ``agent'', clearly explaining how LLMs can\nbe applied based on their specific capabilities, along with the evaluation\nmethods in each stage. Core ability refers to the capabilities that LLMs need\nin order to generate high-quality natural language texts. After confirming LLMs\npossess core ability, they can solve real-world and complex tasks as agent. In\nthe \"core ability\" stage, we discussed the reasoning ability, societal impact,\nand domain knowledge of LLMs. In the ``agent'' stage, we demonstrated embodied\naction, planning, and tool learning of LLMs agent applications. Finally, we\nexamined the challenges currently confronting the evaluation methods for LLMs,\nas well as the directions for future development."}, "hash": "0f9b24732f1c4f6b2c95f405334fcb5d2911b4b922db5efc4cc9b3ab2b13c176", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c21f2425-c959-4425-90ea-16215bacff6d", "node_type": "1", "metadata": {"page_label": "15", "file_name": "2406_00936v1.pdf", "Title of this paper": "A Survey of Useful LLM Evaluation", "Authors": "Ji-Lun Peng, Sijia Cheng, Egil Diau, Yung-Yu Shih, Po-Heng Chen, Yen-Ting Lin, Yun-Nung Chen", "Date published": "06/03/2024", "URL": "http://arxiv.org/abs/2406.00936v1", "summary": "LLMs have gotten attention across various research domains due to their\nexceptional performance on a wide range of complex tasks. Therefore, refined\nmethods to evaluate the capabilities of LLMs are needed to determine the tasks\nand responsibility they should undertake. Our study mainly discussed how LLMs,\nas useful tools, should be effectively assessed. We proposed the two-stage\nframework: from ``core ability'' to ``agent'', clearly explaining how LLMs can\nbe applied based on their specific capabilities, along with the evaluation\nmethods in each stage. Core ability refers to the capabilities that LLMs need\nin order to generate high-quality natural language texts. After confirming LLMs\npossess core ability, they can solve real-world and complex tasks as agent. In\nthe \"core ability\" stage, we discussed the reasoning ability, societal impact,\nand domain knowledge of LLMs. In the ``agent'' stage, we demonstrated embodied\naction, planning, and tool learning of LLMs agent applications. Finally, we\nexamined the challenges currently confronting the evaluation methods for LLMs,\nas well as the directions for future development."}, "hash": "3a55b6c66ff9a43e71d91c121d7172e9cf32e614d99c360436edc0232fa435a5", "class_name": "RelatedNodeInfo"}}, "text": "Furthermore, detailed evaluations of other as-\npects, such as the sim-to-real gap, robustness\nagainst adversarial perturbations, human-robot col-\nlaboration, and multi-robot coordination, remain\ncritical for deploying robots effectively in real-\nworld scenarios. Lastly, as deep learning continues\nto demonstrate success with extensive data train-\ning, evaluating robot foundational models like RT-2\nand PaLM-E will also be essential for advancing\nour understanding and application of robotics in\ncomplex environments.\n5 Conclusion\nBecause of the inexplicability of LLMs, we need\nvarious evaluation methods to understand their ca-\npabilities, and this is the driving force behind the\nprogress of LLMs. This study introduced the two-\nstage framework: from core ability to agent to eval-\nuate the usability of LLMs. We reviewed applica-\ntions, benchmarks, and evaluation methods in each\nsection, aiming to elucidate the advantages and lim-\nitations of current LLM development. Lastly, we\nproposed several directions for the advancement of\nLLMs evaluation methods aimed at making future\nevaluations of LLMs more flexible, automated, and\ncapable of identifying the root causes of issues. We\nlook forward to future research making LLMs a\nmore useful tool for aiding human society.\nAcknowledgements\nReferences\nRania Abdelghani, Yen-Hsiang Wang, Xingdi Yuan,\nTong Wang, Pauline Lucas, H\u00e9l\u00e8ne Sauz\u00e9on, and\nPierre-Yves Oudeyer. 2023. Gpt-3-driven pedagogi-\ncal agents to train children\u2019s curious question-asking\nskills. International Journal of Artificial Intelligence\nin Education , pages 1\u201336.\nJosh Achiam, Steven Adler, Sandhini Agarwal, Lama\nAhmad, Ilge Akkaya, Florencia Leoni Aleman,\nDiogo Almeida, Janko Altenschmidt, Sam Altman,\nShyamal Anadkat, et al. 2023. Gpt-4 technical report.\narXiv preprint arXiv:2303.08774 .\nMonica Agrawal, Stefan Hegselmann, Hunter Lang,\nYoon Kim, and David Sontag. 2022. Large language\n15", "mimetype": "text/plain", "start_char_idx": 2683, "end_char_idx": 4598, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6d4d7748-be5a-489e-8c72-23834ca8d6c2": {"__data__": {"id_": "6d4d7748-be5a-489e-8c72-23834ca8d6c2", "embedding": null, "metadata": {"page_label": "16", "file_name": "2406_00936v1.pdf", "Title of this paper": "A Survey of Useful LLM Evaluation", "Authors": "Ji-Lun Peng, Sijia Cheng, Egil Diau, Yung-Yu Shih, Po-Heng Chen, Yen-Ting Lin, Yun-Nung Chen", "Date published": "06/03/2024", "URL": "http://arxiv.org/abs/2406.00936v1", "summary": "LLMs have gotten attention across various research domains due to their\nexceptional performance on a wide range of complex tasks. Therefore, refined\nmethods to evaluate the capabilities of LLMs are needed to determine the tasks\nand responsibility they should undertake. Our study mainly discussed how LLMs,\nas useful tools, should be effectively assessed. We proposed the two-stage\nframework: from ``core ability'' to ``agent'', clearly explaining how LLMs can\nbe applied based on their specific capabilities, along with the evaluation\nmethods in each stage. Core ability refers to the capabilities that LLMs need\nin order to generate high-quality natural language texts. After confirming LLMs\npossess core ability, they can solve real-world and complex tasks as agent. In\nthe \"core ability\" stage, we discussed the reasoning ability, societal impact,\nand domain knowledge of LLMs. In the ``agent'' stage, we demonstrated embodied\naction, planning, and tool learning of LLMs agent applications. Finally, we\nexamined the challenges currently confronting the evaluation methods for LLMs,\nas well as the directions for future development."}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d00e5ad8-2386-47ee-8e95-46d94b626131", "node_type": "4", "metadata": {"page_label": "16", "file_name": "2406_00936v1.pdf", "Title of this paper": "A Survey of Useful LLM Evaluation", "Authors": "Ji-Lun Peng, Sijia Cheng, Egil Diau, Yung-Yu Shih, Po-Heng Chen, Yen-Ting Lin, Yun-Nung Chen", "Date published": "06/03/2024", "URL": "http://arxiv.org/abs/2406.00936v1", "summary": "LLMs have gotten attention across various research domains due to their\nexceptional performance on a wide range of complex tasks. Therefore, refined\nmethods to evaluate the capabilities of LLMs are needed to determine the tasks\nand responsibility they should undertake. Our study mainly discussed how LLMs,\nas useful tools, should be effectively assessed. We proposed the two-stage\nframework: from ``core ability'' to ``agent'', clearly explaining how LLMs can\nbe applied based on their specific capabilities, along with the evaluation\nmethods in each stage. Core ability refers to the capabilities that LLMs need\nin order to generate high-quality natural language texts. After confirming LLMs\npossess core ability, they can solve real-world and complex tasks as agent. In\nthe \"core ability\" stage, we discussed the reasoning ability, societal impact,\nand domain knowledge of LLMs. In the ``agent'' stage, we demonstrated embodied\naction, planning, and tool learning of LLMs agent applications. Finally, we\nexamined the challenges currently confronting the evaluation methods for LLMs,\nas well as the directions for future development."}, "hash": "b4750cb22732234771b1979b647b51d3aa21a107bd4bb681da11ac24a015fd0a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e0783693-35c8-4c90-988c-008c25098f64", "node_type": "1", "metadata": {}, "hash": "bb9035e1827bf6b23ea8d856fd5b71496cb3263e454dcae1a5d741eac93740c6", "class_name": "RelatedNodeInfo"}}, "text": "models are few-shot clinical information extractors.\narXiv preprint arXiv:2205.12689 .\nJulio Cesar Salinas Alvarado, Karin Verspoor, and Timo-\nthy Baldwin. 2015. Domain adaption of named entity\nrecognition to support credit risk assessment. In Pro-\nceedings of the Australasian Language Technology\nAssociation Workshop 2015 , pages 84\u201390.\nTara Athan, Harold Boley, Guido Governatori, Monica\nPalmirani, Adrian Paschke, and Adam Wyner. 2013.\nOasis legalruleml. In proceedings of the fourteenth\ninternational conference on artificial intelligence and\nlaw, pages 3\u201312.\nMichael Balas, Jordan Joseph Wadden, Philip C H\u00e9bert,\nEric Mathison, Marika D Warren, Victoria Seavillek-\nlein, Daniel Wyzynski, Alison Callahan, Sean A\nCrawford, Parnian Arjmand, et al. 2024. Exploring\nthe potential utility of ai large language models for\nmedical ethics: an expert panel evaluation of gpt-4.\nJournal of Medical Ethics , 50(2):90\u201396.\nSatanjeev Banerjee and Alon Lavie. 2005. Meteor: An\nautomatic metric for mt evaluation with improved cor-\nrelation with human judgments. In Proceedings of\nthe acl workshop on intrinsic and extrinsic evaluation\nmeasures for machine translation and/or summariza-\ntion, pages 65\u201372.\nYejin Bang, Samuel Cahyawijaya, Nayeon Lee, Wen-\nliang Dai, Dan Su, Bryan Wilie, Holy Lovenia, Ziwei\nJi, Tiezheng Yu, Willy Chung, Quyet V . Do, Yan\nXu, and Pascale Fung. 2023. A multitask, multilin-\ngual, multimodal evaluation of chatgpt on reasoning,\nhallucination, and interactivity.\nJames RA Benoit. 2023. Chatgpt for clinical vignette\ngeneration, revision, and evaluation. MedRxiv , pages\n2023\u201302.\nChandra Bhagavatula, Ronan Le Bras, Chaitanya\nMalaviya, Keisuke Sakaguchi, Ari Holtzman, Han-\nnah Rashkin, Doug Downey, Scott Wen-tau Yih, and\nYejin Choi. 2019. Abductive commonsense reason-\ning. arXiv preprint arXiv:1908.05739 .\nNing Bian, Xianpei Han, Le Sun, Hongyu Lin, Yaojie\nLu, Ben He, Shanshan Jiang, and Bin Dong. 2024.\nChatgpt is a knowledgeable but inexperienced solver:\nAn investigation of commonsense problem in large\nlanguage models.\nYonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi,\net al. 2020. Piqa: Reasoning about physical com-\nmonsense in natural language. In Proceedings of the\nAAAI conference on artificial intelligence , volume 34,\npages 7432\u20137439.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2275, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e0783693-35c8-4c90-988c-008c25098f64": {"__data__": {"id_": "e0783693-35c8-4c90-988c-008c25098f64", "embedding": null, "metadata": {"page_label": "16", "file_name": "2406_00936v1.pdf", "Title of this paper": "A Survey of Useful LLM Evaluation", "Authors": "Ji-Lun Peng, Sijia Cheng, Egil Diau, Yung-Yu Shih, Po-Heng Chen, Yen-Ting Lin, Yun-Nung Chen", "Date published": "06/03/2024", "URL": "http://arxiv.org/abs/2406.00936v1", "summary": "LLMs have gotten attention across various research domains due to their\nexceptional performance on a wide range of complex tasks. Therefore, refined\nmethods to evaluate the capabilities of LLMs are needed to determine the tasks\nand responsibility they should undertake. Our study mainly discussed how LLMs,\nas useful tools, should be effectively assessed. We proposed the two-stage\nframework: from ``core ability'' to ``agent'', clearly explaining how LLMs can\nbe applied based on their specific capabilities, along with the evaluation\nmethods in each stage. Core ability refers to the capabilities that LLMs need\nin order to generate high-quality natural language texts. After confirming LLMs\npossess core ability, they can solve real-world and complex tasks as agent. In\nthe \"core ability\" stage, we discussed the reasoning ability, societal impact,\nand domain knowledge of LLMs. In the ``agent'' stage, we demonstrated embodied\naction, planning, and tool learning of LLMs agent applications. Finally, we\nexamined the challenges currently confronting the evaluation methods for LLMs,\nas well as the directions for future development."}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d00e5ad8-2386-47ee-8e95-46d94b626131", "node_type": "4", "metadata": {"page_label": "16", "file_name": "2406_00936v1.pdf", "Title of this paper": "A Survey of Useful LLM Evaluation", "Authors": "Ji-Lun Peng, Sijia Cheng, Egil Diau, Yung-Yu Shih, Po-Heng Chen, Yen-Ting Lin, Yun-Nung Chen", "Date published": "06/03/2024", "URL": "http://arxiv.org/abs/2406.00936v1", "summary": "LLMs have gotten attention across various research domains due to their\nexceptional performance on a wide range of complex tasks. Therefore, refined\nmethods to evaluate the capabilities of LLMs are needed to determine the tasks\nand responsibility they should undertake. Our study mainly discussed how LLMs,\nas useful tools, should be effectively assessed. We proposed the two-stage\nframework: from ``core ability'' to ``agent'', clearly explaining how LLMs can\nbe applied based on their specific capabilities, along with the evaluation\nmethods in each stage. Core ability refers to the capabilities that LLMs need\nin order to generate high-quality natural language texts. After confirming LLMs\npossess core ability, they can solve real-world and complex tasks as agent. In\nthe \"core ability\" stage, we discussed the reasoning ability, societal impact,\nand domain knowledge of LLMs. In the ``agent'' stage, we demonstrated embodied\naction, planning, and tool learning of LLMs agent applications. Finally, we\nexamined the challenges currently confronting the evaluation methods for LLMs,\nas well as the directions for future development."}, "hash": "b4750cb22732234771b1979b647b51d3aa21a107bd4bb681da11ac24a015fd0a", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6d4d7748-be5a-489e-8c72-23834ca8d6c2", "node_type": "1", "metadata": {"page_label": "16", "file_name": "2406_00936v1.pdf", "Title of this paper": "A Survey of Useful LLM Evaluation", "Authors": "Ji-Lun Peng, Sijia Cheng, Egil Diau, Yung-Yu Shih, Po-Heng Chen, Yen-Ting Lin, Yun-Nung Chen", "Date published": "06/03/2024", "URL": "http://arxiv.org/abs/2406.00936v1", "summary": "LLMs have gotten attention across various research domains due to their\nexceptional performance on a wide range of complex tasks. Therefore, refined\nmethods to evaluate the capabilities of LLMs are needed to determine the tasks\nand responsibility they should undertake. Our study mainly discussed how LLMs,\nas useful tools, should be effectively assessed. We proposed the two-stage\nframework: from ``core ability'' to ``agent'', clearly explaining how LLMs can\nbe applied based on their specific capabilities, along with the evaluation\nmethods in each stage. Core ability refers to the capabilities that LLMs need\nin order to generate high-quality natural language texts. After confirming LLMs\npossess core ability, they can solve real-world and complex tasks as agent. In\nthe \"core ability\" stage, we discussed the reasoning ability, societal impact,\nand domain knowledge of LLMs. In the ``agent'' stage, we demonstrated embodied\naction, planning, and tool learning of LLMs agent applications. Finally, we\nexamined the challenges currently confronting the evaluation methods for LLMs,\nas well as the directions for future development."}, "hash": "81066ff49098d26f33ebff078577649d7e5b3c2c6d927be2fb3dd9f4405e3593", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "690095c6-2b60-48f1-92cf-c0e02266d19c", "node_type": "1", "metadata": {}, "hash": "a6cad1a5b0d5e9a16419df7e6b426a5ca581ae0cfe9998d46949b11168a0c1a6", "class_name": "RelatedNodeInfo"}}, "text": "2019. Abductive commonsense reason-\ning. arXiv preprint arXiv:1908.05739 .\nNing Bian, Xianpei Han, Le Sun, Hongyu Lin, Yaojie\nLu, Ben He, Shanshan Jiang, and Bin Dong. 2024.\nChatgpt is a knowledgeable but inexperienced solver:\nAn investigation of commonsense problem in large\nlanguage models.\nYonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi,\net al. 2020. Piqa: Reasoning about physical com-\nmonsense in natural language. In Proceedings of the\nAAAI conference on artificial intelligence , volume 34,\npages 7432\u20137439.\nAndrew Blair-Stanek, Nils Holzenberger, and Benjamin\nVan Durme. 2023. Can gpt-3 perform statutory rea-\nsoning? In Proceedings of the Nineteenth Interna-\ntional Conference on Artificial Intelligence and Law ,\npages 22\u201331.Tianle Cai, Xuezhi Wang, Tengyu Ma, Xinyun Chen,\nand Denny Zhou. 2024. Large language models as\ntool makers.\nNicholas Carlini, Daphne Ippolito, Matthew Jagielski,\nKatherine Lee, Florian Tramer, and Chiyuan Zhang.\n2023. Quantifying memorization across neural lan-\nguage models.\nFran Casino, Thomas K Dasaklis, Georgios P\nSpathoulas, Marios Anagnostopoulos, Amrita\nGhosal, Istvan Borocz, Agusti Solanas, Mauro Conti,\nand Constantinos Patsakis. 2022. Research trends,\nchallenges, and emerging topics in digital forensics:\nA review of reviews. IEEE Access , 10:25464\u201325493.\nYupeng Chang, Xu Wang, Jindong Wang, Yuan Wu,\nLinyi Yang, Kaijie Zhu, Hao Chen, Xiaoyuan Yi,\nCunxiang Wang, Yidong Wang, et al. 2023. A sur-\nvey on evaluation of large language models. ACM\nTransactions on Intelligent Systems and Technology .\nWenhu Chen, Hanwen Zha, Zhiyu Chen, Wenhan Xiong,\nHong Wang, and William Wang. 2020. Hybridqa: A\ndataset of multi-hop question answering over tabular\nand textual data. arXiv preprint arXiv:2004.07347 .\nZhiyu Chen, Shiyang Li, Charese Smiley, Zhiqiang Ma,\nSameena Shah, and William Yang Wang. 2022. Con-\nvfinqa: Exploring the chain of numerical reasoning\nin conversational finance question answering. arXiv\npreprint arXiv:2210.03849 .\nJenny Cifuentes, Ana Lucila Sandoval Orozco, and\nLuis Javier Garcia Villalba. 2022. A survey of arti-\nficial intelligence strategies for automatic detection\nof sexually explicit videos. Multimedia Tools and\nApplications , 81(3):3205\u20133222.", "mimetype": "text/plain", "start_char_idx": 1754, "end_char_idx": 3980, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "690095c6-2b60-48f1-92cf-c0e02266d19c": {"__data__": {"id_": "690095c6-2b60-48f1-92cf-c0e02266d19c", "embedding": null, "metadata": {"page_label": "16", "file_name": "2406_00936v1.pdf", "Title of this paper": "A Survey of Useful LLM Evaluation", "Authors": "Ji-Lun Peng, Sijia Cheng, Egil Diau, Yung-Yu Shih, Po-Heng Chen, Yen-Ting Lin, Yun-Nung Chen", "Date published": "06/03/2024", "URL": "http://arxiv.org/abs/2406.00936v1", "summary": "LLMs have gotten attention across various research domains due to their\nexceptional performance on a wide range of complex tasks. Therefore, refined\nmethods to evaluate the capabilities of LLMs are needed to determine the tasks\nand responsibility they should undertake. Our study mainly discussed how LLMs,\nas useful tools, should be effectively assessed. We proposed the two-stage\nframework: from ``core ability'' to ``agent'', clearly explaining how LLMs can\nbe applied based on their specific capabilities, along with the evaluation\nmethods in each stage. Core ability refers to the capabilities that LLMs need\nin order to generate high-quality natural language texts. After confirming LLMs\npossess core ability, they can solve real-world and complex tasks as agent. In\nthe \"core ability\" stage, we discussed the reasoning ability, societal impact,\nand domain knowledge of LLMs. In the ``agent'' stage, we demonstrated embodied\naction, planning, and tool learning of LLMs agent applications. Finally, we\nexamined the challenges currently confronting the evaluation methods for LLMs,\nas well as the directions for future development."}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d00e5ad8-2386-47ee-8e95-46d94b626131", "node_type": "4", "metadata": {"page_label": "16", "file_name": "2406_00936v1.pdf", "Title of this paper": "A Survey of Useful LLM Evaluation", "Authors": "Ji-Lun Peng, Sijia Cheng, Egil Diau, Yung-Yu Shih, Po-Heng Chen, Yen-Ting Lin, Yun-Nung Chen", "Date published": "06/03/2024", "URL": "http://arxiv.org/abs/2406.00936v1", "summary": "LLMs have gotten attention across various research domains due to their\nexceptional performance on a wide range of complex tasks. Therefore, refined\nmethods to evaluate the capabilities of LLMs are needed to determine the tasks\nand responsibility they should undertake. Our study mainly discussed how LLMs,\nas useful tools, should be effectively assessed. We proposed the two-stage\nframework: from ``core ability'' to ``agent'', clearly explaining how LLMs can\nbe applied based on their specific capabilities, along with the evaluation\nmethods in each stage. Core ability refers to the capabilities that LLMs need\nin order to generate high-quality natural language texts. After confirming LLMs\npossess core ability, they can solve real-world and complex tasks as agent. In\nthe \"core ability\" stage, we discussed the reasoning ability, societal impact,\nand domain knowledge of LLMs. In the ``agent'' stage, we demonstrated embodied\naction, planning, and tool learning of LLMs agent applications. Finally, we\nexamined the challenges currently confronting the evaluation methods for LLMs,\nas well as the directions for future development."}, "hash": "b4750cb22732234771b1979b647b51d3aa21a107bd4bb681da11ac24a015fd0a", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e0783693-35c8-4c90-988c-008c25098f64", "node_type": "1", "metadata": {"page_label": "16", "file_name": "2406_00936v1.pdf", "Title of this paper": "A Survey of Useful LLM Evaluation", "Authors": "Ji-Lun Peng, Sijia Cheng, Egil Diau, Yung-Yu Shih, Po-Heng Chen, Yen-Ting Lin, Yun-Nung Chen", "Date published": "06/03/2024", "URL": "http://arxiv.org/abs/2406.00936v1", "summary": "LLMs have gotten attention across various research domains due to their\nexceptional performance on a wide range of complex tasks. Therefore, refined\nmethods to evaluate the capabilities of LLMs are needed to determine the tasks\nand responsibility they should undertake. Our study mainly discussed how LLMs,\nas useful tools, should be effectively assessed. We proposed the two-stage\nframework: from ``core ability'' to ``agent'', clearly explaining how LLMs can\nbe applied based on their specific capabilities, along with the evaluation\nmethods in each stage. Core ability refers to the capabilities that LLMs need\nin order to generate high-quality natural language texts. After confirming LLMs\npossess core ability, they can solve real-world and complex tasks as agent. In\nthe \"core ability\" stage, we discussed the reasoning ability, societal impact,\nand domain knowledge of LLMs. In the ``agent'' stage, we demonstrated embodied\naction, planning, and tool learning of LLMs agent applications. Finally, we\nexamined the challenges currently confronting the evaluation methods for LLMs,\nas well as the directions for future development."}, "hash": "8d83067c1363da6ef7173ffcdbd158c0ab7cf037668437ea6753fc06c6594d3b", "class_name": "RelatedNodeInfo"}}, "text": "2020. Hybridqa: A\ndataset of multi-hop question answering over tabular\nand textual data. arXiv preprint arXiv:2004.07347 .\nZhiyu Chen, Shiyang Li, Charese Smiley, Zhiqiang Ma,\nSameena Shah, and William Yang Wang. 2022. Con-\nvfinqa: Exploring the chain of numerical reasoning\nin conversational finance question answering. arXiv\npreprint arXiv:2210.03849 .\nJenny Cifuentes, Ana Lucila Sandoval Orozco, and\nLuis Javier Garcia Villalba. 2022. A survey of arti-\nficial intelligence strategies for automatic detection\nof sexually explicit videos. Multimedia Tools and\nApplications , 81(3):3205\u20133222.\nKarl Cobbe, Vineet Kosaraju, Mohammad Bavarian,\nMark Chen, Heewoo Jun, Lukasz Kaiser, Matthias\nPlappert, Jerry Tworek, Jacob Hilton, Reiichiro\nNakano, et al. 2021. Training verifiers to solve math\nword problems. arXiv preprint arXiv:2110.14168 .\nBhavana Dalvi, Peter Jansen, Oyvind Tafjord, Zhengnan\nXie, Hannah Smith, Leighanna Pipatanangkura, and\nPeter Clark. 2021. Explaining answers with entail-\nment trees. arXiv preprint arXiv:2104.08661 .\nGilles Daniel, Didier Sornette, and Peter Wohrmann.\n2008. Look-ahead benchmark bias in port-\nfolio performance evaluation. arXiv preprint\narXiv:0810.1922 .\nBadhan Chandra Das, M Hadi Amini, and Yanzhao\nWu. 2024. Security and privacy challenges of\nlarge language models: A survey. arXiv preprint\narXiv:2402.00888 .\nDorottya Demszky, Diyi Yang, David S Yeager, Christo-\npher J Bryan, Margarett Clapper, Susannah Chand-\nhok, Johannes C Eichstaedt, Cameron Hecht, Jeremy\nJamieson, Meghann Johnson, et al. 2023. Using large\nlanguage models in psychology. Nature Reviews Psy-\nchology , 2(11):688\u2013701.\n16", "mimetype": "text/plain", "start_char_idx": 3387, "end_char_idx": 5024, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f110dc68-9447-4dd5-8397-39dcd13ebba6": {"__data__": {"id_": "f110dc68-9447-4dd5-8397-39dcd13ebba6", "embedding": null, "metadata": {"page_label": "17", "file_name": "2406_00936v1.pdf", "Title of this paper": "A Survey of Useful LLM Evaluation", "Authors": "Ji-Lun Peng, Sijia Cheng, Egil Diau, Yung-Yu Shih, Po-Heng Chen, Yen-Ting Lin, Yun-Nung Chen", "Date published": "06/03/2024", "URL": "http://arxiv.org/abs/2406.00936v1", "summary": "LLMs have gotten attention across various research domains due to their\nexceptional performance on a wide range of complex tasks. Therefore, refined\nmethods to evaluate the capabilities of LLMs are needed to determine the tasks\nand responsibility they should undertake. Our study mainly discussed how LLMs,\nas useful tools, should be effectively assessed. We proposed the two-stage\nframework: from ``core ability'' to ``agent'', clearly explaining how LLMs can\nbe applied based on their specific capabilities, along with the evaluation\nmethods in each stage. Core ability refers to the capabilities that LLMs need\nin order to generate high-quality natural language texts. After confirming LLMs\npossess core ability, they can solve real-world and complex tasks as agent. In\nthe \"core ability\" stage, we discussed the reasoning ability, societal impact,\nand domain knowledge of LLMs. In the ``agent'' stage, we demonstrated embodied\naction, planning, and tool learning of LLMs agent applications. Finally, we\nexamined the challenges currently confronting the evaluation methods for LLMs,\nas well as the directions for future development."}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "9f59d426-38d4-4aa4-8612-0cabbee7afea", "node_type": "4", "metadata": {"page_label": "17", "file_name": "2406_00936v1.pdf", "Title of this paper": "A Survey of Useful LLM Evaluation", "Authors": "Ji-Lun Peng, Sijia Cheng, Egil Diau, Yung-Yu Shih, Po-Heng Chen, Yen-Ting Lin, Yun-Nung Chen", "Date published": "06/03/2024", "URL": "http://arxiv.org/abs/2406.00936v1", "summary": "LLMs have gotten attention across various research domains due to their\nexceptional performance on a wide range of complex tasks. Therefore, refined\nmethods to evaluate the capabilities of LLMs are needed to determine the tasks\nand responsibility they should undertake. Our study mainly discussed how LLMs,\nas useful tools, should be effectively assessed. We proposed the two-stage\nframework: from ``core ability'' to ``agent'', clearly explaining how LLMs can\nbe applied based on their specific capabilities, along with the evaluation\nmethods in each stage. Core ability refers to the capabilities that LLMs need\nin order to generate high-quality natural language texts. After confirming LLMs\npossess core ability, they can solve real-world and complex tasks as agent. In\nthe \"core ability\" stage, we discussed the reasoning ability, societal impact,\nand domain knowledge of LLMs. In the ``agent'' stage, we demonstrated embodied\naction, planning, and tool learning of LLMs agent applications. Finally, we\nexamined the challenges currently confronting the evaluation methods for LLMs,\nas well as the directions for future development."}, "hash": "b455840a8d28a60a62f2e7ac602272ca4d3968ef93b3bf637298ee908fe47708", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "baeb0e7b-8822-4e4a-9e40-b7808694d9f4", "node_type": "1", "metadata": {}, "hash": "dcdc73ac59af417db19eb28cb66e40f529598edb6674285db79f667df448dfbd", "class_name": "RelatedNodeInfo"}}, "text": "Xiang Deng, Ahmed Hassan Awadallah, Christopher\nMeek, Oleksandr Polozov, Huan Sun, and Matthew\nRichardson. 2020. Structure-grounded pretraining\nfor text-to-sql. arXiv preprint arXiv:2010.12773 .\nXiang Deng, Yu Gu, Boyuan Zheng, Shijie Chen,\nSamuel Stevens, Boshi Wang, Huan Sun, and Yu Su.\n2023. Mind2web: Towards a generalist agent for the\nweb.\nAniket Deroy, Kripabandhu Ghosh, and Saptarshi\nGhosh. 2023. How ready are pre-trained abstractive\nmodels and llms for legal case judgement summariza-\ntion? arXiv preprint arXiv:2306.01248 .\nRamon Dijkstra, Z\u00fclk\u00fcf Gen\u00e7, Subhradeep Kayal, Jaap\nKamps, et al. 2022. Reading comprehension quiz\ngeneration using generative pre-trained transformers.\nIniTextbooks@ AIED , pages 4\u201317.\nShichuan Du, Yong Tao, and Aleix M Martinez.\n2014. Compound facial expressions of emotion.\nProceedings of the national academy of sciences ,\n111(15):E1454\u2013E1462.\nShitong Duan, Xiaoyuan Yi, Peng Zhang, Tun Lu, Xing\nXie, and Ning Gu. 2024. Denevil: Towards deci-\nphering and navigating the ethical values of large\nlanguage models via instruction learning.\nYann Dubois, Chen Xuechen Li, Rohan Taori, Tianyi\nZhang, Ishaan Gulrajani, Jimmy Ba, Carlos Guestrin,\nPercy S Liang, and Tatsunori B Hashimoto. 2024.\nAlpacafarm: A simulation framework for methods\nthat learn from human feedback. Advances in Neural\nInformation Processing Systems , 36.\nChristoph Engel and Richard H Mcadams. 2024. Ask-\ning gpt for the ordinary meaning of statutory terms.\nMPI Collective Goods Discussion Paper , (2024/5).\nAngela Fan, Yacine Jernite, Ethan Perez, David Grang-\nier, Jason Weston, and Michael Auli. 2019. Eli5:\nLong form question answering. arXiv preprint\narXiv:1907.09190 .\nPhilip Feldman, James R. Foulds, and Shimei Pan. 2023.\nTrapping llm hallucinations using tagged context\nprompts.\nDawei Gao, Haibin Wang, Yaliang Li, Xiuyu Sun,\nYichen Qian, Bolin Ding, and Jingren Zhou. 2023.\nText-to-sql empowered by large language mod-\nels: A benchmark evaluation. arXiv preprint\narXiv:2308.15363 .\nMor Geva, Daniel Khashabi, Elad Segal, Tushar Khot,\nDan Roth, and Jonathan Berant. 2021. Did aristotle\nuse a laptop? a question answering benchmark with\nimplicit reasoning strategies.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2181, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "baeb0e7b-8822-4e4a-9e40-b7808694d9f4": {"__data__": {"id_": "baeb0e7b-8822-4e4a-9e40-b7808694d9f4", "embedding": null, "metadata": {"page_label": "17", "file_name": "2406_00936v1.pdf", "Title of this paper": "A Survey of Useful LLM Evaluation", "Authors": "Ji-Lun Peng, Sijia Cheng, Egil Diau, Yung-Yu Shih, Po-Heng Chen, Yen-Ting Lin, Yun-Nung Chen", "Date published": "06/03/2024", "URL": "http://arxiv.org/abs/2406.00936v1", "summary": "LLMs have gotten attention across various research domains due to their\nexceptional performance on a wide range of complex tasks. Therefore, refined\nmethods to evaluate the capabilities of LLMs are needed to determine the tasks\nand responsibility they should undertake. Our study mainly discussed how LLMs,\nas useful tools, should be effectively assessed. We proposed the two-stage\nframework: from ``core ability'' to ``agent'', clearly explaining how LLMs can\nbe applied based on their specific capabilities, along with the evaluation\nmethods in each stage. Core ability refers to the capabilities that LLMs need\nin order to generate high-quality natural language texts. After confirming LLMs\npossess core ability, they can solve real-world and complex tasks as agent. In\nthe \"core ability\" stage, we discussed the reasoning ability, societal impact,\nand domain knowledge of LLMs. In the ``agent'' stage, we demonstrated embodied\naction, planning, and tool learning of LLMs agent applications. Finally, we\nexamined the challenges currently confronting the evaluation methods for LLMs,\nas well as the directions for future development."}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "9f59d426-38d4-4aa4-8612-0cabbee7afea", "node_type": "4", "metadata": {"page_label": "17", "file_name": "2406_00936v1.pdf", "Title of this paper": "A Survey of Useful LLM Evaluation", "Authors": "Ji-Lun Peng, Sijia Cheng, Egil Diau, Yung-Yu Shih, Po-Heng Chen, Yen-Ting Lin, Yun-Nung Chen", "Date published": "06/03/2024", "URL": "http://arxiv.org/abs/2406.00936v1", "summary": "LLMs have gotten attention across various research domains due to their\nexceptional performance on a wide range of complex tasks. Therefore, refined\nmethods to evaluate the capabilities of LLMs are needed to determine the tasks\nand responsibility they should undertake. Our study mainly discussed how LLMs,\nas useful tools, should be effectively assessed. We proposed the two-stage\nframework: from ``core ability'' to ``agent'', clearly explaining how LLMs can\nbe applied based on their specific capabilities, along with the evaluation\nmethods in each stage. Core ability refers to the capabilities that LLMs need\nin order to generate high-quality natural language texts. After confirming LLMs\npossess core ability, they can solve real-world and complex tasks as agent. In\nthe \"core ability\" stage, we discussed the reasoning ability, societal impact,\nand domain knowledge of LLMs. In the ``agent'' stage, we demonstrated embodied\naction, planning, and tool learning of LLMs agent applications. Finally, we\nexamined the challenges currently confronting the evaluation methods for LLMs,\nas well as the directions for future development."}, "hash": "b455840a8d28a60a62f2e7ac602272ca4d3968ef93b3bf637298ee908fe47708", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f110dc68-9447-4dd5-8397-39dcd13ebba6", "node_type": "1", "metadata": {"page_label": "17", "file_name": "2406_00936v1.pdf", "Title of this paper": "A Survey of Useful LLM Evaluation", "Authors": "Ji-Lun Peng, Sijia Cheng, Egil Diau, Yung-Yu Shih, Po-Heng Chen, Yen-Ting Lin, Yun-Nung Chen", "Date published": "06/03/2024", "URL": "http://arxiv.org/abs/2406.00936v1", "summary": "LLMs have gotten attention across various research domains due to their\nexceptional performance on a wide range of complex tasks. Therefore, refined\nmethods to evaluate the capabilities of LLMs are needed to determine the tasks\nand responsibility they should undertake. Our study mainly discussed how LLMs,\nas useful tools, should be effectively assessed. We proposed the two-stage\nframework: from ``core ability'' to ``agent'', clearly explaining how LLMs can\nbe applied based on their specific capabilities, along with the evaluation\nmethods in each stage. Core ability refers to the capabilities that LLMs need\nin order to generate high-quality natural language texts. After confirming LLMs\npossess core ability, they can solve real-world and complex tasks as agent. In\nthe \"core ability\" stage, we discussed the reasoning ability, societal impact,\nand domain knowledge of LLMs. In the ``agent'' stage, we demonstrated embodied\naction, planning, and tool learning of LLMs agent applications. Finally, we\nexamined the challenges currently confronting the evaluation methods for LLMs,\nas well as the directions for future development."}, "hash": "fd9234258455e08fe1f8aeb0211749e4d6f9d196dce4b29ef9a7fd8ea3e10821", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d1da1abb-9400-4c8d-8e1f-972dead68c11", "node_type": "1", "metadata": {}, "hash": "274b53eb2ad990a96274a780f1945b9403c78aa15d78699cdddb50bb251ecd0c", "class_name": "RelatedNodeInfo"}}, "text": "2019. Eli5:\nLong form question answering. arXiv preprint\narXiv:1907.09190 .\nPhilip Feldman, James R. Foulds, and Shimei Pan. 2023.\nTrapping llm hallucinations using tagged context\nprompts.\nDawei Gao, Haibin Wang, Yaliang Li, Xiuyu Sun,\nYichen Qian, Bolin Ding, and Jingren Zhou. 2023.\nText-to-sql empowered by large language mod-\nels: A benchmark evaluation. arXiv preprint\narXiv:2308.15363 .\nMor Geva, Daniel Khashabi, Elad Segal, Tushar Khot,\nDan Roth, and Jonathan Berant. 2021. Did aristotle\nuse a laptop? a question answering benchmark with\nimplicit reasoning strategies. Transactions of the\nAssociation for Computational Linguistics , 9:346\u2013\n361.\nMalik Ghallab, Dana Nau, and Paolo Traverso. 2004.\nAutomated Planning: theory and practice . Elsevier.Shaona Ghosh, Prasoon Varshney, Erick Galinkin, and\nChristopher Parisien. 2024. Aegis: Online adaptive\nai content safety moderation with ensemble of llm\nexperts.\nZishan Guo, Renren Jin, Chuang Liu, Yufei Huang, Dan\nShi, Linhao Yu, Yan Liu, Jiaxuan Li, Bojian Xiong,\nDeyi Xiong, et al. 2023. Evaluating large language\nmodels: A comprehensive survey. arXiv preprint\narXiv:2310.19736 .\nSimon J. Han, Keith Ransom, Andrew Perfors, and\nCharles Kemp. 2023. Inductive reasoning in humans\nand large language models.\nDan Hendrycks, Collin Burns, Saurav Kadavath, Akul\nArora, Steven Basart, Eric Tang, Dawn Song, and Ja-\ncob Steinhardt. 2021. Measuring mathematical prob-\nlem solving with the math dataset. arXiv preprint\narXiv:2103.03874 .\nNils Holzenberger, Andrew Blair-Stanek, and Benjamin\nVan Durme. 2020. A dataset for statutory reasoning\nin tax law entailment and question answering. arXiv\npreprint arXiv:2005.05257 .\nMax Hort, Jie M Zhang, Federica Sarro, and Mark Har-\nman. 2021. Fairea: A model behaviour mutation\napproach to benchmarking bias mitigation methods.\nInProceedings of the 29th ACM joint meeting on\neuropean software engineering conference and sym-\nposium on the foundations of software engineering ,\npages 994\u20131006.\nChenxu Hu, Jie Fu, Chenzhuang Du, Simian Luo, Junbo\nZhao, and Hang Zhao. 2023. Chatdb: Augmenting\nllms with databases as their symbolic memory.\nJie Huang and Kevin Chen-Chuan Chang. 2023. To-\nwards reasoning in large language models: A survey.", "mimetype": "text/plain", "start_char_idx": 1605, "end_char_idx": 3832, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d1da1abb-9400-4c8d-8e1f-972dead68c11": {"__data__": {"id_": "d1da1abb-9400-4c8d-8e1f-972dead68c11", "embedding": null, "metadata": {"page_label": "17", "file_name": "2406_00936v1.pdf", "Title of this paper": "A Survey of Useful LLM Evaluation", "Authors": "Ji-Lun Peng, Sijia Cheng, Egil Diau, Yung-Yu Shih, Po-Heng Chen, Yen-Ting Lin, Yun-Nung Chen", "Date published": "06/03/2024", "URL": "http://arxiv.org/abs/2406.00936v1", "summary": "LLMs have gotten attention across various research domains due to their\nexceptional performance on a wide range of complex tasks. Therefore, refined\nmethods to evaluate the capabilities of LLMs are needed to determine the tasks\nand responsibility they should undertake. Our study mainly discussed how LLMs,\nas useful tools, should be effectively assessed. We proposed the two-stage\nframework: from ``core ability'' to ``agent'', clearly explaining how LLMs can\nbe applied based on their specific capabilities, along with the evaluation\nmethods in each stage. Core ability refers to the capabilities that LLMs need\nin order to generate high-quality natural language texts. After confirming LLMs\npossess core ability, they can solve real-world and complex tasks as agent. In\nthe \"core ability\" stage, we discussed the reasoning ability, societal impact,\nand domain knowledge of LLMs. In the ``agent'' stage, we demonstrated embodied\naction, planning, and tool learning of LLMs agent applications. Finally, we\nexamined the challenges currently confronting the evaluation methods for LLMs,\nas well as the directions for future development."}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "9f59d426-38d4-4aa4-8612-0cabbee7afea", "node_type": "4", "metadata": {"page_label": "17", "file_name": "2406_00936v1.pdf", "Title of this paper": "A Survey of Useful LLM Evaluation", "Authors": "Ji-Lun Peng, Sijia Cheng, Egil Diau, Yung-Yu Shih, Po-Heng Chen, Yen-Ting Lin, Yun-Nung Chen", "Date published": "06/03/2024", "URL": "http://arxiv.org/abs/2406.00936v1", "summary": "LLMs have gotten attention across various research domains due to their\nexceptional performance on a wide range of complex tasks. Therefore, refined\nmethods to evaluate the capabilities of LLMs are needed to determine the tasks\nand responsibility they should undertake. Our study mainly discussed how LLMs,\nas useful tools, should be effectively assessed. We proposed the two-stage\nframework: from ``core ability'' to ``agent'', clearly explaining how LLMs can\nbe applied based on their specific capabilities, along with the evaluation\nmethods in each stage. Core ability refers to the capabilities that LLMs need\nin order to generate high-quality natural language texts. After confirming LLMs\npossess core ability, they can solve real-world and complex tasks as agent. In\nthe \"core ability\" stage, we discussed the reasoning ability, societal impact,\nand domain knowledge of LLMs. In the ``agent'' stage, we demonstrated embodied\naction, planning, and tool learning of LLMs agent applications. Finally, we\nexamined the challenges currently confronting the evaluation methods for LLMs,\nas well as the directions for future development."}, "hash": "b455840a8d28a60a62f2e7ac602272ca4d3968ef93b3bf637298ee908fe47708", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "baeb0e7b-8822-4e4a-9e40-b7808694d9f4", "node_type": "1", "metadata": {"page_label": "17", "file_name": "2406_00936v1.pdf", "Title of this paper": "A Survey of Useful LLM Evaluation", "Authors": "Ji-Lun Peng, Sijia Cheng, Egil Diau, Yung-Yu Shih, Po-Heng Chen, Yen-Ting Lin, Yun-Nung Chen", "Date published": "06/03/2024", "URL": "http://arxiv.org/abs/2406.00936v1", "summary": "LLMs have gotten attention across various research domains due to their\nexceptional performance on a wide range of complex tasks. Therefore, refined\nmethods to evaluate the capabilities of LLMs are needed to determine the tasks\nand responsibility they should undertake. Our study mainly discussed how LLMs,\nas useful tools, should be effectively assessed. We proposed the two-stage\nframework: from ``core ability'' to ``agent'', clearly explaining how LLMs can\nbe applied based on their specific capabilities, along with the evaluation\nmethods in each stage. Core ability refers to the capabilities that LLMs need\nin order to generate high-quality natural language texts. After confirming LLMs\npossess core ability, they can solve real-world and complex tasks as agent. In\nthe \"core ability\" stage, we discussed the reasoning ability, societal impact,\nand domain knowledge of LLMs. In the ``agent'' stage, we demonstrated embodied\naction, planning, and tool learning of LLMs agent applications. Finally, we\nexamined the challenges currently confronting the evaluation methods for LLMs,\nas well as the directions for future development."}, "hash": "cf066dbbabd54d4201ac646f84539295e708a859fe8a9a3308c47c2dd6850077", "class_name": "RelatedNodeInfo"}}, "text": "2020. A dataset for statutory reasoning\nin tax law entailment and question answering. arXiv\npreprint arXiv:2005.05257 .\nMax Hort, Jie M Zhang, Federica Sarro, and Mark Har-\nman. 2021. Fairea: A model behaviour mutation\napproach to benchmarking bias mitigation methods.\nInProceedings of the 29th ACM joint meeting on\neuropean software engineering conference and sym-\nposium on the foundations of software engineering ,\npages 994\u20131006.\nChenxu Hu, Jie Fu, Chenzhuang Du, Simian Luo, Junbo\nZhao, and Hang Zhao. 2023. Chatdb: Augmenting\nllms with databases as their symbolic memory.\nJie Huang and Kevin Chen-Chuan Chang. 2023. To-\nwards reasoning in large language models: A survey.\nWenlong Huang, Pieter Abbeel, Deepak Pathak, and\nIgor Mordatch. 2022a. Language models as zero-\nshot planners: Extracting actionable knowledge for\nembodied agents. In International Conference on\nMachine Learning , pages 9118\u20139147. PMLR.\nWenlong Huang, Chen Wang, Ruohan Zhang, Yunzhu\nLi, Jiajun Wu, and Li Fei-Fei. 2023. V oxposer: Com-\nposable 3d value maps for robotic manipulation with\nlanguage models. arXiv preprint arXiv:2307.05973 .\nWenlong Huang, Fei Xia, Ted Xiao, Harris Chan, Jacky\nLiang, Pete Florence, Andy Zeng, Jonathan Tomp-\nson, Igor Mordatch, Yevgen Chebotar, Pierre Ser-\nmanet, Noah Brown, Tomas Jackson, Linda Luu,\nSergey Levine, Karol Hausman, and Brian Ichter.\n2022b. Inner monologue: Embodied reasoning\nthrough planning with language models. In arXiv\npreprint arXiv:2207.05608 .\nJiaming Ji, Mickel Liu, Juntao Dai, Xuehai Pan, Chi\nZhang, Ce Bian, Chi Zhang, Ruiyang Sun, Yizhou\nWang, and Yaodong Yang. 2023. Beavertails: To-\nwards improved safety alignment of llm via a human-\npreference dataset.\n17", "mimetype": "text/plain", "start_char_idx": 3155, "end_char_idx": 4855, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "88cd3980-d3c2-412a-8ee4-75e1b10d220c": {"__data__": {"id_": "88cd3980-d3c2-412a-8ee4-75e1b10d220c", "embedding": null, "metadata": {"page_label": "18", "file_name": "2406_00936v1.pdf", "Title of this paper": "A Survey of Useful LLM Evaluation", "Authors": "Ji-Lun Peng, Sijia Cheng, Egil Diau, Yung-Yu Shih, Po-Heng Chen, Yen-Ting Lin, Yun-Nung Chen", "Date published": "06/03/2024", "URL": "http://arxiv.org/abs/2406.00936v1", "summary": "LLMs have gotten attention across various research domains due to their\nexceptional performance on a wide range of complex tasks. Therefore, refined\nmethods to evaluate the capabilities of LLMs are needed to determine the tasks\nand responsibility they should undertake. Our study mainly discussed how LLMs,\nas useful tools, should be effectively assessed. We proposed the two-stage\nframework: from ``core ability'' to ``agent'', clearly explaining how LLMs can\nbe applied based on their specific capabilities, along with the evaluation\nmethods in each stage. Core ability refers to the capabilities that LLMs need\nin order to generate high-quality natural language texts. After confirming LLMs\npossess core ability, they can solve real-world and complex tasks as agent. In\nthe \"core ability\" stage, we discussed the reasoning ability, societal impact,\nand domain knowledge of LLMs. In the ``agent'' stage, we demonstrated embodied\naction, planning, and tool learning of LLMs agent applications. Finally, we\nexamined the challenges currently confronting the evaluation methods for LLMs,\nas well as the directions for future development."}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "dd73c8d9-67ac-4670-8a4f-8bc81168290b", "node_type": "4", "metadata": {"page_label": "18", "file_name": "2406_00936v1.pdf", "Title of this paper": "A Survey of Useful LLM Evaluation", "Authors": "Ji-Lun Peng, Sijia Cheng, Egil Diau, Yung-Yu Shih, Po-Heng Chen, Yen-Ting Lin, Yun-Nung Chen", "Date published": "06/03/2024", "URL": "http://arxiv.org/abs/2406.00936v1", "summary": "LLMs have gotten attention across various research domains due to their\nexceptional performance on a wide range of complex tasks. Therefore, refined\nmethods to evaluate the capabilities of LLMs are needed to determine the tasks\nand responsibility they should undertake. Our study mainly discussed how LLMs,\nas useful tools, should be effectively assessed. We proposed the two-stage\nframework: from ``core ability'' to ``agent'', clearly explaining how LLMs can\nbe applied based on their specific capabilities, along with the evaluation\nmethods in each stage. Core ability refers to the capabilities that LLMs need\nin order to generate high-quality natural language texts. After confirming LLMs\npossess core ability, they can solve real-world and complex tasks as agent. In\nthe \"core ability\" stage, we discussed the reasoning ability, societal impact,\nand domain knowledge of LLMs. In the ``agent'' stage, we demonstrated embodied\naction, planning, and tool learning of LLMs agent applications. Finally, we\nexamined the challenges currently confronting the evaluation methods for LLMs,\nas well as the directions for future development."}, "hash": "835524bcd01d28e4c181a1f4340c159e59e93efc34461fe3b06b39c7969abf1b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "00f2422b-690e-4b36-bb74-3127acf98f92", "node_type": "1", "metadata": {}, "hash": "6ddc6232e8bc129c9ae0280ce0341ae52fd6ea28ef3b9266b826bc2df32fc9b2", "class_name": "RelatedNodeInfo"}}, "text": "Qinjin Jia, Jialin Cui, Yunkai Xiao, Chengyuan\nLiu, Parvez Rashid, and Edward F Gehringer.\n2021. All-in-one: Multi-task learning bert mod-\nels for evaluating peer assessments. arXiv preprint\narXiv:2110.03895 .\nChaoya Jiang, Wei Ye, Mengfan Dong, Hongrui Jia,\nHaiyang Xu, Ming Yan, Ji Zhang, and Shikun Zhang.\n2024. Hal-eval: A universal and fine-grained hallu-\ncination evaluation framework for large vision lan-\nguage models. arXiv preprint arXiv:2402.15721 .\nYichen Jiang, Shikha Bordia, Zheng Zhong, Charles\nDognin, Maneesh Singh, and Mohit Bansal. 2020.\nHover: A dataset for many-hop fact extraction and\nclaim verification.\nMingyu Jin, Suiyuan Zhu, Beichen Wang, Zihao Zhou,\nChong Zhang, Yongfeng Zhang, et al. 2024. Attack-\neval: How to evaluate the effectiveness of jailbreak\nattacking on large language models. arXiv preprint\narXiv:2401.09002 .\nSasan Karamizadeh, Saman Shojae Chaeikar, and\nAlireza Jolfaei. 2023. Adult content image recogni-\ntion by boltzmann machine limited and deep learning.\nEvolutionary Intelligence , 16(4):1185\u20131194.\nElise Karinshak, Sunny Xun Liu, Joon Sung Park, and\nJeffrey T Hancock. 2023. Working with ai to per-\nsuade: Examining a large language model\u2019s abil-\nity to generate pro-vaccination messages. Proceed-\nings of the ACM on Human-Computer Interaction ,\n7(CSCW1):1\u201329.\nEnkelejda Kasneci, Kathrin Se\u00dfler, Stefan K\u00fcchemann,\nMaria Bannert, Daryna Dementieva, Frank Fischer,\nUrs Gasser, Georg Groh, Stephan G\u00fcnnemann, Eyke\nH\u00fcllermeier, et al. 2023. Chatgpt for good? on op-\nportunities and challenges of large language models\nfor education. Learning and individual differences ,\n103:102274.\nDaniel Martin Katz, Michael James Bommarito, Shang\nGao, and Pablo Arredondo. 2024. Gpt-4 passes the\nbar exam. Philosophical Transactions of the Royal\nSociety A , 382(2270):20230254.\nAkbir Khan, John Hughes, Dan Valentine, Laura\nRuis, Kshitij Sachan, Ansh Radhakrishnan, Edward\nGrefenstette, Samuel R Bowman, Tim Rockt\u00e4schel,\nand Ethan Perez. 2024. Debating with more per-\nsuasive llms leads to more truthful answers. arXiv\npreprint arXiv:2402.06782 .\nMinju Kim, Heuiyeen Yeen, and Myoung-Wan Koo.\n2024a.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2132, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "00f2422b-690e-4b36-bb74-3127acf98f92": {"__data__": {"id_": "00f2422b-690e-4b36-bb74-3127acf98f92", "embedding": null, "metadata": {"page_label": "18", "file_name": "2406_00936v1.pdf", "Title of this paper": "A Survey of Useful LLM Evaluation", "Authors": "Ji-Lun Peng, Sijia Cheng, Egil Diau, Yung-Yu Shih, Po-Heng Chen, Yen-Ting Lin, Yun-Nung Chen", "Date published": "06/03/2024", "URL": "http://arxiv.org/abs/2406.00936v1", "summary": "LLMs have gotten attention across various research domains due to their\nexceptional performance on a wide range of complex tasks. Therefore, refined\nmethods to evaluate the capabilities of LLMs are needed to determine the tasks\nand responsibility they should undertake. Our study mainly discussed how LLMs,\nas useful tools, should be effectively assessed. We proposed the two-stage\nframework: from ``core ability'' to ``agent'', clearly explaining how LLMs can\nbe applied based on their specific capabilities, along with the evaluation\nmethods in each stage. Core ability refers to the capabilities that LLMs need\nin order to generate high-quality natural language texts. After confirming LLMs\npossess core ability, they can solve real-world and complex tasks as agent. In\nthe \"core ability\" stage, we discussed the reasoning ability, societal impact,\nand domain knowledge of LLMs. In the ``agent'' stage, we demonstrated embodied\naction, planning, and tool learning of LLMs agent applications. Finally, we\nexamined the challenges currently confronting the evaluation methods for LLMs,\nas well as the directions for future development."}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "dd73c8d9-67ac-4670-8a4f-8bc81168290b", "node_type": "4", "metadata": {"page_label": "18", "file_name": "2406_00936v1.pdf", "Title of this paper": "A Survey of Useful LLM Evaluation", "Authors": "Ji-Lun Peng, Sijia Cheng, Egil Diau, Yung-Yu Shih, Po-Heng Chen, Yen-Ting Lin, Yun-Nung Chen", "Date published": "06/03/2024", "URL": "http://arxiv.org/abs/2406.00936v1", "summary": "LLMs have gotten attention across various research domains due to their\nexceptional performance on a wide range of complex tasks. Therefore, refined\nmethods to evaluate the capabilities of LLMs are needed to determine the tasks\nand responsibility they should undertake. Our study mainly discussed how LLMs,\nas useful tools, should be effectively assessed. We proposed the two-stage\nframework: from ``core ability'' to ``agent'', clearly explaining how LLMs can\nbe applied based on their specific capabilities, along with the evaluation\nmethods in each stage. Core ability refers to the capabilities that LLMs need\nin order to generate high-quality natural language texts. After confirming LLMs\npossess core ability, they can solve real-world and complex tasks as agent. In\nthe \"core ability\" stage, we discussed the reasoning ability, societal impact,\nand domain knowledge of LLMs. In the ``agent'' stage, we demonstrated embodied\naction, planning, and tool learning of LLMs agent applications. Finally, we\nexamined the challenges currently confronting the evaluation methods for LLMs,\nas well as the directions for future development."}, "hash": "835524bcd01d28e4c181a1f4340c159e59e93efc34461fe3b06b39c7969abf1b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "88cd3980-d3c2-412a-8ee4-75e1b10d220c", "node_type": "1", "metadata": {"page_label": "18", "file_name": "2406_00936v1.pdf", "Title of this paper": "A Survey of Useful LLM Evaluation", "Authors": "Ji-Lun Peng, Sijia Cheng, Egil Diau, Yung-Yu Shih, Po-Heng Chen, Yen-Ting Lin, Yun-Nung Chen", "Date published": "06/03/2024", "URL": "http://arxiv.org/abs/2406.00936v1", "summary": "LLMs have gotten attention across various research domains due to their\nexceptional performance on a wide range of complex tasks. Therefore, refined\nmethods to evaluate the capabilities of LLMs are needed to determine the tasks\nand responsibility they should undertake. Our study mainly discussed how LLMs,\nas useful tools, should be effectively assessed. We proposed the two-stage\nframework: from ``core ability'' to ``agent'', clearly explaining how LLMs can\nbe applied based on their specific capabilities, along with the evaluation\nmethods in each stage. Core ability refers to the capabilities that LLMs need\nin order to generate high-quality natural language texts. After confirming LLMs\npossess core ability, they can solve real-world and complex tasks as agent. In\nthe \"core ability\" stage, we discussed the reasoning ability, societal impact,\nand domain knowledge of LLMs. In the ``agent'' stage, we demonstrated embodied\naction, planning, and tool learning of LLMs agent applications. Finally, we\nexamined the challenges currently confronting the evaluation methods for LLMs,\nas well as the directions for future development."}, "hash": "c45386f27985a2b46c7fd80e573ae9ccac394fcbbf1b4ed2071ab2a47805b112", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ebfc8357-35f4-4fc4-9b44-74eddd833a2b", "node_type": "1", "metadata": {}, "hash": "d18375e0a13c7138dac98e59d57f9152cc94f5f897cf5fc2c4521646fdfbdb32", "class_name": "RelatedNodeInfo"}}, "text": "Learning and individual differences ,\n103:102274.\nDaniel Martin Katz, Michael James Bommarito, Shang\nGao, and Pablo Arredondo. 2024. Gpt-4 passes the\nbar exam. Philosophical Transactions of the Royal\nSociety A , 382(2270):20230254.\nAkbir Khan, John Hughes, Dan Valentine, Laura\nRuis, Kshitij Sachan, Ansh Radhakrishnan, Edward\nGrefenstette, Samuel R Bowman, Tim Rockt\u00e4schel,\nand Ethan Perez. 2024. Debating with more per-\nsuasive llms leads to more truthful answers. arXiv\npreprint arXiv:2402.06782 .\nMinju Kim, Heuiyeen Yeen, and Myoung-Wan Koo.\n2024a. Towards context-based violence detection:\nA korean crime dialogue dataset. In Findings of the\nAssociation for Computational Linguistics: EACL\n2024 , pages 603\u2013623.\nSiwon Kim, Sangdoo Yun, Hwaran Lee, Martin Gubri,\nSungroh Yoon, and Seong Joon Oh. 2024b. Propile:\nProbing privacy leakage in large language models.\nAdvances in Neural Information Processing Systems ,\n36.Michal Kosinski. 2023. Theory of mind may have spon-\ntaneously emerged in large language models. arXiv\npreprint arXiv:2302.02083 , 4:169.\nArun HS Kumar. 2023. Analysis of chatgpt tool to as-\nsess the potential of its utility for academic writing in\nbiomedical domain. Biology, Engineering, Medicine\nand Science Reports , 9(1):24\u201330.\nTiffany H Kung, Morgan Cheatham, Arielle Medenilla,\nCzarina Sillos, Lorie De Leon, Camille Elepa\u00f1o,\nMaria Madriaga, Rimel Aggabao, Giezel Diaz-\nCandido, James Maningo, et al. 2023. Performance\nof chatgpt on usmle: potential for ai-assisted medical\neducation using large language models. PLoS digital\nhealth , 2(2):e0000198.\nTom Kwiatkowski, Jennimaria Palomaki, Olivia Red-\nfield, Michael Collins, Ankur Parikh, Chris Alberti,\nDanielle Epstein, Illia Polosukhin, Jacob Devlin, Ken-\nton Lee, et al. 2019. Natural questions: a benchmark\nfor question answering research. Transactions of the\nAssociation for Computational Linguistics , 7:453\u2013\n466.\nJean Lee, Nicholas Stevens, Soyeon Caren Han, and\nMinseok Song. 2024. A survey of large lan-\nguage models in finance (finllms). arXiv preprint\narXiv:2402.02315 .\nLijun Li, Bowen Dong, Ruohui Wang, Xuhao Hu, Wang-\nmeng Zuo, Dahua Lin, Yu Qiao, and Jing Shao.\n2024a.", "mimetype": "text/plain", "start_char_idx": 1579, "end_char_idx": 3742, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ebfc8357-35f4-4fc4-9b44-74eddd833a2b": {"__data__": {"id_": "ebfc8357-35f4-4fc4-9b44-74eddd833a2b", "embedding": null, "metadata": {"page_label": "18", "file_name": "2406_00936v1.pdf", "Title of this paper": "A Survey of Useful LLM Evaluation", "Authors": "Ji-Lun Peng, Sijia Cheng, Egil Diau, Yung-Yu Shih, Po-Heng Chen, Yen-Ting Lin, Yun-Nung Chen", "Date published": "06/03/2024", "URL": "http://arxiv.org/abs/2406.00936v1", "summary": "LLMs have gotten attention across various research domains due to their\nexceptional performance on a wide range of complex tasks. Therefore, refined\nmethods to evaluate the capabilities of LLMs are needed to determine the tasks\nand responsibility they should undertake. Our study mainly discussed how LLMs,\nas useful tools, should be effectively assessed. We proposed the two-stage\nframework: from ``core ability'' to ``agent'', clearly explaining how LLMs can\nbe applied based on their specific capabilities, along with the evaluation\nmethods in each stage. Core ability refers to the capabilities that LLMs need\nin order to generate high-quality natural language texts. After confirming LLMs\npossess core ability, they can solve real-world and complex tasks as agent. In\nthe \"core ability\" stage, we discussed the reasoning ability, societal impact,\nand domain knowledge of LLMs. In the ``agent'' stage, we demonstrated embodied\naction, planning, and tool learning of LLMs agent applications. Finally, we\nexamined the challenges currently confronting the evaluation methods for LLMs,\nas well as the directions for future development."}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "dd73c8d9-67ac-4670-8a4f-8bc81168290b", "node_type": "4", "metadata": {"page_label": "18", "file_name": "2406_00936v1.pdf", "Title of this paper": "A Survey of Useful LLM Evaluation", "Authors": "Ji-Lun Peng, Sijia Cheng, Egil Diau, Yung-Yu Shih, Po-Heng Chen, Yen-Ting Lin, Yun-Nung Chen", "Date published": "06/03/2024", "URL": "http://arxiv.org/abs/2406.00936v1", "summary": "LLMs have gotten attention across various research domains due to their\nexceptional performance on a wide range of complex tasks. Therefore, refined\nmethods to evaluate the capabilities of LLMs are needed to determine the tasks\nand responsibility they should undertake. Our study mainly discussed how LLMs,\nas useful tools, should be effectively assessed. We proposed the two-stage\nframework: from ``core ability'' to ``agent'', clearly explaining how LLMs can\nbe applied based on their specific capabilities, along with the evaluation\nmethods in each stage. Core ability refers to the capabilities that LLMs need\nin order to generate high-quality natural language texts. After confirming LLMs\npossess core ability, they can solve real-world and complex tasks as agent. In\nthe \"core ability\" stage, we discussed the reasoning ability, societal impact,\nand domain knowledge of LLMs. In the ``agent'' stage, we demonstrated embodied\naction, planning, and tool learning of LLMs agent applications. Finally, we\nexamined the challenges currently confronting the evaluation methods for LLMs,\nas well as the directions for future development."}, "hash": "835524bcd01d28e4c181a1f4340c159e59e93efc34461fe3b06b39c7969abf1b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "00f2422b-690e-4b36-bb74-3127acf98f92", "node_type": "1", "metadata": {"page_label": "18", "file_name": "2406_00936v1.pdf", "Title of this paper": "A Survey of Useful LLM Evaluation", "Authors": "Ji-Lun Peng, Sijia Cheng, Egil Diau, Yung-Yu Shih, Po-Heng Chen, Yen-Ting Lin, Yun-Nung Chen", "Date published": "06/03/2024", "URL": "http://arxiv.org/abs/2406.00936v1", "summary": "LLMs have gotten attention across various research domains due to their\nexceptional performance on a wide range of complex tasks. Therefore, refined\nmethods to evaluate the capabilities of LLMs are needed to determine the tasks\nand responsibility they should undertake. Our study mainly discussed how LLMs,\nas useful tools, should be effectively assessed. We proposed the two-stage\nframework: from ``core ability'' to ``agent'', clearly explaining how LLMs can\nbe applied based on their specific capabilities, along with the evaluation\nmethods in each stage. Core ability refers to the capabilities that LLMs need\nin order to generate high-quality natural language texts. After confirming LLMs\npossess core ability, they can solve real-world and complex tasks as agent. In\nthe \"core ability\" stage, we discussed the reasoning ability, societal impact,\nand domain knowledge of LLMs. In the ``agent'' stage, we demonstrated embodied\naction, planning, and tool learning of LLMs agent applications. Finally, we\nexamined the challenges currently confronting the evaluation methods for LLMs,\nas well as the directions for future development."}, "hash": "42f306f61300ba9133ef67c35b51ebf71e050d605932bf92e6eb7861c72fe3c1", "class_name": "RelatedNodeInfo"}}, "text": "Tom Kwiatkowski, Jennimaria Palomaki, Olivia Red-\nfield, Michael Collins, Ankur Parikh, Chris Alberti,\nDanielle Epstein, Illia Polosukhin, Jacob Devlin, Ken-\nton Lee, et al. 2019. Natural questions: a benchmark\nfor question answering research. Transactions of the\nAssociation for Computational Linguistics , 7:453\u2013\n466.\nJean Lee, Nicholas Stevens, Soyeon Caren Han, and\nMinseok Song. 2024. A survey of large lan-\nguage models in finance (finllms). arXiv preprint\narXiv:2402.02315 .\nLijun Li, Bowen Dong, Ruohui Wang, Xuhao Hu, Wang-\nmeng Zuo, Dahua Lin, Yu Qiao, and Jing Shao.\n2024a. Salad-bench: A hierarchical and compre-\nhensive safety benchmark for large language models.\nMinghao Li, Yingxiu Zhao, Bowen Yu, Feifan Song,\nHangyu Li, Haiyang Yu, Zhoujun Li, Fei Huang, and\nYongbin Li. 2023a. Api-bank: A comprehensive\nbenchmark for tool-augmented llms.\nYinheng Li, Shaofei Wang, Han Ding, and Hang Chen.\n2023b. Large language models in finance: A sur-\nvey. In Proceedings of the Fourth ACM International\nConference on AI in Finance , pages 374\u2013382.\nZhen Li, Xiaohan Xu, Tao Shen, Can Xu, Jia-Chen\nGu, and Chongyang Tao. 2024b. Leveraging large\nlanguage models for nlg evaluation: A survey. arXiv\npreprint arXiv:2401.07103 .\nJacky Liang, Wenlong Huang, Fei Xia, Peng Xu, Karol\nHausman, Brian Ichter, Pete Florence, and Andy\nZeng. 2023. Code as policies: Language model\nprograms for embodied control. In 2023 IEEE In-\nternational Conference on Robotics and Automation\n(ICRA) , pages 9493\u20139500. IEEE.\nPercy Liang, Rishi Bommasani, Tony Lee, Dimitris\nTsipras, Dilara Soylu, Michihiro Yasunaga, Yian\nZhang, Deepak Narayanan, Yuhuai Wu, Ananya Ku-\nmar, et al. 2022. Holistic evaluation of language\nmodels. arXiv preprint arXiv:2211.09110 .\nYichan Liang, Jianheng Li, and Jian Yin. 2019. A new\nmulti-choice reading comprehension dataset for cur-\nriculum learning. In Asian Conference on Machine\nLearning , pages 742\u2013757. PMLR.\n18", "mimetype": "text/plain", "start_char_idx": 3158, "end_char_idx": 5083, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "22021f9f-37fd-464c-a0b3-ff84eadd707f": {"__data__": {"id_": "22021f9f-37fd-464c-a0b3-ff84eadd707f", "embedding": null, "metadata": {"page_label": "19", "file_name": "2406_00936v1.pdf", "Title of this paper": "A Survey of Useful LLM Evaluation", "Authors": "Ji-Lun Peng, Sijia Cheng, Egil Diau, Yung-Yu Shih, Po-Heng Chen, Yen-Ting Lin, Yun-Nung Chen", "Date published": "06/03/2024", "URL": "http://arxiv.org/abs/2406.00936v1", "summary": "LLMs have gotten attention across various research domains due to their\nexceptional performance on a wide range of complex tasks. Therefore, refined\nmethods to evaluate the capabilities of LLMs are needed to determine the tasks\nand responsibility they should undertake. Our study mainly discussed how LLMs,\nas useful tools, should be effectively assessed. We proposed the two-stage\nframework: from ``core ability'' to ``agent'', clearly explaining how LLMs can\nbe applied based on their specific capabilities, along with the evaluation\nmethods in each stage. Core ability refers to the capabilities that LLMs need\nin order to generate high-quality natural language texts. After confirming LLMs\npossess core ability, they can solve real-world and complex tasks as agent. In\nthe \"core ability\" stage, we discussed the reasoning ability, societal impact,\nand domain knowledge of LLMs. In the ``agent'' stage, we demonstrated embodied\naction, planning, and tool learning of LLMs agent applications. Finally, we\nexamined the challenges currently confronting the evaluation methods for LLMs,\nas well as the directions for future development."}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "80d2bc89-634a-4619-b582-8b2e5b6ddead", "node_type": "4", "metadata": {"page_label": "19", "file_name": "2406_00936v1.pdf", "Title of this paper": "A Survey of Useful LLM Evaluation", "Authors": "Ji-Lun Peng, Sijia Cheng, Egil Diau, Yung-Yu Shih, Po-Heng Chen, Yen-Ting Lin, Yun-Nung Chen", "Date published": "06/03/2024", "URL": "http://arxiv.org/abs/2406.00936v1", "summary": "LLMs have gotten attention across various research domains due to their\nexceptional performance on a wide range of complex tasks. Therefore, refined\nmethods to evaluate the capabilities of LLMs are needed to determine the tasks\nand responsibility they should undertake. Our study mainly discussed how LLMs,\nas useful tools, should be effectively assessed. We proposed the two-stage\nframework: from ``core ability'' to ``agent'', clearly explaining how LLMs can\nbe applied based on their specific capabilities, along with the evaluation\nmethods in each stage. Core ability refers to the capabilities that LLMs need\nin order to generate high-quality natural language texts. After confirming LLMs\npossess core ability, they can solve real-world and complex tasks as agent. In\nthe \"core ability\" stage, we discussed the reasoning ability, societal impact,\nand domain knowledge of LLMs. In the ``agent'' stage, we demonstrated embodied\naction, planning, and tool learning of LLMs agent applications. Finally, we\nexamined the challenges currently confronting the evaluation methods for LLMs,\nas well as the directions for future development."}, "hash": "762453126433252ef69022b6b770fbb7353425270b21a5f22cc0970d37e42eb1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3fdfbf4c-1f9b-410a-b4d4-9d0217f8da33", "node_type": "1", "metadata": {}, "hash": "fb98838dd44da2f4c1ccc179f02b37334e5425630cc11d01f113167c8419ac9d", "class_name": "RelatedNodeInfo"}}, "text": "Davide Liga and Livio Robaldo. 2023. Fine-tuning\ngpt-3 for legal rule classification. Computer Law &\nSecurity Review , 51:105864.\nChin-Yew Lin. 2004. Rouge: A package for automatic\nevaluation of summaries. In Text summarization\nbranches out , pages 74\u201381.\nStephanie Lin, Jacob Hilton, and Owain Evans. 2022.\nTruthfulqa: Measuring how models mimic human\nfalsehoods.\nZi Lin, Zihan Wang, Yongqi Tong, Yangkun Wang,\nYuxin Guo, Yujia Wang, and Jingbo Shang. 2023.\nToxicchat: Unveiling hidden challenges of toxicity\ndetection in real-world user-ai conversation.\nFang Liu, Yang Liu, Lin Shi, Houkun Huang, Ruifeng\nWang, Zhen Yang, and Li Zhang. 2024a. Exploring\nand evaluating hallucinations in llm-powered code\ngeneration. arXiv preprint arXiv:2404.00971 .\nHanmeng Liu, Ruoxi Ning, Zhiyang Teng, Jian Liu, Qiji\nZhou, and Yue Zhang. 2023. Evaluating the logical\nreasoning ability of chatgpt and gpt-4.\nXiaogeng Liu, Zhiyuan Yu, Yizhe Zhang, Ning Zhang,\nand Chaowei Xiao. 2024b. Automatic and univer-\nsal prompt injection attacks against large language\nmodels. arXiv preprint arXiv:2403.04957 .\nXin Liu, Henglin Shi, Haoyu Chen, Zitong Yu, Xi-\naobai Li, and Guoying Zhao. 2021a. imigue: An\nidentity-free video dataset for micro-gesture under-\nstanding and emotion analysis. In Proceedings of\nthe IEEE/CVF conference on computer vision and\npattern recognition , pages 10631\u201310642.\nZhuang Liu, Degen Huang, Kaiyu Huang, Zhuang Li,\nand Jun Zhao. 2021b. Finbert: A pre-trained finan-\ncial language representation model for financial text\nmining. In Proceedings of the twenty-ninth interna-\ntional conference on international joint conferences\non artificial intelligence , pages 4513\u20134519.\nHao Lu, Xuesong Niu, Jiyao Wang, Yin Wang, Qingy-\nong Hu, Jiaqi Tang, Yuting Zhang, Kaishen Yuan, Bin\nHuang, Zitong Yu, et al. 2024. Gpt as psychologist?\npreliminary evaluations for gpt-4v on visual affective\ncomputing. arXiv preprint arXiv:2403.05916 .\nKyle Mahowald, Anna A Ivanova, Idan A Blank, Nancy\nKanwisher, Joshua B Tenenbaum, and Evelina Fe-\ndorenko. 2023. Dissociating language and thought\nin large language models: a cognitive perspective.\narXiv preprint arXiv:2301.06627 .", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2162, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3fdfbf4c-1f9b-410a-b4d4-9d0217f8da33": {"__data__": {"id_": "3fdfbf4c-1f9b-410a-b4d4-9d0217f8da33", "embedding": null, "metadata": {"page_label": "19", "file_name": "2406_00936v1.pdf", "Title of this paper": "A Survey of Useful LLM Evaluation", "Authors": "Ji-Lun Peng, Sijia Cheng, Egil Diau, Yung-Yu Shih, Po-Heng Chen, Yen-Ting Lin, Yun-Nung Chen", "Date published": "06/03/2024", "URL": "http://arxiv.org/abs/2406.00936v1", "summary": "LLMs have gotten attention across various research domains due to their\nexceptional performance on a wide range of complex tasks. Therefore, refined\nmethods to evaluate the capabilities of LLMs are needed to determine the tasks\nand responsibility they should undertake. Our study mainly discussed how LLMs,\nas useful tools, should be effectively assessed. We proposed the two-stage\nframework: from ``core ability'' to ``agent'', clearly explaining how LLMs can\nbe applied based on their specific capabilities, along with the evaluation\nmethods in each stage. Core ability refers to the capabilities that LLMs need\nin order to generate high-quality natural language texts. After confirming LLMs\npossess core ability, they can solve real-world and complex tasks as agent. In\nthe \"core ability\" stage, we discussed the reasoning ability, societal impact,\nand domain knowledge of LLMs. In the ``agent'' stage, we demonstrated embodied\naction, planning, and tool learning of LLMs agent applications. Finally, we\nexamined the challenges currently confronting the evaluation methods for LLMs,\nas well as the directions for future development."}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "80d2bc89-634a-4619-b582-8b2e5b6ddead", "node_type": "4", "metadata": {"page_label": "19", "file_name": "2406_00936v1.pdf", "Title of this paper": "A Survey of Useful LLM Evaluation", "Authors": "Ji-Lun Peng, Sijia Cheng, Egil Diau, Yung-Yu Shih, Po-Heng Chen, Yen-Ting Lin, Yun-Nung Chen", "Date published": "06/03/2024", "URL": "http://arxiv.org/abs/2406.00936v1", "summary": "LLMs have gotten attention across various research domains due to their\nexceptional performance on a wide range of complex tasks. Therefore, refined\nmethods to evaluate the capabilities of LLMs are needed to determine the tasks\nand responsibility they should undertake. Our study mainly discussed how LLMs,\nas useful tools, should be effectively assessed. We proposed the two-stage\nframework: from ``core ability'' to ``agent'', clearly explaining how LLMs can\nbe applied based on their specific capabilities, along with the evaluation\nmethods in each stage. Core ability refers to the capabilities that LLMs need\nin order to generate high-quality natural language texts. After confirming LLMs\npossess core ability, they can solve real-world and complex tasks as agent. In\nthe \"core ability\" stage, we discussed the reasoning ability, societal impact,\nand domain knowledge of LLMs. In the ``agent'' stage, we demonstrated embodied\naction, planning, and tool learning of LLMs agent applications. Finally, we\nexamined the challenges currently confronting the evaluation methods for LLMs,\nas well as the directions for future development."}, "hash": "762453126433252ef69022b6b770fbb7353425270b21a5f22cc0970d37e42eb1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "22021f9f-37fd-464c-a0b3-ff84eadd707f", "node_type": "1", "metadata": {"page_label": "19", "file_name": "2406_00936v1.pdf", "Title of this paper": "A Survey of Useful LLM Evaluation", "Authors": "Ji-Lun Peng, Sijia Cheng, Egil Diau, Yung-Yu Shih, Po-Heng Chen, Yen-Ting Lin, Yun-Nung Chen", "Date published": "06/03/2024", "URL": "http://arxiv.org/abs/2406.00936v1", "summary": "LLMs have gotten attention across various research domains due to their\nexceptional performance on a wide range of complex tasks. Therefore, refined\nmethods to evaluate the capabilities of LLMs are needed to determine the tasks\nand responsibility they should undertake. Our study mainly discussed how LLMs,\nas useful tools, should be effectively assessed. We proposed the two-stage\nframework: from ``core ability'' to ``agent'', clearly explaining how LLMs can\nbe applied based on their specific capabilities, along with the evaluation\nmethods in each stage. Core ability refers to the capabilities that LLMs need\nin order to generate high-quality natural language texts. After confirming LLMs\npossess core ability, they can solve real-world and complex tasks as agent. In\nthe \"core ability\" stage, we discussed the reasoning ability, societal impact,\nand domain knowledge of LLMs. In the ``agent'' stage, we demonstrated embodied\naction, planning, and tool learning of LLMs agent applications. Finally, we\nexamined the challenges currently confronting the evaluation methods for LLMs,\nas well as the directions for future development."}, "hash": "c99559c337f0e618b3c4f49e5804b8cfda9a252a846199409e060fdc32fe600b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "27fe7941-c483-4565-bb65-5c9503b5c298", "node_type": "1", "metadata": {}, "hash": "7fbcc5b6742f377fd288d454f8e0b0fd546af6c1a5fdd0bb02417104b9faca34", "class_name": "RelatedNodeInfo"}}, "text": "In Proceedings of the twenty-ninth interna-\ntional conference on international joint conferences\non artificial intelligence , pages 4513\u20134519.\nHao Lu, Xuesong Niu, Jiyao Wang, Yin Wang, Qingy-\nong Hu, Jiaqi Tang, Yuting Zhang, Kaishen Yuan, Bin\nHuang, Zitong Yu, et al. 2024. Gpt as psychologist?\npreliminary evaluations for gpt-4v on visual affective\ncomputing. arXiv preprint arXiv:2403.05916 .\nKyle Mahowald, Anna A Ivanova, Idan A Blank, Nancy\nKanwisher, Joshua B Tenenbaum, and Evelina Fe-\ndorenko. 2023. Dissociating language and thought\nin large language models: a cognitive perspective.\narXiv preprint arXiv:2301.06627 .\nMacedo Maia, Siegfried Handschuh, Andr\u00e9 Freitas,\nBrian Davis, Ross McDermott, Manel Zarrouk, and\nAlexandra Balahur. 2018. Www\u201918 open challenge:\nfinancial opinion mining and question answering. In\nCompanion proceedings of the the web conference\n2018 , pages 1941\u20131942.Pekka Malo, Ankur Sinha, Pekka Korhonen, Jyrki Wal-\nlenius, and Pyry Takala. 2014. Good debt or bad\ndebt: Detecting semantic orientations in economic\ntexts. Journal of the Association for Information\nScience and Technology , 65(4):782\u2013796.\nBen Mann, N Ryder, M Subbiah, J Kaplan, P Dhari-\nwal, A Neelakantan, P Shyam, G Sastry, A Askell,\nS Agarwal, et al. 2020. Language models are few-\nshot learners. arXiv preprint arXiv:2005.14165 .\nTodor Markov, Chong Zhang, Sandhini Agarwal, Tyna\nEloundou, Teddy Lee, Steven Adler, Angela Jiang,\nand Lilian Weng. 2023. A holistic approach to unde-\nsired content detection in the real world.\nS Mohammad Mavadati, Mohammad H Mahoor, Kevin\nBartlett, Philip Trinh, and Jeffrey F Cohn. 2013.\nDisfa: A spontaneous facial action intensity database.\nIEEE Transactions on Affective Computing , 4(2):151\u2013\n160.\nJacob Menick, Maja Trebacz, Vladimir Mikulik, John\nAslanides, Francis Song, Martin Chadwick, Mia\nGlaese, Susannah Young, Lucy Campbell-Gillingam,\nGeoffrey Irving, et al. 2022. Teaching language mod-\nels to support answers with verified quotes. arxiv.\nNing Miao, Yee Whye Teh, and Tom Rainforth.\n2023. Selfcheck: Using llms to zero-shot check\ntheir own step-by-step reasoning. arXiv preprint\narXiv:2308.00436 .\nTodor Mihaylov, Peter Clark, Tushar Khot, and Ashish\nSabharwal.", "mimetype": "text/plain", "start_char_idx": 1534, "end_char_idx": 3743, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "27fe7941-c483-4565-bb65-5c9503b5c298": {"__data__": {"id_": "27fe7941-c483-4565-bb65-5c9503b5c298", "embedding": null, "metadata": {"page_label": "19", "file_name": "2406_00936v1.pdf", "Title of this paper": "A Survey of Useful LLM Evaluation", "Authors": "Ji-Lun Peng, Sijia Cheng, Egil Diau, Yung-Yu Shih, Po-Heng Chen, Yen-Ting Lin, Yun-Nung Chen", "Date published": "06/03/2024", "URL": "http://arxiv.org/abs/2406.00936v1", "summary": "LLMs have gotten attention across various research domains due to their\nexceptional performance on a wide range of complex tasks. Therefore, refined\nmethods to evaluate the capabilities of LLMs are needed to determine the tasks\nand responsibility they should undertake. Our study mainly discussed how LLMs,\nas useful tools, should be effectively assessed. We proposed the two-stage\nframework: from ``core ability'' to ``agent'', clearly explaining how LLMs can\nbe applied based on their specific capabilities, along with the evaluation\nmethods in each stage. Core ability refers to the capabilities that LLMs need\nin order to generate high-quality natural language texts. After confirming LLMs\npossess core ability, they can solve real-world and complex tasks as agent. In\nthe \"core ability\" stage, we discussed the reasoning ability, societal impact,\nand domain knowledge of LLMs. In the ``agent'' stage, we demonstrated embodied\naction, planning, and tool learning of LLMs agent applications. Finally, we\nexamined the challenges currently confronting the evaluation methods for LLMs,\nas well as the directions for future development."}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "80d2bc89-634a-4619-b582-8b2e5b6ddead", "node_type": "4", "metadata": {"page_label": "19", "file_name": "2406_00936v1.pdf", "Title of this paper": "A Survey of Useful LLM Evaluation", "Authors": "Ji-Lun Peng, Sijia Cheng, Egil Diau, Yung-Yu Shih, Po-Heng Chen, Yen-Ting Lin, Yun-Nung Chen", "Date published": "06/03/2024", "URL": "http://arxiv.org/abs/2406.00936v1", "summary": "LLMs have gotten attention across various research domains due to their\nexceptional performance on a wide range of complex tasks. Therefore, refined\nmethods to evaluate the capabilities of LLMs are needed to determine the tasks\nand responsibility they should undertake. Our study mainly discussed how LLMs,\nas useful tools, should be effectively assessed. We proposed the two-stage\nframework: from ``core ability'' to ``agent'', clearly explaining how LLMs can\nbe applied based on their specific capabilities, along with the evaluation\nmethods in each stage. Core ability refers to the capabilities that LLMs need\nin order to generate high-quality natural language texts. After confirming LLMs\npossess core ability, they can solve real-world and complex tasks as agent. In\nthe \"core ability\" stage, we discussed the reasoning ability, societal impact,\nand domain knowledge of LLMs. In the ``agent'' stage, we demonstrated embodied\naction, planning, and tool learning of LLMs agent applications. Finally, we\nexamined the challenges currently confronting the evaluation methods for LLMs,\nas well as the directions for future development."}, "hash": "762453126433252ef69022b6b770fbb7353425270b21a5f22cc0970d37e42eb1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3fdfbf4c-1f9b-410a-b4d4-9d0217f8da33", "node_type": "1", "metadata": {"page_label": "19", "file_name": "2406_00936v1.pdf", "Title of this paper": "A Survey of Useful LLM Evaluation", "Authors": "Ji-Lun Peng, Sijia Cheng, Egil Diau, Yung-Yu Shih, Po-Heng Chen, Yen-Ting Lin, Yun-Nung Chen", "Date published": "06/03/2024", "URL": "http://arxiv.org/abs/2406.00936v1", "summary": "LLMs have gotten attention across various research domains due to their\nexceptional performance on a wide range of complex tasks. Therefore, refined\nmethods to evaluate the capabilities of LLMs are needed to determine the tasks\nand responsibility they should undertake. Our study mainly discussed how LLMs,\nas useful tools, should be effectively assessed. We proposed the two-stage\nframework: from ``core ability'' to ``agent'', clearly explaining how LLMs can\nbe applied based on their specific capabilities, along with the evaluation\nmethods in each stage. Core ability refers to the capabilities that LLMs need\nin order to generate high-quality natural language texts. After confirming LLMs\npossess core ability, they can solve real-world and complex tasks as agent. In\nthe \"core ability\" stage, we discussed the reasoning ability, societal impact,\nand domain knowledge of LLMs. In the ``agent'' stage, we demonstrated embodied\naction, planning, and tool learning of LLMs agent applications. Finally, we\nexamined the challenges currently confronting the evaluation methods for LLMs,\nas well as the directions for future development."}, "hash": "9cd889643b73f4932232a1a540039b8577b045c3b6a5be7443998dee315fbcaf", "class_name": "RelatedNodeInfo"}}, "text": "2013.\nDisfa: A spontaneous facial action intensity database.\nIEEE Transactions on Affective Computing , 4(2):151\u2013\n160.\nJacob Menick, Maja Trebacz, Vladimir Mikulik, John\nAslanides, Francis Song, Martin Chadwick, Mia\nGlaese, Susannah Young, Lucy Campbell-Gillingam,\nGeoffrey Irving, et al. 2022. Teaching language mod-\nels to support answers with verified quotes. arxiv.\nNing Miao, Yee Whye Teh, and Tom Rainforth.\n2023. Selfcheck: Using llms to zero-shot check\ntheir own step-by-step reasoning. arXiv preprint\narXiv:2308.00436 .\nTodor Mihaylov, Peter Clark, Tushar Khot, and Ashish\nSabharwal. 2018. Can a suit of armor conduct elec-\ntricity? a new dataset for open book question answer-\ning. In EMNLP .\nBonan Min, Hayley Ross, Elior Sulem, Amir\nPouran Ben Veyseh, Thien Huu Nguyen, Oscar Sainz,\nEneko Agirre, Ilana Heintz, and Dan Roth. 2023.\nRecent advances in natural language processing via\nlarge pre-trained language models: A survey. ACM\nComputing Surveys , 56(2):1\u201340.\nSungrim Moon, Serguei Pakhomov, Nathan Liu,\nJames O Ryan, and Genevieve B Melton. 2014.\nA sense inventory for clinical abbreviations and\nacronyms created using clinical notes and medical\ndictionary resources. Journal of the American Medi-\ncal Informatics Association , 21(2):299\u2013307.\nNikesh Muthukrishnan, Farhad Maleki, Katie Ovens,\nCaroline Reinhold, Behzad Forghani, Reza Forghani,\net al. 2020. Brief history of artificial intelligence.\nNeuroimaging Clinics of North America , 30(4):393\u2013\n399.\nReiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu,\nLong Ouyang, Christina Kim, Christopher Hesse,\nShantanu Jain, Vineet Kosaraju, William Saunders,\nXu Jiang, Karl Cobbe, Tyna Eloundou, Gretchen\nKrueger, Kevin Button, Matthew Knight, Benjamin\nChess, and John Schulman. 2022. Webgpt: Browser-\nassisted question-answering with human feedback.\n19", "mimetype": "text/plain", "start_char_idx": 3151, "end_char_idx": 4969, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4cdb609f-31c8-42f4-8831-8388be220894": {"__data__": {"id_": "4cdb609f-31c8-42f4-8831-8388be220894", "embedding": null, "metadata": {"page_label": "20", "file_name": "2406_00936v1.pdf", "Title of this paper": "A Survey of Useful LLM Evaluation", "Authors": "Ji-Lun Peng, Sijia Cheng, Egil Diau, Yung-Yu Shih, Po-Heng Chen, Yen-Ting Lin, Yun-Nung Chen", "Date published": "06/03/2024", "URL": "http://arxiv.org/abs/2406.00936v1", "summary": "LLMs have gotten attention across various research domains due to their\nexceptional performance on a wide range of complex tasks. Therefore, refined\nmethods to evaluate the capabilities of LLMs are needed to determine the tasks\nand responsibility they should undertake. Our study mainly discussed how LLMs,\nas useful tools, should be effectively assessed. We proposed the two-stage\nframework: from ``core ability'' to ``agent'', clearly explaining how LLMs can\nbe applied based on their specific capabilities, along with the evaluation\nmethods in each stage. Core ability refers to the capabilities that LLMs need\nin order to generate high-quality natural language texts. After confirming LLMs\npossess core ability, they can solve real-world and complex tasks as agent. In\nthe \"core ability\" stage, we discussed the reasoning ability, societal impact,\nand domain knowledge of LLMs. In the ``agent'' stage, we demonstrated embodied\naction, planning, and tool learning of LLMs agent applications. Finally, we\nexamined the challenges currently confronting the evaluation methods for LLMs,\nas well as the directions for future development."}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e94bfb13-444c-4371-b46f-d7a757f7282b", "node_type": "4", "metadata": {"page_label": "20", "file_name": "2406_00936v1.pdf", "Title of this paper": "A Survey of Useful LLM Evaluation", "Authors": "Ji-Lun Peng, Sijia Cheng, Egil Diau, Yung-Yu Shih, Po-Heng Chen, Yen-Ting Lin, Yun-Nung Chen", "Date published": "06/03/2024", "URL": "http://arxiv.org/abs/2406.00936v1", "summary": "LLMs have gotten attention across various research domains due to their\nexceptional performance on a wide range of complex tasks. Therefore, refined\nmethods to evaluate the capabilities of LLMs are needed to determine the tasks\nand responsibility they should undertake. Our study mainly discussed how LLMs,\nas useful tools, should be effectively assessed. We proposed the two-stage\nframework: from ``core ability'' to ``agent'', clearly explaining how LLMs can\nbe applied based on their specific capabilities, along with the evaluation\nmethods in each stage. Core ability refers to the capabilities that LLMs need\nin order to generate high-quality natural language texts. After confirming LLMs\npossess core ability, they can solve real-world and complex tasks as agent. In\nthe \"core ability\" stage, we discussed the reasoning ability, societal impact,\nand domain knowledge of LLMs. In the ``agent'' stage, we demonstrated embodied\naction, planning, and tool learning of LLMs agent applications. Finally, we\nexamined the challenges currently confronting the evaluation methods for LLMs,\nas well as the directions for future development."}, "hash": "29e31769758a2c84e16e653609dcba6a361b10d163f2b55d7f1bd6d63cd06961", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7086ca5f-f587-4170-abfd-3a0366d70f7d", "node_type": "1", "metadata": {}, "hash": "39a8eb67a77e7b6686f5359f4fdcf8310bc511426195efdee0c762a1dcca45f9", "class_name": "RelatedNodeInfo"}}, "text": "Tahereh Nayerifard, Haleh Amintoosi, Abbas Ghaemi\nBafghi, and Ali Dehghantanha. 2023. Machine learn-\ning in digital forensics: a systematic literature review.\narXiv preprint arXiv:2306.04965 .\nStefano Nolfi. 2023. On the unexpected abilities of large\nlanguage models. arXiv preprint arXiv:2308.09720 .\nOscar Oviedo-Trespalacios, Amy E Peden, Thomas\nCole-Hunter, Arianna Costantini, Milad Haghani,\nJE Rod, Sage Kelly, Helma Torkamaan, Amina Tariq,\nJames David Albert Newton, et al. 2023. The risks\nof using chatgpt to obtain common safety-related\ninformation and advice. Safety science , 167:106244.\nMonica Palmirani and Fabio Vitali. 2011. Akoma-ntoso\nfor legal documents. Legislative XML for the Seman-\ntic Web: Principles, Models, Standards for Document\nManagement , pages 75\u2013100.\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-\nJing Zhu. 2002. Bleu: a method for automatic evalu-\nation of machine translation. In Proceedings of the\n40th annual meeting of the Association for Computa-\ntional Linguistics , pages 311\u2013318.\nAlicia Parrish, Angelica Chen, Nikita Nangia,\nVishakh Padmakumar, Jason Phang, Jana Thompson,\nPhu Mon Htut, and Samuel R Bowman. 2021. Bbq:\nA hand-built bias benchmark for question answering.\narXiv preprint arXiv:2110.08193 .\nShishir G. Patil, Tianjun Zhang, Xin Wang, and\nJoseph E. Gonzalez. 2023. Gorilla: Large language\nmodel connected with massive apis.\nFabio Petroni, Tim Rockt\u00e4schel, Patrick Lewis, An-\nton Bakhtin, Yuxiang Wu, Alexander H Miller, and\nSebastian Riedel. 2019. Language models as knowl-\nedge bases? arXiv preprint arXiv:1909.01066 .\nAyse Pinar Saygin, Ilyas Cicekli, and Varol Akman.\n2000. Turing test: 50 years later. Minds and ma-\nchines , 10(4):463\u2013518.\nCheng Qian, Chi Han, Yi R. Fung, Yujia Qin, Zhiyuan\nLiu, and Heng Ji. 2023. Creator: Tool creation for\ndisentangling abstract and concrete reasoning of large\nlanguage models.\nYujia Qin, Zihan Cai, Dian Jin, Lan Yan, Shihao\nLiang, Kunlun Zhu, Yankai Lin, Xu Han, Ning Ding,\nHuadong Wang, Ruobing Xie, Fanchao Qi, Zhiyuan\nLiu, Maosong Sun, and Jie Zhou. 2023a.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2063, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7086ca5f-f587-4170-abfd-3a0366d70f7d": {"__data__": {"id_": "7086ca5f-f587-4170-abfd-3a0366d70f7d", "embedding": null, "metadata": {"page_label": "20", "file_name": "2406_00936v1.pdf", "Title of this paper": "A Survey of Useful LLM Evaluation", "Authors": "Ji-Lun Peng, Sijia Cheng, Egil Diau, Yung-Yu Shih, Po-Heng Chen, Yen-Ting Lin, Yun-Nung Chen", "Date published": "06/03/2024", "URL": "http://arxiv.org/abs/2406.00936v1", "summary": "LLMs have gotten attention across various research domains due to their\nexceptional performance on a wide range of complex tasks. Therefore, refined\nmethods to evaluate the capabilities of LLMs are needed to determine the tasks\nand responsibility they should undertake. Our study mainly discussed how LLMs,\nas useful tools, should be effectively assessed. We proposed the two-stage\nframework: from ``core ability'' to ``agent'', clearly explaining how LLMs can\nbe applied based on their specific capabilities, along with the evaluation\nmethods in each stage. Core ability refers to the capabilities that LLMs need\nin order to generate high-quality natural language texts. After confirming LLMs\npossess core ability, they can solve real-world and complex tasks as agent. In\nthe \"core ability\" stage, we discussed the reasoning ability, societal impact,\nand domain knowledge of LLMs. In the ``agent'' stage, we demonstrated embodied\naction, planning, and tool learning of LLMs agent applications. Finally, we\nexamined the challenges currently confronting the evaluation methods for LLMs,\nas well as the directions for future development."}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e94bfb13-444c-4371-b46f-d7a757f7282b", "node_type": "4", "metadata": {"page_label": "20", "file_name": "2406_00936v1.pdf", "Title of this paper": "A Survey of Useful LLM Evaluation", "Authors": "Ji-Lun Peng, Sijia Cheng, Egil Diau, Yung-Yu Shih, Po-Heng Chen, Yen-Ting Lin, Yun-Nung Chen", "Date published": "06/03/2024", "URL": "http://arxiv.org/abs/2406.00936v1", "summary": "LLMs have gotten attention across various research domains due to their\nexceptional performance on a wide range of complex tasks. Therefore, refined\nmethods to evaluate the capabilities of LLMs are needed to determine the tasks\nand responsibility they should undertake. Our study mainly discussed how LLMs,\nas useful tools, should be effectively assessed. We proposed the two-stage\nframework: from ``core ability'' to ``agent'', clearly explaining how LLMs can\nbe applied based on their specific capabilities, along with the evaluation\nmethods in each stage. Core ability refers to the capabilities that LLMs need\nin order to generate high-quality natural language texts. After confirming LLMs\npossess core ability, they can solve real-world and complex tasks as agent. In\nthe \"core ability\" stage, we discussed the reasoning ability, societal impact,\nand domain knowledge of LLMs. In the ``agent'' stage, we demonstrated embodied\naction, planning, and tool learning of LLMs agent applications. Finally, we\nexamined the challenges currently confronting the evaluation methods for LLMs,\nas well as the directions for future development."}, "hash": "29e31769758a2c84e16e653609dcba6a361b10d163f2b55d7f1bd6d63cd06961", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4cdb609f-31c8-42f4-8831-8388be220894", "node_type": "1", "metadata": {"page_label": "20", "file_name": "2406_00936v1.pdf", "Title of this paper": "A Survey of Useful LLM Evaluation", "Authors": "Ji-Lun Peng, Sijia Cheng, Egil Diau, Yung-Yu Shih, Po-Heng Chen, Yen-Ting Lin, Yun-Nung Chen", "Date published": "06/03/2024", "URL": "http://arxiv.org/abs/2406.00936v1", "summary": "LLMs have gotten attention across various research domains due to their\nexceptional performance on a wide range of complex tasks. Therefore, refined\nmethods to evaluate the capabilities of LLMs are needed to determine the tasks\nand responsibility they should undertake. Our study mainly discussed how LLMs,\nas useful tools, should be effectively assessed. We proposed the two-stage\nframework: from ``core ability'' to ``agent'', clearly explaining how LLMs can\nbe applied based on their specific capabilities, along with the evaluation\nmethods in each stage. Core ability refers to the capabilities that LLMs need\nin order to generate high-quality natural language texts. After confirming LLMs\npossess core ability, they can solve real-world and complex tasks as agent. In\nthe \"core ability\" stage, we discussed the reasoning ability, societal impact,\nand domain knowledge of LLMs. In the ``agent'' stage, we demonstrated embodied\naction, planning, and tool learning of LLMs agent applications. Finally, we\nexamined the challenges currently confronting the evaluation methods for LLMs,\nas well as the directions for future development."}, "hash": "a810e3d0ad7e07c6c536a506aff9b13993ccd4f91d75b6e12bb3587e3afc2727", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4afef103-bb68-4f8a-b5eb-d2d8aafbbe07", "node_type": "1", "metadata": {}, "hash": "a1949cfdf49a8b3827588d9e88d06c4fe970e6113feb6b4bb2d5375a1cefe8f9", "class_name": "RelatedNodeInfo"}}, "text": "Ayse Pinar Saygin, Ilyas Cicekli, and Varol Akman.\n2000. Turing test: 50 years later. Minds and ma-\nchines , 10(4):463\u2013518.\nCheng Qian, Chi Han, Yi R. Fung, Yujia Qin, Zhiyuan\nLiu, and Heng Ji. 2023. Creator: Tool creation for\ndisentangling abstract and concrete reasoning of large\nlanguage models.\nYujia Qin, Zihan Cai, Dian Jin, Lan Yan, Shihao\nLiang, Kunlun Zhu, Yankai Lin, Xu Han, Ning Ding,\nHuadong Wang, Ruobing Xie, Fanchao Qi, Zhiyuan\nLiu, Maosong Sun, and Jie Zhou. 2023a. Webcpm:\nInteractive web search for chinese long-form ques-\ntion answering.\nYujia Qin, Shengding Hu, Yankai Lin, Weize Chen,\nNing Ding, Ganqu Cui, Zheni Zeng, Yufei Huang,\nChaojun Xiao, Chi Han, Yi Ren Fung, Yusheng Su,\nHuadong Wang, Cheng Qian, Runchu Tian, Kunlun\nZhu, Shihao Liang, Xingyu Shen, Bokai Xu, Zhen\nZhang, Yining Ye, Bowen Li, Ziwei Tang, Jing Yi,\nYuzhang Zhu, Zhenning Dai, Lan Yan, Xin Cong,\nYaxi Lu, Weilin Zhao, Yuxiang Huang, Junxi Yan,\nXu Han, Xian Sun, Dahai Li, Jason Phang, ChengYang, Tongshuang Wu, Heng Ji, Zhiyuan Liu, and\nMaosong Sun. 2023b. Tool learning with foundation\nmodels.\nYujia Qin, Shihao Liang, Yining Ye, Kunlun Zhu, Lan\nYan, Yaxi Lu, Yankai Lin, Xin Cong, Xiangru Tang,\nBill Qian, Sihan Zhao, Lauren Hong, Runchu Tian,\nRuobing Xie, Jie Zhou, Mark Gerstein, Dahai Li,\nZhiyuan Liu, and Maosong Sun. 2023c. Toolllm:\nFacilitating large language models to master 16000+\nreal-world apis.\nAlec Radford, Karthik Narasimhan, Tim Salimans, Ilya\nSutskever, et al. 2018. Improving language under-\nstanding by generative pre-training.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, Ilya Sutskever, et al. 2019. Language\nmodels are unsupervised multitask learners. OpenAI\nblog, 1(8):9.\nVatsal Raina and Mark Gales. 2022. Multiple-choice\nquestion generation: Towards an automated assess-\nment framework. arXiv preprint arXiv:2209.11830 .", "mimetype": "text/plain", "start_char_idx": 1581, "end_char_idx": 3442, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4afef103-bb68-4f8a-b5eb-d2d8aafbbe07": {"__data__": {"id_": "4afef103-bb68-4f8a-b5eb-d2d8aafbbe07", "embedding": null, "metadata": {"page_label": "20", "file_name": "2406_00936v1.pdf", "Title of this paper": "A Survey of Useful LLM Evaluation", "Authors": "Ji-Lun Peng, Sijia Cheng, Egil Diau, Yung-Yu Shih, Po-Heng Chen, Yen-Ting Lin, Yun-Nung Chen", "Date published": "06/03/2024", "URL": "http://arxiv.org/abs/2406.00936v1", "summary": "LLMs have gotten attention across various research domains due to their\nexceptional performance on a wide range of complex tasks. Therefore, refined\nmethods to evaluate the capabilities of LLMs are needed to determine the tasks\nand responsibility they should undertake. Our study mainly discussed how LLMs,\nas useful tools, should be effectively assessed. We proposed the two-stage\nframework: from ``core ability'' to ``agent'', clearly explaining how LLMs can\nbe applied based on their specific capabilities, along with the evaluation\nmethods in each stage. Core ability refers to the capabilities that LLMs need\nin order to generate high-quality natural language texts. After confirming LLMs\npossess core ability, they can solve real-world and complex tasks as agent. In\nthe \"core ability\" stage, we discussed the reasoning ability, societal impact,\nand domain knowledge of LLMs. In the ``agent'' stage, we demonstrated embodied\naction, planning, and tool learning of LLMs agent applications. Finally, we\nexamined the challenges currently confronting the evaluation methods for LLMs,\nas well as the directions for future development."}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e94bfb13-444c-4371-b46f-d7a757f7282b", "node_type": "4", "metadata": {"page_label": "20", "file_name": "2406_00936v1.pdf", "Title of this paper": "A Survey of Useful LLM Evaluation", "Authors": "Ji-Lun Peng, Sijia Cheng, Egil Diau, Yung-Yu Shih, Po-Heng Chen, Yen-Ting Lin, Yun-Nung Chen", "Date published": "06/03/2024", "URL": "http://arxiv.org/abs/2406.00936v1", "summary": "LLMs have gotten attention across various research domains due to their\nexceptional performance on a wide range of complex tasks. Therefore, refined\nmethods to evaluate the capabilities of LLMs are needed to determine the tasks\nand responsibility they should undertake. Our study mainly discussed how LLMs,\nas useful tools, should be effectively assessed. We proposed the two-stage\nframework: from ``core ability'' to ``agent'', clearly explaining how LLMs can\nbe applied based on their specific capabilities, along with the evaluation\nmethods in each stage. Core ability refers to the capabilities that LLMs need\nin order to generate high-quality natural language texts. After confirming LLMs\npossess core ability, they can solve real-world and complex tasks as agent. In\nthe \"core ability\" stage, we discussed the reasoning ability, societal impact,\nand domain knowledge of LLMs. In the ``agent'' stage, we demonstrated embodied\naction, planning, and tool learning of LLMs agent applications. Finally, we\nexamined the challenges currently confronting the evaluation methods for LLMs,\nas well as the directions for future development."}, "hash": "29e31769758a2c84e16e653609dcba6a361b10d163f2b55d7f1bd6d63cd06961", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7086ca5f-f587-4170-abfd-3a0366d70f7d", "node_type": "1", "metadata": {"page_label": "20", "file_name": "2406_00936v1.pdf", "Title of this paper": "A Survey of Useful LLM Evaluation", "Authors": "Ji-Lun Peng, Sijia Cheng, Egil Diau, Yung-Yu Shih, Po-Heng Chen, Yen-Ting Lin, Yun-Nung Chen", "Date published": "06/03/2024", "URL": "http://arxiv.org/abs/2406.00936v1", "summary": "LLMs have gotten attention across various research domains due to their\nexceptional performance on a wide range of complex tasks. Therefore, refined\nmethods to evaluate the capabilities of LLMs are needed to determine the tasks\nand responsibility they should undertake. Our study mainly discussed how LLMs,\nas useful tools, should be effectively assessed. We proposed the two-stage\nframework: from ``core ability'' to ``agent'', clearly explaining how LLMs can\nbe applied based on their specific capabilities, along with the evaluation\nmethods in each stage. Core ability refers to the capabilities that LLMs need\nin order to generate high-quality natural language texts. After confirming LLMs\npossess core ability, they can solve real-world and complex tasks as agent. In\nthe \"core ability\" stage, we discussed the reasoning ability, societal impact,\nand domain knowledge of LLMs. In the ``agent'' stage, we demonstrated embodied\naction, planning, and tool learning of LLMs agent applications. Finally, we\nexamined the challenges currently confronting the evaluation methods for LLMs,\nas well as the directions for future development."}, "hash": "43a902542193bb5f9777269b778307aa7b820e225a519ab56f98f6dc6069e1a0", "class_name": "RelatedNodeInfo"}}, "text": "2023c. Toolllm:\nFacilitating large language models to master 16000+\nreal-world apis.\nAlec Radford, Karthik Narasimhan, Tim Salimans, Ilya\nSutskever, et al. 2018. Improving language under-\nstanding by generative pre-training.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, Ilya Sutskever, et al. 2019. Language\nmodels are unsupervised multitask learners. OpenAI\nblog, 1(8):9.\nVatsal Raina and Mark Gales. 2022. Multiple-choice\nquestion generation: Towards an automated assess-\nment framework. arXiv preprint arXiv:2209.11830 .\nKrishan Rana, Jesse Haviland, Sourav Garg, Jad Abou-\nChakra, Ian Reid, and Niko Suenderhauf. 2023. Say-\nplan: Grounding large language models using 3d\nscene graphs for scalable task planning. In 7th An-\nnual Conference on Robot Learning .\nSteve Rathje, Dan-Mircea Mirea, Ilia Sucholutsky, Raja\nMarjieh, Claire Robertson, and Jay J Van Bavel. 2023.\nGpt is an effective tool for multilingual psychological\ntext analysis.\nAllen Z Ren, Anushri Dixit, Alexandra Bodrova,\nSumeet Singh, Stephen Tu, Noah Brown, Peng Xu,\nLeila Takayama, Fei Xia, Jake Varley, et al. 2023.\nRobots that ask for help: Uncertainty alignment\nfor large language model planners. arXiv preprint\narXiv:2307.01928 .\nJingqing Ruan, Yihong Chen, Bin Zhang, Zhiwei Xu,\nTianpeng Bao, Guoqing Du, Shiwei Shi, Hangyu\nMao, Ziyue Li, Xingyu Zeng, and Rui Zhao. 2023.\nTptu: Large language model-based ai agents for task\nplanning and tool usage.\nHadi Salehi and Rigoberto Burgue\u00f1o. 2018. Emerging\nartificial intelligence methods in structural engineer-\ning. Engineering structures , 171:170\u2013189.\nNino Scherrer, Claudia Shi, Amir Feder, and David M.\nBlei. 2023. Evaluating the moral beliefs encoded in\nllms.\nTimo Schick, Jane Dwivedi-Yu, Roberto Dess\u00ec, Roberta\nRaileanu, Maria Lomeli, Luke Zettlemoyer, Nicola\nCancedda, and Thomas Scialom. 2023. Toolformer:\nLanguage models can teach themselves to use tools.\nHannah L Semigran, Jeffrey A Linder, Courtney Gi-\ndengil, and Ateev Mehrotra. 2015. Evaluation of\nsymptom checkers for self diagnosis and triage: au-\ndit study. bmj, 351.\n20", "mimetype": "text/plain", "start_char_idx": 2899, "end_char_idx": 4979, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b4b4381d-228c-4a51-8d76-05f03f3ea84d": {"__data__": {"id_": "b4b4381d-228c-4a51-8d76-05f03f3ea84d", "embedding": null, "metadata": {"page_label": "21", "file_name": "2406_00936v1.pdf", "Title of this paper": "A Survey of Useful LLM Evaluation", "Authors": "Ji-Lun Peng, Sijia Cheng, Egil Diau, Yung-Yu Shih, Po-Heng Chen, Yen-Ting Lin, Yun-Nung Chen", "Date published": "06/03/2024", "URL": "http://arxiv.org/abs/2406.00936v1", "summary": "LLMs have gotten attention across various research domains due to their\nexceptional performance on a wide range of complex tasks. Therefore, refined\nmethods to evaluate the capabilities of LLMs are needed to determine the tasks\nand responsibility they should undertake. Our study mainly discussed how LLMs,\nas useful tools, should be effectively assessed. We proposed the two-stage\nframework: from ``core ability'' to ``agent'', clearly explaining how LLMs can\nbe applied based on their specific capabilities, along with the evaluation\nmethods in each stage. Core ability refers to the capabilities that LLMs need\nin order to generate high-quality natural language texts. After confirming LLMs\npossess core ability, they can solve real-world and complex tasks as agent. In\nthe \"core ability\" stage, we discussed the reasoning ability, societal impact,\nand domain knowledge of LLMs. In the ``agent'' stage, we demonstrated embodied\naction, planning, and tool learning of LLMs agent applications. Finally, we\nexamined the challenges currently confronting the evaluation methods for LLMs,\nas well as the directions for future development."}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "aa873ac2-8693-49b7-91c3-b69b4a1f04cb", "node_type": "4", "metadata": {"page_label": "21", "file_name": "2406_00936v1.pdf", "Title of this paper": "A Survey of Useful LLM Evaluation", "Authors": "Ji-Lun Peng, Sijia Cheng, Egil Diau, Yung-Yu Shih, Po-Heng Chen, Yen-Ting Lin, Yun-Nung Chen", "Date published": "06/03/2024", "URL": "http://arxiv.org/abs/2406.00936v1", "summary": "LLMs have gotten attention across various research domains due to their\nexceptional performance on a wide range of complex tasks. Therefore, refined\nmethods to evaluate the capabilities of LLMs are needed to determine the tasks\nand responsibility they should undertake. Our study mainly discussed how LLMs,\nas useful tools, should be effectively assessed. We proposed the two-stage\nframework: from ``core ability'' to ``agent'', clearly explaining how LLMs can\nbe applied based on their specific capabilities, along with the evaluation\nmethods in each stage. Core ability refers to the capabilities that LLMs need\nin order to generate high-quality natural language texts. After confirming LLMs\npossess core ability, they can solve real-world and complex tasks as agent. In\nthe \"core ability\" stage, we discussed the reasoning ability, societal impact,\nand domain knowledge of LLMs. In the ``agent'' stage, we demonstrated embodied\naction, planning, and tool learning of LLMs agent applications. Finally, we\nexamined the challenges currently confronting the evaluation methods for LLMs,\nas well as the directions for future development."}, "hash": "d943a2dc771af4d7a031a5f8eb2922d51ebd19710a7d07787b8a7fe2896ecd5b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "83af5fda-2789-4967-aaf1-467644174aab", "node_type": "1", "metadata": {}, "hash": "001112bad2ee3b786902d85450936ef7f37c756ab5dd89e1d2081172a583d7d1", "class_name": "RelatedNodeInfo"}}, "text": "Dhruv Shah, Michael Robert Equi, B\u0142a \u02d9zej Osi \u00b4nski, Fei\nXia, Brian Ichter, and Sergey Levine. 2023. Navi-\ngation with large language models: Semantic guess-\nwork as a heuristic for planning. In Conference on\nRobot Learning , pages 2683\u20132699. PMLR.\nDhruv Shah, Blazej Osinski, Brian Ichter, and Sergey\nLevine. 2022. LM-nav: Robotic navigation with\nlarge pre-trained models of language, vision, and\naction. In 6th Annual Conference on Robot Learning .\nLi Shan and Weihong Deng. 2018. Reliable crowd-\nsourcing and deep locality-preserving learning for\nunconstrained facial expression recognition. IEEE\nTransactions on Image Processing , 28(1):356\u2013370.\nGaurav Sharma and Abhishek Thakur. 2023. Chatgpt\nin drug discovery.\nNoah Shinn, Federico Cassano, Edward Berman, Ash-\nwin Gopinath, Karthik Narasimhan, and Shunyu Yao.\n2023. Reflexion: Language agents with verbal rein-\nforcement learning.\nIshika Singh, Valts Blukis, Arsalan Mousavian, Ankit\nGoyal, Danfei Xu, Jonathan Tremblay, Dieter Fox,\nJesse Thomason, and Animesh Garg. 2023. Prog-\nprompt: Generating situated robot task plans using\nlarge language models. In 2023 IEEE International\nConference on Robotics and Automation (ICRA) ,\npages 11523\u201311530. IEEE.\nAnkur Sinha and Tanmay Khandait. 2021. Impact of\nnews on the commodity market: Dataset and results.\nInAdvances in Information and Communication:\nProceedings of the 2021 Future of Information and\nCommunication Conference (FICC), Volume 2 , pages\n589\u2013601. Springer.\nChan Hee Song, Jiaman Wu, Clayton Washington,\nBrian M. Sadler, Wei-Lun Chao, and Yu Su. 2023a.\nLlm-planner: Few-shot grounded planning for em-\nbodied agents with large language models. In Pro-\nceedings of the IEEE/CVF International Conference\non Computer Vision (ICCV) .\nYifan Song, Weimin Xiong, Dawei Zhu, Wenhao Wu,\nHan Qian, Mingbo Song, Hailiang Huang, Cheng\nLi, Ke Wang, Rong Yao, Ye Tian, and Sujian Li.\n2023b. Restgpt: Connecting large language models\nwith real-world restful apis.\nTaylor Sorensen, Liwei Jiang, Jena D Hwang, Sydney\nLevine, Valentina Pyatkin, Peter West, Nouha Dziri,\nXiming Lu, Kavel Rao, Chandra Bhagavatula, et al.\n2024. Value kaleidoscope: Engaging ai with pluralis-\ntic human values, rights, and duties. In Proceedings\nof the AAAI Conference on Artificial Intelligence ,\nvolume 38, pages 19937\u201319947.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2304, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "83af5fda-2789-4967-aaf1-467644174aab": {"__data__": {"id_": "83af5fda-2789-4967-aaf1-467644174aab", "embedding": null, "metadata": {"page_label": "21", "file_name": "2406_00936v1.pdf", "Title of this paper": "A Survey of Useful LLM Evaluation", "Authors": "Ji-Lun Peng, Sijia Cheng, Egil Diau, Yung-Yu Shih, Po-Heng Chen, Yen-Ting Lin, Yun-Nung Chen", "Date published": "06/03/2024", "URL": "http://arxiv.org/abs/2406.00936v1", "summary": "LLMs have gotten attention across various research domains due to their\nexceptional performance on a wide range of complex tasks. Therefore, refined\nmethods to evaluate the capabilities of LLMs are needed to determine the tasks\nand responsibility they should undertake. Our study mainly discussed how LLMs,\nas useful tools, should be effectively assessed. We proposed the two-stage\nframework: from ``core ability'' to ``agent'', clearly explaining how LLMs can\nbe applied based on their specific capabilities, along with the evaluation\nmethods in each stage. Core ability refers to the capabilities that LLMs need\nin order to generate high-quality natural language texts. After confirming LLMs\npossess core ability, they can solve real-world and complex tasks as agent. In\nthe \"core ability\" stage, we discussed the reasoning ability, societal impact,\nand domain knowledge of LLMs. In the ``agent'' stage, we demonstrated embodied\naction, planning, and tool learning of LLMs agent applications. Finally, we\nexamined the challenges currently confronting the evaluation methods for LLMs,\nas well as the directions for future development."}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "aa873ac2-8693-49b7-91c3-b69b4a1f04cb", "node_type": "4", "metadata": {"page_label": "21", "file_name": "2406_00936v1.pdf", "Title of this paper": "A Survey of Useful LLM Evaluation", "Authors": "Ji-Lun Peng, Sijia Cheng, Egil Diau, Yung-Yu Shih, Po-Heng Chen, Yen-Ting Lin, Yun-Nung Chen", "Date published": "06/03/2024", "URL": "http://arxiv.org/abs/2406.00936v1", "summary": "LLMs have gotten attention across various research domains due to their\nexceptional performance on a wide range of complex tasks. Therefore, refined\nmethods to evaluate the capabilities of LLMs are needed to determine the tasks\nand responsibility they should undertake. Our study mainly discussed how LLMs,\nas useful tools, should be effectively assessed. We proposed the two-stage\nframework: from ``core ability'' to ``agent'', clearly explaining how LLMs can\nbe applied based on their specific capabilities, along with the evaluation\nmethods in each stage. Core ability refers to the capabilities that LLMs need\nin order to generate high-quality natural language texts. After confirming LLMs\npossess core ability, they can solve real-world and complex tasks as agent. In\nthe \"core ability\" stage, we discussed the reasoning ability, societal impact,\nand domain knowledge of LLMs. In the ``agent'' stage, we demonstrated embodied\naction, planning, and tool learning of LLMs agent applications. Finally, we\nexamined the challenges currently confronting the evaluation methods for LLMs,\nas well as the directions for future development."}, "hash": "d943a2dc771af4d7a031a5f8eb2922d51ebd19710a7d07787b8a7fe2896ecd5b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b4b4381d-228c-4a51-8d76-05f03f3ea84d", "node_type": "1", "metadata": {"page_label": "21", "file_name": "2406_00936v1.pdf", "Title of this paper": "A Survey of Useful LLM Evaluation", "Authors": "Ji-Lun Peng, Sijia Cheng, Egil Diau, Yung-Yu Shih, Po-Heng Chen, Yen-Ting Lin, Yun-Nung Chen", "Date published": "06/03/2024", "URL": "http://arxiv.org/abs/2406.00936v1", "summary": "LLMs have gotten attention across various research domains due to their\nexceptional performance on a wide range of complex tasks. Therefore, refined\nmethods to evaluate the capabilities of LLMs are needed to determine the tasks\nand responsibility they should undertake. Our study mainly discussed how LLMs,\nas useful tools, should be effectively assessed. We proposed the two-stage\nframework: from ``core ability'' to ``agent'', clearly explaining how LLMs can\nbe applied based on their specific capabilities, along with the evaluation\nmethods in each stage. Core ability refers to the capabilities that LLMs need\nin order to generate high-quality natural language texts. After confirming LLMs\npossess core ability, they can solve real-world and complex tasks as agent. In\nthe \"core ability\" stage, we discussed the reasoning ability, societal impact,\nand domain knowledge of LLMs. In the ``agent'' stage, we demonstrated embodied\naction, planning, and tool learning of LLMs agent applications. Finally, we\nexamined the challenges currently confronting the evaluation methods for LLMs,\nas well as the directions for future development."}, "hash": "9e88141477ad214eaf72bd2a5038877654df5023609e8a73c664333c279f069a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1f061f59-3379-4e37-be71-43dc40effbbe", "node_type": "1", "metadata": {}, "hash": "ee39014296f3539f8df5362af35186bd25d40ece34ff985221661c0687b37d25", "class_name": "RelatedNodeInfo"}}, "text": "In Pro-\nceedings of the IEEE/CVF International Conference\non Computer Vision (ICCV) .\nYifan Song, Weimin Xiong, Dawei Zhu, Wenhao Wu,\nHan Qian, Mingbo Song, Hailiang Huang, Cheng\nLi, Ke Wang, Rong Yao, Ye Tian, and Sujian Li.\n2023b. Restgpt: Connecting large language models\nwith real-world restful apis.\nTaylor Sorensen, Liwei Jiang, Jena D Hwang, Sydney\nLevine, Valentina Pyatkin, Peter West, Nouha Dziri,\nXiming Lu, Kavel Rao, Chandra Bhagavatula, et al.\n2024. Value kaleidoscope: Engaging ai with pluralis-\ntic human values, rights, and duties. In Proceedings\nof the AAAI Conference on Artificial Intelligence ,\nvolume 38, pages 19937\u201319947.\nAarohi Srivastava, Abhinav Rastogi, Abhishek Rao,\nAbu Awal Md Shoeb, Abubakar Abid, Adam Fisch,\nAdam R Brown, Adam Santoro, Aditya Gupta,\nAdri\u00e0 Garriga-Alonso, et al. 2022. Beyond the\nimitation game: Quantifying and extrapolating the\ncapabilities of language models. arXiv preprint\narXiv:2206.04615 .Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao,\nAbu Awal Md Shoeb, Abubakar Abid, Adam\nFisch, Adam R. Brown, Adam Santoro, Aditya\nGupta, Adri\u00e0 Garriga-Alonso, Agnieszka Kluska,\nAitor Lewkowycz, Akshat Agarwal, Alethea Power,\nAlex Ray, Alex Warstadt, Alexander W. Kocurek,\nAli Safaya, Ali Tazarv, Alice Xiang, Alicia Par-\nrish, Allen Nie, Aman Hussain, Amanda Askell,\nAmanda Dsouza, Ambrose Slone, Ameet Rahane,\nAnantharaman S. Iyer, Anders Andreassen, Andrea\nMadotto, Andrea Santilli, Andreas Stuhlm\u00fcller, An-\ndrew Dai, Andrew La, Andrew Lampinen, Andy\nZou, Angela Jiang, Angelica Chen, Anh Vuong,\nAnimesh Gupta, Anna Gottardi, Antonio Norelli,\nAnu Venkatesh, Arash Gholamidavoodi, Arfa Tabas-\nsum, Arul Menezes, Arun Kirubarajan, Asher Mul-\nlokandov, Ashish Sabharwal, Austin Herrick, Avia\nEfrat, Aykut Erdem, Ayla Karaka\u00b8 s, B. Ryan Roberts,\nBao Sheng Loe, Barret Zoph, Bart\u0142omiej Bojanowski,\nBatuhan \u00d6zyurt, Behnam Hedayatnia, Behnam\nNeyshabur, Benjamin Inden, Benno Stein, Berk\nEkmekci, Bill Yuchen Lin, Blake Howald, Bryan\nOrinion, Cameron Diao,", "mimetype": "text/plain", "start_char_idx": 1659, "end_char_idx": 3657, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1f061f59-3379-4e37-be71-43dc40effbbe": {"__data__": {"id_": "1f061f59-3379-4e37-be71-43dc40effbbe", "embedding": null, "metadata": {"page_label": "21", "file_name": "2406_00936v1.pdf", "Title of this paper": "A Survey of Useful LLM Evaluation", "Authors": "Ji-Lun Peng, Sijia Cheng, Egil Diau, Yung-Yu Shih, Po-Heng Chen, Yen-Ting Lin, Yun-Nung Chen", "Date published": "06/03/2024", "URL": "http://arxiv.org/abs/2406.00936v1", "summary": "LLMs have gotten attention across various research domains due to their\nexceptional performance on a wide range of complex tasks. Therefore, refined\nmethods to evaluate the capabilities of LLMs are needed to determine the tasks\nand responsibility they should undertake. Our study mainly discussed how LLMs,\nas useful tools, should be effectively assessed. We proposed the two-stage\nframework: from ``core ability'' to ``agent'', clearly explaining how LLMs can\nbe applied based on their specific capabilities, along with the evaluation\nmethods in each stage. Core ability refers to the capabilities that LLMs need\nin order to generate high-quality natural language texts. After confirming LLMs\npossess core ability, they can solve real-world and complex tasks as agent. In\nthe \"core ability\" stage, we discussed the reasoning ability, societal impact,\nand domain knowledge of LLMs. In the ``agent'' stage, we demonstrated embodied\naction, planning, and tool learning of LLMs agent applications. Finally, we\nexamined the challenges currently confronting the evaluation methods for LLMs,\nas well as the directions for future development."}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "aa873ac2-8693-49b7-91c3-b69b4a1f04cb", "node_type": "4", "metadata": {"page_label": "21", "file_name": "2406_00936v1.pdf", "Title of this paper": "A Survey of Useful LLM Evaluation", "Authors": "Ji-Lun Peng, Sijia Cheng, Egil Diau, Yung-Yu Shih, Po-Heng Chen, Yen-Ting Lin, Yun-Nung Chen", "Date published": "06/03/2024", "URL": "http://arxiv.org/abs/2406.00936v1", "summary": "LLMs have gotten attention across various research domains due to their\nexceptional performance on a wide range of complex tasks. Therefore, refined\nmethods to evaluate the capabilities of LLMs are needed to determine the tasks\nand responsibility they should undertake. Our study mainly discussed how LLMs,\nas useful tools, should be effectively assessed. We proposed the two-stage\nframework: from ``core ability'' to ``agent'', clearly explaining how LLMs can\nbe applied based on their specific capabilities, along with the evaluation\nmethods in each stage. Core ability refers to the capabilities that LLMs need\nin order to generate high-quality natural language texts. After confirming LLMs\npossess core ability, they can solve real-world and complex tasks as agent. In\nthe \"core ability\" stage, we discussed the reasoning ability, societal impact,\nand domain knowledge of LLMs. In the ``agent'' stage, we demonstrated embodied\naction, planning, and tool learning of LLMs agent applications. Finally, we\nexamined the challenges currently confronting the evaluation methods for LLMs,\nas well as the directions for future development."}, "hash": "d943a2dc771af4d7a031a5f8eb2922d51ebd19710a7d07787b8a7fe2896ecd5b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "83af5fda-2789-4967-aaf1-467644174aab", "node_type": "1", "metadata": {"page_label": "21", "file_name": "2406_00936v1.pdf", "Title of this paper": "A Survey of Useful LLM Evaluation", "Authors": "Ji-Lun Peng, Sijia Cheng, Egil Diau, Yung-Yu Shih, Po-Heng Chen, Yen-Ting Lin, Yun-Nung Chen", "Date published": "06/03/2024", "URL": "http://arxiv.org/abs/2406.00936v1", "summary": "LLMs have gotten attention across various research domains due to their\nexceptional performance on a wide range of complex tasks. Therefore, refined\nmethods to evaluate the capabilities of LLMs are needed to determine the tasks\nand responsibility they should undertake. Our study mainly discussed how LLMs,\nas useful tools, should be effectively assessed. We proposed the two-stage\nframework: from ``core ability'' to ``agent'', clearly explaining how LLMs can\nbe applied based on their specific capabilities, along with the evaluation\nmethods in each stage. Core ability refers to the capabilities that LLMs need\nin order to generate high-quality natural language texts. After confirming LLMs\npossess core ability, they can solve real-world and complex tasks as agent. In\nthe \"core ability\" stage, we discussed the reasoning ability, societal impact,\nand domain knowledge of LLMs. In the ``agent'' stage, we demonstrated embodied\naction, planning, and tool learning of LLMs agent applications. Finally, we\nexamined the challenges currently confronting the evaluation methods for LLMs,\nas well as the directions for future development."}, "hash": "ea785979397dbadc5b2014dbda755c310c910df3700a5af183524ba7ca7d2c70", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "978362a6-fad9-4c7f-81c7-a9f2f040cbc6", "node_type": "1", "metadata": {}, "hash": "084ad6bc3cdd0dc3a69b8405b136a91e5a19ee5857194aa42d4ae1e9d973913d", "class_name": "RelatedNodeInfo"}}, "text": "Andrew La, Andrew Lampinen, Andy\nZou, Angela Jiang, Angelica Chen, Anh Vuong,\nAnimesh Gupta, Anna Gottardi, Antonio Norelli,\nAnu Venkatesh, Arash Gholamidavoodi, Arfa Tabas-\nsum, Arul Menezes, Arun Kirubarajan, Asher Mul-\nlokandov, Ashish Sabharwal, Austin Herrick, Avia\nEfrat, Aykut Erdem, Ayla Karaka\u00b8 s, B. Ryan Roberts,\nBao Sheng Loe, Barret Zoph, Bart\u0142omiej Bojanowski,\nBatuhan \u00d6zyurt, Behnam Hedayatnia, Behnam\nNeyshabur, Benjamin Inden, Benno Stein, Berk\nEkmekci, Bill Yuchen Lin, Blake Howald, Bryan\nOrinion, Cameron Diao, Cameron Dour, Cather-\nine Stinson, Cedrick Argueta, C\u00e9sar Ferri Ram\u00edrez,\nChandan Singh, Charles Rathkopf, Chenlin Meng,\nChitta Baral, Chiyu Wu, Chris Callison-Burch, Chris\nWaites, Christian V oigt, Christopher D. Manning,\nChristopher Potts, Cindy Ramirez, Clara E. Rivera,\nClemencia Siro, Colin Raffel, Courtney Ashcraft,\nCristina Garbacea, Damien Sileo, Dan Garrette, Dan\nHendrycks, Dan Kilman, Dan Roth, Daniel Free-\nman, Daniel Khashabi, Daniel Levy, Daniel Mosegu\u00ed\nGonz\u00e1lez, Danielle Perszyk, Danny Hernandez,\nDanqi Chen, Daphne Ippolito, Dar Gilboa, David Do-\nhan, David Drakard, David Jurgens, Debajyoti Datta,\nDeep Ganguli, Denis Emelin, Denis Kleyko, Deniz\nYuret, Derek Chen, Derek Tam, Dieuwke Hupkes,\nDiganta Misra, Dilyar Buzan, Dimitri Coelho Mollo,\nDiyi Yang, Dong-Ho Lee, Dylan Schrader, Ekaterina\nShutova, Ekin Dogus Cubuk, Elad Segal, Eleanor\nHagerman, Elizabeth Barnes, Elizabeth Donoway, El-\nlie Pavlick, Emanuele Rodola, Emma Lam, Eric Chu,\nEric Tang, Erkut Erdem, Ernie Chang, Ethan A. Chi,\nEthan Dyer, Ethan Jerzak, Ethan Kim, Eunice En-\ngefu Manyasi, Evgenii Zheltonozhskii, Fanyue Xia,\nFatemeh Siar, Fernando Mart\u00ednez-Plumed, Francesca\nHapp\u00e9, Francois Chollet, Frieda Rong, Gaurav\nMishra, Genta Indra Winata, Gerard de Melo, Ger-\nm\u00e1n Kruszewski, Giambattista Parascandolo, Gior-\ngio Mariani, Gloria Wang, Gonzalo Jaimovitch-\nL\u00f3pez, Gregor Betz, Guy Gur-Ari, Hana Galijase-\nvic, Hannah Kim, Hannah Rashkin,", "mimetype": "text/plain", "start_char_idx": 3127, "end_char_idx": 5086, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "978362a6-fad9-4c7f-81c7-a9f2f040cbc6": {"__data__": {"id_": "978362a6-fad9-4c7f-81c7-a9f2f040cbc6", "embedding": null, "metadata": {"page_label": "21", "file_name": "2406_00936v1.pdf", "Title of this paper": "A Survey of Useful LLM Evaluation", "Authors": "Ji-Lun Peng, Sijia Cheng, Egil Diau, Yung-Yu Shih, Po-Heng Chen, Yen-Ting Lin, Yun-Nung Chen", "Date published": "06/03/2024", "URL": "http://arxiv.org/abs/2406.00936v1", "summary": "LLMs have gotten attention across various research domains due to their\nexceptional performance on a wide range of complex tasks. Therefore, refined\nmethods to evaluate the capabilities of LLMs are needed to determine the tasks\nand responsibility they should undertake. Our study mainly discussed how LLMs,\nas useful tools, should be effectively assessed. We proposed the two-stage\nframework: from ``core ability'' to ``agent'', clearly explaining how LLMs can\nbe applied based on their specific capabilities, along with the evaluation\nmethods in each stage. Core ability refers to the capabilities that LLMs need\nin order to generate high-quality natural language texts. After confirming LLMs\npossess core ability, they can solve real-world and complex tasks as agent. In\nthe \"core ability\" stage, we discussed the reasoning ability, societal impact,\nand domain knowledge of LLMs. In the ``agent'' stage, we demonstrated embodied\naction, planning, and tool learning of LLMs agent applications. Finally, we\nexamined the challenges currently confronting the evaluation methods for LLMs,\nas well as the directions for future development."}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "aa873ac2-8693-49b7-91c3-b69b4a1f04cb", "node_type": "4", "metadata": {"page_label": "21", "file_name": "2406_00936v1.pdf", "Title of this paper": "A Survey of Useful LLM Evaluation", "Authors": "Ji-Lun Peng, Sijia Cheng, Egil Diau, Yung-Yu Shih, Po-Heng Chen, Yen-Ting Lin, Yun-Nung Chen", "Date published": "06/03/2024", "URL": "http://arxiv.org/abs/2406.00936v1", "summary": "LLMs have gotten attention across various research domains due to their\nexceptional performance on a wide range of complex tasks. Therefore, refined\nmethods to evaluate the capabilities of LLMs are needed to determine the tasks\nand responsibility they should undertake. Our study mainly discussed how LLMs,\nas useful tools, should be effectively assessed. We proposed the two-stage\nframework: from ``core ability'' to ``agent'', clearly explaining how LLMs can\nbe applied based on their specific capabilities, along with the evaluation\nmethods in each stage. Core ability refers to the capabilities that LLMs need\nin order to generate high-quality natural language texts. After confirming LLMs\npossess core ability, they can solve real-world and complex tasks as agent. In\nthe \"core ability\" stage, we discussed the reasoning ability, societal impact,\nand domain knowledge of LLMs. In the ``agent'' stage, we demonstrated embodied\naction, planning, and tool learning of LLMs agent applications. Finally, we\nexamined the challenges currently confronting the evaluation methods for LLMs,\nas well as the directions for future development."}, "hash": "d943a2dc771af4d7a031a5f8eb2922d51ebd19710a7d07787b8a7fe2896ecd5b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1f061f59-3379-4e37-be71-43dc40effbbe", "node_type": "1", "metadata": {"page_label": "21", "file_name": "2406_00936v1.pdf", "Title of this paper": "A Survey of Useful LLM Evaluation", "Authors": "Ji-Lun Peng, Sijia Cheng, Egil Diau, Yung-Yu Shih, Po-Heng Chen, Yen-Ting Lin, Yun-Nung Chen", "Date published": "06/03/2024", "URL": "http://arxiv.org/abs/2406.00936v1", "summary": "LLMs have gotten attention across various research domains due to their\nexceptional performance on a wide range of complex tasks. Therefore, refined\nmethods to evaluate the capabilities of LLMs are needed to determine the tasks\nand responsibility they should undertake. Our study mainly discussed how LLMs,\nas useful tools, should be effectively assessed. We proposed the two-stage\nframework: from ``core ability'' to ``agent'', clearly explaining how LLMs can\nbe applied based on their specific capabilities, along with the evaluation\nmethods in each stage. Core ability refers to the capabilities that LLMs need\nin order to generate high-quality natural language texts. After confirming LLMs\npossess core ability, they can solve real-world and complex tasks as agent. In\nthe \"core ability\" stage, we discussed the reasoning ability, societal impact,\nand domain knowledge of LLMs. In the ``agent'' stage, we demonstrated embodied\naction, planning, and tool learning of LLMs agent applications. Finally, we\nexamined the challenges currently confronting the evaluation methods for LLMs,\nas well as the directions for future development."}, "hash": "00adde96e6130b7e85c44e9b8ff1307b812b99f9bc3eadbb0913e3a5c6345bcb", "class_name": "RelatedNodeInfo"}}, "text": "Elizabeth Barnes, Elizabeth Donoway, El-\nlie Pavlick, Emanuele Rodola, Emma Lam, Eric Chu,\nEric Tang, Erkut Erdem, Ernie Chang, Ethan A. Chi,\nEthan Dyer, Ethan Jerzak, Ethan Kim, Eunice En-\ngefu Manyasi, Evgenii Zheltonozhskii, Fanyue Xia,\nFatemeh Siar, Fernando Mart\u00ednez-Plumed, Francesca\nHapp\u00e9, Francois Chollet, Frieda Rong, Gaurav\nMishra, Genta Indra Winata, Gerard de Melo, Ger-\nm\u00e1n Kruszewski, Giambattista Parascandolo, Gior-\ngio Mariani, Gloria Wang, Gonzalo Jaimovitch-\nL\u00f3pez, Gregor Betz, Guy Gur-Ari, Hana Galijase-\nvic, Hannah Kim, Hannah Rashkin, Hannaneh Ha-\njishirzi, Harsh Mehta, Hayden Bogar, Henry Shevlin,\nHinrich Sch\u00fctze, Hiromu Yakura, Hongming Zhang,\nHugh Mee Wong, Ian Ng, Isaac Noble, Jaap Jumelet,\nJack Geissinger, Jackson Kernion, Jacob Hilton, Jae-\nhoon Lee, Jaime Fern\u00e1ndez Fisac, James B. Simon,\nJames Koppel, James Zheng, James Zou, Jan Koco \u00b4n,\nJana Thompson, Janelle Wingfield, Jared Kaplan,\nJarema Radom, Jascha Sohl-Dickstein, Jason Phang,\nJason Wei, Jason Yosinski, Jekaterina Novikova,\nJelle Bosscher, Jennifer Marsh, Jeremy Kim, Jeroen\nTaal, Jesse Engel, Jesujoba Alabi, Jiacheng Xu, Ji-\naming Song, Jillian Tang, Joan Waweru, John Bur-\n21", "mimetype": "text/plain", "start_char_idx": 4527, "end_char_idx": 5703, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "769544b4-94fa-4b9b-a7c3-cf05ba175036": {"__data__": {"id_": "769544b4-94fa-4b9b-a7c3-cf05ba175036", "embedding": null, "metadata": {"page_label": "22", "file_name": "2406_00936v1.pdf", "Title of this paper": "A Survey of Useful LLM Evaluation", "Authors": "Ji-Lun Peng, Sijia Cheng, Egil Diau, Yung-Yu Shih, Po-Heng Chen, Yen-Ting Lin, Yun-Nung Chen", "Date published": "06/03/2024", "URL": "http://arxiv.org/abs/2406.00936v1", "summary": "LLMs have gotten attention across various research domains due to their\nexceptional performance on a wide range of complex tasks. Therefore, refined\nmethods to evaluate the capabilities of LLMs are needed to determine the tasks\nand responsibility they should undertake. Our study mainly discussed how LLMs,\nas useful tools, should be effectively assessed. We proposed the two-stage\nframework: from ``core ability'' to ``agent'', clearly explaining how LLMs can\nbe applied based on their specific capabilities, along with the evaluation\nmethods in each stage. Core ability refers to the capabilities that LLMs need\nin order to generate high-quality natural language texts. After confirming LLMs\npossess core ability, they can solve real-world and complex tasks as agent. In\nthe \"core ability\" stage, we discussed the reasoning ability, societal impact,\nand domain knowledge of LLMs. In the ``agent'' stage, we demonstrated embodied\naction, planning, and tool learning of LLMs agent applications. Finally, we\nexamined the challenges currently confronting the evaluation methods for LLMs,\nas well as the directions for future development."}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a1643132-b9a2-40b5-a12a-3db21b4804b3", "node_type": "4", "metadata": {"page_label": "22", "file_name": "2406_00936v1.pdf", "Title of this paper": "A Survey of Useful LLM Evaluation", "Authors": "Ji-Lun Peng, Sijia Cheng, Egil Diau, Yung-Yu Shih, Po-Heng Chen, Yen-Ting Lin, Yun-Nung Chen", "Date published": "06/03/2024", "URL": "http://arxiv.org/abs/2406.00936v1", "summary": "LLMs have gotten attention across various research domains due to their\nexceptional performance on a wide range of complex tasks. Therefore, refined\nmethods to evaluate the capabilities of LLMs are needed to determine the tasks\nand responsibility they should undertake. Our study mainly discussed how LLMs,\nas useful tools, should be effectively assessed. We proposed the two-stage\nframework: from ``core ability'' to ``agent'', clearly explaining how LLMs can\nbe applied based on their specific capabilities, along with the evaluation\nmethods in each stage. Core ability refers to the capabilities that LLMs need\nin order to generate high-quality natural language texts. After confirming LLMs\npossess core ability, they can solve real-world and complex tasks as agent. In\nthe \"core ability\" stage, we discussed the reasoning ability, societal impact,\nand domain knowledge of LLMs. In the ``agent'' stage, we demonstrated embodied\naction, planning, and tool learning of LLMs agent applications. Finally, we\nexamined the challenges currently confronting the evaluation methods for LLMs,\nas well as the directions for future development."}, "hash": "a8adfe99f5579e526c964514aea43203cbe76486cff49319bfd57aeaa2aa84c1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d574d45c-7f22-4e51-91bb-6ea1f7065184", "node_type": "1", "metadata": {}, "hash": "3c487b0ded24f75c948dc25d602a102623d7e23c2f2d76c538d9068ee205ee9d", "class_name": "RelatedNodeInfo"}}, "text": "den, John Miller, John U. Balis, Jonathan Batchelder,\nJonathan Berant, J\u00f6rg Frohberg, Jos Rozen, Jose\nHernandez-Orallo, Joseph Boudeman, Joseph Guerr,\nJoseph Jones, Joshua B. Tenenbaum, Joshua S. Rule,\nJoyce Chua, Kamil Kanclerz, Karen Livescu, Karl\nKrauth, Karthik Gopalakrishnan, Katerina Ignatyeva,\nKatja Markert, Kaustubh D. Dhole, Kevin Gim-\npel, Kevin Omondi, Kory Mathewson, Kristen Chi-\nafullo, Ksenia Shkaruta, Kumar Shridhar, Kyle Mc-\nDonell, Kyle Richardson, Laria Reynolds, Leo Gao,\nLi Zhang, Liam Dugan, Lianhui Qin, Lidia Contreras-\nOchando, Louis-Philippe Morency, Luca Moschella,\nLucas Lam, Lucy Noble, Ludwig Schmidt, Luheng\nHe, Luis Oliveros Col\u00f3n, Luke Metz, L\u00fctfi Kerem\n\u00b8 Senel, Maarten Bosma, Maarten Sap, Maartje ter\nHoeve, Maheen Farooqi, Manaal Faruqui, Mantas\nMazeika, Marco Baturan, Marco Marelli, Marco\nMaru, Maria Jose Ram\u00edrez Quintana, Marie Tolkiehn,\nMario Giulianelli, Martha Lewis, Martin Potthast,\nMatthew L. Leavitt, Matthias Hagen, M\u00e1ty\u00e1s Schu-\nbert, Medina Orduna Baitemirova, Melody Arnaud,\nMelvin McElrath, Michael A. Yee, Michael Co-\nhen, Michael Gu, Michael Ivanitskiy, Michael Star-\nritt, Michael Strube, Micha\u0142 Sw\u02db edrowski, Michele\nBevilacqua, Michihiro Yasunaga, Mihir Kale, Mike\nCain, Mimee Xu, Mirac Suzgun, Mitch Walker,\nMo Tiwari, Mohit Bansal, Moin Aminnaseri, Mor\nGeva, Mozhdeh Gheini, Mukund Varma T, Nanyun\nPeng, Nathan A. Chi, Nayeon Lee, Neta Gur-Ari\nKrakover, Nicholas Cameron, Nicholas Roberts,\nNick Doiron, Nicole Martinez, Nikita Nangia, Niklas\nDeckers, Niklas Muennighoff, Nitish Shirish Keskar,\nNiveditha S. Iyer, Noah Constant, Noah Fiedel, Nuan\nWen, Oliver Zhang, Omar Agha, Omar Elbaghdadi,\nOmer Levy, Owain Evans, Pablo Antonio Moreno\nCasares, Parth Doshi, Pascale Fung, Paul Pu Liang,\nPaul Vicol, Pegah Alipoormolabashi, Peiyuan Liao,\nPercy Liang, Peter Chang, Peter Eckersley, Phu Mon\nHtut, Pinyu Hwang, Piotr Mi\u0142kowski, Piyush Patil,\nPouya Pezeshkpour, Priti Oli, Qiaozhu Mei,", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1943, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d574d45c-7f22-4e51-91bb-6ea1f7065184": {"__data__": {"id_": "d574d45c-7f22-4e51-91bb-6ea1f7065184", "embedding": null, "metadata": {"page_label": "22", "file_name": "2406_00936v1.pdf", "Title of this paper": "A Survey of Useful LLM Evaluation", "Authors": "Ji-Lun Peng, Sijia Cheng, Egil Diau, Yung-Yu Shih, Po-Heng Chen, Yen-Ting Lin, Yun-Nung Chen", "Date published": "06/03/2024", "URL": "http://arxiv.org/abs/2406.00936v1", "summary": "LLMs have gotten attention across various research domains due to their\nexceptional performance on a wide range of complex tasks. Therefore, refined\nmethods to evaluate the capabilities of LLMs are needed to determine the tasks\nand responsibility they should undertake. Our study mainly discussed how LLMs,\nas useful tools, should be effectively assessed. We proposed the two-stage\nframework: from ``core ability'' to ``agent'', clearly explaining how LLMs can\nbe applied based on their specific capabilities, along with the evaluation\nmethods in each stage. Core ability refers to the capabilities that LLMs need\nin order to generate high-quality natural language texts. After confirming LLMs\npossess core ability, they can solve real-world and complex tasks as agent. In\nthe \"core ability\" stage, we discussed the reasoning ability, societal impact,\nand domain knowledge of LLMs. In the ``agent'' stage, we demonstrated embodied\naction, planning, and tool learning of LLMs agent applications. Finally, we\nexamined the challenges currently confronting the evaluation methods for LLMs,\nas well as the directions for future development."}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a1643132-b9a2-40b5-a12a-3db21b4804b3", "node_type": "4", "metadata": {"page_label": "22", "file_name": "2406_00936v1.pdf", "Title of this paper": "A Survey of Useful LLM Evaluation", "Authors": "Ji-Lun Peng, Sijia Cheng, Egil Diau, Yung-Yu Shih, Po-Heng Chen, Yen-Ting Lin, Yun-Nung Chen", "Date published": "06/03/2024", "URL": "http://arxiv.org/abs/2406.00936v1", "summary": "LLMs have gotten attention across various research domains due to their\nexceptional performance on a wide range of complex tasks. Therefore, refined\nmethods to evaluate the capabilities of LLMs are needed to determine the tasks\nand responsibility they should undertake. Our study mainly discussed how LLMs,\nas useful tools, should be effectively assessed. We proposed the two-stage\nframework: from ``core ability'' to ``agent'', clearly explaining how LLMs can\nbe applied based on their specific capabilities, along with the evaluation\nmethods in each stage. Core ability refers to the capabilities that LLMs need\nin order to generate high-quality natural language texts. After confirming LLMs\npossess core ability, they can solve real-world and complex tasks as agent. In\nthe \"core ability\" stage, we discussed the reasoning ability, societal impact,\nand domain knowledge of LLMs. In the ``agent'' stage, we demonstrated embodied\naction, planning, and tool learning of LLMs agent applications. Finally, we\nexamined the challenges currently confronting the evaluation methods for LLMs,\nas well as the directions for future development."}, "hash": "a8adfe99f5579e526c964514aea43203cbe76486cff49319bfd57aeaa2aa84c1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "769544b4-94fa-4b9b-a7c3-cf05ba175036", "node_type": "1", "metadata": {"page_label": "22", "file_name": "2406_00936v1.pdf", "Title of this paper": "A Survey of Useful LLM Evaluation", "Authors": "Ji-Lun Peng, Sijia Cheng, Egil Diau, Yung-Yu Shih, Po-Heng Chen, Yen-Ting Lin, Yun-Nung Chen", "Date published": "06/03/2024", "URL": "http://arxiv.org/abs/2406.00936v1", "summary": "LLMs have gotten attention across various research domains due to their\nexceptional performance on a wide range of complex tasks. Therefore, refined\nmethods to evaluate the capabilities of LLMs are needed to determine the tasks\nand responsibility they should undertake. Our study mainly discussed how LLMs,\nas useful tools, should be effectively assessed. We proposed the two-stage\nframework: from ``core ability'' to ``agent'', clearly explaining how LLMs can\nbe applied based on their specific capabilities, along with the evaluation\nmethods in each stage. Core ability refers to the capabilities that LLMs need\nin order to generate high-quality natural language texts. After confirming LLMs\npossess core ability, they can solve real-world and complex tasks as agent. In\nthe \"core ability\" stage, we discussed the reasoning ability, societal impact,\nand domain knowledge of LLMs. In the ``agent'' stage, we demonstrated embodied\naction, planning, and tool learning of LLMs agent applications. Finally, we\nexamined the challenges currently confronting the evaluation methods for LLMs,\nas well as the directions for future development."}, "hash": "30288307dd05616c1ee523317ff8ec2c2d67ed979ae30a37569ec0ad1ff4f9b7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f6eaa4a7-d73c-4ec0-8c30-48534ca645cf", "node_type": "1", "metadata": {}, "hash": "4fd4f06da9539340b0e529558ceee1dc2687dbb74683c86d257104588e7b0f78", "class_name": "RelatedNodeInfo"}}, "text": "Nicholas Cameron, Nicholas Roberts,\nNick Doiron, Nicole Martinez, Nikita Nangia, Niklas\nDeckers, Niklas Muennighoff, Nitish Shirish Keskar,\nNiveditha S. Iyer, Noah Constant, Noah Fiedel, Nuan\nWen, Oliver Zhang, Omar Agha, Omar Elbaghdadi,\nOmer Levy, Owain Evans, Pablo Antonio Moreno\nCasares, Parth Doshi, Pascale Fung, Paul Pu Liang,\nPaul Vicol, Pegah Alipoormolabashi, Peiyuan Liao,\nPercy Liang, Peter Chang, Peter Eckersley, Phu Mon\nHtut, Pinyu Hwang, Piotr Mi\u0142kowski, Piyush Patil,\nPouya Pezeshkpour, Priti Oli, Qiaozhu Mei, Qing\nLyu, Qinlang Chen, Rabin Banjade, Rachel Etta\nRudolph, Raefer Gabriel, Rahel Habacker, Ramon\nRisco, Rapha\u00ebl Milli\u00e8re, Rhythm Garg, Richard\nBarnes, Rif A. Saurous, Riku Arakawa, Robbe\nRaymaekers, Robert Frank, Rohan Sikand, Roman\nNovak, Roman Sitelew, Ronan LeBras, Rosanne\nLiu, Rowan Jacobs, Rui Zhang, Ruslan Salakhut-\ndinov, Ryan Chi, Ryan Lee, Ryan Stovall, Ryan\nTeehan, Rylan Yang, Sahib Singh, Saif M. Moham-\nmad, Sajant Anand, Sam Dillavou, Sam Shleifer,\nSam Wiseman, Samuel Gruetter, Samuel R. Bow-\nman, Samuel S. Schoenholz, Sanghyun Han, San-\njeev Kwatra, Sarah A. Rous, Sarik Ghazarian, Sayan\nGhosh, Sean Casey, Sebastian Bischoff, Sebastian\nGehrmann, Sebastian Schuster, Sepideh Sadeghi,\nShadi Hamdan, Sharon Zhou, Shashank Srivastava,\nSherry Shi, Shikhar Singh, Shima Asaadi, Shixi-\nang Shane Gu, Shubh Pachchigar, Shubham Tosh-\nniwal, Shyam Upadhyay, Shyamolima, Debnath,\nSiamak Shakeri, Simon Thormeyer, Simone Melzi,\nSiva Reddy, Sneha Priscilla Makini, Soo-Hwan Lee,\nSpencer Torene, Sriharsha Hatwar, Stanislas De-\nhaene, Stefan Divic, Stefano Ermon, Stella Bider-\nman, Stephanie Lin, Stephen Prasad, Steven T. Pi-antadosi, Stuart M. Shieber, Summer Misherghi, Svet-\nlana Kiritchenko, Swaroop Mishra, Tal Linzen, Tal\nSchuster, Tao Li, Tao Yu, Tariq Ali, Tatsu Hashimoto,\nTe-Lin Wu, Th\u00e9o Desbordes, Theodore Rothschild,\nThomas Phan, Tianle Wang, Tiberius Nkinyili,", "mimetype": "text/plain", "start_char_idx": 1415, "end_char_idx": 3327, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f6eaa4a7-d73c-4ec0-8c30-48534ca645cf": {"__data__": {"id_": "f6eaa4a7-d73c-4ec0-8c30-48534ca645cf", "embedding": null, "metadata": {"page_label": "22", "file_name": "2406_00936v1.pdf", "Title of this paper": "A Survey of Useful LLM Evaluation", "Authors": "Ji-Lun Peng, Sijia Cheng, Egil Diau, Yung-Yu Shih, Po-Heng Chen, Yen-Ting Lin, Yun-Nung Chen", "Date published": "06/03/2024", "URL": "http://arxiv.org/abs/2406.00936v1", "summary": "LLMs have gotten attention across various research domains due to their\nexceptional performance on a wide range of complex tasks. Therefore, refined\nmethods to evaluate the capabilities of LLMs are needed to determine the tasks\nand responsibility they should undertake. Our study mainly discussed how LLMs,\nas useful tools, should be effectively assessed. We proposed the two-stage\nframework: from ``core ability'' to ``agent'', clearly explaining how LLMs can\nbe applied based on their specific capabilities, along with the evaluation\nmethods in each stage. Core ability refers to the capabilities that LLMs need\nin order to generate high-quality natural language texts. After confirming LLMs\npossess core ability, they can solve real-world and complex tasks as agent. In\nthe \"core ability\" stage, we discussed the reasoning ability, societal impact,\nand domain knowledge of LLMs. In the ``agent'' stage, we demonstrated embodied\naction, planning, and tool learning of LLMs agent applications. Finally, we\nexamined the challenges currently confronting the evaluation methods for LLMs,\nas well as the directions for future development."}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a1643132-b9a2-40b5-a12a-3db21b4804b3", "node_type": "4", "metadata": {"page_label": "22", "file_name": "2406_00936v1.pdf", "Title of this paper": "A Survey of Useful LLM Evaluation", "Authors": "Ji-Lun Peng, Sijia Cheng, Egil Diau, Yung-Yu Shih, Po-Heng Chen, Yen-Ting Lin, Yun-Nung Chen", "Date published": "06/03/2024", "URL": "http://arxiv.org/abs/2406.00936v1", "summary": "LLMs have gotten attention across various research domains due to their\nexceptional performance on a wide range of complex tasks. Therefore, refined\nmethods to evaluate the capabilities of LLMs are needed to determine the tasks\nand responsibility they should undertake. Our study mainly discussed how LLMs,\nas useful tools, should be effectively assessed. We proposed the two-stage\nframework: from ``core ability'' to ``agent'', clearly explaining how LLMs can\nbe applied based on their specific capabilities, along with the evaluation\nmethods in each stage. Core ability refers to the capabilities that LLMs need\nin order to generate high-quality natural language texts. After confirming LLMs\npossess core ability, they can solve real-world and complex tasks as agent. In\nthe \"core ability\" stage, we discussed the reasoning ability, societal impact,\nand domain knowledge of LLMs. In the ``agent'' stage, we demonstrated embodied\naction, planning, and tool learning of LLMs agent applications. Finally, we\nexamined the challenges currently confronting the evaluation methods for LLMs,\nas well as the directions for future development."}, "hash": "a8adfe99f5579e526c964514aea43203cbe76486cff49319bfd57aeaa2aa84c1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d574d45c-7f22-4e51-91bb-6ea1f7065184", "node_type": "1", "metadata": {"page_label": "22", "file_name": "2406_00936v1.pdf", "Title of this paper": "A Survey of Useful LLM Evaluation", "Authors": "Ji-Lun Peng, Sijia Cheng, Egil Diau, Yung-Yu Shih, Po-Heng Chen, Yen-Ting Lin, Yun-Nung Chen", "Date published": "06/03/2024", "URL": "http://arxiv.org/abs/2406.00936v1", "summary": "LLMs have gotten attention across various research domains due to their\nexceptional performance on a wide range of complex tasks. Therefore, refined\nmethods to evaluate the capabilities of LLMs are needed to determine the tasks\nand responsibility they should undertake. Our study mainly discussed how LLMs,\nas useful tools, should be effectively assessed. We proposed the two-stage\nframework: from ``core ability'' to ``agent'', clearly explaining how LLMs can\nbe applied based on their specific capabilities, along with the evaluation\nmethods in each stage. Core ability refers to the capabilities that LLMs need\nin order to generate high-quality natural language texts. After confirming LLMs\npossess core ability, they can solve real-world and complex tasks as agent. In\nthe \"core ability\" stage, we discussed the reasoning ability, societal impact,\nand domain knowledge of LLMs. In the ``agent'' stage, we demonstrated embodied\naction, planning, and tool learning of LLMs agent applications. Finally, we\nexamined the challenges currently confronting the evaluation methods for LLMs,\nas well as the directions for future development."}, "hash": "aa585d8a64929c7e69311956cdc8344fe8eb631ceea0ca03dc2aeeaa2c514682", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d5d82fcb-345c-4f93-b420-5e49ae10cc81", "node_type": "1", "metadata": {}, "hash": "4ac1013ac37f7e8bc2c001396875cc3711105c46617e023a1c5e3ca0a6b81a1a", "class_name": "RelatedNodeInfo"}}, "text": "Shubham Tosh-\nniwal, Shyam Upadhyay, Shyamolima, Debnath,\nSiamak Shakeri, Simon Thormeyer, Simone Melzi,\nSiva Reddy, Sneha Priscilla Makini, Soo-Hwan Lee,\nSpencer Torene, Sriharsha Hatwar, Stanislas De-\nhaene, Stefan Divic, Stefano Ermon, Stella Bider-\nman, Stephanie Lin, Stephen Prasad, Steven T. Pi-antadosi, Stuart M. Shieber, Summer Misherghi, Svet-\nlana Kiritchenko, Swaroop Mishra, Tal Linzen, Tal\nSchuster, Tao Li, Tao Yu, Tariq Ali, Tatsu Hashimoto,\nTe-Lin Wu, Th\u00e9o Desbordes, Theodore Rothschild,\nThomas Phan, Tianle Wang, Tiberius Nkinyili, Timo\nSchick, Timofei Kornev, Titus Tunduny, Tobias Ger-\nstenberg, Trenton Chang, Trishala Neeraj, Tushar\nKhot, Tyler Shultz, Uri Shaham, Vedant Misra, Vera\nDemberg, Victoria Nyamai, Vikas Raunak, Vinay\nRamasesh, Vinay Uday Prabhu, Vishakh Padmaku-\nmar, Vivek Srikumar, William Fedus, William Saun-\nders, William Zhang, Wout V ossen, Xiang Ren, Xi-\naoyu Tong, Xinran Zhao, Xinyi Wu, Xudong Shen,\nYadollah Yaghoobzadeh, Yair Lakretz, Yangqiu Song,\nYasaman Bahri, Yejin Choi, Yichi Yang, Yiding\nHao, Yifu Chen, Yonatan Belinkov, Yu Hou, Yufang\nHou, Yuntao Bai, Zachary Seid, Zhuoye Zhao, Zi-\njian Wang, Zijie J. Wang, Zirui Wang, and Ziyi Wu.\n2023. Beyond the imitation game: Quantifying and\nextrapolating the capabilities of language models.\nRobin Staab, Mark Vero, Mislav Balunovi \u00b4c, and Martin\nVechev. 2023. Beyond memorization: Violating pri-\nvacy via inference with large language models. arXiv\npreprint arXiv:2310.07298 .\nAlessandro Stolfo, Zhijing Jin, Kumar Shridhar, Bern-\nhard Sch\u00f6lkopf, and Mrinmaya Sachan. 2023. A\ncausal framework to quantify the robustness of math-\nematical reasoning with language models.\nHaotian Sun, Yuchen Zhuang, Lingkai Kong, Bo Dai,\nand Chao Zhang. 2023. Adaplanner: Adaptive plan-\nning from feedback with language models. In Ad-\nvances in Neural Information Processing Systems ,\nvolume 36, pages 58202\u201358245. Curran Associates,\nInc.", "mimetype": "text/plain", "start_char_idx": 2776, "end_char_idx": 4697, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d5d82fcb-345c-4f93-b420-5e49ae10cc81": {"__data__": {"id_": "d5d82fcb-345c-4f93-b420-5e49ae10cc81", "embedding": null, "metadata": {"page_label": "22", "file_name": "2406_00936v1.pdf", "Title of this paper": "A Survey of Useful LLM Evaluation", "Authors": "Ji-Lun Peng, Sijia Cheng, Egil Diau, Yung-Yu Shih, Po-Heng Chen, Yen-Ting Lin, Yun-Nung Chen", "Date published": "06/03/2024", "URL": "http://arxiv.org/abs/2406.00936v1", "summary": "LLMs have gotten attention across various research domains due to their\nexceptional performance on a wide range of complex tasks. Therefore, refined\nmethods to evaluate the capabilities of LLMs are needed to determine the tasks\nand responsibility they should undertake. Our study mainly discussed how LLMs,\nas useful tools, should be effectively assessed. We proposed the two-stage\nframework: from ``core ability'' to ``agent'', clearly explaining how LLMs can\nbe applied based on their specific capabilities, along with the evaluation\nmethods in each stage. Core ability refers to the capabilities that LLMs need\nin order to generate high-quality natural language texts. After confirming LLMs\npossess core ability, they can solve real-world and complex tasks as agent. In\nthe \"core ability\" stage, we discussed the reasoning ability, societal impact,\nand domain knowledge of LLMs. In the ``agent'' stage, we demonstrated embodied\naction, planning, and tool learning of LLMs agent applications. Finally, we\nexamined the challenges currently confronting the evaluation methods for LLMs,\nas well as the directions for future development."}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a1643132-b9a2-40b5-a12a-3db21b4804b3", "node_type": "4", "metadata": {"page_label": "22", "file_name": "2406_00936v1.pdf", "Title of this paper": "A Survey of Useful LLM Evaluation", "Authors": "Ji-Lun Peng, Sijia Cheng, Egil Diau, Yung-Yu Shih, Po-Heng Chen, Yen-Ting Lin, Yun-Nung Chen", "Date published": "06/03/2024", "URL": "http://arxiv.org/abs/2406.00936v1", "summary": "LLMs have gotten attention across various research domains due to their\nexceptional performance on a wide range of complex tasks. Therefore, refined\nmethods to evaluate the capabilities of LLMs are needed to determine the tasks\nand responsibility they should undertake. Our study mainly discussed how LLMs,\nas useful tools, should be effectively assessed. We proposed the two-stage\nframework: from ``core ability'' to ``agent'', clearly explaining how LLMs can\nbe applied based on their specific capabilities, along with the evaluation\nmethods in each stage. Core ability refers to the capabilities that LLMs need\nin order to generate high-quality natural language texts. After confirming LLMs\npossess core ability, they can solve real-world and complex tasks as agent. In\nthe \"core ability\" stage, we discussed the reasoning ability, societal impact,\nand domain knowledge of LLMs. In the ``agent'' stage, we demonstrated embodied\naction, planning, and tool learning of LLMs agent applications. Finally, we\nexamined the challenges currently confronting the evaluation methods for LLMs,\nas well as the directions for future development."}, "hash": "a8adfe99f5579e526c964514aea43203cbe76486cff49319bfd57aeaa2aa84c1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f6eaa4a7-d73c-4ec0-8c30-48534ca645cf", "node_type": "1", "metadata": {"page_label": "22", "file_name": "2406_00936v1.pdf", "Title of this paper": "A Survey of Useful LLM Evaluation", "Authors": "Ji-Lun Peng, Sijia Cheng, Egil Diau, Yung-Yu Shih, Po-Heng Chen, Yen-Ting Lin, Yun-Nung Chen", "Date published": "06/03/2024", "URL": "http://arxiv.org/abs/2406.00936v1", "summary": "LLMs have gotten attention across various research domains due to their\nexceptional performance on a wide range of complex tasks. Therefore, refined\nmethods to evaluate the capabilities of LLMs are needed to determine the tasks\nand responsibility they should undertake. Our study mainly discussed how LLMs,\nas useful tools, should be effectively assessed. We proposed the two-stage\nframework: from ``core ability'' to ``agent'', clearly explaining how LLMs can\nbe applied based on their specific capabilities, along with the evaluation\nmethods in each stage. Core ability refers to the capabilities that LLMs need\nin order to generate high-quality natural language texts. After confirming LLMs\npossess core ability, they can solve real-world and complex tasks as agent. In\nthe \"core ability\" stage, we discussed the reasoning ability, societal impact,\nand domain knowledge of LLMs. In the ``agent'' stage, we demonstrated embodied\naction, planning, and tool learning of LLMs agent applications. Finally, we\nexamined the challenges currently confronting the evaluation methods for LLMs,\nas well as the directions for future development."}, "hash": "54e37e66eb5ab6cb88305493d7bc9b817b45e05fade8c9aae95e5f4c67b3f1b2", "class_name": "RelatedNodeInfo"}}, "text": "Robin Staab, Mark Vero, Mislav Balunovi \u00b4c, and Martin\nVechev. 2023. Beyond memorization: Violating pri-\nvacy via inference with large language models. arXiv\npreprint arXiv:2310.07298 .\nAlessandro Stolfo, Zhijing Jin, Kumar Shridhar, Bern-\nhard Sch\u00f6lkopf, and Mrinmaya Sachan. 2023. A\ncausal framework to quantify the robustness of math-\nematical reasoning with language models.\nHaotian Sun, Yuchen Zhuang, Lingkai Kong, Bo Dai,\nand Chao Zhang. 2023. Adaplanner: Adaptive plan-\nning from feedback with language models. In Ad-\nvances in Neural Information Processing Systems ,\nvolume 36, pages 58202\u201358245. Curran Associates,\nInc.\nJiankai Sun, Chuanyang Zheng, Enze Xie, Zhengying\nLiu, Ruihang Chu, Jianing Qiu, Jiaqi Xu, Mingyu\nDing, Hongyang Li, Mengzhe Geng, Yue Wu, Wen-\nhai Wang, Junsong Chen, Zhangyue Yin, Xiaozhe\nRen, Jie Fu, Junxian He, Wu Yuan, Qi Liu, Xihui\nLiu, Yu Li, Hao Dong, Yu Cheng, Ming Zhang,\nPheng Ann Heng, Jifeng Dai, Ping Luo, Jingdong\nWang, Ji-Rong Wen, Xipeng Qiu, Yike Guo, Hui\nXiong, Qun Liu, and Zhenguo Li. 2024. A survey of\nreasoning with foundation models.\nMirac Suzgun, Nathan Scales, Nathanael Sch\u00e4rli, Se-\nbastian Gehrmann, Yi Tay, Hyung Won Chung,\nAakanksha Chowdhery, Quoc V Le, Ed H Chi, Denny\nZhou, et al. 2022. Challenging big-bench tasks and\nwhether chain-of-thought can solve them. arXiv\npreprint arXiv:2210.09261 .\nAlon Talmor, Jonathan Herzig, Nicholas Lourie, and\nJonathan Berant. 2018. Commonsenseqa: A question\nanswering challenge targeting commonsense knowl-\nedge. arXiv preprint arXiv:1811.00937 .\nLiyan Tang, Igor Shalyminov, Amy Wing mei Wong,\nJon Burnsky, Jake W. Vincent, Yu\u2019an Yang, Siffi\nSingh, Song Feng, Hwanjun Song, Hang Su, Lijia\n22", "mimetype": "text/plain", "start_char_idx": 4068, "end_char_idx": 5759, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "47f7130d-2124-4425-907a-92d492aef41c": {"__data__": {"id_": "47f7130d-2124-4425-907a-92d492aef41c", "embedding": null, "metadata": {"page_label": "23", "file_name": "2406_00936v1.pdf", "Title of this paper": "A Survey of Useful LLM Evaluation", "Authors": "Ji-Lun Peng, Sijia Cheng, Egil Diau, Yung-Yu Shih, Po-Heng Chen, Yen-Ting Lin, Yun-Nung Chen", "Date published": "06/03/2024", "URL": "http://arxiv.org/abs/2406.00936v1", "summary": "LLMs have gotten attention across various research domains due to their\nexceptional performance on a wide range of complex tasks. Therefore, refined\nmethods to evaluate the capabilities of LLMs are needed to determine the tasks\nand responsibility they should undertake. Our study mainly discussed how LLMs,\nas useful tools, should be effectively assessed. We proposed the two-stage\nframework: from ``core ability'' to ``agent'', clearly explaining how LLMs can\nbe applied based on their specific capabilities, along with the evaluation\nmethods in each stage. Core ability refers to the capabilities that LLMs need\nin order to generate high-quality natural language texts. After confirming LLMs\npossess core ability, they can solve real-world and complex tasks as agent. In\nthe \"core ability\" stage, we discussed the reasoning ability, societal impact,\nand domain knowledge of LLMs. In the ``agent'' stage, we demonstrated embodied\naction, planning, and tool learning of LLMs agent applications. Finally, we\nexamined the challenges currently confronting the evaluation methods for LLMs,\nas well as the directions for future development."}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b54ab185-43c9-49d9-9a3f-075efccae227", "node_type": "4", "metadata": {"page_label": "23", "file_name": "2406_00936v1.pdf", "Title of this paper": "A Survey of Useful LLM Evaluation", "Authors": "Ji-Lun Peng, Sijia Cheng, Egil Diau, Yung-Yu Shih, Po-Heng Chen, Yen-Ting Lin, Yun-Nung Chen", "Date published": "06/03/2024", "URL": "http://arxiv.org/abs/2406.00936v1", "summary": "LLMs have gotten attention across various research domains due to their\nexceptional performance on a wide range of complex tasks. Therefore, refined\nmethods to evaluate the capabilities of LLMs are needed to determine the tasks\nand responsibility they should undertake. Our study mainly discussed how LLMs,\nas useful tools, should be effectively assessed. We proposed the two-stage\nframework: from ``core ability'' to ``agent'', clearly explaining how LLMs can\nbe applied based on their specific capabilities, along with the evaluation\nmethods in each stage. Core ability refers to the capabilities that LLMs need\nin order to generate high-quality natural language texts. After confirming LLMs\npossess core ability, they can solve real-world and complex tasks as agent. In\nthe \"core ability\" stage, we discussed the reasoning ability, societal impact,\nand domain knowledge of LLMs. In the ``agent'' stage, we demonstrated embodied\naction, planning, and tool learning of LLMs agent applications. Finally, we\nexamined the challenges currently confronting the evaluation methods for LLMs,\nas well as the directions for future development."}, "hash": "dbd98d358ed52a3c1f8711f1cd9c6a4674b40c12f6fedddd26df2f8d97a100d9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "da74fc8b-a3f2-429d-ad5d-6882036548a0", "node_type": "1", "metadata": {}, "hash": "6fe47ad0703c1a3d357ae1b5d382dbf03eaf6c7c9cd2f03c18bb8d1a781fc019", "class_name": "RelatedNodeInfo"}}, "text": "Sun, Yi Zhang, Saab Mansour, and Kathleen McK-\neown. 2024. Tofueval: Evaluating hallucinations of\nllms on topic-focused dialogue summarization.\nQiaoyu Tang, Ziliang Deng, Hongyu Lin, Xianpei Han,\nQiao Liang, Boxi Cao, and Le Sun. 2023. Toolalpaca:\nGeneralized tool learning for language models with\n3000 simulated cases.\nArun James Thirunavukarasu, Darren Shu Jeng Ting,\nKabilan Elangovan, Laura Gutierrez, Ting Fang Tan,\nand Daniel Shu Wei Ting. 2023. Large language\nmodels in medicine. Nature medicine , 29(8):1930\u2013\n1940.\nKevin P Tobia. 2020. Testing ordinary meaning. Harv.\nL. Rev. , 134:726.\nMiles Turpin, Julian Michael, Ethan Perez, and\nSamuel R. Bowman. 2023. Language models don\u2019t\nalways say what they think: Unfaithful explanations\nin chain-of-thought prompting.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. Advances in neural information processing\nsystems , 30.\nDiederick Vermetten, Bas van Stein, Fabio Caraffini, Le-\nandro L Minku, and Anna V Kononova. 2022. Bias:\nA toolbox for benchmarking structural bias in the con-\ntinuous domain. IEEE Transactions on Evolutionary\nComputation , 26(6):1380\u20131393.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1226, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "da74fc8b-a3f2-429d-ad5d-6882036548a0": {"__data__": {"id_": "da74fc8b-a3f2-429d-ad5d-6882036548a0", "embedding": null, "metadata": {"page_label": "23", "file_name": "2406_00936v1.pdf", "Title of this paper": "A Survey of Useful LLM Evaluation", "Authors": "Ji-Lun Peng, Sijia Cheng, Egil Diau, Yung-Yu Shih, Po-Heng Chen, Yen-Ting Lin, Yun-Nung Chen", "Date published": "06/03/2024", "URL": "http://arxiv.org/abs/2406.00936v1", "summary": "LLMs have gotten attention across various research domains due to their\nexceptional performance on a wide range of complex tasks. Therefore, refined\nmethods to evaluate the capabilities of LLMs are needed to determine the tasks\nand responsibility they should undertake. Our study mainly discussed how LLMs,\nas useful tools, should be effectively assessed. We proposed the two-stage\nframework: from ``core ability'' to ``agent'', clearly explaining how LLMs can\nbe applied based on their specific capabilities, along with the evaluation\nmethods in each stage. Core ability refers to the capabilities that LLMs need\nin order to generate high-quality natural language texts. After confirming LLMs\npossess core ability, they can solve real-world and complex tasks as agent. In\nthe \"core ability\" stage, we discussed the reasoning ability, societal impact,\nand domain knowledge of LLMs. In the ``agent'' stage, we demonstrated embodied\naction, planning, and tool learning of LLMs agent applications. Finally, we\nexamined the challenges currently confronting the evaluation methods for LLMs,\nas well as the directions for future development."}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b54ab185-43c9-49d9-9a3f-075efccae227", "node_type": "4", "metadata": {"page_label": "23", "file_name": "2406_00936v1.pdf", "Title of this paper": "A Survey of Useful LLM Evaluation", "Authors": "Ji-Lun Peng, Sijia Cheng, Egil Diau, Yung-Yu Shih, Po-Heng Chen, Yen-Ting Lin, Yun-Nung Chen", "Date published": "06/03/2024", "URL": "http://arxiv.org/abs/2406.00936v1", "summary": "LLMs have gotten attention across various research domains due to their\nexceptional performance on a wide range of complex tasks. Therefore, refined\nmethods to evaluate the capabilities of LLMs are needed to determine the tasks\nand responsibility they should undertake. Our study mainly discussed how LLMs,\nas useful tools, should be effectively assessed. We proposed the two-stage\nframework: from ``core ability'' to ``agent'', clearly explaining how LLMs can\nbe applied based on their specific capabilities, along with the evaluation\nmethods in each stage. Core ability refers to the capabilities that LLMs need\nin order to generate high-quality natural language texts. After confirming LLMs\npossess core ability, they can solve real-world and complex tasks as agent. In\nthe \"core ability\" stage, we discussed the reasoning ability, societal impact,\nand domain knowledge of LLMs. In the ``agent'' stage, we demonstrated embodied\naction, planning, and tool learning of LLMs agent applications. Finally, we\nexamined the challenges currently confronting the evaluation methods for LLMs,\nas well as the directions for future development."}, "hash": "dbd98d358ed52a3c1f8711f1cd9c6a4674b40c12f6fedddd26df2f8d97a100d9", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "47f7130d-2124-4425-907a-92d492aef41c", "node_type": "1", "metadata": {"page_label": "23", "file_name": "2406_00936v1.pdf", "Title of this paper": "A Survey of Useful LLM Evaluation", "Authors": "Ji-Lun Peng, Sijia Cheng, Egil Diau, Yung-Yu Shih, Po-Heng Chen, Yen-Ting Lin, Yun-Nung Chen", "Date published": "06/03/2024", "URL": "http://arxiv.org/abs/2406.00936v1", "summary": "LLMs have gotten attention across various research domains due to their\nexceptional performance on a wide range of complex tasks. Therefore, refined\nmethods to evaluate the capabilities of LLMs are needed to determine the tasks\nand responsibility they should undertake. Our study mainly discussed how LLMs,\nas useful tools, should be effectively assessed. We proposed the two-stage\nframework: from ``core ability'' to ``agent'', clearly explaining how LLMs can\nbe applied based on their specific capabilities, along with the evaluation\nmethods in each stage. Core ability refers to the capabilities that LLMs need\nin order to generate high-quality natural language texts. After confirming LLMs\npossess core ability, they can solve real-world and complex tasks as agent. In\nthe \"core ability\" stage, we discussed the reasoning ability, societal impact,\nand domain knowledge of LLMs. In the ``agent'' stage, we demonstrated embodied\naction, planning, and tool learning of LLMs agent applications. Finally, we\nexamined the challenges currently confronting the evaluation methods for LLMs,\nas well as the directions for future development."}, "hash": "a597ac50a8937d226002702aca457a3ae3a21ded6aea8d171b93241b9bb34bdd", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "fa3166c8-9bfd-40e9-a2a4-3f73e7f1b635", "node_type": "1", "metadata": {}, "hash": "dae49695292eb93ae28a822a1ce2c226e770d1c431a953977af1e3b919f2b7e6", "class_name": "RelatedNodeInfo"}}, "text": "Miles Turpin, Julian Michael, Ethan Perez, and\nSamuel R. Bowman. 2023. Language models don\u2019t\nalways say what they think: Unfaithful explanations\nin chain-of-thought prompting.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. Advances in neural information processing\nsystems , 30.\nDiederick Vermetten, Bas van Stein, Fabio Caraffini, Le-\nandro L Minku, and Anna V Kononova. 2022. Bias:\nA toolbox for benchmarking structural bias in the con-\ntinuous domain. IEEE Transactions on Evolutionary\nComputation , 26(6):1380\u20131393.\nBertie Vidgen, Adarsh Agrawal, Ahmed M. Ahmed,\nVictor Akinwande, Namir Al-Nuaimi, Najla Alfaraj,\nElie Alhajjar, Lora Aroyo, Trupti Bavalatti, Borhane\nBlili-Hamelin, Kurt Bollacker, Rishi Bomassani,\nMarisa Ferrara Boston, Sim\u00e9on Campos, Kal Chakra,\nCanyu Chen, Cody Coleman, Zacharie Delpierre\nCoudert, Leon Derczynski, Debojyoti Dutta, Ian\nEisenberg, James Ezick, Heather Frase, Brian Fuller,\nRam Gandikota, Agasthya Gangavarapu, Ananya\nGangavarapu, James Gealy, Rajat Ghosh, James\nGoel, Usman Gohar, Sujata Goswami, Scott A.\nHale, Wiebke Hutiri, Joseph Marvin Imperial, Sur-\ngan Jandial, Nick Judd, Felix Juefei-Xu, Foutse\nKhomh, Bhavya Kailkhura, Hannah Rose Kirk,\nKevin Klyman, Chris Knotz, Michael Kuchnik,\nShachi H. Kumar, Chris Lengerich, Bo Li, Zeyi\nLiao, Eileen Peters Long, Victor Lu, Yifan Mai,\nPriyanka Mary Mammen, Kelvin Manyeki, Sean\nMcGregor, Virendra Mehta, Shafee Mohammed,\nEmanuel Moss, Lama Nachman, Dinesh Jinenhally\nNaganna, Amin Nikanjam, Besmira Nushi, Luis Oala,\nIftach Orr, Alicia Parrish, Cigdem Patlak, William\nPietri, Forough Poursabzi-Sangdeh, Eleonora Pre-\nsani, Fabrizio Puletti, Paul R\u00f6ttger, Saurav Sahay,\nTim Santos, Nino Scherrer, Alice Schoenauer Se-\nbag, Patrick Schramowski, Abolfazl Shahbazi, Vin\nSharma, Xudong Shen, Vamsi Sistla, Leonard Tang,\nDavide Testuggine, Vithursan Thangarasa, Eliza-\nbeth Anne Watkins, Rebecca Weiss, Chris Welty,\nTyler Wilbers, Adina Williams, Carole-Jean Wu,Poonam Yadav, Xianjun Yang, Yi Zeng, Wenhui\nZhang, Fedor Zhdanov, Jiacheng Zhu, Percy Liang,\nPeter Mattson, and Joaquin Vanschoren.", "mimetype": "text/plain", "start_char_idx": 596, "end_char_idx": 2784, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "fa3166c8-9bfd-40e9-a2a4-3f73e7f1b635": {"__data__": {"id_": "fa3166c8-9bfd-40e9-a2a4-3f73e7f1b635", "embedding": null, "metadata": {"page_label": "23", "file_name": "2406_00936v1.pdf", "Title of this paper": "A Survey of Useful LLM Evaluation", "Authors": "Ji-Lun Peng, Sijia Cheng, Egil Diau, Yung-Yu Shih, Po-Heng Chen, Yen-Ting Lin, Yun-Nung Chen", "Date published": "06/03/2024", "URL": "http://arxiv.org/abs/2406.00936v1", "summary": "LLMs have gotten attention across various research domains due to their\nexceptional performance on a wide range of complex tasks. Therefore, refined\nmethods to evaluate the capabilities of LLMs are needed to determine the tasks\nand responsibility they should undertake. Our study mainly discussed how LLMs,\nas useful tools, should be effectively assessed. We proposed the two-stage\nframework: from ``core ability'' to ``agent'', clearly explaining how LLMs can\nbe applied based on their specific capabilities, along with the evaluation\nmethods in each stage. Core ability refers to the capabilities that LLMs need\nin order to generate high-quality natural language texts. After confirming LLMs\npossess core ability, they can solve real-world and complex tasks as agent. In\nthe \"core ability\" stage, we discussed the reasoning ability, societal impact,\nand domain knowledge of LLMs. In the ``agent'' stage, we demonstrated embodied\naction, planning, and tool learning of LLMs agent applications. Finally, we\nexamined the challenges currently confronting the evaluation methods for LLMs,\nas well as the directions for future development."}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b54ab185-43c9-49d9-9a3f-075efccae227", "node_type": "4", "metadata": {"page_label": "23", "file_name": "2406_00936v1.pdf", "Title of this paper": "A Survey of Useful LLM Evaluation", "Authors": "Ji-Lun Peng, Sijia Cheng, Egil Diau, Yung-Yu Shih, Po-Heng Chen, Yen-Ting Lin, Yun-Nung Chen", "Date published": "06/03/2024", "URL": "http://arxiv.org/abs/2406.00936v1", "summary": "LLMs have gotten attention across various research domains due to their\nexceptional performance on a wide range of complex tasks. Therefore, refined\nmethods to evaluate the capabilities of LLMs are needed to determine the tasks\nand responsibility they should undertake. Our study mainly discussed how LLMs,\nas useful tools, should be effectively assessed. We proposed the two-stage\nframework: from ``core ability'' to ``agent'', clearly explaining how LLMs can\nbe applied based on their specific capabilities, along with the evaluation\nmethods in each stage. Core ability refers to the capabilities that LLMs need\nin order to generate high-quality natural language texts. After confirming LLMs\npossess core ability, they can solve real-world and complex tasks as agent. In\nthe \"core ability\" stage, we discussed the reasoning ability, societal impact,\nand domain knowledge of LLMs. In the ``agent'' stage, we demonstrated embodied\naction, planning, and tool learning of LLMs agent applications. Finally, we\nexamined the challenges currently confronting the evaluation methods for LLMs,\nas well as the directions for future development."}, "hash": "dbd98d358ed52a3c1f8711f1cd9c6a4674b40c12f6fedddd26df2f8d97a100d9", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "da74fc8b-a3f2-429d-ad5d-6882036548a0", "node_type": "1", "metadata": {"page_label": "23", "file_name": "2406_00936v1.pdf", "Title of this paper": "A Survey of Useful LLM Evaluation", "Authors": "Ji-Lun Peng, Sijia Cheng, Egil Diau, Yung-Yu Shih, Po-Heng Chen, Yen-Ting Lin, Yun-Nung Chen", "Date published": "06/03/2024", "URL": "http://arxiv.org/abs/2406.00936v1", "summary": "LLMs have gotten attention across various research domains due to their\nexceptional performance on a wide range of complex tasks. Therefore, refined\nmethods to evaluate the capabilities of LLMs are needed to determine the tasks\nand responsibility they should undertake. Our study mainly discussed how LLMs,\nas useful tools, should be effectively assessed. We proposed the two-stage\nframework: from ``core ability'' to ``agent'', clearly explaining how LLMs can\nbe applied based on their specific capabilities, along with the evaluation\nmethods in each stage. Core ability refers to the capabilities that LLMs need\nin order to generate high-quality natural language texts. After confirming LLMs\npossess core ability, they can solve real-world and complex tasks as agent. In\nthe \"core ability\" stage, we discussed the reasoning ability, societal impact,\nand domain knowledge of LLMs. In the ``agent'' stage, we demonstrated embodied\naction, planning, and tool learning of LLMs agent applications. Finally, we\nexamined the challenges currently confronting the evaluation methods for LLMs,\nas well as the directions for future development."}, "hash": "05e3d5f7d5e331c88eb9cc592505dde31d56a8c84f7c6c3de0fbfefa0fe83fde", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "53292e7f-b531-4eb9-92eb-46ee6a25be54", "node_type": "1", "metadata": {}, "hash": "771f17aacbbf0a746af0de673d4998f68cbf984e8afd4cd51739389f05c6dc6f", "class_name": "RelatedNodeInfo"}}, "text": "2024. Intro-\nducing v0.5 of the ai safety benchmark from mlcom-\nmons.\nJunyang Wang, Yuhang Wang, Guohai Xu, Jing Zhang,\nYukai Gu, Haitao Jia, Jiaqi Wang, Haiyang Xu, Ming\nYan, Ji Zhang, and Jitao Sang. 2024a. Amber: An\nllm-free multi-dimensional benchmark for mllms hal-\nlucination evaluation.\nLei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao\nYang, Jingsen Zhang, Zhiyuan Chen, Jiakai Tang,\nXu Chen, Yankai Lin, Wayne Xin Zhao, Zhewei Wei,\nand Jirong Wen. 2024b. A survey on large language\nmodel based autonomous agents. Frontiers of Com-\nputer Science , 18(6).\nSiyuan Wang, Zhuohan Long, Zhihao Fan, Zhongyu\nWei, and Xuanjing Huang. 2024c. Benchmark self-\nevolving: A multi-agent framework for dynamic llm\nevaluation. arXiv preprint arXiv:2402.11443 .\nSu Wang, Greg Durrett, and Katrin Erk. 2018. Modeling\nsemantic plausibility by injecting world knowledge.\narXiv preprint arXiv:1804.00619 .\nYizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa\nLiu, Noah A. Smith, Daniel Khashabi, and Hannaneh\nHajishirzi. 2023a. Self-instruct: Aligning language\nmodels with self-generated instructions.\nZihao Wang, Shaofei Cai, Guanzhou Chen, Anji Liu,\nXiaojian Ma, and Yitao Liang. 2023b. Describe,\nexplain, plan and select: Interactive planning with\nlarge language models enables open-world multi-task\nagents. arXiv preprint arXiv:2302.01560 .\nJiaheng Wei, Yuanshun Yao, Jean-Francois Ton, Hongyi\nGuo, Andrew Estornell, and Yang Liu. 2024. Mea-\nsuring and reducing llm hallucination without gold-\nstandard answers via expertise-weighting. arXiv\npreprint arXiv:2402.10412 .\nMartin Wessel, Tom\u00e1s Horych, Terry Ruas, Akiko\nAizawa, Bela Gipp, and Timo Spinde. 2023. Intro-\nducing mbib-the first media bias identification bench-\nmark task and dataset collection. In Proceedings of\nthe 46th International ACM SIGIR Conference on\nResearch and Development in Information Retrieval ,\npages 2765\u20132774.\nJason Weston, Antoine Bordes, Sumit Chopra, Alexan-\nder M Rush, Bart Van Merri\u00ebnboer, Armand Joulin,\nand Tomas Mikolov. 2015. Towards ai-complete\nquestion answering: A set of prerequisite toy tasks.\narXiv preprint arXiv:1502.05698 .", "mimetype": "text/plain", "start_char_idx": 2785, "end_char_idx": 4895, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "53292e7f-b531-4eb9-92eb-46ee6a25be54": {"__data__": {"id_": "53292e7f-b531-4eb9-92eb-46ee6a25be54", "embedding": null, "metadata": {"page_label": "23", "file_name": "2406_00936v1.pdf", "Title of this paper": "A Survey of Useful LLM Evaluation", "Authors": "Ji-Lun Peng, Sijia Cheng, Egil Diau, Yung-Yu Shih, Po-Heng Chen, Yen-Ting Lin, Yun-Nung Chen", "Date published": "06/03/2024", "URL": "http://arxiv.org/abs/2406.00936v1", "summary": "LLMs have gotten attention across various research domains due to their\nexceptional performance on a wide range of complex tasks. Therefore, refined\nmethods to evaluate the capabilities of LLMs are needed to determine the tasks\nand responsibility they should undertake. Our study mainly discussed how LLMs,\nas useful tools, should be effectively assessed. We proposed the two-stage\nframework: from ``core ability'' to ``agent'', clearly explaining how LLMs can\nbe applied based on their specific capabilities, along with the evaluation\nmethods in each stage. Core ability refers to the capabilities that LLMs need\nin order to generate high-quality natural language texts. After confirming LLMs\npossess core ability, they can solve real-world and complex tasks as agent. In\nthe \"core ability\" stage, we discussed the reasoning ability, societal impact,\nand domain knowledge of LLMs. In the ``agent'' stage, we demonstrated embodied\naction, planning, and tool learning of LLMs agent applications. Finally, we\nexamined the challenges currently confronting the evaluation methods for LLMs,\nas well as the directions for future development."}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b54ab185-43c9-49d9-9a3f-075efccae227", "node_type": "4", "metadata": {"page_label": "23", "file_name": "2406_00936v1.pdf", "Title of this paper": "A Survey of Useful LLM Evaluation", "Authors": "Ji-Lun Peng, Sijia Cheng, Egil Diau, Yung-Yu Shih, Po-Heng Chen, Yen-Ting Lin, Yun-Nung Chen", "Date published": "06/03/2024", "URL": "http://arxiv.org/abs/2406.00936v1", "summary": "LLMs have gotten attention across various research domains due to their\nexceptional performance on a wide range of complex tasks. Therefore, refined\nmethods to evaluate the capabilities of LLMs are needed to determine the tasks\nand responsibility they should undertake. Our study mainly discussed how LLMs,\nas useful tools, should be effectively assessed. We proposed the two-stage\nframework: from ``core ability'' to ``agent'', clearly explaining how LLMs can\nbe applied based on their specific capabilities, along with the evaluation\nmethods in each stage. Core ability refers to the capabilities that LLMs need\nin order to generate high-quality natural language texts. After confirming LLMs\npossess core ability, they can solve real-world and complex tasks as agent. In\nthe \"core ability\" stage, we discussed the reasoning ability, societal impact,\nand domain knowledge of LLMs. In the ``agent'' stage, we demonstrated embodied\naction, planning, and tool learning of LLMs agent applications. Finally, we\nexamined the challenges currently confronting the evaluation methods for LLMs,\nas well as the directions for future development."}, "hash": "dbd98d358ed52a3c1f8711f1cd9c6a4674b40c12f6fedddd26df2f8d97a100d9", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "fa3166c8-9bfd-40e9-a2a4-3f73e7f1b635", "node_type": "1", "metadata": {"page_label": "23", "file_name": "2406_00936v1.pdf", "Title of this paper": "A Survey of Useful LLM Evaluation", "Authors": "Ji-Lun Peng, Sijia Cheng, Egil Diau, Yung-Yu Shih, Po-Heng Chen, Yen-Ting Lin, Yun-Nung Chen", "Date published": "06/03/2024", "URL": "http://arxiv.org/abs/2406.00936v1", "summary": "LLMs have gotten attention across various research domains due to their\nexceptional performance on a wide range of complex tasks. Therefore, refined\nmethods to evaluate the capabilities of LLMs are needed to determine the tasks\nand responsibility they should undertake. Our study mainly discussed how LLMs,\nas useful tools, should be effectively assessed. We proposed the two-stage\nframework: from ``core ability'' to ``agent'', clearly explaining how LLMs can\nbe applied based on their specific capabilities, along with the evaluation\nmethods in each stage. Core ability refers to the capabilities that LLMs need\nin order to generate high-quality natural language texts. After confirming LLMs\npossess core ability, they can solve real-world and complex tasks as agent. In\nthe \"core ability\" stage, we discussed the reasoning ability, societal impact,\nand domain knowledge of LLMs. In the ``agent'' stage, we demonstrated embodied\naction, planning, and tool learning of LLMs agent applications. Finally, we\nexamined the challenges currently confronting the evaluation methods for LLMs,\nas well as the directions for future development."}, "hash": "af1dfe7c1ab81e322ea94b2e55385aa8749d125236122d52ec7745830008a110", "class_name": "RelatedNodeInfo"}}, "text": "Mea-\nsuring and reducing llm hallucination without gold-\nstandard answers via expertise-weighting. arXiv\npreprint arXiv:2402.10412 .\nMartin Wessel, Tom\u00e1s Horych, Terry Ruas, Akiko\nAizawa, Bela Gipp, and Timo Spinde. 2023. Intro-\nducing mbib-the first media bias identification bench-\nmark task and dataset collection. In Proceedings of\nthe 46th International ACM SIGIR Conference on\nResearch and Development in Information Retrieval ,\npages 2765\u20132774.\nJason Weston, Antoine Bordes, Sumit Chopra, Alexan-\nder M Rush, Bart Van Merri\u00ebnboer, Armand Joulin,\nand Tomas Mikolov. 2015. Towards ai-complete\nquestion answering: A set of prerequisite toy tasks.\narXiv preprint arXiv:1502.05698 .\nMinghao Wu and Alham Fikri Aji. 2023. Style over\nsubstance: Evaluation biases for large language mod-\nels.\nShijie Wu, Ozan Irsoy, Steven Lu, Vadim Dabravolski,\nMark Dredze, Sebastian Gehrmann, Prabhanjan Kam-\nbadur, David Rosenberg, and Gideon Mann. 2023.\n23", "mimetype": "text/plain", "start_char_idx": 4211, "end_char_idx": 5154, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "78de34ad-7e41-467a-bc06-ee07fa064bdb": {"__data__": {"id_": "78de34ad-7e41-467a-bc06-ee07fa064bdb", "embedding": null, "metadata": {"page_label": "24", "file_name": "2406_00936v1.pdf", "Title of this paper": "A Survey of Useful LLM Evaluation", "Authors": "Ji-Lun Peng, Sijia Cheng, Egil Diau, Yung-Yu Shih, Po-Heng Chen, Yen-Ting Lin, Yun-Nung Chen", "Date published": "06/03/2024", "URL": "http://arxiv.org/abs/2406.00936v1", "summary": "LLMs have gotten attention across various research domains due to their\nexceptional performance on a wide range of complex tasks. Therefore, refined\nmethods to evaluate the capabilities of LLMs are needed to determine the tasks\nand responsibility they should undertake. Our study mainly discussed how LLMs,\nas useful tools, should be effectively assessed. We proposed the two-stage\nframework: from ``core ability'' to ``agent'', clearly explaining how LLMs can\nbe applied based on their specific capabilities, along with the evaluation\nmethods in each stage. Core ability refers to the capabilities that LLMs need\nin order to generate high-quality natural language texts. After confirming LLMs\npossess core ability, they can solve real-world and complex tasks as agent. In\nthe \"core ability\" stage, we discussed the reasoning ability, societal impact,\nand domain knowledge of LLMs. In the ``agent'' stage, we demonstrated embodied\naction, planning, and tool learning of LLMs agent applications. Finally, we\nexamined the challenges currently confronting the evaluation methods for LLMs,\nas well as the directions for future development."}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8307e32b-8dfa-4e7a-8281-17f204dd5dee", "node_type": "4", "metadata": {"page_label": "24", "file_name": "2406_00936v1.pdf", "Title of this paper": "A Survey of Useful LLM Evaluation", "Authors": "Ji-Lun Peng, Sijia Cheng, Egil Diau, Yung-Yu Shih, Po-Heng Chen, Yen-Ting Lin, Yun-Nung Chen", "Date published": "06/03/2024", "URL": "http://arxiv.org/abs/2406.00936v1", "summary": "LLMs have gotten attention across various research domains due to their\nexceptional performance on a wide range of complex tasks. Therefore, refined\nmethods to evaluate the capabilities of LLMs are needed to determine the tasks\nand responsibility they should undertake. Our study mainly discussed how LLMs,\nas useful tools, should be effectively assessed. We proposed the two-stage\nframework: from ``core ability'' to ``agent'', clearly explaining how LLMs can\nbe applied based on their specific capabilities, along with the evaluation\nmethods in each stage. Core ability refers to the capabilities that LLMs need\nin order to generate high-quality natural language texts. After confirming LLMs\npossess core ability, they can solve real-world and complex tasks as agent. In\nthe \"core ability\" stage, we discussed the reasoning ability, societal impact,\nand domain knowledge of LLMs. In the ``agent'' stage, we demonstrated embodied\naction, planning, and tool learning of LLMs agent applications. Finally, we\nexamined the challenges currently confronting the evaluation methods for LLMs,\nas well as the directions for future development."}, "hash": "44df4c57e041f2319a521dfd75f9e225233b5efe4034e21ddc241cfbaf46c8d4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9333929f-04f6-4109-a3fa-930626962947", "node_type": "1", "metadata": {}, "hash": "9f56334a0928e119f001b53263f091c5bd40ff223da957888af84b042704da73", "class_name": "RelatedNodeInfo"}}, "text": "Bloomberggpt: A large language model for finance.\narXiv preprint arXiv:2303.17564 .\nQianqian Xie, Weiguang Han, Xiao Zhang, Yanzhao\nLai, Min Peng, Alejandro Lopez-Lira, and Jimin\nHuang. 2023. Pixiu: A large language model, in-\nstruction data and evaluation benchmark for finance.\narXiv preprint arXiv:2306.05443 .\nFangzhi Xu, Qika Lin, Jiawei Han, Tianzhe Zhao, Jun\nLiu, and Erik Cambria. 2023a. Are large language\nmodels really good logical reasoners? a comprehen-\nsive evaluation and beyond.\nGuohai Xu, Jiayi Liu, Ming Yan, Haotian Xu, Jinghui\nSi, Zhuoran Zhou, Peng Yi, Xing Gao, Jitao Sang,\nRong Zhang, Ji Zhang, Chao Peng, Fei Huang, and\nJingren Zhou. 2023b. Cvalues: Measuring the val-\nues of chinese large language models from safety to\nresponsibility.\nQiantong Xu, Fenglu Hong, Bo Li, Changran Hu,\nZhengyu Chen, and Jian Zhang. 2023c. On the\ntool manipulation capability of open-source large\nlanguage models.\nBiwei Yan, Kun Li, Minghui Xu, Yueyan Dong, Yue\nZhang, Zhaochun Ren, and Xiuzhen Cheng. 2024a.\nOn protecting the data privacy of large language mod-\nels (llms): A survey.\nFanjia Yan, Huanzhi Mao, Charlie Cheng-Jie Ji, Tianjun\nZhang, Shishir G. Patil, Ion Stoica, and Joseph E.\nGonzalez. 2024b. Berkeley function calling leader-\nboard.\nWen-Jing Yan, Xiaobai Li, Su-Jing Wang, Guoying\nZhao, Yong-Jin Liu, Yu-Hsin Chen, and Xiaolan\nFu. 2014. Casme ii: An improved spontaneous\nmicro-expression database and the baseline evalu-\nation. PloS one , 9(1):e86041.\nShiping Yang, Renliang Sun, and Xiaojun Wan. 2023.\nA new benchmark and reverse validation method for\npassage-level hallucination detection.\nZhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Ben-\ngio, William W Cohen, Ruslan Salakhutdinov, and\nChristopher D Manning. 2018. Hotpotqa: A dataset\nfor diverse, explainable multi-hop question answer-\ning. arXiv preprint arXiv:1809.09600 .\nShunyu Yao, Howard Chen, John Yang, and Karthik\nNarasimhan. 2023a. Webshop: Towards scalable\nreal-world web interaction with grounded language\nagents.\nShunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak\nShafran, Karthik Narasimhan, and Yuan Cao. 2023b.\nReact: Synergizing reasoning and acting in language\nmodels.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2158, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9333929f-04f6-4109-a3fa-930626962947": {"__data__": {"id_": "9333929f-04f6-4109-a3fa-930626962947", "embedding": null, "metadata": {"page_label": "24", "file_name": "2406_00936v1.pdf", "Title of this paper": "A Survey of Useful LLM Evaluation", "Authors": "Ji-Lun Peng, Sijia Cheng, Egil Diau, Yung-Yu Shih, Po-Heng Chen, Yen-Ting Lin, Yun-Nung Chen", "Date published": "06/03/2024", "URL": "http://arxiv.org/abs/2406.00936v1", "summary": "LLMs have gotten attention across various research domains due to their\nexceptional performance on a wide range of complex tasks. Therefore, refined\nmethods to evaluate the capabilities of LLMs are needed to determine the tasks\nand responsibility they should undertake. Our study mainly discussed how LLMs,\nas useful tools, should be effectively assessed. We proposed the two-stage\nframework: from ``core ability'' to ``agent'', clearly explaining how LLMs can\nbe applied based on their specific capabilities, along with the evaluation\nmethods in each stage. Core ability refers to the capabilities that LLMs need\nin order to generate high-quality natural language texts. After confirming LLMs\npossess core ability, they can solve real-world and complex tasks as agent. In\nthe \"core ability\" stage, we discussed the reasoning ability, societal impact,\nand domain knowledge of LLMs. In the ``agent'' stage, we demonstrated embodied\naction, planning, and tool learning of LLMs agent applications. Finally, we\nexamined the challenges currently confronting the evaluation methods for LLMs,\nas well as the directions for future development."}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8307e32b-8dfa-4e7a-8281-17f204dd5dee", "node_type": "4", "metadata": {"page_label": "24", "file_name": "2406_00936v1.pdf", "Title of this paper": "A Survey of Useful LLM Evaluation", "Authors": "Ji-Lun Peng, Sijia Cheng, Egil Diau, Yung-Yu Shih, Po-Heng Chen, Yen-Ting Lin, Yun-Nung Chen", "Date published": "06/03/2024", "URL": "http://arxiv.org/abs/2406.00936v1", "summary": "LLMs have gotten attention across various research domains due to their\nexceptional performance on a wide range of complex tasks. Therefore, refined\nmethods to evaluate the capabilities of LLMs are needed to determine the tasks\nand responsibility they should undertake. Our study mainly discussed how LLMs,\nas useful tools, should be effectively assessed. We proposed the two-stage\nframework: from ``core ability'' to ``agent'', clearly explaining how LLMs can\nbe applied based on their specific capabilities, along with the evaluation\nmethods in each stage. Core ability refers to the capabilities that LLMs need\nin order to generate high-quality natural language texts. After confirming LLMs\npossess core ability, they can solve real-world and complex tasks as agent. In\nthe \"core ability\" stage, we discussed the reasoning ability, societal impact,\nand domain knowledge of LLMs. In the ``agent'' stage, we demonstrated embodied\naction, planning, and tool learning of LLMs agent applications. Finally, we\nexamined the challenges currently confronting the evaluation methods for LLMs,\nas well as the directions for future development."}, "hash": "44df4c57e041f2319a521dfd75f9e225233b5efe4034e21ddc241cfbaf46c8d4", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "78de34ad-7e41-467a-bc06-ee07fa064bdb", "node_type": "1", "metadata": {"page_label": "24", "file_name": "2406_00936v1.pdf", "Title of this paper": "A Survey of Useful LLM Evaluation", "Authors": "Ji-Lun Peng, Sijia Cheng, Egil Diau, Yung-Yu Shih, Po-Heng Chen, Yen-Ting Lin, Yun-Nung Chen", "Date published": "06/03/2024", "URL": "http://arxiv.org/abs/2406.00936v1", "summary": "LLMs have gotten attention across various research domains due to their\nexceptional performance on a wide range of complex tasks. Therefore, refined\nmethods to evaluate the capabilities of LLMs are needed to determine the tasks\nand responsibility they should undertake. Our study mainly discussed how LLMs,\nas useful tools, should be effectively assessed. We proposed the two-stage\nframework: from ``core ability'' to ``agent'', clearly explaining how LLMs can\nbe applied based on their specific capabilities, along with the evaluation\nmethods in each stage. Core ability refers to the capabilities that LLMs need\nin order to generate high-quality natural language texts. After confirming LLMs\npossess core ability, they can solve real-world and complex tasks as agent. In\nthe \"core ability\" stage, we discussed the reasoning ability, societal impact,\nand domain knowledge of LLMs. In the ``agent'' stage, we demonstrated embodied\naction, planning, and tool learning of LLMs agent applications. Finally, we\nexamined the challenges currently confronting the evaluation methods for LLMs,\nas well as the directions for future development."}, "hash": "9c64af6f37b24d324f762b267e2b9b088d3cbb6b3d552e1cb4b46008d18efa61", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2cbe94c4-539a-4157-8796-010450784b18", "node_type": "1", "metadata": {}, "hash": "961a9693f67e299839d955e255594df2c9061305b10ac3b7a4d2581f54512ae5", "class_name": "RelatedNodeInfo"}}, "text": "2023.\nA new benchmark and reverse validation method for\npassage-level hallucination detection.\nZhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Ben-\ngio, William W Cohen, Ruslan Salakhutdinov, and\nChristopher D Manning. 2018. Hotpotqa: A dataset\nfor diverse, explainable multi-hop question answer-\ning. arXiv preprint arXiv:1809.09600 .\nShunyu Yao, Howard Chen, John Yang, and Karthik\nNarasimhan. 2023a. Webshop: Towards scalable\nreal-world web interaction with grounded language\nagents.\nShunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak\nShafran, Karthik Narasimhan, and Yuan Cao. 2023b.\nReact: Synergizing reasoning and acting in language\nmodels.\nYifan Yao, Jinhao Duan, Kaidi Xu, Yuanfang Cai, Zhibo\nSun, and Yue Zhang. 2024. A survey on large lan-\nguage model (llm) security and privacy: The good,\nthe bad, and the ugly. High-Confidence Computing ,\npage 100211.Daniel Wankit Yip, Aysan Esmradi, and Chun Fai Chan.\n2024. A novel evaluation framework for assessing\nresilience against prompt injection attacks in large\nlanguage models.\nNathan Young, Qiming Bao, Joshua Bensemann, and\nMichael Witbrock. 2022. Abductionrules: Training\ntransformers to explain unexpected inputs. arXiv\npreprint arXiv:2203.12186 .\nWenhao Yu, Nimrod Gileadi, Chuyuan Fu, Sean Kir-\nmani, Kuang-Huei Lee, Montse Gonzalez Arenas,\nHao-Tien Lewis Chiang, Tom Erez, Leonard Hasen-\nclever, Jan Humplik, Brian Ichter, Ted Xiao, Peng Xu,\nAndy Zeng, Tingnan Zhang, Nicolas Heess, Dorsa\nSadigh, Jie Tan, Yuval Tassa, and Fei Xia. 2023. Lan-\nguage to rewards for robotic skill synthesis. Arxiv\npreprint arXiv:2306.08647 .\nZheng Yuan, Hongyi Yuan, Chuanqi Tan, Wei Wang,\nand Songfang Huang. 2023. How well do large lan-\nguage models perform in arithmetic tasks?\nZhuowen Yuan, Zidi Xiong, Yi Zeng, Ning Yu, Ruoxi\nJia, Dawn Song, and Bo Li. 2024. Rigorllm: Re-\nsilient guardrails for large language models against\nundesired content.\nQiusi Zhan, Zhixiang Liang, Zifan Ying, and Daniel\nKang. 2024. Injecagent: Benchmarking indirect\nprompt injections in tool-integrated large language\nmodel agents. arXiv preprint arXiv:2403.02691 .\nJizhi Zhang, Keqin Bao, Yang Zhang, Wenjie Wang,\nFuli Feng, and Xiangnan He. 2023.", "mimetype": "text/plain", "start_char_idx": 1516, "end_char_idx": 3686, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2cbe94c4-539a-4157-8796-010450784b18": {"__data__": {"id_": "2cbe94c4-539a-4157-8796-010450784b18", "embedding": null, "metadata": {"page_label": "24", "file_name": "2406_00936v1.pdf", "Title of this paper": "A Survey of Useful LLM Evaluation", "Authors": "Ji-Lun Peng, Sijia Cheng, Egil Diau, Yung-Yu Shih, Po-Heng Chen, Yen-Ting Lin, Yun-Nung Chen", "Date published": "06/03/2024", "URL": "http://arxiv.org/abs/2406.00936v1", "summary": "LLMs have gotten attention across various research domains due to their\nexceptional performance on a wide range of complex tasks. Therefore, refined\nmethods to evaluate the capabilities of LLMs are needed to determine the tasks\nand responsibility they should undertake. Our study mainly discussed how LLMs,\nas useful tools, should be effectively assessed. We proposed the two-stage\nframework: from ``core ability'' to ``agent'', clearly explaining how LLMs can\nbe applied based on their specific capabilities, along with the evaluation\nmethods in each stage. Core ability refers to the capabilities that LLMs need\nin order to generate high-quality natural language texts. After confirming LLMs\npossess core ability, they can solve real-world and complex tasks as agent. In\nthe \"core ability\" stage, we discussed the reasoning ability, societal impact,\nand domain knowledge of LLMs. In the ``agent'' stage, we demonstrated embodied\naction, planning, and tool learning of LLMs agent applications. Finally, we\nexamined the challenges currently confronting the evaluation methods for LLMs,\nas well as the directions for future development."}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8307e32b-8dfa-4e7a-8281-17f204dd5dee", "node_type": "4", "metadata": {"page_label": "24", "file_name": "2406_00936v1.pdf", "Title of this paper": "A Survey of Useful LLM Evaluation", "Authors": "Ji-Lun Peng, Sijia Cheng, Egil Diau, Yung-Yu Shih, Po-Heng Chen, Yen-Ting Lin, Yun-Nung Chen", "Date published": "06/03/2024", "URL": "http://arxiv.org/abs/2406.00936v1", "summary": "LLMs have gotten attention across various research domains due to their\nexceptional performance on a wide range of complex tasks. Therefore, refined\nmethods to evaluate the capabilities of LLMs are needed to determine the tasks\nand responsibility they should undertake. Our study mainly discussed how LLMs,\nas useful tools, should be effectively assessed. We proposed the two-stage\nframework: from ``core ability'' to ``agent'', clearly explaining how LLMs can\nbe applied based on their specific capabilities, along with the evaluation\nmethods in each stage. Core ability refers to the capabilities that LLMs need\nin order to generate high-quality natural language texts. After confirming LLMs\npossess core ability, they can solve real-world and complex tasks as agent. In\nthe \"core ability\" stage, we discussed the reasoning ability, societal impact,\nand domain knowledge of LLMs. In the ``agent'' stage, we demonstrated embodied\naction, planning, and tool learning of LLMs agent applications. Finally, we\nexamined the challenges currently confronting the evaluation methods for LLMs,\nas well as the directions for future development."}, "hash": "44df4c57e041f2319a521dfd75f9e225233b5efe4034e21ddc241cfbaf46c8d4", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9333929f-04f6-4109-a3fa-930626962947", "node_type": "1", "metadata": {"page_label": "24", "file_name": "2406_00936v1.pdf", "Title of this paper": "A Survey of Useful LLM Evaluation", "Authors": "Ji-Lun Peng, Sijia Cheng, Egil Diau, Yung-Yu Shih, Po-Heng Chen, Yen-Ting Lin, Yun-Nung Chen", "Date published": "06/03/2024", "URL": "http://arxiv.org/abs/2406.00936v1", "summary": "LLMs have gotten attention across various research domains due to their\nexceptional performance on a wide range of complex tasks. Therefore, refined\nmethods to evaluate the capabilities of LLMs are needed to determine the tasks\nand responsibility they should undertake. Our study mainly discussed how LLMs,\nas useful tools, should be effectively assessed. We proposed the two-stage\nframework: from ``core ability'' to ``agent'', clearly explaining how LLMs can\nbe applied based on their specific capabilities, along with the evaluation\nmethods in each stage. Core ability refers to the capabilities that LLMs need\nin order to generate high-quality natural language texts. After confirming LLMs\npossess core ability, they can solve real-world and complex tasks as agent. In\nthe \"core ability\" stage, we discussed the reasoning ability, societal impact,\nand domain knowledge of LLMs. In the ``agent'' stage, we demonstrated embodied\naction, planning, and tool learning of LLMs agent applications. Finally, we\nexamined the challenges currently confronting the evaluation methods for LLMs,\nas well as the directions for future development."}, "hash": "e591473ffd3dba8ae8283c8d99f71ac6b7803c999c10e5188fa44abf352ffeb4", "class_name": "RelatedNodeInfo"}}, "text": "Zheng Yuan, Hongyi Yuan, Chuanqi Tan, Wei Wang,\nand Songfang Huang. 2023. How well do large lan-\nguage models perform in arithmetic tasks?\nZhuowen Yuan, Zidi Xiong, Yi Zeng, Ning Yu, Ruoxi\nJia, Dawn Song, and Bo Li. 2024. Rigorllm: Re-\nsilient guardrails for large language models against\nundesired content.\nQiusi Zhan, Zhixiang Liang, Zifan Ying, and Daniel\nKang. 2024. Injecagent: Benchmarking indirect\nprompt injections in tool-integrated large language\nmodel agents. arXiv preprint arXiv:2403.02691 .\nJizhi Zhang, Keqin Bao, Yang Zhang, Wenjie Wang,\nFuli Feng, and Xiangnan He. 2023. Is chatgpt fair\nfor recommendation? evaluating fairness in large\nlanguage model recommendation. In Proceedings of\nthe 17th ACM Conference on Recommender Systems ,\nRecSys \u201923. ACM.\nKechi Zhang, Jia Li, Ge Li, Xianjie Shi, and Zhi Jin.\n2024a. Codeagent: Enhancing code generation with\ntool-integrated agent systems for real-world repo-\nlevel coding challenges.\nXiaoying Zhang, Baolin Peng, Ye Tian, Jingyan Zhou,\nLifeng Jin, Linfeng Song, Haitao Mi, and Helen\nMeng. 2024b. Self-alignment for factuality: Mitigat-\ning hallucinations in llms via self-evaluation. arXiv\npreprint arXiv:2402.09267 .\nXuanyu Zhang and Qing Yang. 2023. Xuanyuan 2.0:\nA large chinese financial chat model with hundreds\nof billions parameters. In Proceedings of the 32nd\nACM International Conference on Information and\nKnowledge Management , pages 4435\u20134439.\nYuyu Zhang, Hanjun Dai, Zornitsa Kozareva, Alexan-\nder J Smola, and Le Song. 2018. Variational reason-\ning for question answering with knowledge graph. In\nAAAI .\nGuoying Zhao, Xiaobai Li, Yante Li, and Matti Pietik\u00e4i-\nnen. 2023. Facial micro-expressions: an overview.\nProceedings of the IEEE .\n24", "mimetype": "text/plain", "start_char_idx": 3099, "end_char_idx": 4814, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f5610056-b377-4fd6-8398-016560fc786d": {"__data__": {"id_": "f5610056-b377-4fd6-8398-016560fc786d", "embedding": null, "metadata": {"page_label": "25", "file_name": "2406_00936v1.pdf", "Title of this paper": "A Survey of Useful LLM Evaluation", "Authors": "Ji-Lun Peng, Sijia Cheng, Egil Diau, Yung-Yu Shih, Po-Heng Chen, Yen-Ting Lin, Yun-Nung Chen", "Date published": "06/03/2024", "URL": "http://arxiv.org/abs/2406.00936v1", "summary": "LLMs have gotten attention across various research domains due to their\nexceptional performance on a wide range of complex tasks. Therefore, refined\nmethods to evaluate the capabilities of LLMs are needed to determine the tasks\nand responsibility they should undertake. Our study mainly discussed how LLMs,\nas useful tools, should be effectively assessed. We proposed the two-stage\nframework: from ``core ability'' to ``agent'', clearly explaining how LLMs can\nbe applied based on their specific capabilities, along with the evaluation\nmethods in each stage. Core ability refers to the capabilities that LLMs need\nin order to generate high-quality natural language texts. After confirming LLMs\npossess core ability, they can solve real-world and complex tasks as agent. In\nthe \"core ability\" stage, we discussed the reasoning ability, societal impact,\nand domain knowledge of LLMs. In the ``agent'' stage, we demonstrated embodied\naction, planning, and tool learning of LLMs agent applications. Finally, we\nexamined the challenges currently confronting the evaluation methods for LLMs,\nas well as the directions for future development."}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "536bcd2a-4e21-4f29-9367-0b9d91fc9705", "node_type": "4", "metadata": {"page_label": "25", "file_name": "2406_00936v1.pdf", "Title of this paper": "A Survey of Useful LLM Evaluation", "Authors": "Ji-Lun Peng, Sijia Cheng, Egil Diau, Yung-Yu Shih, Po-Heng Chen, Yen-Ting Lin, Yun-Nung Chen", "Date published": "06/03/2024", "URL": "http://arxiv.org/abs/2406.00936v1", "summary": "LLMs have gotten attention across various research domains due to their\nexceptional performance on a wide range of complex tasks. Therefore, refined\nmethods to evaluate the capabilities of LLMs are needed to determine the tasks\nand responsibility they should undertake. Our study mainly discussed how LLMs,\nas useful tools, should be effectively assessed. We proposed the two-stage\nframework: from ``core ability'' to ``agent'', clearly explaining how LLMs can\nbe applied based on their specific capabilities, along with the evaluation\nmethods in each stage. Core ability refers to the capabilities that LLMs need\nin order to generate high-quality natural language texts. After confirming LLMs\npossess core ability, they can solve real-world and complex tasks as agent. In\nthe \"core ability\" stage, we discussed the reasoning ability, societal impact,\nand domain knowledge of LLMs. In the ``agent'' stage, we demonstrated embodied\naction, planning, and tool learning of LLMs agent applications. Finally, we\nexamined the challenges currently confronting the evaluation methods for LLMs,\nas well as the directions for future development."}, "hash": "a8ab5d9476bf19f77e72d5879b41503515d43eaf5b75cf4cd403897f9765fe6d", "class_name": "RelatedNodeInfo"}}, "text": "Duo Zheng, Shijia Huang, Lin Zhao, Yiwu Zhong, and\nLiwei Wang. 2023a. Towards learning a generalist\nmodel for embodied navigation.\nLianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan\nZhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin,\nZhuohan Li, Dacheng Li, Eric Xing, et al. 2024.\nJudging llm-as-a-judge with mt-bench and chatbot\narena. Advances in Neural Information Processing\nSystems , 36.\nShen Zheng, Jie Huang, and Kevin Chen-Chuan Chang.\n2023b. Why does chatgpt fall short in providing\ntruthful answers?\nGengze Zhou, Yicong Hong, and Qi Wu. 2023a.\nNavgpt: Explicit reasoning in vision-and-language\nnavigation with large language models. arXiv\npreprint arXiv:2305.16986 .\nShuyan Zhou, Frank F. Xu, Hao Zhu, Xuhui Zhou,\nRobert Lo, Abishek Sridhar, Xianyi Cheng, Tianyue\nOu, Yonatan Bisk, Daniel Fried, Uri Alon, and Gra-\nham Neubig. 2023b. Webarena: A realistic web\nenvironment for building autonomous agents.\nChris Zielinski, Margaret Winker, Rakesh Aggarwal,\nLorraine Ferris, Markus Heinemann, Jose Florencio\nLape\u00f1a Jr, Sanjay Pai, Edsel Ing, Leslie Citrome,\net al. 2023. Wame recommendations on chatgpt and\nchatbots in relation to scholarly publications.\n25", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1157, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}}, "docstore/metadata": {"faf3f149-be4f-437d-98ea-1fde6a5aab5f": {"doc_hash": "e02840c5fd1a2b8a6512f41f6bee58d30093f8ddbee02e648be0b6c930654e86", "ref_doc_id": "dfbb3e44-557a-400b-8d6d-50e507564097"}, "3b447e69-caaf-412b-86bf-f9462ad40a34": {"doc_hash": "5ec336e663fcdc694ada225542d705b3d92a2e1efa4b2b0b0f199dab68fb43c3", "ref_doc_id": "dfbb3e44-557a-400b-8d6d-50e507564097"}, "bff77fd8-1eec-4b50-98d5-9b3cbeb18f27": {"doc_hash": "2f4b57056ab17d151cb18f3734f043af4a10eec7af1c7323cbf8a834c277e5f9", "ref_doc_id": "edd28199-ceb8-4311-af14-1989bf72ea1c"}, "5ede0fec-360e-41da-98aa-8d12086dd129": {"doc_hash": "23f443e33352ff7600f05c74fd58645dba72771d9bddf1912f7cb658504b9a99", "ref_doc_id": "edd28199-ceb8-4311-af14-1989bf72ea1c"}, "e77797fa-eb70-4b54-807d-db1fdec77128": {"doc_hash": "02f1274e47dbc9251bfd21c7a326aa3c0f00ac303cf9b8e4d599cd6804f00fd6", "ref_doc_id": "90bdc65e-3c51-4860-9904-ba44dedf5c3d"}, "a6aa3233-b842-4d11-a76c-f29c68cd8a77": {"doc_hash": "685d04063e54f95914fb6780a651224554ab518141288a8c4bb4af8650223bce", "ref_doc_id": "90bdc65e-3c51-4860-9904-ba44dedf5c3d"}, "c78be11e-f963-4dcd-b6e9-62453327ab2f": {"doc_hash": "67749fa6d9edaa0cfa231d9788a18e33deaaf4482f9e0f83bf00a2f7e50c9994", "ref_doc_id": "ef3b04be-211f-422b-8af0-ff52fe993fd8"}, "990643d2-e51f-4773-89d7-6460837d2efe": {"doc_hash": "430d966d8caae0a1b8099d082234faf3c8cb39f63d996bd27b4ed1eb8c6f3b9d", "ref_doc_id": "ef3b04be-211f-422b-8af0-ff52fe993fd8"}, "098bc478-c3ba-4f37-a3cb-41e532ba7bbc": {"doc_hash": "4ff3f3c9d1390c769fb97a73731063083029190527a4d1dc1380f616e300d168", "ref_doc_id": "ef3b04be-211f-422b-8af0-ff52fe993fd8"}, "388ad8ee-add3-43f4-b285-5761813e73e4": {"doc_hash": "fbe371ab6c2e9e016bf64edc6e3f6d9331708c2d6cc7f6322f625731ccf0ceea", "ref_doc_id": "020929b8-9749-415c-93be-f98fdd7e0106"}, "cbb04b97-c8c3-48c6-9a6b-d237d998829e": {"doc_hash": "26765bc50052c530445533ccfec09c9388c0b2439bb75e690ece1178d02aa514", "ref_doc_id": "020929b8-9749-415c-93be-f98fdd7e0106"}, "b0adc3f3-eb87-4348-9280-92d4b18c017c": {"doc_hash": "cc8900eed240f542ba3b567a749ecbe59990aea35fc5213611d743f0e2726bb6", "ref_doc_id": "140c55bd-2358-4977-8253-be2cd1bd7bd1"}, "f895595b-da20-4b6e-a13d-413989804509": {"doc_hash": "c22f06c9bbe55e3a66281e7fc3002ac8e3a24b4b33917648a5171d83f420c98c", "ref_doc_id": "140c55bd-2358-4977-8253-be2cd1bd7bd1"}, "27030cf0-94e5-46bb-8fb0-67a42eaf9b60": {"doc_hash": "eaec12bef04e10df13d8e76b2d5db484f746d69d1f0b50578d185266a798a7a0", "ref_doc_id": "ad91309c-b7e2-48ea-9862-10a6b3fecb85"}, "d3394261-fd6b-4ca7-958e-14236d6ee8ad": {"doc_hash": "616f69c6b527c46232aba9569a521c554a77a23bcf7da30b0a6248ec7e2c76bb", "ref_doc_id": "ad91309c-b7e2-48ea-9862-10a6b3fecb85"}, "fb4e3b69-9876-443a-8255-9f28f2fa024f": {"doc_hash": "47682bc185a0067fed369f358e6fd6e6e1f2c7257b573d570ad5213208503359", "ref_doc_id": "1c13d026-9938-4364-b3f4-868560c845c6"}, "8ebaf79f-f834-4843-91b1-ad1369f30306": {"doc_hash": "5e31fccb685f0c4c3f91db8618a57dfff8d4b0c3b5c31395b3c3d8b85f7e13a9", "ref_doc_id": "1c13d026-9938-4364-b3f4-868560c845c6"}, "619e0387-ae89-4e2d-abb3-50e88ff6fb14": {"doc_hash": "126cb97c1881ef4a2ce6e7d6007ed30b30df59c704c1fa98599ab37d2e4e0402", "ref_doc_id": "1c13d026-9938-4364-b3f4-868560c845c6"}, "7e559ea7-8b79-4422-b482-86e371b2fdd0": {"doc_hash": "0f0d8e4243a86a58e04d574d6c5d34b3bdadc361be12d1a6ecae8e5b0d8c51c5", "ref_doc_id": "8ac0feb1-b2b0-4c5b-85d7-09a71e899465"}, "5ec34214-7ce0-44b5-a1c7-f883d66e1d07": {"doc_hash": "8844063efbfd76e23c282b1cf4cb41f888cae2ada6f92f77a9797346f250b9ac", "ref_doc_id": "8ac0feb1-b2b0-4c5b-85d7-09a71e899465"}, "4f33cd9c-7fd1-47ce-ab0d-0abfef6515b4": {"doc_hash": "5fe3e7b9a964b71e52d51ffdb5bb225c614692ea67fd0ffd957c84d3029c0ad4", "ref_doc_id": "e86913ac-b0fb-4b1d-b560-ef83650f9e04"}, "e16c1863-2375-4d21-99ed-4f22adb70f92": {"doc_hash": "227b12f4f2d87d26c548ee33ac681f0a2629034e3986f4d1a10b3766e4bb4d2f", "ref_doc_id": "e86913ac-b0fb-4b1d-b560-ef83650f9e04"}, "005ab259-0cc2-4005-ba60-943d1c38eaab": {"doc_hash": "721952848493eb59aac8739a6b152975011ef85f64f410876e6e4da49dea77e0", "ref_doc_id": "c29d64c4-4e80-435d-b7cc-1f6f7a36fc05"}, "ef399f6f-3a54-4ebc-ae03-965ddedd7899": {"doc_hash": "e46caf89f6acb09db7c2a09835bee5ad3b608283a260f9e9717944a033813b85", "ref_doc_id": "c29d64c4-4e80-435d-b7cc-1f6f7a36fc05"}, "905987af-b042-4707-855a-576549b66d90": {"doc_hash": "5dd9de524878ab9510be4707360d326a1d0f6b67810711014834d7cf26c86dc8", "ref_doc_id": "5063287c-0de4-48b1-ac66-5b850b96c8f8"}, "9912b965-bee8-464d-980a-d05ddb223b69": {"doc_hash": "1b9e000b1c6c777d6d82058062ea3350a222c79937af5e3b61d67b0bd0e72e24", "ref_doc_id": "5063287c-0de4-48b1-ac66-5b850b96c8f8"}, "2eb7af28-b022-4292-8cd3-10c5e2cd4814": {"doc_hash": "ea0010b8932ba569ac1ee980c31a32c216e3a6068064a1808862d1dacc668786", "ref_doc_id": "54c4af3d-4982-41f9-9b7b-2decb7dfbd22"}, "b1610f7f-1c90-4b0b-8dbc-64bf701fb59f": {"doc_hash": "97ced6e2ee152c239dac74366cddbee23466cd0278fc4417d704efbbb4c423e2", "ref_doc_id": "54c4af3d-4982-41f9-9b7b-2decb7dfbd22"}, "a798d9b0-47ff-47fa-9d16-1798054f7200": {"doc_hash": "3f87208938f359408bf7d3b967c84b06c8dbba7623431142a898ee0b8fbc50fe", "ref_doc_id": "d4f8389b-10d6-41b7-b3f2-49feabf83e1e"}, "80addb88-835e-4094-8559-cdb01a07d4a0": {"doc_hash": "6adfe6e7e8fabfbad1de855cd4e7f79ba9579e96bea110d8d78de6a560e9391b", "ref_doc_id": "d4f8389b-10d6-41b7-b3f2-49feabf83e1e"}, "c21f2425-c959-4425-90ea-16215bacff6d": {"doc_hash": "3a55b6c66ff9a43e71d91c121d7172e9cf32e614d99c360436edc0232fa435a5", "ref_doc_id": "f26764b5-771e-4c62-b511-3f23004e1492"}, "928c472d-3d3a-43a9-bb78-4f0fb4c3b5fd": {"doc_hash": "1086b283f9318877c9e566c17aef08380c5a3e8e94f04530ab58657b05c480ef", "ref_doc_id": "f26764b5-771e-4c62-b511-3f23004e1492"}, "6d4d7748-be5a-489e-8c72-23834ca8d6c2": {"doc_hash": "81066ff49098d26f33ebff078577649d7e5b3c2c6d927be2fb3dd9f4405e3593", "ref_doc_id": "d00e5ad8-2386-47ee-8e95-46d94b626131"}, "e0783693-35c8-4c90-988c-008c25098f64": {"doc_hash": "8d83067c1363da6ef7173ffcdbd158c0ab7cf037668437ea6753fc06c6594d3b", "ref_doc_id": "d00e5ad8-2386-47ee-8e95-46d94b626131"}, "690095c6-2b60-48f1-92cf-c0e02266d19c": {"doc_hash": "de0c3b3c8273620208bd5240b1b00e82772395014118284861415bb02a4e4003", "ref_doc_id": "d00e5ad8-2386-47ee-8e95-46d94b626131"}, "f110dc68-9447-4dd5-8397-39dcd13ebba6": {"doc_hash": "fd9234258455e08fe1f8aeb0211749e4d6f9d196dce4b29ef9a7fd8ea3e10821", "ref_doc_id": "9f59d426-38d4-4aa4-8612-0cabbee7afea"}, "baeb0e7b-8822-4e4a-9e40-b7808694d9f4": {"doc_hash": "cf066dbbabd54d4201ac646f84539295e708a859fe8a9a3308c47c2dd6850077", "ref_doc_id": "9f59d426-38d4-4aa4-8612-0cabbee7afea"}, "d1da1abb-9400-4c8d-8e1f-972dead68c11": {"doc_hash": "0898acf14f494a41497c83e633872f9ac3271aac4106fa39bd0c78838ce5b688", "ref_doc_id": "9f59d426-38d4-4aa4-8612-0cabbee7afea"}, "88cd3980-d3c2-412a-8ee4-75e1b10d220c": {"doc_hash": "c45386f27985a2b46c7fd80e573ae9ccac394fcbbf1b4ed2071ab2a47805b112", "ref_doc_id": "dd73c8d9-67ac-4670-8a4f-8bc81168290b"}, "00f2422b-690e-4b36-bb74-3127acf98f92": {"doc_hash": "42f306f61300ba9133ef67c35b51ebf71e050d605932bf92e6eb7861c72fe3c1", "ref_doc_id": "dd73c8d9-67ac-4670-8a4f-8bc81168290b"}, "ebfc8357-35f4-4fc4-9b44-74eddd833a2b": {"doc_hash": "0459358994c8dcf3cb72d36d13b1dbe672175f8b0d6c6131183dcb974e7f89bd", "ref_doc_id": "dd73c8d9-67ac-4670-8a4f-8bc81168290b"}, "22021f9f-37fd-464c-a0b3-ff84eadd707f": {"doc_hash": "c99559c337f0e618b3c4f49e5804b8cfda9a252a846199409e060fdc32fe600b", "ref_doc_id": "80d2bc89-634a-4619-b582-8b2e5b6ddead"}, "3fdfbf4c-1f9b-410a-b4d4-9d0217f8da33": {"doc_hash": "9cd889643b73f4932232a1a540039b8577b045c3b6a5be7443998dee315fbcaf", "ref_doc_id": "80d2bc89-634a-4619-b582-8b2e5b6ddead"}, "27fe7941-c483-4565-bb65-5c9503b5c298": {"doc_hash": "49eae821f6577179115d5590ae51dcae3433463a997892bdc0dafbc90902057c", "ref_doc_id": "80d2bc89-634a-4619-b582-8b2e5b6ddead"}, "4cdb609f-31c8-42f4-8831-8388be220894": {"doc_hash": "a810e3d0ad7e07c6c536a506aff9b13993ccd4f91d75b6e12bb3587e3afc2727", "ref_doc_id": "e94bfb13-444c-4371-b46f-d7a757f7282b"}, "7086ca5f-f587-4170-abfd-3a0366d70f7d": {"doc_hash": "43a902542193bb5f9777269b778307aa7b820e225a519ab56f98f6dc6069e1a0", "ref_doc_id": "e94bfb13-444c-4371-b46f-d7a757f7282b"}, "4afef103-bb68-4f8a-b5eb-d2d8aafbbe07": {"doc_hash": "a55c1d4817fee9a1dde65f4b118ad53ddad443659d4b4ef47928d078ebdd3a20", "ref_doc_id": "e94bfb13-444c-4371-b46f-d7a757f7282b"}, "b4b4381d-228c-4a51-8d76-05f03f3ea84d": {"doc_hash": "9e88141477ad214eaf72bd2a5038877654df5023609e8a73c664333c279f069a", "ref_doc_id": "aa873ac2-8693-49b7-91c3-b69b4a1f04cb"}, "83af5fda-2789-4967-aaf1-467644174aab": {"doc_hash": "ea785979397dbadc5b2014dbda755c310c910df3700a5af183524ba7ca7d2c70", "ref_doc_id": "aa873ac2-8693-49b7-91c3-b69b4a1f04cb"}, "1f061f59-3379-4e37-be71-43dc40effbbe": {"doc_hash": "00adde96e6130b7e85c44e9b8ff1307b812b99f9bc3eadbb0913e3a5c6345bcb", "ref_doc_id": "aa873ac2-8693-49b7-91c3-b69b4a1f04cb"}, "978362a6-fad9-4c7f-81c7-a9f2f040cbc6": {"doc_hash": "07ba28aea409fcb269527834a003ba1ef19b359f4e79775a933b396c3555cd54", "ref_doc_id": "aa873ac2-8693-49b7-91c3-b69b4a1f04cb"}, "769544b4-94fa-4b9b-a7c3-cf05ba175036": {"doc_hash": "30288307dd05616c1ee523317ff8ec2c2d67ed979ae30a37569ec0ad1ff4f9b7", "ref_doc_id": "a1643132-b9a2-40b5-a12a-3db21b4804b3"}, "d574d45c-7f22-4e51-91bb-6ea1f7065184": {"doc_hash": "aa585d8a64929c7e69311956cdc8344fe8eb631ceea0ca03dc2aeeaa2c514682", "ref_doc_id": "a1643132-b9a2-40b5-a12a-3db21b4804b3"}, "f6eaa4a7-d73c-4ec0-8c30-48534ca645cf": {"doc_hash": "54e37e66eb5ab6cb88305493d7bc9b817b45e05fade8c9aae95e5f4c67b3f1b2", "ref_doc_id": "a1643132-b9a2-40b5-a12a-3db21b4804b3"}, "d5d82fcb-345c-4f93-b420-5e49ae10cc81": {"doc_hash": "9cfe37102d6812a6ab3033e1fe30199bb4733b21caabe9386116901a141e974e", "ref_doc_id": "a1643132-b9a2-40b5-a12a-3db21b4804b3"}, "47f7130d-2124-4425-907a-92d492aef41c": {"doc_hash": "a597ac50a8937d226002702aca457a3ae3a21ded6aea8d171b93241b9bb34bdd", "ref_doc_id": "b54ab185-43c9-49d9-9a3f-075efccae227"}, "da74fc8b-a3f2-429d-ad5d-6882036548a0": {"doc_hash": "05e3d5f7d5e331c88eb9cc592505dde31d56a8c84f7c6c3de0fbfefa0fe83fde", "ref_doc_id": "b54ab185-43c9-49d9-9a3f-075efccae227"}, "fa3166c8-9bfd-40e9-a2a4-3f73e7f1b635": {"doc_hash": "af1dfe7c1ab81e322ea94b2e55385aa8749d125236122d52ec7745830008a110", "ref_doc_id": "b54ab185-43c9-49d9-9a3f-075efccae227"}, "53292e7f-b531-4eb9-92eb-46ee6a25be54": {"doc_hash": "9b09e6fc2d3484edf37166ccc41edc52d3a1e04d318cfc6af5294982f3cacb8e", "ref_doc_id": "b54ab185-43c9-49d9-9a3f-075efccae227"}, "78de34ad-7e41-467a-bc06-ee07fa064bdb": {"doc_hash": "9c64af6f37b24d324f762b267e2b9b088d3cbb6b3d552e1cb4b46008d18efa61", "ref_doc_id": "8307e32b-8dfa-4e7a-8281-17f204dd5dee"}, "9333929f-04f6-4109-a3fa-930626962947": {"doc_hash": "e591473ffd3dba8ae8283c8d99f71ac6b7803c999c10e5188fa44abf352ffeb4", "ref_doc_id": "8307e32b-8dfa-4e7a-8281-17f204dd5dee"}, "2cbe94c4-539a-4157-8796-010450784b18": {"doc_hash": "7c4783acb0cd9c1e6c7c919017d3940d2e31ae331db82ad82ac39a5fd5e1eabd", "ref_doc_id": "8307e32b-8dfa-4e7a-8281-17f204dd5dee"}, "f5610056-b377-4fd6-8398-016560fc786d": {"doc_hash": "a8ab5d9476bf19f77e72d5879b41503515d43eaf5b75cf4cd403897f9765fe6d", "ref_doc_id": "536bcd2a-4e21-4f29-9367-0b9d91fc9705"}}, "docstore/ref_doc_info": {"dfbb3e44-557a-400b-8d6d-50e507564097": {"node_ids": ["faf3f149-be4f-437d-98ea-1fde6a5aab5f", "3b447e69-caaf-412b-86bf-f9462ad40a34"], "metadata": {"page_label": "1", "file_name": "2406_00936v1.pdf", "Title of this paper": "A Survey of Useful LLM Evaluation", "Authors": "Ji-Lun Peng, Sijia Cheng, Egil Diau, Yung-Yu Shih, Po-Heng Chen, Yen-Ting Lin, Yun-Nung Chen", "Date published": "06/03/2024", "URL": "http://arxiv.org/abs/2406.00936v1", "summary": "LLMs have gotten attention across various research domains due to their\nexceptional performance on a wide range of complex tasks. Therefore, refined\nmethods to evaluate the capabilities of LLMs are needed to determine the tasks\nand responsibility they should undertake. Our study mainly discussed how LLMs,\nas useful tools, should be effectively assessed. We proposed the two-stage\nframework: from ``core ability'' to ``agent'', clearly explaining how LLMs can\nbe applied based on their specific capabilities, along with the evaluation\nmethods in each stage. Core ability refers to the capabilities that LLMs need\nin order to generate high-quality natural language texts. After confirming LLMs\npossess core ability, they can solve real-world and complex tasks as agent. In\nthe \"core ability\" stage, we discussed the reasoning ability, societal impact,\nand domain knowledge of LLMs. In the ``agent'' stage, we demonstrated embodied\naction, planning, and tool learning of LLMs agent applications. Finally, we\nexamined the challenges currently confronting the evaluation methods for LLMs,\nas well as the directions for future development."}}, "edd28199-ceb8-4311-af14-1989bf72ea1c": {"node_ids": ["bff77fd8-1eec-4b50-98d5-9b3cbeb18f27", "5ede0fec-360e-41da-98aa-8d12086dd129"], "metadata": {"page_label": "2", "file_name": "2406_00936v1.pdf", "Title of this paper": "A Survey of Useful LLM Evaluation", "Authors": "Ji-Lun Peng, Sijia Cheng, Egil Diau, Yung-Yu Shih, Po-Heng Chen, Yen-Ting Lin, Yun-Nung Chen", "Date published": "06/03/2024", "URL": "http://arxiv.org/abs/2406.00936v1", "summary": "LLMs have gotten attention across various research domains due to their\nexceptional performance on a wide range of complex tasks. Therefore, refined\nmethods to evaluate the capabilities of LLMs are needed to determine the tasks\nand responsibility they should undertake. Our study mainly discussed how LLMs,\nas useful tools, should be effectively assessed. We proposed the two-stage\nframework: from ``core ability'' to ``agent'', clearly explaining how LLMs can\nbe applied based on their specific capabilities, along with the evaluation\nmethods in each stage. Core ability refers to the capabilities that LLMs need\nin order to generate high-quality natural language texts. After confirming LLMs\npossess core ability, they can solve real-world and complex tasks as agent. In\nthe \"core ability\" stage, we discussed the reasoning ability, societal impact,\nand domain knowledge of LLMs. In the ``agent'' stage, we demonstrated embodied\naction, planning, and tool learning of LLMs agent applications. Finally, we\nexamined the challenges currently confronting the evaluation methods for LLMs,\nas well as the directions for future development."}}, "90bdc65e-3c51-4860-9904-ba44dedf5c3d": {"node_ids": ["e77797fa-eb70-4b54-807d-db1fdec77128", "a6aa3233-b842-4d11-a76c-f29c68cd8a77"], "metadata": {"page_label": "3", "file_name": "2406_00936v1.pdf", "Title of this paper": "A Survey of Useful LLM Evaluation", "Authors": "Ji-Lun Peng, Sijia Cheng, Egil Diau, Yung-Yu Shih, Po-Heng Chen, Yen-Ting Lin, Yun-Nung Chen", "Date published": "06/03/2024", "URL": "http://arxiv.org/abs/2406.00936v1", "summary": "LLMs have gotten attention across various research domains due to their\nexceptional performance on a wide range of complex tasks. Therefore, refined\nmethods to evaluate the capabilities of LLMs are needed to determine the tasks\nand responsibility they should undertake. Our study mainly discussed how LLMs,\nas useful tools, should be effectively assessed. We proposed the two-stage\nframework: from ``core ability'' to ``agent'', clearly explaining how LLMs can\nbe applied based on their specific capabilities, along with the evaluation\nmethods in each stage. Core ability refers to the capabilities that LLMs need\nin order to generate high-quality natural language texts. After confirming LLMs\npossess core ability, they can solve real-world and complex tasks as agent. In\nthe \"core ability\" stage, we discussed the reasoning ability, societal impact,\nand domain knowledge of LLMs. In the ``agent'' stage, we demonstrated embodied\naction, planning, and tool learning of LLMs agent applications. Finally, we\nexamined the challenges currently confronting the evaluation methods for LLMs,\nas well as the directions for future development."}}, "ef3b04be-211f-422b-8af0-ff52fe993fd8": {"node_ids": ["c78be11e-f963-4dcd-b6e9-62453327ab2f", "990643d2-e51f-4773-89d7-6460837d2efe", "098bc478-c3ba-4f37-a3cb-41e532ba7bbc"], "metadata": {"page_label": "4", "file_name": "2406_00936v1.pdf", "Title of this paper": "A Survey of Useful LLM Evaluation", "Authors": "Ji-Lun Peng, Sijia Cheng, Egil Diau, Yung-Yu Shih, Po-Heng Chen, Yen-Ting Lin, Yun-Nung Chen", "Date published": "06/03/2024", "URL": "http://arxiv.org/abs/2406.00936v1", "summary": "LLMs have gotten attention across various research domains due to their\nexceptional performance on a wide range of complex tasks. Therefore, refined\nmethods to evaluate the capabilities of LLMs are needed to determine the tasks\nand responsibility they should undertake. Our study mainly discussed how LLMs,\nas useful tools, should be effectively assessed. We proposed the two-stage\nframework: from ``core ability'' to ``agent'', clearly explaining how LLMs can\nbe applied based on their specific capabilities, along with the evaluation\nmethods in each stage. Core ability refers to the capabilities that LLMs need\nin order to generate high-quality natural language texts. After confirming LLMs\npossess core ability, they can solve real-world and complex tasks as agent. In\nthe \"core ability\" stage, we discussed the reasoning ability, societal impact,\nand domain knowledge of LLMs. In the ``agent'' stage, we demonstrated embodied\naction, planning, and tool learning of LLMs agent applications. Finally, we\nexamined the challenges currently confronting the evaluation methods for LLMs,\nas well as the directions for future development."}}, "020929b8-9749-415c-93be-f98fdd7e0106": {"node_ids": ["388ad8ee-add3-43f4-b285-5761813e73e4", "cbb04b97-c8c3-48c6-9a6b-d237d998829e"], "metadata": {"page_label": "5", "file_name": "2406_00936v1.pdf", "Title of this paper": "A Survey of Useful LLM Evaluation", "Authors": "Ji-Lun Peng, Sijia Cheng, Egil Diau, Yung-Yu Shih, Po-Heng Chen, Yen-Ting Lin, Yun-Nung Chen", "Date published": "06/03/2024", "URL": "http://arxiv.org/abs/2406.00936v1", "summary": "LLMs have gotten attention across various research domains due to their\nexceptional performance on a wide range of complex tasks. Therefore, refined\nmethods to evaluate the capabilities of LLMs are needed to determine the tasks\nand responsibility they should undertake. Our study mainly discussed how LLMs,\nas useful tools, should be effectively assessed. We proposed the two-stage\nframework: from ``core ability'' to ``agent'', clearly explaining how LLMs can\nbe applied based on their specific capabilities, along with the evaluation\nmethods in each stage. Core ability refers to the capabilities that LLMs need\nin order to generate high-quality natural language texts. After confirming LLMs\npossess core ability, they can solve real-world and complex tasks as agent. In\nthe \"core ability\" stage, we discussed the reasoning ability, societal impact,\nand domain knowledge of LLMs. In the ``agent'' stage, we demonstrated embodied\naction, planning, and tool learning of LLMs agent applications. Finally, we\nexamined the challenges currently confronting the evaluation methods for LLMs,\nas well as the directions for future development."}}, "140c55bd-2358-4977-8253-be2cd1bd7bd1": {"node_ids": ["b0adc3f3-eb87-4348-9280-92d4b18c017c", "f895595b-da20-4b6e-a13d-413989804509"], "metadata": {"page_label": "6", "file_name": "2406_00936v1.pdf", "Title of this paper": "A Survey of Useful LLM Evaluation", "Authors": "Ji-Lun Peng, Sijia Cheng, Egil Diau, Yung-Yu Shih, Po-Heng Chen, Yen-Ting Lin, Yun-Nung Chen", "Date published": "06/03/2024", "URL": "http://arxiv.org/abs/2406.00936v1", "summary": "LLMs have gotten attention across various research domains due to their\nexceptional performance on a wide range of complex tasks. Therefore, refined\nmethods to evaluate the capabilities of LLMs are needed to determine the tasks\nand responsibility they should undertake. Our study mainly discussed how LLMs,\nas useful tools, should be effectively assessed. We proposed the two-stage\nframework: from ``core ability'' to ``agent'', clearly explaining how LLMs can\nbe applied based on their specific capabilities, along with the evaluation\nmethods in each stage. Core ability refers to the capabilities that LLMs need\nin order to generate high-quality natural language texts. After confirming LLMs\npossess core ability, they can solve real-world and complex tasks as agent. In\nthe \"core ability\" stage, we discussed the reasoning ability, societal impact,\nand domain knowledge of LLMs. In the ``agent'' stage, we demonstrated embodied\naction, planning, and tool learning of LLMs agent applications. Finally, we\nexamined the challenges currently confronting the evaluation methods for LLMs,\nas well as the directions for future development."}}, "ad91309c-b7e2-48ea-9862-10a6b3fecb85": {"node_ids": ["27030cf0-94e5-46bb-8fb0-67a42eaf9b60", "d3394261-fd6b-4ca7-958e-14236d6ee8ad"], "metadata": {"page_label": "7", "file_name": "2406_00936v1.pdf", "Title of this paper": "A Survey of Useful LLM Evaluation", "Authors": "Ji-Lun Peng, Sijia Cheng, Egil Diau, Yung-Yu Shih, Po-Heng Chen, Yen-Ting Lin, Yun-Nung Chen", "Date published": "06/03/2024", "URL": "http://arxiv.org/abs/2406.00936v1", "summary": "LLMs have gotten attention across various research domains due to their\nexceptional performance on a wide range of complex tasks. Therefore, refined\nmethods to evaluate the capabilities of LLMs are needed to determine the tasks\nand responsibility they should undertake. Our study mainly discussed how LLMs,\nas useful tools, should be effectively assessed. We proposed the two-stage\nframework: from ``core ability'' to ``agent'', clearly explaining how LLMs can\nbe applied based on their specific capabilities, along with the evaluation\nmethods in each stage. Core ability refers to the capabilities that LLMs need\nin order to generate high-quality natural language texts. After confirming LLMs\npossess core ability, they can solve real-world and complex tasks as agent. In\nthe \"core ability\" stage, we discussed the reasoning ability, societal impact,\nand domain knowledge of LLMs. In the ``agent'' stage, we demonstrated embodied\naction, planning, and tool learning of LLMs agent applications. Finally, we\nexamined the challenges currently confronting the evaluation methods for LLMs,\nas well as the directions for future development."}}, "1c13d026-9938-4364-b3f4-868560c845c6": {"node_ids": ["fb4e3b69-9876-443a-8255-9f28f2fa024f", "8ebaf79f-f834-4843-91b1-ad1369f30306", "619e0387-ae89-4e2d-abb3-50e88ff6fb14"], "metadata": {"page_label": "8", "file_name": "2406_00936v1.pdf", "Title of this paper": "A Survey of Useful LLM Evaluation", "Authors": "Ji-Lun Peng, Sijia Cheng, Egil Diau, Yung-Yu Shih, Po-Heng Chen, Yen-Ting Lin, Yun-Nung Chen", "Date published": "06/03/2024", "URL": "http://arxiv.org/abs/2406.00936v1", "summary": "LLMs have gotten attention across various research domains due to their\nexceptional performance on a wide range of complex tasks. Therefore, refined\nmethods to evaluate the capabilities of LLMs are needed to determine the tasks\nand responsibility they should undertake. Our study mainly discussed how LLMs,\nas useful tools, should be effectively assessed. We proposed the two-stage\nframework: from ``core ability'' to ``agent'', clearly explaining how LLMs can\nbe applied based on their specific capabilities, along with the evaluation\nmethods in each stage. Core ability refers to the capabilities that LLMs need\nin order to generate high-quality natural language texts. After confirming LLMs\npossess core ability, they can solve real-world and complex tasks as agent. In\nthe \"core ability\" stage, we discussed the reasoning ability, societal impact,\nand domain knowledge of LLMs. In the ``agent'' stage, we demonstrated embodied\naction, planning, and tool learning of LLMs agent applications. Finally, we\nexamined the challenges currently confronting the evaluation methods for LLMs,\nas well as the directions for future development."}}, "8ac0feb1-b2b0-4c5b-85d7-09a71e899465": {"node_ids": ["7e559ea7-8b79-4422-b482-86e371b2fdd0", "5ec34214-7ce0-44b5-a1c7-f883d66e1d07"], "metadata": {"page_label": "9", "file_name": "2406_00936v1.pdf", "Title of this paper": "A Survey of Useful LLM Evaluation", "Authors": "Ji-Lun Peng, Sijia Cheng, Egil Diau, Yung-Yu Shih, Po-Heng Chen, Yen-Ting Lin, Yun-Nung Chen", "Date published": "06/03/2024", "URL": "http://arxiv.org/abs/2406.00936v1", "summary": "LLMs have gotten attention across various research domains due to their\nexceptional performance on a wide range of complex tasks. Therefore, refined\nmethods to evaluate the capabilities of LLMs are needed to determine the tasks\nand responsibility they should undertake. Our study mainly discussed how LLMs,\nas useful tools, should be effectively assessed. We proposed the two-stage\nframework: from ``core ability'' to ``agent'', clearly explaining how LLMs can\nbe applied based on their specific capabilities, along with the evaluation\nmethods in each stage. Core ability refers to the capabilities that LLMs need\nin order to generate high-quality natural language texts. After confirming LLMs\npossess core ability, they can solve real-world and complex tasks as agent. In\nthe \"core ability\" stage, we discussed the reasoning ability, societal impact,\nand domain knowledge of LLMs. In the ``agent'' stage, we demonstrated embodied\naction, planning, and tool learning of LLMs agent applications. Finally, we\nexamined the challenges currently confronting the evaluation methods for LLMs,\nas well as the directions for future development."}}, "e86913ac-b0fb-4b1d-b560-ef83650f9e04": {"node_ids": ["4f33cd9c-7fd1-47ce-ab0d-0abfef6515b4", "e16c1863-2375-4d21-99ed-4f22adb70f92"], "metadata": {"page_label": "10", "file_name": "2406_00936v1.pdf", "Title of this paper": "A Survey of Useful LLM Evaluation", "Authors": "Ji-Lun Peng, Sijia Cheng, Egil Diau, Yung-Yu Shih, Po-Heng Chen, Yen-Ting Lin, Yun-Nung Chen", "Date published": "06/03/2024", "URL": "http://arxiv.org/abs/2406.00936v1", "summary": "LLMs have gotten attention across various research domains due to their\nexceptional performance on a wide range of complex tasks. Therefore, refined\nmethods to evaluate the capabilities of LLMs are needed to determine the tasks\nand responsibility they should undertake. Our study mainly discussed how LLMs,\nas useful tools, should be effectively assessed. We proposed the two-stage\nframework: from ``core ability'' to ``agent'', clearly explaining how LLMs can\nbe applied based on their specific capabilities, along with the evaluation\nmethods in each stage. Core ability refers to the capabilities that LLMs need\nin order to generate high-quality natural language texts. After confirming LLMs\npossess core ability, they can solve real-world and complex tasks as agent. In\nthe \"core ability\" stage, we discussed the reasoning ability, societal impact,\nand domain knowledge of LLMs. In the ``agent'' stage, we demonstrated embodied\naction, planning, and tool learning of LLMs agent applications. Finally, we\nexamined the challenges currently confronting the evaluation methods for LLMs,\nas well as the directions for future development."}}, "c29d64c4-4e80-435d-b7cc-1f6f7a36fc05": {"node_ids": ["005ab259-0cc2-4005-ba60-943d1c38eaab", "ef399f6f-3a54-4ebc-ae03-965ddedd7899"], "metadata": {"page_label": "11", "file_name": "2406_00936v1.pdf", "Title of this paper": "A Survey of Useful LLM Evaluation", "Authors": "Ji-Lun Peng, Sijia Cheng, Egil Diau, Yung-Yu Shih, Po-Heng Chen, Yen-Ting Lin, Yun-Nung Chen", "Date published": "06/03/2024", "URL": "http://arxiv.org/abs/2406.00936v1", "summary": "LLMs have gotten attention across various research domains due to their\nexceptional performance on a wide range of complex tasks. Therefore, refined\nmethods to evaluate the capabilities of LLMs are needed to determine the tasks\nand responsibility they should undertake. Our study mainly discussed how LLMs,\nas useful tools, should be effectively assessed. We proposed the two-stage\nframework: from ``core ability'' to ``agent'', clearly explaining how LLMs can\nbe applied based on their specific capabilities, along with the evaluation\nmethods in each stage. Core ability refers to the capabilities that LLMs need\nin order to generate high-quality natural language texts. After confirming LLMs\npossess core ability, they can solve real-world and complex tasks as agent. In\nthe \"core ability\" stage, we discussed the reasoning ability, societal impact,\nand domain knowledge of LLMs. In the ``agent'' stage, we demonstrated embodied\naction, planning, and tool learning of LLMs agent applications. Finally, we\nexamined the challenges currently confronting the evaluation methods for LLMs,\nas well as the directions for future development."}}, "5063287c-0de4-48b1-ac66-5b850b96c8f8": {"node_ids": ["905987af-b042-4707-855a-576549b66d90", "9912b965-bee8-464d-980a-d05ddb223b69"], "metadata": {"page_label": "12", "file_name": "2406_00936v1.pdf", "Title of this paper": "A Survey of Useful LLM Evaluation", "Authors": "Ji-Lun Peng, Sijia Cheng, Egil Diau, Yung-Yu Shih, Po-Heng Chen, Yen-Ting Lin, Yun-Nung Chen", "Date published": "06/03/2024", "URL": "http://arxiv.org/abs/2406.00936v1", "summary": "LLMs have gotten attention across various research domains due to their\nexceptional performance on a wide range of complex tasks. Therefore, refined\nmethods to evaluate the capabilities of LLMs are needed to determine the tasks\nand responsibility they should undertake. Our study mainly discussed how LLMs,\nas useful tools, should be effectively assessed. We proposed the two-stage\nframework: from ``core ability'' to ``agent'', clearly explaining how LLMs can\nbe applied based on their specific capabilities, along with the evaluation\nmethods in each stage. Core ability refers to the capabilities that LLMs need\nin order to generate high-quality natural language texts. After confirming LLMs\npossess core ability, they can solve real-world and complex tasks as agent. In\nthe \"core ability\" stage, we discussed the reasoning ability, societal impact,\nand domain knowledge of LLMs. In the ``agent'' stage, we demonstrated embodied\naction, planning, and tool learning of LLMs agent applications. Finally, we\nexamined the challenges currently confronting the evaluation methods for LLMs,\nas well as the directions for future development."}}, "54c4af3d-4982-41f9-9b7b-2decb7dfbd22": {"node_ids": ["2eb7af28-b022-4292-8cd3-10c5e2cd4814", "b1610f7f-1c90-4b0b-8dbc-64bf701fb59f"], "metadata": {"page_label": "13", "file_name": "2406_00936v1.pdf", "Title of this paper": "A Survey of Useful LLM Evaluation", "Authors": "Ji-Lun Peng, Sijia Cheng, Egil Diau, Yung-Yu Shih, Po-Heng Chen, Yen-Ting Lin, Yun-Nung Chen", "Date published": "06/03/2024", "URL": "http://arxiv.org/abs/2406.00936v1", "summary": "LLMs have gotten attention across various research domains due to their\nexceptional performance on a wide range of complex tasks. Therefore, refined\nmethods to evaluate the capabilities of LLMs are needed to determine the tasks\nand responsibility they should undertake. Our study mainly discussed how LLMs,\nas useful tools, should be effectively assessed. We proposed the two-stage\nframework: from ``core ability'' to ``agent'', clearly explaining how LLMs can\nbe applied based on their specific capabilities, along with the evaluation\nmethods in each stage. Core ability refers to the capabilities that LLMs need\nin order to generate high-quality natural language texts. After confirming LLMs\npossess core ability, they can solve real-world and complex tasks as agent. In\nthe \"core ability\" stage, we discussed the reasoning ability, societal impact,\nand domain knowledge of LLMs. In the ``agent'' stage, we demonstrated embodied\naction, planning, and tool learning of LLMs agent applications. Finally, we\nexamined the challenges currently confronting the evaluation methods for LLMs,\nas well as the directions for future development."}}, "d4f8389b-10d6-41b7-b3f2-49feabf83e1e": {"node_ids": ["a798d9b0-47ff-47fa-9d16-1798054f7200", "80addb88-835e-4094-8559-cdb01a07d4a0"], "metadata": {"page_label": "14", "file_name": "2406_00936v1.pdf", "Title of this paper": "A Survey of Useful LLM Evaluation", "Authors": "Ji-Lun Peng, Sijia Cheng, Egil Diau, Yung-Yu Shih, Po-Heng Chen, Yen-Ting Lin, Yun-Nung Chen", "Date published": "06/03/2024", "URL": "http://arxiv.org/abs/2406.00936v1", "summary": "LLMs have gotten attention across various research domains due to their\nexceptional performance on a wide range of complex tasks. Therefore, refined\nmethods to evaluate the capabilities of LLMs are needed to determine the tasks\nand responsibility they should undertake. Our study mainly discussed how LLMs,\nas useful tools, should be effectively assessed. We proposed the two-stage\nframework: from ``core ability'' to ``agent'', clearly explaining how LLMs can\nbe applied based on their specific capabilities, along with the evaluation\nmethods in each stage. Core ability refers to the capabilities that LLMs need\nin order to generate high-quality natural language texts. After confirming LLMs\npossess core ability, they can solve real-world and complex tasks as agent. In\nthe \"core ability\" stage, we discussed the reasoning ability, societal impact,\nand domain knowledge of LLMs. In the ``agent'' stage, we demonstrated embodied\naction, planning, and tool learning of LLMs agent applications. Finally, we\nexamined the challenges currently confronting the evaluation methods for LLMs,\nas well as the directions for future development."}}, "f26764b5-771e-4c62-b511-3f23004e1492": {"node_ids": ["c21f2425-c959-4425-90ea-16215bacff6d", "928c472d-3d3a-43a9-bb78-4f0fb4c3b5fd"], "metadata": {"page_label": "15", "file_name": "2406_00936v1.pdf", "Title of this paper": "A Survey of Useful LLM Evaluation", "Authors": "Ji-Lun Peng, Sijia Cheng, Egil Diau, Yung-Yu Shih, Po-Heng Chen, Yen-Ting Lin, Yun-Nung Chen", "Date published": "06/03/2024", "URL": "http://arxiv.org/abs/2406.00936v1", "summary": "LLMs have gotten attention across various research domains due to their\nexceptional performance on a wide range of complex tasks. Therefore, refined\nmethods to evaluate the capabilities of LLMs are needed to determine the tasks\nand responsibility they should undertake. Our study mainly discussed how LLMs,\nas useful tools, should be effectively assessed. We proposed the two-stage\nframework: from ``core ability'' to ``agent'', clearly explaining how LLMs can\nbe applied based on their specific capabilities, along with the evaluation\nmethods in each stage. Core ability refers to the capabilities that LLMs need\nin order to generate high-quality natural language texts. After confirming LLMs\npossess core ability, they can solve real-world and complex tasks as agent. In\nthe \"core ability\" stage, we discussed the reasoning ability, societal impact,\nand domain knowledge of LLMs. In the ``agent'' stage, we demonstrated embodied\naction, planning, and tool learning of LLMs agent applications. Finally, we\nexamined the challenges currently confronting the evaluation methods for LLMs,\nas well as the directions for future development."}}, "d00e5ad8-2386-47ee-8e95-46d94b626131": {"node_ids": ["6d4d7748-be5a-489e-8c72-23834ca8d6c2", "e0783693-35c8-4c90-988c-008c25098f64", "690095c6-2b60-48f1-92cf-c0e02266d19c"], "metadata": {"page_label": "16", "file_name": "2406_00936v1.pdf", "Title of this paper": "A Survey of Useful LLM Evaluation", "Authors": "Ji-Lun Peng, Sijia Cheng, Egil Diau, Yung-Yu Shih, Po-Heng Chen, Yen-Ting Lin, Yun-Nung Chen", "Date published": "06/03/2024", "URL": "http://arxiv.org/abs/2406.00936v1", "summary": "LLMs have gotten attention across various research domains due to their\nexceptional performance on a wide range of complex tasks. Therefore, refined\nmethods to evaluate the capabilities of LLMs are needed to determine the tasks\nand responsibility they should undertake. Our study mainly discussed how LLMs,\nas useful tools, should be effectively assessed. We proposed the two-stage\nframework: from ``core ability'' to ``agent'', clearly explaining how LLMs can\nbe applied based on their specific capabilities, along with the evaluation\nmethods in each stage. Core ability refers to the capabilities that LLMs need\nin order to generate high-quality natural language texts. After confirming LLMs\npossess core ability, they can solve real-world and complex tasks as agent. In\nthe \"core ability\" stage, we discussed the reasoning ability, societal impact,\nand domain knowledge of LLMs. In the ``agent'' stage, we demonstrated embodied\naction, planning, and tool learning of LLMs agent applications. Finally, we\nexamined the challenges currently confronting the evaluation methods for LLMs,\nas well as the directions for future development."}}, "9f59d426-38d4-4aa4-8612-0cabbee7afea": {"node_ids": ["f110dc68-9447-4dd5-8397-39dcd13ebba6", "baeb0e7b-8822-4e4a-9e40-b7808694d9f4", "d1da1abb-9400-4c8d-8e1f-972dead68c11"], "metadata": {"page_label": "17", "file_name": "2406_00936v1.pdf", "Title of this paper": "A Survey of Useful LLM Evaluation", "Authors": "Ji-Lun Peng, Sijia Cheng, Egil Diau, Yung-Yu Shih, Po-Heng Chen, Yen-Ting Lin, Yun-Nung Chen", "Date published": "06/03/2024", "URL": "http://arxiv.org/abs/2406.00936v1", "summary": "LLMs have gotten attention across various research domains due to their\nexceptional performance on a wide range of complex tasks. Therefore, refined\nmethods to evaluate the capabilities of LLMs are needed to determine the tasks\nand responsibility they should undertake. Our study mainly discussed how LLMs,\nas useful tools, should be effectively assessed. We proposed the two-stage\nframework: from ``core ability'' to ``agent'', clearly explaining how LLMs can\nbe applied based on their specific capabilities, along with the evaluation\nmethods in each stage. Core ability refers to the capabilities that LLMs need\nin order to generate high-quality natural language texts. After confirming LLMs\npossess core ability, they can solve real-world and complex tasks as agent. In\nthe \"core ability\" stage, we discussed the reasoning ability, societal impact,\nand domain knowledge of LLMs. In the ``agent'' stage, we demonstrated embodied\naction, planning, and tool learning of LLMs agent applications. Finally, we\nexamined the challenges currently confronting the evaluation methods for LLMs,\nas well as the directions for future development."}}, "dd73c8d9-67ac-4670-8a4f-8bc81168290b": {"node_ids": ["88cd3980-d3c2-412a-8ee4-75e1b10d220c", "00f2422b-690e-4b36-bb74-3127acf98f92", "ebfc8357-35f4-4fc4-9b44-74eddd833a2b"], "metadata": {"page_label": "18", "file_name": "2406_00936v1.pdf", "Title of this paper": "A Survey of Useful LLM Evaluation", "Authors": "Ji-Lun Peng, Sijia Cheng, Egil Diau, Yung-Yu Shih, Po-Heng Chen, Yen-Ting Lin, Yun-Nung Chen", "Date published": "06/03/2024", "URL": "http://arxiv.org/abs/2406.00936v1", "summary": "LLMs have gotten attention across various research domains due to their\nexceptional performance on a wide range of complex tasks. Therefore, refined\nmethods to evaluate the capabilities of LLMs are needed to determine the tasks\nand responsibility they should undertake. Our study mainly discussed how LLMs,\nas useful tools, should be effectively assessed. We proposed the two-stage\nframework: from ``core ability'' to ``agent'', clearly explaining how LLMs can\nbe applied based on their specific capabilities, along with the evaluation\nmethods in each stage. Core ability refers to the capabilities that LLMs need\nin order to generate high-quality natural language texts. After confirming LLMs\npossess core ability, they can solve real-world and complex tasks as agent. In\nthe \"core ability\" stage, we discussed the reasoning ability, societal impact,\nand domain knowledge of LLMs. In the ``agent'' stage, we demonstrated embodied\naction, planning, and tool learning of LLMs agent applications. Finally, we\nexamined the challenges currently confronting the evaluation methods for LLMs,\nas well as the directions for future development."}}, "80d2bc89-634a-4619-b582-8b2e5b6ddead": {"node_ids": ["22021f9f-37fd-464c-a0b3-ff84eadd707f", "3fdfbf4c-1f9b-410a-b4d4-9d0217f8da33", "27fe7941-c483-4565-bb65-5c9503b5c298"], "metadata": {"page_label": "19", "file_name": "2406_00936v1.pdf", "Title of this paper": "A Survey of Useful LLM Evaluation", "Authors": "Ji-Lun Peng, Sijia Cheng, Egil Diau, Yung-Yu Shih, Po-Heng Chen, Yen-Ting Lin, Yun-Nung Chen", "Date published": "06/03/2024", "URL": "http://arxiv.org/abs/2406.00936v1", "summary": "LLMs have gotten attention across various research domains due to their\nexceptional performance on a wide range of complex tasks. Therefore, refined\nmethods to evaluate the capabilities of LLMs are needed to determine the tasks\nand responsibility they should undertake. Our study mainly discussed how LLMs,\nas useful tools, should be effectively assessed. We proposed the two-stage\nframework: from ``core ability'' to ``agent'', clearly explaining how LLMs can\nbe applied based on their specific capabilities, along with the evaluation\nmethods in each stage. Core ability refers to the capabilities that LLMs need\nin order to generate high-quality natural language texts. After confirming LLMs\npossess core ability, they can solve real-world and complex tasks as agent. In\nthe \"core ability\" stage, we discussed the reasoning ability, societal impact,\nand domain knowledge of LLMs. In the ``agent'' stage, we demonstrated embodied\naction, planning, and tool learning of LLMs agent applications. Finally, we\nexamined the challenges currently confronting the evaluation methods for LLMs,\nas well as the directions for future development."}}, "e94bfb13-444c-4371-b46f-d7a757f7282b": {"node_ids": ["4cdb609f-31c8-42f4-8831-8388be220894", "7086ca5f-f587-4170-abfd-3a0366d70f7d", "4afef103-bb68-4f8a-b5eb-d2d8aafbbe07"], "metadata": {"page_label": "20", "file_name": "2406_00936v1.pdf", "Title of this paper": "A Survey of Useful LLM Evaluation", "Authors": "Ji-Lun Peng, Sijia Cheng, Egil Diau, Yung-Yu Shih, Po-Heng Chen, Yen-Ting Lin, Yun-Nung Chen", "Date published": "06/03/2024", "URL": "http://arxiv.org/abs/2406.00936v1", "summary": "LLMs have gotten attention across various research domains due to their\nexceptional performance on a wide range of complex tasks. Therefore, refined\nmethods to evaluate the capabilities of LLMs are needed to determine the tasks\nand responsibility they should undertake. Our study mainly discussed how LLMs,\nas useful tools, should be effectively assessed. We proposed the two-stage\nframework: from ``core ability'' to ``agent'', clearly explaining how LLMs can\nbe applied based on their specific capabilities, along with the evaluation\nmethods in each stage. Core ability refers to the capabilities that LLMs need\nin order to generate high-quality natural language texts. After confirming LLMs\npossess core ability, they can solve real-world and complex tasks as agent. In\nthe \"core ability\" stage, we discussed the reasoning ability, societal impact,\nand domain knowledge of LLMs. In the ``agent'' stage, we demonstrated embodied\naction, planning, and tool learning of LLMs agent applications. Finally, we\nexamined the challenges currently confronting the evaluation methods for LLMs,\nas well as the directions for future development."}}, "aa873ac2-8693-49b7-91c3-b69b4a1f04cb": {"node_ids": ["b4b4381d-228c-4a51-8d76-05f03f3ea84d", "83af5fda-2789-4967-aaf1-467644174aab", "1f061f59-3379-4e37-be71-43dc40effbbe", "978362a6-fad9-4c7f-81c7-a9f2f040cbc6"], "metadata": {"page_label": "21", "file_name": "2406_00936v1.pdf", "Title of this paper": "A Survey of Useful LLM Evaluation", "Authors": "Ji-Lun Peng, Sijia Cheng, Egil Diau, Yung-Yu Shih, Po-Heng Chen, Yen-Ting Lin, Yun-Nung Chen", "Date published": "06/03/2024", "URL": "http://arxiv.org/abs/2406.00936v1", "summary": "LLMs have gotten attention across various research domains due to their\nexceptional performance on a wide range of complex tasks. Therefore, refined\nmethods to evaluate the capabilities of LLMs are needed to determine the tasks\nand responsibility they should undertake. Our study mainly discussed how LLMs,\nas useful tools, should be effectively assessed. We proposed the two-stage\nframework: from ``core ability'' to ``agent'', clearly explaining how LLMs can\nbe applied based on their specific capabilities, along with the evaluation\nmethods in each stage. Core ability refers to the capabilities that LLMs need\nin order to generate high-quality natural language texts. After confirming LLMs\npossess core ability, they can solve real-world and complex tasks as agent. In\nthe \"core ability\" stage, we discussed the reasoning ability, societal impact,\nand domain knowledge of LLMs. In the ``agent'' stage, we demonstrated embodied\naction, planning, and tool learning of LLMs agent applications. Finally, we\nexamined the challenges currently confronting the evaluation methods for LLMs,\nas well as the directions for future development."}}, "a1643132-b9a2-40b5-a12a-3db21b4804b3": {"node_ids": ["769544b4-94fa-4b9b-a7c3-cf05ba175036", "d574d45c-7f22-4e51-91bb-6ea1f7065184", "f6eaa4a7-d73c-4ec0-8c30-48534ca645cf", "d5d82fcb-345c-4f93-b420-5e49ae10cc81"], "metadata": {"page_label": "22", "file_name": "2406_00936v1.pdf", "Title of this paper": "A Survey of Useful LLM Evaluation", "Authors": "Ji-Lun Peng, Sijia Cheng, Egil Diau, Yung-Yu Shih, Po-Heng Chen, Yen-Ting Lin, Yun-Nung Chen", "Date published": "06/03/2024", "URL": "http://arxiv.org/abs/2406.00936v1", "summary": "LLMs have gotten attention across various research domains due to their\nexceptional performance on a wide range of complex tasks. Therefore, refined\nmethods to evaluate the capabilities of LLMs are needed to determine the tasks\nand responsibility they should undertake. Our study mainly discussed how LLMs,\nas useful tools, should be effectively assessed. We proposed the two-stage\nframework: from ``core ability'' to ``agent'', clearly explaining how LLMs can\nbe applied based on their specific capabilities, along with the evaluation\nmethods in each stage. Core ability refers to the capabilities that LLMs need\nin order to generate high-quality natural language texts. After confirming LLMs\npossess core ability, they can solve real-world and complex tasks as agent. In\nthe \"core ability\" stage, we discussed the reasoning ability, societal impact,\nand domain knowledge of LLMs. In the ``agent'' stage, we demonstrated embodied\naction, planning, and tool learning of LLMs agent applications. Finally, we\nexamined the challenges currently confronting the evaluation methods for LLMs,\nas well as the directions for future development."}}, "b54ab185-43c9-49d9-9a3f-075efccae227": {"node_ids": ["47f7130d-2124-4425-907a-92d492aef41c", "da74fc8b-a3f2-429d-ad5d-6882036548a0", "fa3166c8-9bfd-40e9-a2a4-3f73e7f1b635", "53292e7f-b531-4eb9-92eb-46ee6a25be54"], "metadata": {"page_label": "23", "file_name": "2406_00936v1.pdf", "Title of this paper": "A Survey of Useful LLM Evaluation", "Authors": "Ji-Lun Peng, Sijia Cheng, Egil Diau, Yung-Yu Shih, Po-Heng Chen, Yen-Ting Lin, Yun-Nung Chen", "Date published": "06/03/2024", "URL": "http://arxiv.org/abs/2406.00936v1", "summary": "LLMs have gotten attention across various research domains due to their\nexceptional performance on a wide range of complex tasks. Therefore, refined\nmethods to evaluate the capabilities of LLMs are needed to determine the tasks\nand responsibility they should undertake. Our study mainly discussed how LLMs,\nas useful tools, should be effectively assessed. We proposed the two-stage\nframework: from ``core ability'' to ``agent'', clearly explaining how LLMs can\nbe applied based on their specific capabilities, along with the evaluation\nmethods in each stage. Core ability refers to the capabilities that LLMs need\nin order to generate high-quality natural language texts. After confirming LLMs\npossess core ability, they can solve real-world and complex tasks as agent. In\nthe \"core ability\" stage, we discussed the reasoning ability, societal impact,\nand domain knowledge of LLMs. In the ``agent'' stage, we demonstrated embodied\naction, planning, and tool learning of LLMs agent applications. Finally, we\nexamined the challenges currently confronting the evaluation methods for LLMs,\nas well as the directions for future development."}}, "8307e32b-8dfa-4e7a-8281-17f204dd5dee": {"node_ids": ["78de34ad-7e41-467a-bc06-ee07fa064bdb", "9333929f-04f6-4109-a3fa-930626962947", "2cbe94c4-539a-4157-8796-010450784b18"], "metadata": {"page_label": "24", "file_name": "2406_00936v1.pdf", "Title of this paper": "A Survey of Useful LLM Evaluation", "Authors": "Ji-Lun Peng, Sijia Cheng, Egil Diau, Yung-Yu Shih, Po-Heng Chen, Yen-Ting Lin, Yun-Nung Chen", "Date published": "06/03/2024", "URL": "http://arxiv.org/abs/2406.00936v1", "summary": "LLMs have gotten attention across various research domains due to their\nexceptional performance on a wide range of complex tasks. Therefore, refined\nmethods to evaluate the capabilities of LLMs are needed to determine the tasks\nand responsibility they should undertake. Our study mainly discussed how LLMs,\nas useful tools, should be effectively assessed. We proposed the two-stage\nframework: from ``core ability'' to ``agent'', clearly explaining how LLMs can\nbe applied based on their specific capabilities, along with the evaluation\nmethods in each stage. Core ability refers to the capabilities that LLMs need\nin order to generate high-quality natural language texts. After confirming LLMs\npossess core ability, they can solve real-world and complex tasks as agent. In\nthe \"core ability\" stage, we discussed the reasoning ability, societal impact,\nand domain knowledge of LLMs. In the ``agent'' stage, we demonstrated embodied\naction, planning, and tool learning of LLMs agent applications. Finally, we\nexamined the challenges currently confronting the evaluation methods for LLMs,\nas well as the directions for future development."}}, "536bcd2a-4e21-4f29-9367-0b9d91fc9705": {"node_ids": ["f5610056-b377-4fd6-8398-016560fc786d"], "metadata": {"page_label": "25", "file_name": "2406_00936v1.pdf", "Title of this paper": "A Survey of Useful LLM Evaluation", "Authors": "Ji-Lun Peng, Sijia Cheng, Egil Diau, Yung-Yu Shih, Po-Heng Chen, Yen-Ting Lin, Yun-Nung Chen", "Date published": "06/03/2024", "URL": "http://arxiv.org/abs/2406.00936v1", "summary": "LLMs have gotten attention across various research domains due to their\nexceptional performance on a wide range of complex tasks. Therefore, refined\nmethods to evaluate the capabilities of LLMs are needed to determine the tasks\nand responsibility they should undertake. Our study mainly discussed how LLMs,\nas useful tools, should be effectively assessed. We proposed the two-stage\nframework: from ``core ability'' to ``agent'', clearly explaining how LLMs can\nbe applied based on their specific capabilities, along with the evaluation\nmethods in each stage. Core ability refers to the capabilities that LLMs need\nin order to generate high-quality natural language texts. After confirming LLMs\npossess core ability, they can solve real-world and complex tasks as agent. In\nthe \"core ability\" stage, we discussed the reasoning ability, societal impact,\nand domain knowledge of LLMs. In the ``agent'' stage, we demonstrated embodied\naction, planning, and tool learning of LLMs agent applications. Finally, we\nexamined the challenges currently confronting the evaluation methods for LLMs,\nas well as the directions for future development."}}}}