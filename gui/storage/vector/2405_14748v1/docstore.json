{"docstore/data": {"c1428036-c081-4445-b8c8-7afd1632f30e": {"__data__": {"id_": "c1428036-c081-4445-b8c8-7afd1632f30e", "embedding": null, "metadata": {"page_label": "1", "file_name": "2405_14748v1.pdf", "Title of this paper": "MultiCast: Zero-Shot Multivariate Time Series Forecasting Using LLMs", "Authors": "Georgios Chatzigeorgakidis, Konstantinos Lentzos, Dimitrios Skoutas", "Date published": "05/23/2024", "URL": "http://arxiv.org/abs/2405.14748v1", "summary": "Predicting future values in multivariate time series is vital across various\ndomains. This work explores the use of large language models (LLMs) for this\ntask. However, LLMs typically handle one-dimensional data. We introduce\nMultiCast, a zero-shot LLM-based approach for multivariate time series\nforecasting. It allows LLMs to receive multivariate time series as input,\nthrough three novel token multiplexing solutions that effectively reduce\ndimensionality while preserving key repetitive patterns. Additionally, a\nquantization scheme helps LLMs to better learn these patterns, while\nsignificantly reducing token use for practical applications. We showcase the\nperformance of our approach in terms of RMSE and execution time against\nstate-of-the-art approaches on three real-world datasets."}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d48e1395-7828-4f3b-8f36-b268756432e7", "node_type": "4", "metadata": {"page_label": "1", "file_name": "2405_14748v1.pdf", "Title of this paper": "MultiCast: Zero-Shot Multivariate Time Series Forecasting Using LLMs", "Authors": "Georgios Chatzigeorgakidis, Konstantinos Lentzos, Dimitrios Skoutas", "Date published": "05/23/2024", "URL": "http://arxiv.org/abs/2405.14748v1", "summary": "Predicting future values in multivariate time series is vital across various\ndomains. This work explores the use of large language models (LLMs) for this\ntask. However, LLMs typically handle one-dimensional data. We introduce\nMultiCast, a zero-shot LLM-based approach for multivariate time series\nforecasting. It allows LLMs to receive multivariate time series as input,\nthrough three novel token multiplexing solutions that effectively reduce\ndimensionality while preserving key repetitive patterns. Additionally, a\nquantization scheme helps LLMs to better learn these patterns, while\nsignificantly reducing token use for practical applications. We showcase the\nperformance of our approach in terms of RMSE and execution time against\nstate-of-the-art approaches on three real-world datasets."}, "hash": "119740df623c41c7163df529c57abc59d5cf2d44a96941a5c1817e363b190e62", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5235a6f0-41d6-43ab-a2f1-091a334bf8ac", "node_type": "1", "metadata": {}, "hash": "490315b37f6b5ef3bb5b04a2f0768a3258a2152b4225c386002aaa8504e886a3", "class_name": "RelatedNodeInfo"}}, "text": "MultiCast: Zero-Shot Multivariate Time Series\nForecasting Using LLMs\nGeorgios Chatzigeorgakidis\n\u201cAthena\u201d Research Center\nGreece\ngchatzi@athenarc.grKonstantinos Lentzos\n\u201cAthena\u201d Research Center\nGreece\nklentzos@athenarc.grDimitrios Skoutas\n\u201cAthena\u201d Research Center\nGreece\ndskoutas@athenarc.gr\nAbstract \u2014Predicting future values in multivariate time series\nis vital across various domains. This work explores the use of\nlarge language models (LLMs) for this task. However, LLMs\ntypically handle one-dimensional data. We introduce MultiCast,\na zero-shot LLM-based approach for multivariate time series\nforecasting. It allows LLMs to receive multivariate time series\nas input, through three novel token multiplexing solutions that\neffectively reduce dimensionality while preserving key repetitive\npatterns. Additionally, a quantization scheme helps LLMs to\nbetter learn these patterns, while significantly reducing token\nuse for practical applications. We showcase the performance of\nour approach in terms of RMSE and execution time against\nstate-of-the-art approaches on three real-world datasets.\nIndex Terms \u2014large language models, multivariate time series,\nforecasting\nI. I NTRODUCTION\nA time series is a sequence of data points, typically recorded\nat successive equally spaced intervals of time. These data\npoints can represent various measurements, observations, or\nreadings taken over time, such as temperature readings, stock\nprices, sales figures, or sensor readings. Time series analysis\ninvolves studying the patterns, trends, and relationships present\nin the data to understand its behavior over time [1]. Time series\nforecasting predicts future values of a time series based on its\npast observations.\nTraditional time series forecasting methods have demon-\nstrated considerable efficacy over the years and continue to\nmaintain relevance and widespread adoption in contemporary\npractice [2]. In general, these methods can be categorized into\nlinear [3], [4] and non-linear models [5], [6].\nArguably, the most popular traditional time series method\nis AutoRegressive Integrated Moving Average (ARIMA) [7].\nARIMA consists of three independent components; (i) the\nAutoRegressive (AR) component assumes that the current\nvalue of a time series is a linear combination of its past\nvalues, with the addition of a white-noise term; (ii) the Moving\nAverage (MA) component assumes that the current value of a\ntime series variable is a linear combination of past white-noise\nterms, with no dependence on past values of the variable itself;\n(iii) the integrated (I) component incorporates differencing to\nmake the time series stationary, allowing for the modeling of\nnonstationary time series data.\nMachine learning and, in particular, deep learning has\nemerged as a transformative approach in the field of timeseries forecasting, offering new advances [8]\u2013[11]. Moreover,\npre-training has been used in deep learning, to significantly\naccelerate the training process and increase performance [12].\nIn domains such as computer vision and Natural Language\nProcessing (NLP), pre-training facilitates scaling of perfor-\nmance with the availability of data. However, in the context\nof time series modeling, access to sizable pretraining datasets\nis often limited.\nLarge Language Models (LLMs) have emerged as a popular\ntool for Natural Language Processing (NLP) tasks, and have\nreceived considerable attention in recent years. LLMs are\npretrained models, trained on vast amounts of text data.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3482, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5235a6f0-41d6-43ab-a2f1-091a334bf8ac": {"__data__": {"id_": "5235a6f0-41d6-43ab-a2f1-091a334bf8ac", "embedding": null, "metadata": {"page_label": "1", "file_name": "2405_14748v1.pdf", "Title of this paper": "MultiCast: Zero-Shot Multivariate Time Series Forecasting Using LLMs", "Authors": "Georgios Chatzigeorgakidis, Konstantinos Lentzos, Dimitrios Skoutas", "Date published": "05/23/2024", "URL": "http://arxiv.org/abs/2405.14748v1", "summary": "Predicting future values in multivariate time series is vital across various\ndomains. This work explores the use of large language models (LLMs) for this\ntask. However, LLMs typically handle one-dimensional data. We introduce\nMultiCast, a zero-shot LLM-based approach for multivariate time series\nforecasting. It allows LLMs to receive multivariate time series as input,\nthrough three novel token multiplexing solutions that effectively reduce\ndimensionality while preserving key repetitive patterns. Additionally, a\nquantization scheme helps LLMs to better learn these patterns, while\nsignificantly reducing token use for practical applications. We showcase the\nperformance of our approach in terms of RMSE and execution time against\nstate-of-the-art approaches on three real-world datasets."}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d48e1395-7828-4f3b-8f36-b268756432e7", "node_type": "4", "metadata": {"page_label": "1", "file_name": "2405_14748v1.pdf", "Title of this paper": "MultiCast: Zero-Shot Multivariate Time Series Forecasting Using LLMs", "Authors": "Georgios Chatzigeorgakidis, Konstantinos Lentzos, Dimitrios Skoutas", "Date published": "05/23/2024", "URL": "http://arxiv.org/abs/2405.14748v1", "summary": "Predicting future values in multivariate time series is vital across various\ndomains. This work explores the use of large language models (LLMs) for this\ntask. However, LLMs typically handle one-dimensional data. We introduce\nMultiCast, a zero-shot LLM-based approach for multivariate time series\nforecasting. It allows LLMs to receive multivariate time series as input,\nthrough three novel token multiplexing solutions that effectively reduce\ndimensionality while preserving key repetitive patterns. Additionally, a\nquantization scheme helps LLMs to better learn these patterns, while\nsignificantly reducing token use for practical applications. We showcase the\nperformance of our approach in terms of RMSE and execution time against\nstate-of-the-art approaches on three real-world datasets."}, "hash": "119740df623c41c7163df529c57abc59d5cf2d44a96941a5c1817e363b190e62", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c1428036-c081-4445-b8c8-7afd1632f30e", "node_type": "1", "metadata": {"page_label": "1", "file_name": "2405_14748v1.pdf", "Title of this paper": "MultiCast: Zero-Shot Multivariate Time Series Forecasting Using LLMs", "Authors": "Georgios Chatzigeorgakidis, Konstantinos Lentzos, Dimitrios Skoutas", "Date published": "05/23/2024", "URL": "http://arxiv.org/abs/2405.14748v1", "summary": "Predicting future values in multivariate time series is vital across various\ndomains. This work explores the use of large language models (LLMs) for this\ntask. However, LLMs typically handle one-dimensional data. We introduce\nMultiCast, a zero-shot LLM-based approach for multivariate time series\nforecasting. It allows LLMs to receive multivariate time series as input,\nthrough three novel token multiplexing solutions that effectively reduce\ndimensionality while preserving key repetitive patterns. Additionally, a\nquantization scheme helps LLMs to better learn these patterns, while\nsignificantly reducing token use for practical applications. We showcase the\nperformance of our approach in terms of RMSE and execution time against\nstate-of-the-art approaches on three real-world datasets."}, "hash": "639a88323ff16b5a53d7bfe898d9c9aa20db79feeb0dd898550a82d61a3b74d6", "class_name": "RelatedNodeInfo"}}, "text": "Machine learning and, in particular, deep learning has\nemerged as a transformative approach in the field of timeseries forecasting, offering new advances [8]\u2013[11]. Moreover,\npre-training has been used in deep learning, to significantly\naccelerate the training process and increase performance [12].\nIn domains such as computer vision and Natural Language\nProcessing (NLP), pre-training facilitates scaling of perfor-\nmance with the availability of data. However, in the context\nof time series modeling, access to sizable pretraining datasets\nis often limited.\nLarge Language Models (LLMs) have emerged as a popular\ntool for Natural Language Processing (NLP) tasks, and have\nreceived considerable attention in recent years. LLMs are\npretrained models, trained on vast amounts of text data. Their\nability to learn rich representations of language has drawn the\nattention of the scientific community over the past few years.\nSpecifically, LLMs are quite capable of capturing syntactic,\nsemantic, and contextual information [13]. Another interesting\naspect of LLMs are emergent abilities [14], which are capabil-\nities that are not explicitly programmed or designed, but rather\nspontaneously emerge from the complex internal processes of\nthe models. In the past few years, scientists have focused on\nleveraging the LLMs\u2019 potential to solve problems from other\nfields than NLP. In particular, in time series forecasting, by\ntaking advantage of pre-learned representations of language,\nLLMs can potentially capture temporal relationships and time\nseries dynamics [15]. However, most works have focused\non univariate time series forecasting, requiring either fine-\ntuning [16], or a few-shot prompting approach [17] (i.e.,\nproviding a few examples via prompting to guide the model\u2019s\nbehavior for a specific task).\nIn this work, we examine the utility of LLMs for multi-\nvariate time series forecasting via zero-shot prompting (i.e.,\nno additional examples are provided). To the best of our\nknowledge, ours is the first work that addresses this problem.\nOur contributions are summarized as follows:\n\u2022We introduce three dimensional multiplexing techniques\nto combine all dimensions into a single string, passed to\nan LLM as input.\n\u2022We employ SAX quantization on the time series to\nfacilitate inference by the model and to significantly\nreduce the computational cost and token usage.\n\u2022We present an experimental evaluation against existing\ntraditional, machine learning, and LLM-based methods\nfor time series forecasting.arXiv:2405.14748v1  [cs.LG]  23 May 2024", "mimetype": "text/plain", "start_char_idx": 2694, "end_char_idx": 5245, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1930c7ba-fcd1-4f0d-8ff3-5cadb253b660": {"__data__": {"id_": "1930c7ba-fcd1-4f0d-8ff3-5cadb253b660", "embedding": null, "metadata": {"page_label": "2", "file_name": "2405_14748v1.pdf", "Title of this paper": "MultiCast: Zero-Shot Multivariate Time Series Forecasting Using LLMs", "Authors": "Georgios Chatzigeorgakidis, Konstantinos Lentzos, Dimitrios Skoutas", "Date published": "05/23/2024", "URL": "http://arxiv.org/abs/2405.14748v1", "summary": "Predicting future values in multivariate time series is vital across various\ndomains. This work explores the use of large language models (LLMs) for this\ntask. However, LLMs typically handle one-dimensional data. We introduce\nMultiCast, a zero-shot LLM-based approach for multivariate time series\nforecasting. It allows LLMs to receive multivariate time series as input,\nthrough three novel token multiplexing solutions that effectively reduce\ndimensionality while preserving key repetitive patterns. Additionally, a\nquantization scheme helps LLMs to better learn these patterns, while\nsignificantly reducing token use for practical applications. We showcase the\nperformance of our approach in terms of RMSE and execution time against\nstate-of-the-art approaches on three real-world datasets."}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "19a31078-f89d-49dc-adb7-173adbb44952", "node_type": "4", "metadata": {"page_label": "2", "file_name": "2405_14748v1.pdf", "Title of this paper": "MultiCast: Zero-Shot Multivariate Time Series Forecasting Using LLMs", "Authors": "Georgios Chatzigeorgakidis, Konstantinos Lentzos, Dimitrios Skoutas", "Date published": "05/23/2024", "URL": "http://arxiv.org/abs/2405.14748v1", "summary": "Predicting future values in multivariate time series is vital across various\ndomains. This work explores the use of large language models (LLMs) for this\ntask. However, LLMs typically handle one-dimensional data. We introduce\nMultiCast, a zero-shot LLM-based approach for multivariate time series\nforecasting. It allows LLMs to receive multivariate time series as input,\nthrough three novel token multiplexing solutions that effectively reduce\ndimensionality while preserving key repetitive patterns. Additionally, a\nquantization scheme helps LLMs to better learn these patterns, while\nsignificantly reducing token use for practical applications. We showcase the\nperformance of our approach in terms of RMSE and execution time against\nstate-of-the-art approaches on three real-world datasets."}, "hash": "1ed77d1107339620688df1445c94c52a82f1db6a67e0eb20cb17e7335a6eb2cf", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c11d69b3-3f61-4c01-bcc9-cd95e1984e8c", "node_type": "1", "metadata": {}, "hash": "cfb6d6a353254957290465fede4ddc1a8eb129afb3044eac425f0de3e110043d", "class_name": "RelatedNodeInfo"}}, "text": "II. R ELATED WORK\nLLMs have been applied into many different domains and\ncontexts such as healthcare [18], [19], [20], financial modeling\n[21], [22], [23], and education and research [24], [25], as\nwell as in time series data [26] for many different tasks and\napplication domains [27].\nThe authors of TIME-LLM [28], introduce a reprogramming\nframework aimed at adapting LLMs for time series forecasting\nwithout altering their pre-trained structure. TIME-LLM re-\nprograms input time series into text prototype representations\nthat suit LLMs\u2019 capabilities. By introducing Prompt-as-Prefix\n(PaP), which enriches the input context with natural language\ninstructions, the reprogrammed input is then processed by the\nfrozen LLM. The output is projected to generate time series\nforecasts updating only lightweight input transformation and\noutput projection parameters, while the backbone language\nmodel remains frozen. Scenarios for both short- and long-term\nare addressed, as well as few- and one-shot learning.\nLLMTIME [15] is the first approach to apply zero-shot\nforecasting on time series using LLMs. The authors argue that\nthe output of LLMs when predicting digit-by-digit follows\na multimodal distribution, which fits well in the case of\ntime series. To apply forecasting, the time series values are\ntokenized and rescaled to a predefined number of digits to use\nfewer tokens. Then, to apply forecasting, the time series with\ntheir tokenized values separated by commas are passed to the\nmodel. Notably, the model\u2019s output is limited to producing\nonly digits and commas (i.e., [0\u22129,]). At each time step, a\npredefined number of samples is drawn and the final forecast\nis built using the median of all samples after descaling the\noutputted values.\nDespite the potential of LLMs for time series forecasting,\nthere are several limitations that need to be addressed.\n\u2022No multivariate support : Most current approaches using\nLLMs for time series forecasting focus on univariate time\nseries data. This limitation restricts the applicability of\nLLMs to certain types of time series data.\n\u2022Fine-tuning requirement : Fine-tuning can be time-\nconsuming and computationally expensive, particularly\nfor large models. It also requires a substantial amount\nof training data, which may not always be available.\n\u2022Number of tokens required : LLMs are extremely large\nmodels, capable of efficiently running on computers\nequipped with GPUs of high capacity in RAM. Thus,\ntheir broad availability depends on services that host\nsuch models, which usually charge queries by token.\nConsequently, very large queries (e.g., a large time series\nin our context) would be rather expensive to run.\nIII. M ULTI CAST\nIn the following, we describe our approach to zero-shot\nmultivariate time series forecasting using LLMs. First, we\ngo through the three separate token multiplexing approaches\nthat we propose. Then, we describe our approach to reducing\ncomplexity using the SAX representation.A. Dimensional Multiplexing\nThe dimensional multiplexing process takes place after each\ndimension has been rescaled to avoid decimals. Then, each\ndigit is treated separately. An example of this process is\nillustrated in the top row of Figure 1. Of course, depending on\nthe LLM used, its tokenizer must be adapted accordingly, as\ndiscussed in [15]. After multiplexing, the tokens are replaced\nwith their corresponding corpus id before being passed onto\nthe model for inference.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3436, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c11d69b3-3f61-4c01-bcc9-cd95e1984e8c": {"__data__": {"id_": "c11d69b3-3f61-4c01-bcc9-cd95e1984e8c", "embedding": null, "metadata": {"page_label": "2", "file_name": "2405_14748v1.pdf", "Title of this paper": "MultiCast: Zero-Shot Multivariate Time Series Forecasting Using LLMs", "Authors": "Georgios Chatzigeorgakidis, Konstantinos Lentzos, Dimitrios Skoutas", "Date published": "05/23/2024", "URL": "http://arxiv.org/abs/2405.14748v1", "summary": "Predicting future values in multivariate time series is vital across various\ndomains. This work explores the use of large language models (LLMs) for this\ntask. However, LLMs typically handle one-dimensional data. We introduce\nMultiCast, a zero-shot LLM-based approach for multivariate time series\nforecasting. It allows LLMs to receive multivariate time series as input,\nthrough three novel token multiplexing solutions that effectively reduce\ndimensionality while preserving key repetitive patterns. Additionally, a\nquantization scheme helps LLMs to better learn these patterns, while\nsignificantly reducing token use for practical applications. We showcase the\nperformance of our approach in terms of RMSE and execution time against\nstate-of-the-art approaches on three real-world datasets."}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "19a31078-f89d-49dc-adb7-173adbb44952", "node_type": "4", "metadata": {"page_label": "2", "file_name": "2405_14748v1.pdf", "Title of this paper": "MultiCast: Zero-Shot Multivariate Time Series Forecasting Using LLMs", "Authors": "Georgios Chatzigeorgakidis, Konstantinos Lentzos, Dimitrios Skoutas", "Date published": "05/23/2024", "URL": "http://arxiv.org/abs/2405.14748v1", "summary": "Predicting future values in multivariate time series is vital across various\ndomains. This work explores the use of large language models (LLMs) for this\ntask. However, LLMs typically handle one-dimensional data. We introduce\nMultiCast, a zero-shot LLM-based approach for multivariate time series\nforecasting. It allows LLMs to receive multivariate time series as input,\nthrough three novel token multiplexing solutions that effectively reduce\ndimensionality while preserving key repetitive patterns. Additionally, a\nquantization scheme helps LLMs to better learn these patterns, while\nsignificantly reducing token use for practical applications. We showcase the\nperformance of our approach in terms of RMSE and execution time against\nstate-of-the-art approaches on three real-world datasets."}, "hash": "1ed77d1107339620688df1445c94c52a82f1db6a67e0eb20cb17e7335a6eb2cf", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1930c7ba-fcd1-4f0d-8ff3-5cadb253b660", "node_type": "1", "metadata": {"page_label": "2", "file_name": "2405_14748v1.pdf", "Title of this paper": "MultiCast: Zero-Shot Multivariate Time Series Forecasting Using LLMs", "Authors": "Georgios Chatzigeorgakidis, Konstantinos Lentzos, Dimitrios Skoutas", "Date published": "05/23/2024", "URL": "http://arxiv.org/abs/2405.14748v1", "summary": "Predicting future values in multivariate time series is vital across various\ndomains. This work explores the use of large language models (LLMs) for this\ntask. However, LLMs typically handle one-dimensional data. We introduce\nMultiCast, a zero-shot LLM-based approach for multivariate time series\nforecasting. It allows LLMs to receive multivariate time series as input,\nthrough three novel token multiplexing solutions that effectively reduce\ndimensionality while preserving key repetitive patterns. Additionally, a\nquantization scheme helps LLMs to better learn these patterns, while\nsignificantly reducing token use for practical applications. We showcase the\nperformance of our approach in terms of RMSE and execution time against\nstate-of-the-art approaches on three real-world datasets."}, "hash": "e2993ac430c821628ee33bcd71c448927cd72f022e62cfb10be39481041a8a4d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b91a5516-6a4f-403b-91bb-9b8840ee856f", "node_type": "1", "metadata": {}, "hash": "94009efbefd201eab52555954ce7cbb9923b0101ec917fcbc7bd508cf42897c7", "class_name": "RelatedNodeInfo"}}, "text": "III. M ULTI CAST\nIn the following, we describe our approach to zero-shot\nmultivariate time series forecasting using LLMs. First, we\ngo through the three separate token multiplexing approaches\nthat we propose. Then, we describe our approach to reducing\ncomplexity using the SAX representation.A. Dimensional Multiplexing\nThe dimensional multiplexing process takes place after each\ndimension has been rescaled to avoid decimals. Then, each\ndigit is treated separately. An example of this process is\nillustrated in the top row of Figure 1. Of course, depending on\nthe LLM used, its tokenizer must be adapted accordingly, as\ndiscussed in [15]. After multiplexing, the tokens are replaced\nwith their corresponding corpus id before being passed onto\nthe model for inference. When the model produces the output,\nthis process is reversed to obtain the final result. We introduce\nthree separate dimensional multiplexing techniques, namely\n(i)digit-interleaving , (ii) value-interleaving , and (iii) value-\nconcatenation .\nid111 id112    id211 id212    id121 id122       id221 id222       \u2026 id111  id211  id112  id212    id121 id221 id122 id222  \u2026 2  3 , 3  1 , \u20261  7 , 2  6 , \u2026\n2.3 , 3.1 , \u20261.7 , 2.6 , \u2026 17 , 26 , \u2026\n23 , 31 , \u2026t1 t2\nd1\nd2t1t2\nt1t2\nd1d2d1d2 d1d2d1d2\nt1 t2\nd1d1 d2d2 d1d1 d2d2Digit -\ninterleaving\nV alue -\ninterleaving\nV alue -\nconcatenation(a)\n(b)\n(c)t1t2\n1  2  7  3  2  \nidc idc3  6  1  ,  ,  \nid111  id112  id211  id212    id121 id122 id221 id222  \u2026 t1t2\nd1d1d2d2 d1d1d2d21  7  2  3  2  \nidc idc6  3  1  ,  ,  \n1  7  2  3  2  6  3  1  ,  ,  ,  ,  \nidc idc idc idc\nFig. 1. The three token multiplexing techniques.\n1) Digit-Interleaving: After each dimension has been\nrescaled, the Digit-Interleaving (DI) multiplexing technique\nplaces the digits of each dimension per timestamp inter-\nchangeably. This is exemplified in Figure 1a. Consider a 2-\ndimensional time series. Specifically, d1= [1.7,2.6, ...]and\nd2= [2.3,3.1, ...]are the two dimensions (i.e., we only\nshow the first two timestamps for brevity). After rescaling, the\ndimensions become d1= [17 ,26, ...]andd2= [23 ,31, ...], re-\nspectively. Then, as described previously, each digit is consid-\nered a separate token.", "mimetype": "text/plain", "start_char_idx": 2668, "end_char_idx": 4852, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b91a5516-6a4f-403b-91bb-9b8840ee856f": {"__data__": {"id_": "b91a5516-6a4f-403b-91bb-9b8840ee856f", "embedding": null, "metadata": {"page_label": "2", "file_name": "2405_14748v1.pdf", "Title of this paper": "MultiCast: Zero-Shot Multivariate Time Series Forecasting Using LLMs", "Authors": "Georgios Chatzigeorgakidis, Konstantinos Lentzos, Dimitrios Skoutas", "Date published": "05/23/2024", "URL": "http://arxiv.org/abs/2405.14748v1", "summary": "Predicting future values in multivariate time series is vital across various\ndomains. This work explores the use of large language models (LLMs) for this\ntask. However, LLMs typically handle one-dimensional data. We introduce\nMultiCast, a zero-shot LLM-based approach for multivariate time series\nforecasting. It allows LLMs to receive multivariate time series as input,\nthrough three novel token multiplexing solutions that effectively reduce\ndimensionality while preserving key repetitive patterns. Additionally, a\nquantization scheme helps LLMs to better learn these patterns, while\nsignificantly reducing token use for practical applications. We showcase the\nperformance of our approach in terms of RMSE and execution time against\nstate-of-the-art approaches on three real-world datasets."}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "19a31078-f89d-49dc-adb7-173adbb44952", "node_type": "4", "metadata": {"page_label": "2", "file_name": "2405_14748v1.pdf", "Title of this paper": "MultiCast: Zero-Shot Multivariate Time Series Forecasting Using LLMs", "Authors": "Georgios Chatzigeorgakidis, Konstantinos Lentzos, Dimitrios Skoutas", "Date published": "05/23/2024", "URL": "http://arxiv.org/abs/2405.14748v1", "summary": "Predicting future values in multivariate time series is vital across various\ndomains. This work explores the use of large language models (LLMs) for this\ntask. However, LLMs typically handle one-dimensional data. We introduce\nMultiCast, a zero-shot LLM-based approach for multivariate time series\nforecasting. It allows LLMs to receive multivariate time series as input,\nthrough three novel token multiplexing solutions that effectively reduce\ndimensionality while preserving key repetitive patterns. Additionally, a\nquantization scheme helps LLMs to better learn these patterns, while\nsignificantly reducing token use for practical applications. We showcase the\nperformance of our approach in terms of RMSE and execution time against\nstate-of-the-art approaches on three real-world datasets."}, "hash": "1ed77d1107339620688df1445c94c52a82f1db6a67e0eb20cb17e7335a6eb2cf", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c11d69b3-3f61-4c01-bcc9-cd95e1984e8c", "node_type": "1", "metadata": {"page_label": "2", "file_name": "2405_14748v1.pdf", "Title of this paper": "MultiCast: Zero-Shot Multivariate Time Series Forecasting Using LLMs", "Authors": "Georgios Chatzigeorgakidis, Konstantinos Lentzos, Dimitrios Skoutas", "Date published": "05/23/2024", "URL": "http://arxiv.org/abs/2405.14748v1", "summary": "Predicting future values in multivariate time series is vital across various\ndomains. This work explores the use of large language models (LLMs) for this\ntask. However, LLMs typically handle one-dimensional data. We introduce\nMultiCast, a zero-shot LLM-based approach for multivariate time series\nforecasting. It allows LLMs to receive multivariate time series as input,\nthrough three novel token multiplexing solutions that effectively reduce\ndimensionality while preserving key repetitive patterns. Additionally, a\nquantization scheme helps LLMs to better learn these patterns, while\nsignificantly reducing token use for practical applications. We showcase the\nperformance of our approach in terms of RMSE and execution time against\nstate-of-the-art approaches on three real-world datasets."}, "hash": "ca79418fa5ed63ab2708db6000bd0fff52c4bac12fc8659da4f6c564fd18d798", "class_name": "RelatedNodeInfo"}}, "text": "1. The three token multiplexing techniques.\n1) Digit-Interleaving: After each dimension has been\nrescaled, the Digit-Interleaving (DI) multiplexing technique\nplaces the digits of each dimension per timestamp inter-\nchangeably. This is exemplified in Figure 1a. Consider a 2-\ndimensional time series. Specifically, d1= [1.7,2.6, ...]and\nd2= [2.3,3.1, ...]are the two dimensions (i.e., we only\nshow the first two timestamps for brevity). After rescaling, the\ndimensions become d1= [17 ,26, ...]andd2= [23 ,31, ...], re-\nspectively. Then, as described previously, each digit is consid-\nered a separate token. Before being assigned the corresponding\ncorpus id, tokens are interchangeably placed per dimension\nfor each timestamp, reducing the dimensions to 1. The re-\nsulting series in the example would be d= [1273 ,2361, ...].\nThen, each digit (token) and comma are assigned with the\ncorresponding id. This technique attempts to take advantage\nof the fact that, in many multivariate time series, all values\nare correlated and similarly scaled. Such an example are z-\nnormalized series, which have zero mean with values differing\na few standard deviations from it. In such a case, the left-wise", "mimetype": "text/plain", "start_char_idx": 4247, "end_char_idx": 5437, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6e49a334-f59e-40e0-8d79-76da823feb78": {"__data__": {"id_": "6e49a334-f59e-40e0-8d79-76da823feb78", "embedding": null, "metadata": {"page_label": "3", "file_name": "2405_14748v1.pdf", "Title of this paper": "MultiCast: Zero-Shot Multivariate Time Series Forecasting Using LLMs", "Authors": "Georgios Chatzigeorgakidis, Konstantinos Lentzos, Dimitrios Skoutas", "Date published": "05/23/2024", "URL": "http://arxiv.org/abs/2405.14748v1", "summary": "Predicting future values in multivariate time series is vital across various\ndomains. This work explores the use of large language models (LLMs) for this\ntask. However, LLMs typically handle one-dimensional data. We introduce\nMultiCast, a zero-shot LLM-based approach for multivariate time series\nforecasting. It allows LLMs to receive multivariate time series as input,\nthrough three novel token multiplexing solutions that effectively reduce\ndimensionality while preserving key repetitive patterns. Additionally, a\nquantization scheme helps LLMs to better learn these patterns, while\nsignificantly reducing token use for practical applications. We showcase the\nperformance of our approach in terms of RMSE and execution time against\nstate-of-the-art approaches on three real-world datasets."}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "732108b4-c6bc-432e-8958-5d8f781f7957", "node_type": "4", "metadata": {"page_label": "3", "file_name": "2405_14748v1.pdf", "Title of this paper": "MultiCast: Zero-Shot Multivariate Time Series Forecasting Using LLMs", "Authors": "Georgios Chatzigeorgakidis, Konstantinos Lentzos, Dimitrios Skoutas", "Date published": "05/23/2024", "URL": "http://arxiv.org/abs/2405.14748v1", "summary": "Predicting future values in multivariate time series is vital across various\ndomains. This work explores the use of large language models (LLMs) for this\ntask. However, LLMs typically handle one-dimensional data. We introduce\nMultiCast, a zero-shot LLM-based approach for multivariate time series\nforecasting. It allows LLMs to receive multivariate time series as input,\nthrough three novel token multiplexing solutions that effectively reduce\ndimensionality while preserving key repetitive patterns. Additionally, a\nquantization scheme helps LLMs to better learn these patterns, while\nsignificantly reducing token use for practical applications. We showcase the\nperformance of our approach in terms of RMSE and execution time against\nstate-of-the-art approaches on three real-world datasets."}, "hash": "21f5786498548bfc76eca00a62a296efb2ec3083f0489ab9f141d12bcac844aa", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d309a49d-ac36-4180-b02b-5fe1d8ba350d", "node_type": "1", "metadata": {}, "hash": "33044ab15d2afbed79d44da686e9b9d7aefe14b4b6d18dff15f6c392c7676a77", "class_name": "RelatedNodeInfo"}}, "text": "digits per dimension will be all placed first; since the model is\nproducing the output token-by-token, this can help it infer the\ncorrect scaling of the series. More formally, DI multiplexing\ncan be formulated as follows.\nId={t111...td11t11b...td1b}tc{t1n1...tdn1t1nb...tdnb}(1)\nwhere dis the number of dimensions, bthe predefined\nnumber of digits per timestamp, and nthe time series length.\n2) Value-Interleaving: Figure 1b shows the Value-\nInterleaving (VI) dimensional multiplexing technique. This\ntime, instead of interchangeably placing the digits per\ntimestamp and dimension, we place the whole values of each\ndimension per timestamp one after the other. Thus, in the\nexample, the 1-dimensional result will be d= [1723 ,2631, ...].\nIntuitively, this technique is more suitable in cases where the\ndimensions of the series are on a different scale. We expect\nthe model to be able to distinguish between the different\ndimensions \u2013especially when they differ in scale\u2013, and\nmanage to internally demultiplex the input before inference.\nThe VI multiplexing can be formulated as follows.\nIts={t111...t11btd11...td1b}tc{t1n1...t1nbtdn1...tdnb}(2)\nwhere dis the number of dimensions, bthe predefined\nnumber of digits per timestamp, and nthe time series length.\n3) Value-Concatenation: Finally, Figure 1b shows the\nValue-Concatenation (VC) dimensional multiplexing tech-\nnique which is an extension of the value-interleaving tech-\nnique; for each timestamp, we now place the values of each\ndimension separated by commas, thus considering them as\ndifferent values (e.g., in the figure, the 1-dimensional result\nwill be d= [17 ,23,26,31, ...]. We expect this to further\nfaciliate the internal demultiplexing by the model before\ndetecting any patterns. The VC multiplexing can be formulated\nas follows.\nItd={t111...t11b}tc{td11...td1b}tc{t1n1...t1nb}tc{tdn1...tdnb}(3)\nwhere dis the number of dimensions, bthe predefined\nnumber of digits per timestamp and nthe length of the time\nseries. Of course, in all cases, upon receiving the multiplexed\noutput from the model, the tokens must be properly decoded,\ndemultiplexed, and brought back to their initial scale for each\ndimension, depending on the selected technique. A significant\nadvantage of this multiplexing technique against forecasting\neach dimension separately is the fact that multivariate time\nseries tend to have high interdimensional correlations (e.g.,\ntemperature and humidity in weather data). We expect that\nproviding them altogether in the model can lead to the detec-\ntion of such interdimensional patterns, yielding better results.\nB. Quantization Using SAX\nThe Symbolic Aggregate approXimation (SAX) is a multi-\nresolution representation of a time series introduced in [29]. It\ncan be derived from its Piecewise Aggregate Approximation(PAA) [30], [31] by quantizing the PAA segments on the\nv-axis. A time series is first transformed into a PAA rep-\nresentation of wsegments with real-valued coefficients.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2965, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d309a49d-ac36-4180-b02b-5fe1d8ba350d": {"__data__": {"id_": "d309a49d-ac36-4180-b02b-5fe1d8ba350d", "embedding": null, "metadata": {"page_label": "3", "file_name": "2405_14748v1.pdf", "Title of this paper": "MultiCast: Zero-Shot Multivariate Time Series Forecasting Using LLMs", "Authors": "Georgios Chatzigeorgakidis, Konstantinos Lentzos, Dimitrios Skoutas", "Date published": "05/23/2024", "URL": "http://arxiv.org/abs/2405.14748v1", "summary": "Predicting future values in multivariate time series is vital across various\ndomains. This work explores the use of large language models (LLMs) for this\ntask. However, LLMs typically handle one-dimensional data. We introduce\nMultiCast, a zero-shot LLM-based approach for multivariate time series\nforecasting. It allows LLMs to receive multivariate time series as input,\nthrough three novel token multiplexing solutions that effectively reduce\ndimensionality while preserving key repetitive patterns. Additionally, a\nquantization scheme helps LLMs to better learn these patterns, while\nsignificantly reducing token use for practical applications. We showcase the\nperformance of our approach in terms of RMSE and execution time against\nstate-of-the-art approaches on three real-world datasets."}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "732108b4-c6bc-432e-8958-5d8f781f7957", "node_type": "4", "metadata": {"page_label": "3", "file_name": "2405_14748v1.pdf", "Title of this paper": "MultiCast: Zero-Shot Multivariate Time Series Forecasting Using LLMs", "Authors": "Georgios Chatzigeorgakidis, Konstantinos Lentzos, Dimitrios Skoutas", "Date published": "05/23/2024", "URL": "http://arxiv.org/abs/2405.14748v1", "summary": "Predicting future values in multivariate time series is vital across various\ndomains. This work explores the use of large language models (LLMs) for this\ntask. However, LLMs typically handle one-dimensional data. We introduce\nMultiCast, a zero-shot LLM-based approach for multivariate time series\nforecasting. It allows LLMs to receive multivariate time series as input,\nthrough three novel token multiplexing solutions that effectively reduce\ndimensionality while preserving key repetitive patterns. Additionally, a\nquantization scheme helps LLMs to better learn these patterns, while\nsignificantly reducing token use for practical applications. We showcase the\nperformance of our approach in terms of RMSE and execution time against\nstate-of-the-art approaches on three real-world datasets."}, "hash": "21f5786498548bfc76eca00a62a296efb2ec3083f0489ab9f141d12bcac844aa", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6e49a334-f59e-40e0-8d79-76da823feb78", "node_type": "1", "metadata": {"page_label": "3", "file_name": "2405_14748v1.pdf", "Title of this paper": "MultiCast: Zero-Shot Multivariate Time Series Forecasting Using LLMs", "Authors": "Georgios Chatzigeorgakidis, Konstantinos Lentzos, Dimitrios Skoutas", "Date published": "05/23/2024", "URL": "http://arxiv.org/abs/2405.14748v1", "summary": "Predicting future values in multivariate time series is vital across various\ndomains. This work explores the use of large language models (LLMs) for this\ntask. However, LLMs typically handle one-dimensional data. We introduce\nMultiCast, a zero-shot LLM-based approach for multivariate time series\nforecasting. It allows LLMs to receive multivariate time series as input,\nthrough three novel token multiplexing solutions that effectively reduce\ndimensionality while preserving key repetitive patterns. Additionally, a\nquantization scheme helps LLMs to better learn these patterns, while\nsignificantly reducing token use for practical applications. We showcase the\nperformance of our approach in terms of RMSE and execution time against\nstate-of-the-art approaches on three real-world datasets."}, "hash": "65ca6397fa4c7bd29c88bc672d09110e257a3df6f20a86c989cd6f2ff449538d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8bbabe7e-3006-48a4-9f49-508c7470fd99", "node_type": "1", "metadata": {}, "hash": "665539cf80627b0ee4d0a7a9b6f437b052694998ebfd77993ec000d4d3196bef", "class_name": "RelatedNodeInfo"}}, "text": "A significant\nadvantage of this multiplexing technique against forecasting\neach dimension separately is the fact that multivariate time\nseries tend to have high interdimensional correlations (e.g.,\ntemperature and humidity in weather data). We expect that\nproviding them altogether in the model can lead to the detec-\ntion of such interdimensional patterns, yielding better results.\nB. Quantization Using SAX\nThe Symbolic Aggregate approXimation (SAX) is a multi-\nresolution representation of a time series introduced in [29]. It\ncan be derived from its Piecewise Aggregate Approximation(PAA) [30], [31] by quantizing the PAA segments on the\nv-axis. A time series is first transformed into a PAA rep-\nresentation of wsegments with real-valued coefficients. To\nobtain a SAX word for a time series, these coefficients are\ndiscretized along the value axis using breakpoints assuming\naN(0,1)Gaussian distribution that enables the generation\nof equiprobable symbols for a given cardinality. Although\nbitwise representations were used for these symbols in the\noriginal paper, other encoding types are also possible. Two\nsuch popular alternatives are using alphabetical characters or\ndigits for each symbol.\nForecasting time series is an inherently difficult task due\nto the nature of the data. This is also the case for zero-shot\nforeasting using LLMs, since (i.e., as also described in [15])\nthey have to infer a sequence of tokens for each timestamp,\nthus simulating a multi-modal distribution. This becomes even\nharder when applying the above-mentioned dimensional multi-\nplexing techniques. Also, for large time series, such a process\nbecomes significantly more computationally intensive; plus, it\nrequires many tokens, which, depending on the application,\ncan be rather expensive to infer according to currently LLM\npricing policies. To alleviate these issues, we quantize the\ntime series across all dimensions in both axes using the SAX\nrepresentation, before applying tokenization. We support two\ndifferent quantization types, either using an alphabetical or\na digital SAX alphabet. Now, each value per timestamp is\nconsisted of only one token instead of multiple. For example,\nthe time series in Figure 1 could become dsax\n1= [a, b, ... ]and\ndsax\n2= [b, c, ... ]after alphabetical quantization. We expect that\nit will be easier for the model to detect patterns when dealing\nonly with one token per timestamp.\nIV. E XPERIMENTAL EVALUATION\nThis section presents the results of our experiments. We\nfirst explain how we set up our tests and assess the suggested\nmethods.\nA. Experimental Setup\n1) System: We used Python and the Hugging Face API1.\nThe experiments were run on a server with an AMD Ryzen\nThreadripper 3960X 24-core CPU and 256GB memory. The\nexperiments were run on CPU.\n2) Datasets: We employ three real-world multivariate time\nseries datasets.\nGas Rate : This is a 2-dimensional dataset containing carbon\ndioxide (CO 2) emissions. The first dimension contains the\ninput CO 2measurements (ft3/min) in a gas furnace. The\nsecond dimension contains the output CO 2percentage. The\ndataset is obtained from the darts library2. Of course, the two\ndimensions are correlated, which makes this dataset ideal for\nmultivariate forecasting.\nElectricity : This multivariate time series is part of the Elec-\ntricity Transformer Dataset (ETDataset)3.", "mimetype": "text/plain", "start_char_idx": 2209, "end_char_idx": 5555, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8bbabe7e-3006-48a4-9f49-508c7470fd99": {"__data__": {"id_": "8bbabe7e-3006-48a4-9f49-508c7470fd99", "embedding": null, "metadata": {"page_label": "3", "file_name": "2405_14748v1.pdf", "Title of this paper": "MultiCast: Zero-Shot Multivariate Time Series Forecasting Using LLMs", "Authors": "Georgios Chatzigeorgakidis, Konstantinos Lentzos, Dimitrios Skoutas", "Date published": "05/23/2024", "URL": "http://arxiv.org/abs/2405.14748v1", "summary": "Predicting future values in multivariate time series is vital across various\ndomains. This work explores the use of large language models (LLMs) for this\ntask. However, LLMs typically handle one-dimensional data. We introduce\nMultiCast, a zero-shot LLM-based approach for multivariate time series\nforecasting. It allows LLMs to receive multivariate time series as input,\nthrough three novel token multiplexing solutions that effectively reduce\ndimensionality while preserving key repetitive patterns. Additionally, a\nquantization scheme helps LLMs to better learn these patterns, while\nsignificantly reducing token use for practical applications. We showcase the\nperformance of our approach in terms of RMSE and execution time against\nstate-of-the-art approaches on three real-world datasets."}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "732108b4-c6bc-432e-8958-5d8f781f7957", "node_type": "4", "metadata": {"page_label": "3", "file_name": "2405_14748v1.pdf", "Title of this paper": "MultiCast: Zero-Shot Multivariate Time Series Forecasting Using LLMs", "Authors": "Georgios Chatzigeorgakidis, Konstantinos Lentzos, Dimitrios Skoutas", "Date published": "05/23/2024", "URL": "http://arxiv.org/abs/2405.14748v1", "summary": "Predicting future values in multivariate time series is vital across various\ndomains. This work explores the use of large language models (LLMs) for this\ntask. However, LLMs typically handle one-dimensional data. We introduce\nMultiCast, a zero-shot LLM-based approach for multivariate time series\nforecasting. It allows LLMs to receive multivariate time series as input,\nthrough three novel token multiplexing solutions that effectively reduce\ndimensionality while preserving key repetitive patterns. Additionally, a\nquantization scheme helps LLMs to better learn these patterns, while\nsignificantly reducing token use for practical applications. We showcase the\nperformance of our approach in terms of RMSE and execution time against\nstate-of-the-art approaches on three real-world datasets."}, "hash": "21f5786498548bfc76eca00a62a296efb2ec3083f0489ab9f141d12bcac844aa", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d309a49d-ac36-4180-b02b-5fe1d8ba350d", "node_type": "1", "metadata": {"page_label": "3", "file_name": "2405_14748v1.pdf", "Title of this paper": "MultiCast: Zero-Shot Multivariate Time Series Forecasting Using LLMs", "Authors": "Georgios Chatzigeorgakidis, Konstantinos Lentzos, Dimitrios Skoutas", "Date published": "05/23/2024", "URL": "http://arxiv.org/abs/2405.14748v1", "summary": "Predicting future values in multivariate time series is vital across various\ndomains. This work explores the use of large language models (LLMs) for this\ntask. However, LLMs typically handle one-dimensional data. We introduce\nMultiCast, a zero-shot LLM-based approach for multivariate time series\nforecasting. It allows LLMs to receive multivariate time series as input,\nthrough three novel token multiplexing solutions that effectively reduce\ndimensionality while preserving key repetitive patterns. Additionally, a\nquantization scheme helps LLMs to better learn these patterns, while\nsignificantly reducing token use for practical applications. We showcase the\nperformance of our approach in terms of RMSE and execution time against\nstate-of-the-art approaches on three real-world datasets."}, "hash": "ad0eac94128f74d29432f181f9a3bc87b1c5e4257bf1ed10db7fb717693fdffb", "class_name": "RelatedNodeInfo"}}, "text": "A. Experimental Setup\n1) System: We used Python and the Hugging Face API1.\nThe experiments were run on a server with an AMD Ryzen\nThreadripper 3960X 24-core CPU and 256GB memory. The\nexperiments were run on CPU.\n2) Datasets: We employ three real-world multivariate time\nseries datasets.\nGas Rate : This is a 2-dimensional dataset containing carbon\ndioxide (CO 2) emissions. The first dimension contains the\ninput CO 2measurements (ft3/min) in a gas furnace. The\nsecond dimension contains the output CO 2percentage. The\ndataset is obtained from the darts library2. Of course, the two\ndimensions are correlated, which makes this dataset ideal for\nmultivariate forecasting.\nElectricity : This multivariate time series is part of the Elec-\ntricity Transformer Dataset (ETDataset)3. It contains hourly\n1https://huggingface.co/\n2https://unit8co.github.io/darts\n3https://github.com/zhouhaoyi/ETDataset", "mimetype": "text/plain", "start_char_idx": 4778, "end_char_idx": 5672, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "31e24330-1adb-4395-9de1-5fdbd7970ecd": {"__data__": {"id_": "31e24330-1adb-4395-9de1-5fdbd7970ecd", "embedding": null, "metadata": {"page_label": "4", "file_name": "2405_14748v1.pdf", "Title of this paper": "MultiCast: Zero-Shot Multivariate Time Series Forecasting Using LLMs", "Authors": "Georgios Chatzigeorgakidis, Konstantinos Lentzos, Dimitrios Skoutas", "Date published": "05/23/2024", "URL": "http://arxiv.org/abs/2405.14748v1", "summary": "Predicting future values in multivariate time series is vital across various\ndomains. This work explores the use of large language models (LLMs) for this\ntask. However, LLMs typically handle one-dimensional data. We introduce\nMultiCast, a zero-shot LLM-based approach for multivariate time series\nforecasting. It allows LLMs to receive multivariate time series as input,\nthrough three novel token multiplexing solutions that effectively reduce\ndimensionality while preserving key repetitive patterns. Additionally, a\nquantization scheme helps LLMs to better learn these patterns, while\nsignificantly reducing token use for practical applications. We showcase the\nperformance of our approach in terms of RMSE and execution time against\nstate-of-the-art approaches on three real-world datasets."}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "0865d4e7-44c6-48fc-9fca-a5db256d411b", "node_type": "4", "metadata": {"page_label": "4", "file_name": "2405_14748v1.pdf", "Title of this paper": "MultiCast: Zero-Shot Multivariate Time Series Forecasting Using LLMs", "Authors": "Georgios Chatzigeorgakidis, Konstantinos Lentzos, Dimitrios Skoutas", "Date published": "05/23/2024", "URL": "http://arxiv.org/abs/2405.14748v1", "summary": "Predicting future values in multivariate time series is vital across various\ndomains. This work explores the use of large language models (LLMs) for this\ntask. However, LLMs typically handle one-dimensional data. We introduce\nMultiCast, a zero-shot LLM-based approach for multivariate time series\nforecasting. It allows LLMs to receive multivariate time series as input,\nthrough three novel token multiplexing solutions that effectively reduce\ndimensionality while preserving key repetitive patterns. Additionally, a\nquantization scheme helps LLMs to better learn these patterns, while\nsignificantly reducing token use for practical applications. We showcase the\nperformance of our approach in terms of RMSE and execution time against\nstate-of-the-art approaches on three real-world datasets."}, "hash": "f493a8d1cdd35713a7297971e6175cc6783fd76e2ee1f8f542673c026f223fe0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "118f70fc-e5b2-420f-8883-394407df9f8b", "node_type": "1", "metadata": {}, "hash": "83f1164fc783f3c32a0915a43de69bc17045dadc1f718b9e49649c07643b175f", "class_name": "RelatedNodeInfo"}}, "text": "measurements of various metrics, which were resampled on a\n3-day basis, for a total of 242 timestamps. From this dataset,\nwe extracted 3 dimensions of electricity measurements, specif-\nically the High UseFul Load (HUFL), High UseLess Load\n(HULL), and Oil Temperature (OT). Again, the dimensions\nare correlated; specifically, OT is used as a target variable in\nregression problems.\nWeather : The weather dataset was generated by the Max\nPlanck Institute4and contains 21 weather-related metrics ob-\ntained from a weather station located in Germany. From the 21\nvariables, we extracted the air temperatures (Tlog) measured\nin Celsius degrees, the water vapor concentration (H2OC)\nmeasured in mmol/mol, the saturation water vapor pressure\n(VPmax), measured in mbar, and the potential temperature\n(Tpot) measured in Kelvin degrees. Again, being weather-\nrelated, all dimensions are correlated.\nTABLE I\nDATASETS .\nDataset Dimensions Length\nGas Rate 2 296\nElectricity 3 242\nWeather 4 217\nTABLE II\nPARAMETERS .\nParameter Range\nDimensions 2, 3, 4\nNumber of samples 5, 10, 20\nSAX segment length 3,6, 9\nSAX alphabet size 5, 10, 20\n3) Competitors: We evaluate the following methods:\n\u2022MultiCast (DI) : MultiCast using the digit-interleaving\ndimensional multiplexing method.\n\u2022MultiCast (VI) : MultiCast using the value-interleaving\ndimensional multiplexing method on the same value.\n\u2022MultiCast (VC) : MultiCast using the value-\nconcatenation dimensional multiplexing method on\nconsecutive values.\n\u2022LLMTIME : The state-of-the-art in LLM-based zero-shot\ntime series forecasting (i.e., applied in each dimension\nseparately).\n\u2022ARIMA : Autoregressive Integrated Moving Average\n(ARIMA) is one of the most widely used univariate time\nseries forecasting methods.\n\u2022LSTM [32]: Termed as Long-Short-Term Memory\n(LSTM), LSTMs are Recurrent Neural Networks (RNNs)\ndesigned to handle the vanishing gradient problem. This\nability allows LSTMs to learn and remember information\nover time, making them ideal for time series forecasting.\nLSTMs have been used successfully for multivariate time\nseries forecasting [33], [34].\n4) Parameters: The parameters utilized in our experimental\nassessment are listed in Table II. For each parameter, we\n4https://www.bgc-jena.mpg.de/wetter/performed tuning tests to establish their ranges and default\nvalues, which are highlighted in bold within the table. More\nspecifically, the dimensions parameter corresponds to the di-\nmensionality of each dataset; the number of samples only ap-\nplies to the LLM-related models and is the number of inference\nvalues taken for each timestamp; the SAX segment length is the\nlevel of quantization on the x-axis, which determines the level\nof compression of a time series; the SAX alphabet size is the\nlevel of quantization on the y-axis, as performed by the SAX\nmethod. Regarding the LSTM parametrization, we performed\na grid search, which yielded a 1-hidden-level network of 128\nunits and a dropout rate of 0.2. It was trained for 30 epochs\nusing the Adam [35] optimizer with the Mean Squared Error\n(MAE) as loss function.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3065, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "118f70fc-e5b2-420f-8883-394407df9f8b": {"__data__": {"id_": "118f70fc-e5b2-420f-8883-394407df9f8b", "embedding": null, "metadata": {"page_label": "4", "file_name": "2405_14748v1.pdf", "Title of this paper": "MultiCast: Zero-Shot Multivariate Time Series Forecasting Using LLMs", "Authors": "Georgios Chatzigeorgakidis, Konstantinos Lentzos, Dimitrios Skoutas", "Date published": "05/23/2024", "URL": "http://arxiv.org/abs/2405.14748v1", "summary": "Predicting future values in multivariate time series is vital across various\ndomains. This work explores the use of large language models (LLMs) for this\ntask. However, LLMs typically handle one-dimensional data. We introduce\nMultiCast, a zero-shot LLM-based approach for multivariate time series\nforecasting. It allows LLMs to receive multivariate time series as input,\nthrough three novel token multiplexing solutions that effectively reduce\ndimensionality while preserving key repetitive patterns. Additionally, a\nquantization scheme helps LLMs to better learn these patterns, while\nsignificantly reducing token use for practical applications. We showcase the\nperformance of our approach in terms of RMSE and execution time against\nstate-of-the-art approaches on three real-world datasets."}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "0865d4e7-44c6-48fc-9fca-a5db256d411b", "node_type": "4", "metadata": {"page_label": "4", "file_name": "2405_14748v1.pdf", "Title of this paper": "MultiCast: Zero-Shot Multivariate Time Series Forecasting Using LLMs", "Authors": "Georgios Chatzigeorgakidis, Konstantinos Lentzos, Dimitrios Skoutas", "Date published": "05/23/2024", "URL": "http://arxiv.org/abs/2405.14748v1", "summary": "Predicting future values in multivariate time series is vital across various\ndomains. This work explores the use of large language models (LLMs) for this\ntask. However, LLMs typically handle one-dimensional data. We introduce\nMultiCast, a zero-shot LLM-based approach for multivariate time series\nforecasting. It allows LLMs to receive multivariate time series as input,\nthrough three novel token multiplexing solutions that effectively reduce\ndimensionality while preserving key repetitive patterns. Additionally, a\nquantization scheme helps LLMs to better learn these patterns, while\nsignificantly reducing token use for practical applications. We showcase the\nperformance of our approach in terms of RMSE and execution time against\nstate-of-the-art approaches on three real-world datasets."}, "hash": "f493a8d1cdd35713a7297971e6175cc6783fd76e2ee1f8f542673c026f223fe0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "31e24330-1adb-4395-9de1-5fdbd7970ecd", "node_type": "1", "metadata": {"page_label": "4", "file_name": "2405_14748v1.pdf", "Title of this paper": "MultiCast: Zero-Shot Multivariate Time Series Forecasting Using LLMs", "Authors": "Georgios Chatzigeorgakidis, Konstantinos Lentzos, Dimitrios Skoutas", "Date published": "05/23/2024", "URL": "http://arxiv.org/abs/2405.14748v1", "summary": "Predicting future values in multivariate time series is vital across various\ndomains. This work explores the use of large language models (LLMs) for this\ntask. However, LLMs typically handle one-dimensional data. We introduce\nMultiCast, a zero-shot LLM-based approach for multivariate time series\nforecasting. It allows LLMs to receive multivariate time series as input,\nthrough three novel token multiplexing solutions that effectively reduce\ndimensionality while preserving key repetitive patterns. Additionally, a\nquantization scheme helps LLMs to better learn these patterns, while\nsignificantly reducing token use for practical applications. We showcase the\nperformance of our approach in terms of RMSE and execution time against\nstate-of-the-art approaches on three real-world datasets."}, "hash": "3d0bccbce765b9a2f22bcbf7200652b9fc8fb2e48b94e803ba4a7dafdd3d8c15", "class_name": "RelatedNodeInfo"}}, "text": "More\nspecifically, the dimensions parameter corresponds to the di-\nmensionality of each dataset; the number of samples only ap-\nplies to the LLM-related models and is the number of inference\nvalues taken for each timestamp; the SAX segment length is the\nlevel of quantization on the x-axis, which determines the level\nof compression of a time series; the SAX alphabet size is the\nlevel of quantization on the y-axis, as performed by the SAX\nmethod. Regarding the LSTM parametrization, we performed\na grid search, which yielded a 1-hidden-level network of 128\nunits and a dropout rate of 0.2. It was trained for 30 epochs\nusing the Adam [35] optimizer with the Mean Squared Error\n(MAE) as loss function.\n5) Metrics: In accordance with standard practices in time\nseries forecasting, the Root Mean Squared Error (RMSE)\nmetric was employed to evaluate our methods. RMSE is\nformulated asp\n\u03a3n\ni=1(yi\u2212\u02c6yi)2/n, where yiis the actual\nvalue, \u02c6yiis the predicted value at timestamp iandnis the\nnumber of timestamps on which forecasting was applied.\nB. LLM Model Selection\nMultiCast can be used with any LLM to apply multivariate\ntime series forecasting. In the following, we evaluate its\naccuracy using LLaMA2 (i.e., the 7B parameter variant) and\nPhi-2 [36] as back-end models. LLaMA2 is one of the most\npopular LLMs that achieves good performance with fewer\nparameters. Phi-2 is a math-oriented LLM (i.e., 2.7B param-\neters), tailored to solving math problems. Table III lists the\nforecast RMSE for the Gas Rate data set in both dimensions\nfor both LLMs. In both cases, the VI variant of MultiCast\nwas used. LLaMA2 achieves better performance (i.e., approx.\ntwice as good) in all cases. This can be attributed to the\nfewer parameters of the Phi-2 model; while it is math-oriented\nand quite capable of solving complex problems described by\ntextual prompting, it seems to not properly detect the patterns\nin the series, leading to larger errors.\nTABLE III\nLLM MODEL COMPARISON .\nModelDimension\nGasRate CO2\nMultiCast (LLaMA2 / 7B) 1.154 2.71\nMultiCast (Phi-2 / 2.7B) 2.106 4.676\nFigures 2a and b depict two indicative examples of fore-\ncasting the first dimension of the Gas Rate dataset using\nthe LLaMA2 and Phi-2 models, respectively. Clearly, the\nLLaMA2 model performs better, being able to properly follow\nthe upward trend of the time series and even infer two local\nmaxima of the original time series. Phi-2, on the other hand,\nfails to accurately forecast the time series in this dimension;\nwhile it seems to successfully detect the upward trend, its\nentire output is shifted 1 to 2 units on the y-axis. Since\nLLaMA2 seems to perform significantly better in all cases,\nfor the rest of the experiential evaluation, we will be using\nLLaMA2 as the back-end model for MultiCast.", "mimetype": "text/plain", "start_char_idx": 2363, "end_char_idx": 5130, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f20b7855-9bcb-472d-bbfd-367251c7ca03": {"__data__": {"id_": "f20b7855-9bcb-472d-bbfd-367251c7ca03", "embedding": null, "metadata": {"page_label": "5", "file_name": "2405_14748v1.pdf", "Title of this paper": "MultiCast: Zero-Shot Multivariate Time Series Forecasting Using LLMs", "Authors": "Georgios Chatzigeorgakidis, Konstantinos Lentzos, Dimitrios Skoutas", "Date published": "05/23/2024", "URL": "http://arxiv.org/abs/2405.14748v1", "summary": "Predicting future values in multivariate time series is vital across various\ndomains. This work explores the use of large language models (LLMs) for this\ntask. However, LLMs typically handle one-dimensional data. We introduce\nMultiCast, a zero-shot LLM-based approach for multivariate time series\nforecasting. It allows LLMs to receive multivariate time series as input,\nthrough three novel token multiplexing solutions that effectively reduce\ndimensionality while preserving key repetitive patterns. Additionally, a\nquantization scheme helps LLMs to better learn these patterns, while\nsignificantly reducing token use for practical applications. We showcase the\nperformance of our approach in terms of RMSE and execution time against\nstate-of-the-art approaches on three real-world datasets."}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "adfdfd5f-2ace-4d7e-8f2f-1991cb419a2e", "node_type": "4", "metadata": {"page_label": "5", "file_name": "2405_14748v1.pdf", "Title of this paper": "MultiCast: Zero-Shot Multivariate Time Series Forecasting Using LLMs", "Authors": "Georgios Chatzigeorgakidis, Konstantinos Lentzos, Dimitrios Skoutas", "Date published": "05/23/2024", "URL": "http://arxiv.org/abs/2405.14748v1", "summary": "Predicting future values in multivariate time series is vital across various\ndomains. This work explores the use of large language models (LLMs) for this\ntask. However, LLMs typically handle one-dimensional data. We introduce\nMultiCast, a zero-shot LLM-based approach for multivariate time series\nforecasting. It allows LLMs to receive multivariate time series as input,\nthrough three novel token multiplexing solutions that effectively reduce\ndimensionality while preserving key repetitive patterns. Additionally, a\nquantization scheme helps LLMs to better learn these patterns, while\nsignificantly reducing token use for practical applications. We showcase the\nperformance of our approach in terms of RMSE and execution time against\nstate-of-the-art approaches on three real-world datasets."}, "hash": "8dea88bc7d698a5908d4cf96bae4017a4e7651d5257e0636e838a1b241bc6bea", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "fbeff2ff-4a57-4df4-aac6-cf0a659e757b", "node_type": "1", "metadata": {}, "hash": "7657606f6938dca23cde2c46713b242721b043cd83a0d697f6a0f393ea54d46a", "class_name": "RelatedNodeInfo"}}, "text": "(a) LLaMA2 GasRate prediction\n(b) Phi-2 GasRate prediction\nFig. 2. Comparison of the two models.\nC. Forecasting Accuracy\nNext, we compare the prediction precision in terms of\nRMSE of all MultiCast variants against the rest of the com-\npetitor approaches. Table IV lists the results for the Gas Rate\ndataset. To better comprehend the insights behind the results\nand acquire knowledge regarding the differences in forecasting\nability between the LLM-based models and the rest of the\ncompetition, for each dimension, we denote the first best over-\nall performance using bold font and the second best using italic\nfont. Interestingly, for the GasRate dimension, the best overall\napproach was LLMTIME ( 0.703), followed by MultiCast (DI)\nwith0.781. The LLM-based approaches all seem to cope well\nwith detecting the underlying patterns for this dimension, thus\nproducing good results. The case is different for the second\ndimension (CO2), where the conventional methods seem to\nyield a better overall performance, with ARIMA being the\nbest (2.63). MultiCast (VI) was the second-best overall and\nthe best LLM-based performer (2.71).\nTABLE IV\nFORECASTING RMSE FOR THE GASRATE DATASET .\nModelDimension\nGasRate CO2\nMultiCast (DI) 0.781 4.639\nMultiCast (VI) 1.154 2.71\nMultiCast (VC) 0.965 3.626\nLLMTIME 0.703 2.75\nARIMA 0.92 2.63\nLSTM 1.122 3.89\nFigure 3 depicts two indicative forecast outputs of the best\nMultiCast approach (DI) for the first dimension of the GasTABLE V\nFORECASTING RMSE FOR THE ELECTRICITY DATASET .\nModelDimension\nHUFL HULL OT\nMultiCast (DI) 5.914 1.444 9.198\nMultiCast (VI) 8.63 1.882 13.752\nMultiCast (VC) 2.424 1.913 10.230\nLLMTIME 4.299 1.432 7.543\nARIMA 7.063 1.572 4.181\nLSTM 4.892 1.43 8.740\nTABLE VI\nFORECASTING RMSE FOR THE WEATHER DATASET .\nModelDimension\nTlog H2OC VPmax Tpot\nMultiCast (DI) 3.711 2.43 3.025 6.888\nMultiCast (VI) 3.26 2.122 2.387 11.352\nMultiCast (VC) 4.983 3.819 5.776 5.993\nLLMTIME 3.14 1.746 4.044 6.981\nARIMA 3.324 2.686 4.331 6.067\nLSTM 3.524 1.796 2.708 5.559\nRate data set against the corresponding ARIMA result. Both\nseem to yield a good result here; MultiCast seems to properly\ndetect a continuously upward trend in the time series; however,\nthe result seems to have larger variance than that of the original\ntime series. On the other hand, the ARIMA approach does not\nclearly follow the upward trend; however, its variance seems\nto be on par with the one of the original time series.\n(a) Multicast for the gas rate dataset (GasRate).", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2481, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "fbeff2ff-4a57-4df4-aac6-cf0a659e757b": {"__data__": {"id_": "fbeff2ff-4a57-4df4-aac6-cf0a659e757b", "embedding": null, "metadata": {"page_label": "5", "file_name": "2405_14748v1.pdf", "Title of this paper": "MultiCast: Zero-Shot Multivariate Time Series Forecasting Using LLMs", "Authors": "Georgios Chatzigeorgakidis, Konstantinos Lentzos, Dimitrios Skoutas", "Date published": "05/23/2024", "URL": "http://arxiv.org/abs/2405.14748v1", "summary": "Predicting future values in multivariate time series is vital across various\ndomains. This work explores the use of large language models (LLMs) for this\ntask. However, LLMs typically handle one-dimensional data. We introduce\nMultiCast, a zero-shot LLM-based approach for multivariate time series\nforecasting. It allows LLMs to receive multivariate time series as input,\nthrough three novel token multiplexing solutions that effectively reduce\ndimensionality while preserving key repetitive patterns. Additionally, a\nquantization scheme helps LLMs to better learn these patterns, while\nsignificantly reducing token use for practical applications. We showcase the\nperformance of our approach in terms of RMSE and execution time against\nstate-of-the-art approaches on three real-world datasets."}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "adfdfd5f-2ace-4d7e-8f2f-1991cb419a2e", "node_type": "4", "metadata": {"page_label": "5", "file_name": "2405_14748v1.pdf", "Title of this paper": "MultiCast: Zero-Shot Multivariate Time Series Forecasting Using LLMs", "Authors": "Georgios Chatzigeorgakidis, Konstantinos Lentzos, Dimitrios Skoutas", "Date published": "05/23/2024", "URL": "http://arxiv.org/abs/2405.14748v1", "summary": "Predicting future values in multivariate time series is vital across various\ndomains. This work explores the use of large language models (LLMs) for this\ntask. However, LLMs typically handle one-dimensional data. We introduce\nMultiCast, a zero-shot LLM-based approach for multivariate time series\nforecasting. It allows LLMs to receive multivariate time series as input,\nthrough three novel token multiplexing solutions that effectively reduce\ndimensionality while preserving key repetitive patterns. Additionally, a\nquantization scheme helps LLMs to better learn these patterns, while\nsignificantly reducing token use for practical applications. We showcase the\nperformance of our approach in terms of RMSE and execution time against\nstate-of-the-art approaches on three real-world datasets."}, "hash": "8dea88bc7d698a5908d4cf96bae4017a4e7651d5257e0636e838a1b241bc6bea", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f20b7855-9bcb-472d-bbfd-367251c7ca03", "node_type": "1", "metadata": {"page_label": "5", "file_name": "2405_14748v1.pdf", "Title of this paper": "MultiCast: Zero-Shot Multivariate Time Series Forecasting Using LLMs", "Authors": "Georgios Chatzigeorgakidis, Konstantinos Lentzos, Dimitrios Skoutas", "Date published": "05/23/2024", "URL": "http://arxiv.org/abs/2405.14748v1", "summary": "Predicting future values in multivariate time series is vital across various\ndomains. This work explores the use of large language models (LLMs) for this\ntask. However, LLMs typically handle one-dimensional data. We introduce\nMultiCast, a zero-shot LLM-based approach for multivariate time series\nforecasting. It allows LLMs to receive multivariate time series as input,\nthrough three novel token multiplexing solutions that effectively reduce\ndimensionality while preserving key repetitive patterns. Additionally, a\nquantization scheme helps LLMs to better learn these patterns, while\nsignificantly reducing token use for practical applications. We showcase the\nperformance of our approach in terms of RMSE and execution time against\nstate-of-the-art approaches on three real-world datasets."}, "hash": "feef5641476958f227cf04d7564b69a0971e150b93a21ba796d177a233de0cea", "class_name": "RelatedNodeInfo"}}, "text": "Both\nseem to yield a good result here; MultiCast seems to properly\ndetect a continuously upward trend in the time series; however,\nthe result seems to have larger variance than that of the original\ntime series. On the other hand, the ARIMA approach does not\nclearly follow the upward trend; however, its variance seems\nto be on par with the one of the original time series.\n(a) Multicast for the gas rate dataset (GasRate).\n(b) ARIMA for the gas rate dataset (GasRate).\nFig. 3. MultiCast (DI) versus ARIMA for the GasRate dimension.\nTable V lists the results for the Electricity dataset. For the\nHUFL dimension, MultiCast (VC) seems to yield significantly\nbetter RMSE than the rest of the approaches. However, the", "mimetype": "text/plain", "start_char_idx": 2058, "end_char_idx": 2771, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "390acbed-d6fc-4a90-b144-bea4c9d268f6": {"__data__": {"id_": "390acbed-d6fc-4a90-b144-bea4c9d268f6", "embedding": null, "metadata": {"page_label": "6", "file_name": "2405_14748v1.pdf", "Title of this paper": "MultiCast: Zero-Shot Multivariate Time Series Forecasting Using LLMs", "Authors": "Georgios Chatzigeorgakidis, Konstantinos Lentzos, Dimitrios Skoutas", "Date published": "05/23/2024", "URL": "http://arxiv.org/abs/2405.14748v1", "summary": "Predicting future values in multivariate time series is vital across various\ndomains. This work explores the use of large language models (LLMs) for this\ntask. However, LLMs typically handle one-dimensional data. We introduce\nMultiCast, a zero-shot LLM-based approach for multivariate time series\nforecasting. It allows LLMs to receive multivariate time series as input,\nthrough three novel token multiplexing solutions that effectively reduce\ndimensionality while preserving key repetitive patterns. Additionally, a\nquantization scheme helps LLMs to better learn these patterns, while\nsignificantly reducing token use for practical applications. We showcase the\nperformance of our approach in terms of RMSE and execution time against\nstate-of-the-art approaches on three real-world datasets."}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a3db5c01-9a8f-451e-8696-3fabdf5058c6", "node_type": "4", "metadata": {"page_label": "6", "file_name": "2405_14748v1.pdf", "Title of this paper": "MultiCast: Zero-Shot Multivariate Time Series Forecasting Using LLMs", "Authors": "Georgios Chatzigeorgakidis, Konstantinos Lentzos, Dimitrios Skoutas", "Date published": "05/23/2024", "URL": "http://arxiv.org/abs/2405.14748v1", "summary": "Predicting future values in multivariate time series is vital across various\ndomains. This work explores the use of large language models (LLMs) for this\ntask. However, LLMs typically handle one-dimensional data. We introduce\nMultiCast, a zero-shot LLM-based approach for multivariate time series\nforecasting. It allows LLMs to receive multivariate time series as input,\nthrough three novel token multiplexing solutions that effectively reduce\ndimensionality while preserving key repetitive patterns. Additionally, a\nquantization scheme helps LLMs to better learn these patterns, while\nsignificantly reducing token use for practical applications. We showcase the\nperformance of our approach in terms of RMSE and execution time against\nstate-of-the-art approaches on three real-world datasets."}, "hash": "fe0f1759e276dbcdfb92a7ac886b67a44848f6df40f27c1e041932b561604d80", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "424424d2-e089-4003-aaea-f1ee1f9a1713", "node_type": "1", "metadata": {}, "hash": "f0d43cd2b42bc953f94049b8018e86b63e532c3a8eb2ced9924dd036add2df35", "class_name": "RelatedNodeInfo"}}, "text": "rest of the MultiCast variants do not cope as well. For\nthe HULL dimension, all approaches seem to produce good\nresults, with LLMTIME achieving the best RMSE. Finally, for\nthe OT dimension, ARIMA performs significantly better than\nthe competition. The MultiCast approaches do not perform\nwell. LLMTIME is the best among LLM-based models. This\nsuggests a possible drop in the performance of MultiCast as\nthe dimensionality of the time series increases since there is\nthe extra step of demultiplexing the input that the LLMs must\ninfer. However, the error in the best LLM-based model (9.198)\nis very close to that of the LSTM model (8.740).\nFigure 4 illustrates an indicative example of the MultiCast\n(VC) forecast output (Figure 4a) against the LSTM (Figure 4b)\nfor the HUFL dimension of the electricity data set. Clearly,\nMultiCast manages to correctly infer both the trend and\nvariance of the time series. On the other hand, the LSTM\nseems to perform rather poorly, falsely yielding a non-existent\nlinear upward trend.\n(a) Multicast for the electricity dataset (HUFL).\n(b) LSTM for the electricity dataset (HUFL).\nFig. 4. MultiCast (VC) versus LSTM for the HUFL dimension.\nThe RMSE results for the Weather dataset are listed in Ta-\nble VI. LLMTIME achieves the best performance in the Tlog\ndimension, though, all approaches except MultiCast (VC) are\nclose. This is also the case for the H2OC dimension. For the\nVPmax dimension, the best overall approach was MultiCast\n(VI), with MultiCast (VC) again performing worse than the\nrest. However, this is reversed in the Tpot dimension, where\nthe MultiCast variant (VC) yields the best performance among\nall LLM-based approaches. LSTM is the better performer\nin this dimension. Notice that the degradation in forecasting\naccuracy for more dimensions is not present in this case;\nthe MultiCast variants are all either close to, or outperform\nthe rest in all dimensions. Another key takeaway here is\nthat the optimal multiplexing method differs from dimensionto dimension and from dataset to dataset. A comprehensive\nanalysis on which dataset characteristics cause this behavior\nis an interesting future work.\nAs in the rest of the cases, an indicative example of\nMultiCast against a conventional method is illustrated in\nFigure 5. Clearly, the DI variant of MultiCast (Figure 5a)\nyields better results than ARIMA (Figure 5b) here, able to\naccurately estimate the upward trend and fluctuation at the\nend of the time series.\n(a) Multicast for the weather dataset (Tlog).\n(b) ARIMA for the weather dataset (Tlog).\nFig. 5. MultiCast (VI) versus ARIMA for the Tlog dimension.\nOverall, we notice a trade-off when using MultiCast for\nmultivariate time series forecasting, as opposed to LLMTIME.\nForecasting each dimension separately using LLMTIME will\ncompletely ignore the interdimensional correlations, which is\nnot desirable in such scenarios. On the other hand, MultiCast\nposes an additional challenge to the LLM models, which now\nhave to also infer the demultiplexing of the dimensions. Both\ncases hinder the accuracy of the obtained result. Having in\nmind the interesting aspect of emergent abilities, we argue\nthat using very large LLMs (e.g., GPT-4, Gemini) will further\nimprove MultiCast\u2019s performance.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3248, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "424424d2-e089-4003-aaea-f1ee1f9a1713": {"__data__": {"id_": "424424d2-e089-4003-aaea-f1ee1f9a1713", "embedding": null, "metadata": {"page_label": "6", "file_name": "2405_14748v1.pdf", "Title of this paper": "MultiCast: Zero-Shot Multivariate Time Series Forecasting Using LLMs", "Authors": "Georgios Chatzigeorgakidis, Konstantinos Lentzos, Dimitrios Skoutas", "Date published": "05/23/2024", "URL": "http://arxiv.org/abs/2405.14748v1", "summary": "Predicting future values in multivariate time series is vital across various\ndomains. This work explores the use of large language models (LLMs) for this\ntask. However, LLMs typically handle one-dimensional data. We introduce\nMultiCast, a zero-shot LLM-based approach for multivariate time series\nforecasting. It allows LLMs to receive multivariate time series as input,\nthrough three novel token multiplexing solutions that effectively reduce\ndimensionality while preserving key repetitive patterns. Additionally, a\nquantization scheme helps LLMs to better learn these patterns, while\nsignificantly reducing token use for practical applications. We showcase the\nperformance of our approach in terms of RMSE and execution time against\nstate-of-the-art approaches on three real-world datasets."}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a3db5c01-9a8f-451e-8696-3fabdf5058c6", "node_type": "4", "metadata": {"page_label": "6", "file_name": "2405_14748v1.pdf", "Title of this paper": "MultiCast: Zero-Shot Multivariate Time Series Forecasting Using LLMs", "Authors": "Georgios Chatzigeorgakidis, Konstantinos Lentzos, Dimitrios Skoutas", "Date published": "05/23/2024", "URL": "http://arxiv.org/abs/2405.14748v1", "summary": "Predicting future values in multivariate time series is vital across various\ndomains. This work explores the use of large language models (LLMs) for this\ntask. However, LLMs typically handle one-dimensional data. We introduce\nMultiCast, a zero-shot LLM-based approach for multivariate time series\nforecasting. It allows LLMs to receive multivariate time series as input,\nthrough three novel token multiplexing solutions that effectively reduce\ndimensionality while preserving key repetitive patterns. Additionally, a\nquantization scheme helps LLMs to better learn these patterns, while\nsignificantly reducing token use for practical applications. We showcase the\nperformance of our approach in terms of RMSE and execution time against\nstate-of-the-art approaches on three real-world datasets."}, "hash": "fe0f1759e276dbcdfb92a7ac886b67a44848f6df40f27c1e041932b561604d80", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "390acbed-d6fc-4a90-b144-bea4c9d268f6", "node_type": "1", "metadata": {"page_label": "6", "file_name": "2405_14748v1.pdf", "Title of this paper": "MultiCast: Zero-Shot Multivariate Time Series Forecasting Using LLMs", "Authors": "Georgios Chatzigeorgakidis, Konstantinos Lentzos, Dimitrios Skoutas", "Date published": "05/23/2024", "URL": "http://arxiv.org/abs/2405.14748v1", "summary": "Predicting future values in multivariate time series is vital across various\ndomains. This work explores the use of large language models (LLMs) for this\ntask. However, LLMs typically handle one-dimensional data. We introduce\nMultiCast, a zero-shot LLM-based approach for multivariate time series\nforecasting. It allows LLMs to receive multivariate time series as input,\nthrough three novel token multiplexing solutions that effectively reduce\ndimensionality while preserving key repetitive patterns. Additionally, a\nquantization scheme helps LLMs to better learn these patterns, while\nsignificantly reducing token use for practical applications. We showcase the\nperformance of our approach in terms of RMSE and execution time against\nstate-of-the-art approaches on three real-world datasets."}, "hash": "4e2635b7beeab203aa9102e8f4be40d66eecac92d38cafc1fa5fa02afe6c6320", "class_name": "RelatedNodeInfo"}}, "text": "(a) Multicast for the weather dataset (Tlog).\n(b) ARIMA for the weather dataset (Tlog).\nFig. 5. MultiCast (VI) versus ARIMA for the Tlog dimension.\nOverall, we notice a trade-off when using MultiCast for\nmultivariate time series forecasting, as opposed to LLMTIME.\nForecasting each dimension separately using LLMTIME will\ncompletely ignore the interdimensional correlations, which is\nnot desirable in such scenarios. On the other hand, MultiCast\nposes an additional challenge to the LLM models, which now\nhave to also infer the demultiplexing of the dimensions. Both\ncases hinder the accuracy of the obtained result. Having in\nmind the interesting aspect of emergent abilities, we argue\nthat using very large LLMs (e.g., GPT-4, Gemini) will further\nimprove MultiCast\u2019s performance.\nD. Increasing Number of Samples\nTable VII lists the accuracy in terms of RMSE of all LLM-\nbased models for an increasing number of samples. As a\nreminder, all LLM-based models draw several samples of the\nvalues of each timestamp, and the final estimated value is\nderived by computing the median among all samples. The\nLLMTIME approach seems to produce better results for 5 and\n10 samples. Interestingly, the error worsens for 20 samples.\nThis could be because the inherent variance of the produced\nseries tends to be averaged out as we draw more samples.", "mimetype": "text/plain", "start_char_idx": 2467, "end_char_idx": 3803, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "89301b3b-8316-49a6-a9f5-4191280a6185": {"__data__": {"id_": "89301b3b-8316-49a6-a9f5-4191280a6185", "embedding": null, "metadata": {"page_label": "7", "file_name": "2405_14748v1.pdf", "Title of this paper": "MultiCast: Zero-Shot Multivariate Time Series Forecasting Using LLMs", "Authors": "Georgios Chatzigeorgakidis, Konstantinos Lentzos, Dimitrios Skoutas", "Date published": "05/23/2024", "URL": "http://arxiv.org/abs/2405.14748v1", "summary": "Predicting future values in multivariate time series is vital across various\ndomains. This work explores the use of large language models (LLMs) for this\ntask. However, LLMs typically handle one-dimensional data. We introduce\nMultiCast, a zero-shot LLM-based approach for multivariate time series\nforecasting. It allows LLMs to receive multivariate time series as input,\nthrough three novel token multiplexing solutions that effectively reduce\ndimensionality while preserving key repetitive patterns. Additionally, a\nquantization scheme helps LLMs to better learn these patterns, while\nsignificantly reducing token use for practical applications. We showcase the\nperformance of our approach in terms of RMSE and execution time against\nstate-of-the-art approaches on three real-world datasets."}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e09d5e8b-0c88-453c-bdad-a9ae32acd092", "node_type": "4", "metadata": {"page_label": "7", "file_name": "2405_14748v1.pdf", "Title of this paper": "MultiCast: Zero-Shot Multivariate Time Series Forecasting Using LLMs", "Authors": "Georgios Chatzigeorgakidis, Konstantinos Lentzos, Dimitrios Skoutas", "Date published": "05/23/2024", "URL": "http://arxiv.org/abs/2405.14748v1", "summary": "Predicting future values in multivariate time series is vital across various\ndomains. This work explores the use of large language models (LLMs) for this\ntask. However, LLMs typically handle one-dimensional data. We introduce\nMultiCast, a zero-shot LLM-based approach for multivariate time series\nforecasting. It allows LLMs to receive multivariate time series as input,\nthrough three novel token multiplexing solutions that effectively reduce\ndimensionality while preserving key repetitive patterns. Additionally, a\nquantization scheme helps LLMs to better learn these patterns, while\nsignificantly reducing token use for practical applications. We showcase the\nperformance of our approach in terms of RMSE and execution time against\nstate-of-the-art approaches on three real-world datasets."}, "hash": "ccb33577abd70e5a37edcfbcb5bd4c722fe3b7118d72bd2c0fbea368e1436c48", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "fda47b5a-4199-4831-bb2b-373348f93cd3", "node_type": "1", "metadata": {}, "hash": "edeea5237f6a8541b1eec553b74ca1af76df12e3ccea73811824a6e4df3800ce", "class_name": "RelatedNodeInfo"}}, "text": "However, this is not the case for the MultiCast method; all\nthree approaches seem to produce better results for more\nsamples, and the MultiCast DI variant achieves the best\nperformance for 20 samples. A drawback of drawing many\nsamples is the performance deterioration in execution time\n(i.e., each execution time is listed below each corresponding\nRMSE in Table VII). Notice that, in all cases, the execution\ntime doubles when the number of samples is doubled, which\nis expected since the model must infer twice as many tokens.\nInterestingly, the LLMTIME requires slightly less total time\n(i.e., sum of time needed per dimension) than its MultiCast\ncounterparts, since the latter also need to infer the multiplex-\ning/demultiplexing of the tokens.\nTABLE VII\nPERFORMANCE FOR AN INCREASING NUMBER OF SAMPLES .\nMethodNumber of samples\n5 10 20\nMultiCast (DI)0.781\n1036 sec0.762\n2050 sec0.592\n4159 sec\nMultiCast (VI)0.965\n1041 sec1.302\n2068 sec0.877\n4131 sec\nMultiCast (VC)1.154\n1168 sec0.704\n2468 sec0.63\n4981 sec\nLLMTIME0.703\n1023 sec0.606\n1939 sec0.842\n3684 sec\nE. Performance Boost Using SAX\nNext, we will show the results obtained when quantization is\napplied using the SAX method, as described in Section III-A.\nSpecifically, we evaluate the effects of increasing the length of\nthe SAX segment and the size of the alphabet on the perfor-\nmance of zero-shot time series forecasting using MultiCast.\n1) Increasing SAX Segment length: Table VIII lists the\nresults for an increasing number of SAX segments for the\nCO2%dimension of the Gas Rate dataset, in terms of RMSE\nand execution time. We also list the results for using a different\nkind of SAX quantization; either using alphabetical characters\nor digits to encode SAX words. Compressing the time series\nsignificantly facilitates the inference process since now the\nmodel has to generate only one symbol per timestamp, instead\nof three or more. This is reflected in the execution times shown\nin Table VIII; inference after applying SAX compression is\nmore than an order of magnitude faster, from 52 seconds in the\nbest case (i.e., using 9 SAX segments) to 1168 seconds, when\nno quantization is applied. The large difference in performance\ncan have a big impact on forecasting tasks that are run on CPU,\nwhich may often be the case in scenarios where access to a\nGPU with large enough memory to fit an LLM is not possible.\nAs expected, quantizing the time series leads to a loss of\ninformation. Again, this is reflected in the RMSE scores for\nthe SAX approaches in Table VIII, which are worse than when\nno quantization is applied. However, this may not always be\nthe case; having to infer only one symbol per timestamp is\neasier for the LLM. Patterns, if they exist, will be easier to\ndetect and guess. The higher RMSE scores in these cases can\n(a) 3 SAX segments.\n(b) 6 SAX segments.\n(c) 9 SAX segments.\nFig. 6. Forecasting for various SAX segments (CO2%).\nTABLE VIII\nINCREASING SAX SEGMENT LENGTH .", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2952, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "fda47b5a-4199-4831-bb2b-373348f93cd3": {"__data__": {"id_": "fda47b5a-4199-4831-bb2b-373348f93cd3", "embedding": null, "metadata": {"page_label": "7", "file_name": "2405_14748v1.pdf", "Title of this paper": "MultiCast: Zero-Shot Multivariate Time Series Forecasting Using LLMs", "Authors": "Georgios Chatzigeorgakidis, Konstantinos Lentzos, Dimitrios Skoutas", "Date published": "05/23/2024", "URL": "http://arxiv.org/abs/2405.14748v1", "summary": "Predicting future values in multivariate time series is vital across various\ndomains. This work explores the use of large language models (LLMs) for this\ntask. However, LLMs typically handle one-dimensional data. We introduce\nMultiCast, a zero-shot LLM-based approach for multivariate time series\nforecasting. It allows LLMs to receive multivariate time series as input,\nthrough three novel token multiplexing solutions that effectively reduce\ndimensionality while preserving key repetitive patterns. Additionally, a\nquantization scheme helps LLMs to better learn these patterns, while\nsignificantly reducing token use for practical applications. We showcase the\nperformance of our approach in terms of RMSE and execution time against\nstate-of-the-art approaches on three real-world datasets."}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e09d5e8b-0c88-453c-bdad-a9ae32acd092", "node_type": "4", "metadata": {"page_label": "7", "file_name": "2405_14748v1.pdf", "Title of this paper": "MultiCast: Zero-Shot Multivariate Time Series Forecasting Using LLMs", "Authors": "Georgios Chatzigeorgakidis, Konstantinos Lentzos, Dimitrios Skoutas", "Date published": "05/23/2024", "URL": "http://arxiv.org/abs/2405.14748v1", "summary": "Predicting future values in multivariate time series is vital across various\ndomains. This work explores the use of large language models (LLMs) for this\ntask. However, LLMs typically handle one-dimensional data. We introduce\nMultiCast, a zero-shot LLM-based approach for multivariate time series\nforecasting. It allows LLMs to receive multivariate time series as input,\nthrough three novel token multiplexing solutions that effectively reduce\ndimensionality while preserving key repetitive patterns. Additionally, a\nquantization scheme helps LLMs to better learn these patterns, while\nsignificantly reducing token use for practical applications. We showcase the\nperformance of our approach in terms of RMSE and execution time against\nstate-of-the-art approaches on three real-world datasets."}, "hash": "ccb33577abd70e5a37edcfbcb5bd4c722fe3b7118d72bd2c0fbea368e1436c48", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "89301b3b-8316-49a6-a9f5-4191280a6185", "node_type": "1", "metadata": {"page_label": "7", "file_name": "2405_14748v1.pdf", "Title of this paper": "MultiCast: Zero-Shot Multivariate Time Series Forecasting Using LLMs", "Authors": "Georgios Chatzigeorgakidis, Konstantinos Lentzos, Dimitrios Skoutas", "Date published": "05/23/2024", "URL": "http://arxiv.org/abs/2405.14748v1", "summary": "Predicting future values in multivariate time series is vital across various\ndomains. This work explores the use of large language models (LLMs) for this\ntask. However, LLMs typically handle one-dimensional data. We introduce\nMultiCast, a zero-shot LLM-based approach for multivariate time series\nforecasting. It allows LLMs to receive multivariate time series as input,\nthrough three novel token multiplexing solutions that effectively reduce\ndimensionality while preserving key repetitive patterns. Additionally, a\nquantization scheme helps LLMs to better learn these patterns, while\nsignificantly reducing token use for practical applications. We showcase the\nperformance of our approach in terms of RMSE and execution time against\nstate-of-the-art approaches on three real-world datasets."}, "hash": "8c85f7167e2fb00d3b23f147c1c2aafaaadfd39a0077dc93dba45b6ef256f924", "class_name": "RelatedNodeInfo"}}, "text": "As expected, quantizing the time series leads to a loss of\ninformation. Again, this is reflected in the RMSE scores for\nthe SAX approaches in Table VIII, which are worse than when\nno quantization is applied. However, this may not always be\nthe case; having to infer only one symbol per timestamp is\neasier for the LLM. Patterns, if they exist, will be easier to\ndetect and guess. The higher RMSE scores in these cases can\n(a) 3 SAX segments.\n(b) 6 SAX segments.\n(c) 9 SAX segments.\nFig. 6. Forecasting for various SAX segments (CO2%).\nTABLE VIII\nINCREASING SAX SEGMENT LENGTH .\nMethodSAX Segment Length\n3 6 9\nMultiCast SAX (alphabetical)1.089\n148 sec0.983\n77 sec0.888\n54 sec\nMultiCast SAX (digital)0.992\n156 sec0.99\n71 sec0.912\n52 sec\nMultiCast0.781\n1168 sec\nbe attributed to the quantization that SAX applies on both axes.\nHowever, the final result, when plotted, could properly follow\nthe initial time series. This effect is illustrated in Figures 6b\nand 6c, for the CO2 %dimension of the Gas Rate dataset. On\nthe other hand, in this case, MultiCast using 3 SAX segments\nmanaged to detect the initial upward trend (Figure 6a), but the\nresult worsened afterwards.\nFigure 8 shows an indicative example of the prediction\nresult when applying SAX quantization using digits to encode\nsymbols, for the CO2 %dimension of the Gas Rate dataset. It", "mimetype": "text/plain", "start_char_idx": 2375, "end_char_idx": 3715, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f9160b0a-1b8b-4914-90c6-00e3dc2ac67f": {"__data__": {"id_": "f9160b0a-1b8b-4914-90c6-00e3dc2ac67f", "embedding": null, "metadata": {"page_label": "8", "file_name": "2405_14748v1.pdf", "Title of this paper": "MultiCast: Zero-Shot Multivariate Time Series Forecasting Using LLMs", "Authors": "Georgios Chatzigeorgakidis, Konstantinos Lentzos, Dimitrios Skoutas", "Date published": "05/23/2024", "URL": "http://arxiv.org/abs/2405.14748v1", "summary": "Predicting future values in multivariate time series is vital across various\ndomains. This work explores the use of large language models (LLMs) for this\ntask. However, LLMs typically handle one-dimensional data. We introduce\nMultiCast, a zero-shot LLM-based approach for multivariate time series\nforecasting. It allows LLMs to receive multivariate time series as input,\nthrough three novel token multiplexing solutions that effectively reduce\ndimensionality while preserving key repetitive patterns. Additionally, a\nquantization scheme helps LLMs to better learn these patterns, while\nsignificantly reducing token use for practical applications. We showcase the\nperformance of our approach in terms of RMSE and execution time against\nstate-of-the-art approaches on three real-world datasets."}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1c5d696c-ebec-42f9-bd75-c9243ef6ce94", "node_type": "4", "metadata": {"page_label": "8", "file_name": "2405_14748v1.pdf", "Title of this paper": "MultiCast: Zero-Shot Multivariate Time Series Forecasting Using LLMs", "Authors": "Georgios Chatzigeorgakidis, Konstantinos Lentzos, Dimitrios Skoutas", "Date published": "05/23/2024", "URL": "http://arxiv.org/abs/2405.14748v1", "summary": "Predicting future values in multivariate time series is vital across various\ndomains. This work explores the use of large language models (LLMs) for this\ntask. However, LLMs typically handle one-dimensional data. We introduce\nMultiCast, a zero-shot LLM-based approach for multivariate time series\nforecasting. It allows LLMs to receive multivariate time series as input,\nthrough three novel token multiplexing solutions that effectively reduce\ndimensionality while preserving key repetitive patterns. Additionally, a\nquantization scheme helps LLMs to better learn these patterns, while\nsignificantly reducing token use for practical applications. We showcase the\nperformance of our approach in terms of RMSE and execution time against\nstate-of-the-art approaches on three real-world datasets."}, "hash": "72d723a24279b84debbb5581684b8a3ce7055588cc46a2288e9f7e8c8b4ec82e", "class_name": "RelatedNodeInfo"}}, "text": "(a) 5 SAX symbols.\n(b) 10 SAX symbols.\n(c) 20 SAX symbols.\nFig. 7. Forecasting for different SAX alphabet sizes (CO2%).\nFig. 8. Forecasting using digits instead of letters as symbols.\nis easily noticeable that the resulting line (red in the figure)\nclosely follows the initial time series in this dimension. This\ncould suggest that it may be easier for the LLMs to detect\npatterns in time series represented by numbers rather than\nalphabetical characters.2) Increasing SAX Alphabet Size: In the following, we\nevaluate the performance of MultiCast, in terms of RMSE\nand execution time, when increasing the size of the SAX\nalphabet. Table IX lists the results. We should note that for\ndigits we can only go up to an alphabet of size 10. Again, the\nnon-quantized MultiCast yields better performance in terms of\nRMSE, but is significantly slower. Increasing the size of the\nalphabet does not seem to affect the execution time. Also, in\nterms of RMSE, larger alphabet sizes produced higher errors,\npossibly due to the increase in complexity that the use of more\nsymbols induces.\nTABLE IX\nINCREASING SAX ALPHABET SIZE .\nMethodSAX Alphabet Size\n5 10 20\nMultiCast SAX (alphabetical)0.983\n77 sec1.198\n81 sec1.273\n83 sec\nMultiCast SAX (digital)0.99\n71 sec1.21\n75 secN/A\nMultiCast0.781\n1168 sec\nFinally, Figure 7 shows an indicative forecasting example\nfor 5, 10 and 20 SAX symbols for the CO2 %dimension of the\nGas Rate dataset. The drop in RMSE scores is also reflected\nhere; only in the case of using five symbols does the forecasted\ntime series follow the trend of the original.\nV. C ONCLUSIONS\nIn this paper, we presented MultiCast, an approach that\nleverages LLMs for zero-shot multivariate time series fore-\ncasting. To make this model work with multiple dimensions,\nwe proposed three token multiplexing solutions that reduce\nthe dimensionality of the time series to one. This allows\nthe time series to be compatible with the input of an LLM,\nwhile still preserving its ability to detect repetitive patterns.\nAdditionally, we presented a quantization solution that aims\nto facilitate the learning of existing patterns in the series by\nLLMs. This solution also significantly reduces the execution\ntime. In our comprehensive experimental analysis using three\nreal-world datasets, we found that the use of LLMs for\nmultivariate zero-shot time series forecasting shows promise\nand offers a significant advantage compared to other similar\nmethods available in the literature: No expert knowledge or\ntime and resource-consuming parameter search processes are\nrequired . In the future, we plan to expand our research on\nemploying LLMs for zero-shot solutions on other similar time\nseries-related tasks, such as imputation, anomaly detection,\nand change point detection, as well as evaluate MultiCast\u2019s\ninference performance using more LLMs as back-end models\nand further improving it in more dimensions.\nACKNOWLEDGMENT\nThis work was partially funded by the EU Horizon Europe\nprojects STELAR (101070122) and DT4GS (101056799).", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3014, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "90084695-289f-4605-9d3d-edef11b81eed": {"__data__": {"id_": "90084695-289f-4605-9d3d-edef11b81eed", "embedding": null, "metadata": {"page_label": "9", "file_name": "2405_14748v1.pdf", "Title of this paper": "MultiCast: Zero-Shot Multivariate Time Series Forecasting Using LLMs", "Authors": "Georgios Chatzigeorgakidis, Konstantinos Lentzos, Dimitrios Skoutas", "Date published": "05/23/2024", "URL": "http://arxiv.org/abs/2405.14748v1", "summary": "Predicting future values in multivariate time series is vital across various\ndomains. This work explores the use of large language models (LLMs) for this\ntask. However, LLMs typically handle one-dimensional data. We introduce\nMultiCast, a zero-shot LLM-based approach for multivariate time series\nforecasting. It allows LLMs to receive multivariate time series as input,\nthrough three novel token multiplexing solutions that effectively reduce\ndimensionality while preserving key repetitive patterns. Additionally, a\nquantization scheme helps LLMs to better learn these patterns, while\nsignificantly reducing token use for practical applications. We showcase the\nperformance of our approach in terms of RMSE and execution time against\nstate-of-the-art approaches on three real-world datasets."}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "54339c96-3d24-48c6-ad67-22b6a7fa8e82", "node_type": "4", "metadata": {"page_label": "9", "file_name": "2405_14748v1.pdf", "Title of this paper": "MultiCast: Zero-Shot Multivariate Time Series Forecasting Using LLMs", "Authors": "Georgios Chatzigeorgakidis, Konstantinos Lentzos, Dimitrios Skoutas", "Date published": "05/23/2024", "URL": "http://arxiv.org/abs/2405.14748v1", "summary": "Predicting future values in multivariate time series is vital across various\ndomains. This work explores the use of large language models (LLMs) for this\ntask. However, LLMs typically handle one-dimensional data. We introduce\nMultiCast, a zero-shot LLM-based approach for multivariate time series\nforecasting. It allows LLMs to receive multivariate time series as input,\nthrough three novel token multiplexing solutions that effectively reduce\ndimensionality while preserving key repetitive patterns. Additionally, a\nquantization scheme helps LLMs to better learn these patterns, while\nsignificantly reducing token use for practical applications. We showcase the\nperformance of our approach in terms of RMSE and execution time against\nstate-of-the-art approaches on three real-world datasets."}, "hash": "46a69ffff59fc1e9f4ac7bfac5e5c23578894b130b1025151d7649e14ebbd0a0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "79acdc70-bf45-4526-b97e-293029310334", "node_type": "1", "metadata": {}, "hash": "4227aa6e48a681ff2ff063c585a0e053c94b5a4ceaca0a680dac5f7983235af2", "class_name": "RelatedNodeInfo"}}, "text": "REFERENCES\n[1] R. H. Shumway, D. S. Stoffer, and D. S. Stoffer, Time series analysis\nand its applications . Springer, 2000, vol. 3.\n[2] A. Zeng, M. Chen, L. Zhang, and Q. Xu, \u201cAre transformers effective\nfor time series forecasting?\u201d Proceedings of the AAAI Conference on\nArtificial Intelligence , vol. 37, no. 9, pp. 11 121\u201311 128, Jun. 2023.\n[3] Z. Liu, Z. Zhu, J. Gao, and C. Xu, \u201cForecast methods for time series\ndata: A survey,\u201d IEEE Access , vol. 9, pp. 91 896\u201391 912, 2021.\n[4] J. G. De Gooijer and R. J. Hyndman, \u201c25 years of time series forecast-\ning,\u201d International Journal of Forecasting , vol. 22, no. 3, pp. 443\u2013473,\n2006.\n[5] J. Kuvulmaz, S. Usanmaz, and S. N. Engin, \u201cTime-series forecasting by\nmeans of linear and nonlinear models,\u201d in MICAI 2005: Advances in\nArtificial Intelligence , A. Gelbukh, \u00b4A. de Albornoz, and H. Terashima-\nMar\u00b4\u0131n, Eds. Berlin, Heidelberg: Springer Berlin Heidelberg, 2005, pp.\n504\u2013513.\n[6] C. Cheng, A. Sa-Ngasoongsong, O. Beyca, T. Le, H. Yang, Z. Kong,\nand S. Bukkapatnam, \u201cTime series forecasting for nonlinear and non-\nstationary processes: a review and comparative study,\u201d IIE Transactions ,\nvol. 47, no. 10, pp. 1053\u20131071, 2015.\n[7] G. E. Box, G. M. Jenkins, G. C. Reinsel, and G. M. Ljung, Time series\nanalysis: forecasting and control . John Wiley & Sons, 2015.\n[8] K. e. a. Benidis, \u201cDeep learning for time series forecasting: Tutorial\nand literature survey,\u201d ACM Comput. Surv. , vol. 55, no. 6, dec 2022.\n[Online]. Available: https://doi.org/10.1145/3533382\n[9] J. F. Torres, D. Hadjout, A. Sebaa, F. Mart \u00b4\u0131nez- \u00b4Alvarez, and A. Troncoso,\n\u201cDeep learning for time series forecasting: A survey,\u201d Big Data , vol. 9,\nno. 1, pp. 3\u201321, 2021.\n[10] A. Mahmoud and A. Mohammed, A Survey on Deep Learning for Time-\nSeries Forecasting . Springer International Publishing, 2021, pp. 365\u2013\n392.\n[11] S. Du, T. Li, Y . Yang, and S.-J. Horng, \u201cMultivariate time series forecast-\ning via attention-based encoder\u2013decoder framework,\u201d Neurocomputing ,\nvol. 388, pp. 269\u2013279, 2020.\n[12] J. Jiang, Y . Shu, J. Wang, and M. Long, \u201cTransferability in deep learning:\nA survey,\u201d 2022.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2111, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "79acdc70-bf45-4526-b97e-293029310334": {"__data__": {"id_": "79acdc70-bf45-4526-b97e-293029310334", "embedding": null, "metadata": {"page_label": "9", "file_name": "2405_14748v1.pdf", "Title of this paper": "MultiCast: Zero-Shot Multivariate Time Series Forecasting Using LLMs", "Authors": "Georgios Chatzigeorgakidis, Konstantinos Lentzos, Dimitrios Skoutas", "Date published": "05/23/2024", "URL": "http://arxiv.org/abs/2405.14748v1", "summary": "Predicting future values in multivariate time series is vital across various\ndomains. This work explores the use of large language models (LLMs) for this\ntask. However, LLMs typically handle one-dimensional data. We introduce\nMultiCast, a zero-shot LLM-based approach for multivariate time series\nforecasting. It allows LLMs to receive multivariate time series as input,\nthrough three novel token multiplexing solutions that effectively reduce\ndimensionality while preserving key repetitive patterns. Additionally, a\nquantization scheme helps LLMs to better learn these patterns, while\nsignificantly reducing token use for practical applications. We showcase the\nperformance of our approach in terms of RMSE and execution time against\nstate-of-the-art approaches on three real-world datasets."}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "54339c96-3d24-48c6-ad67-22b6a7fa8e82", "node_type": "4", "metadata": {"page_label": "9", "file_name": "2405_14748v1.pdf", "Title of this paper": "MultiCast: Zero-Shot Multivariate Time Series Forecasting Using LLMs", "Authors": "Georgios Chatzigeorgakidis, Konstantinos Lentzos, Dimitrios Skoutas", "Date published": "05/23/2024", "URL": "http://arxiv.org/abs/2405.14748v1", "summary": "Predicting future values in multivariate time series is vital across various\ndomains. This work explores the use of large language models (LLMs) for this\ntask. However, LLMs typically handle one-dimensional data. We introduce\nMultiCast, a zero-shot LLM-based approach for multivariate time series\nforecasting. It allows LLMs to receive multivariate time series as input,\nthrough three novel token multiplexing solutions that effectively reduce\ndimensionality while preserving key repetitive patterns. Additionally, a\nquantization scheme helps LLMs to better learn these patterns, while\nsignificantly reducing token use for practical applications. We showcase the\nperformance of our approach in terms of RMSE and execution time against\nstate-of-the-art approaches on three real-world datasets."}, "hash": "46a69ffff59fc1e9f4ac7bfac5e5c23578894b130b1025151d7649e14ebbd0a0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "90084695-289f-4605-9d3d-edef11b81eed", "node_type": "1", "metadata": {"page_label": "9", "file_name": "2405_14748v1.pdf", "Title of this paper": "MultiCast: Zero-Shot Multivariate Time Series Forecasting Using LLMs", "Authors": "Georgios Chatzigeorgakidis, Konstantinos Lentzos, Dimitrios Skoutas", "Date published": "05/23/2024", "URL": "http://arxiv.org/abs/2405.14748v1", "summary": "Predicting future values in multivariate time series is vital across various\ndomains. This work explores the use of large language models (LLMs) for this\ntask. However, LLMs typically handle one-dimensional data. We introduce\nMultiCast, a zero-shot LLM-based approach for multivariate time series\nforecasting. It allows LLMs to receive multivariate time series as input,\nthrough three novel token multiplexing solutions that effectively reduce\ndimensionality while preserving key repetitive patterns. Additionally, a\nquantization scheme helps LLMs to better learn these patterns, while\nsignificantly reducing token use for practical applications. We showcase the\nperformance of our approach in terms of RMSE and execution time against\nstate-of-the-art approaches on three real-world datasets."}, "hash": "c757eb5a442aa10626a8901de26ca810ab308f276edea7295d8e1aa76f30d631", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7228c0de-e674-4cbe-a94c-7ffee87d1438", "node_type": "1", "metadata": {}, "hash": "d3cf41fc7828336b4966302a8767f0edcb53796e87cff4859957a476e7f2523d", "class_name": "RelatedNodeInfo"}}, "text": "9,\nno. 1, pp. 3\u201321, 2021.\n[10] A. Mahmoud and A. Mohammed, A Survey on Deep Learning for Time-\nSeries Forecasting . Springer International Publishing, 2021, pp. 365\u2013\n392.\n[11] S. Du, T. Li, Y . Yang, and S.-J. Horng, \u201cMultivariate time series forecast-\ning via attention-based encoder\u2013decoder framework,\u201d Neurocomputing ,\nvol. 388, pp. 269\u2013279, 2020.\n[12] J. Jiang, Y . Shu, J. Wang, and M. Long, \u201cTransferability in deep learning:\nA survey,\u201d 2022.\n[13] W. X. Zhao, K. Zhou, J. Li, T. Tang, X. Wang, Y . Hou, Y . Min, B. Zhang,\nJ. Zhang, Z. Dong et al. , \u201cA survey of large language models,\u201d arXiv\npreprint arXiv:2303.18223 , 2023.\n[14] J. Wei, Y . Tay, R. Bommasani, C. Raffel, B. Zoph, S. Borgeaud,\nD. Yogatama, M. Bosma, D. Zhou, D. Metzler et al. , \u201cEmergent abilities\nof large language models,\u201d arXiv preprint arXiv:2206.07682 , 2022.\n[15] N. Gruver, M. Finzi, S. Qiu, and A. G. Wilson, \u201cLarge language models\nare zero-shot time series forecasters,\u201d in NeurIPS , 2023.\n[16] C. Chang, W.-C. Peng, and T.-F. Chen, \u201cLlm4ts: Two-stage fine-\ntuning for time-series forecasting with pre-trained llms,\u201d arXiv preprint\narXiv:2308.08469 , 2023.\n[17] H. Xue and F. D. Salim, \u201cPromptcast: A new prompt-based learning\nparadigm for time series forecasting,\u201d IEEE Transactions on Knowledge\nand Data Engineering , 2023.\n[18] P. Schmiedmayer, A. Rao, P. Zagar, V . Ravi, A. Zahedivash, A. Ferey-\ndooni, and O. Aalami, \u201cLlm on fhir\u2013demystifying health records,\u201d arXiv\npreprint arXiv:2402.01711 , 2024.\n[19] A. J. Nashwan, A. A. AbuJaber, and A. AbuJaber, \u201cHarnessing the\npower of large language models (llms) for electronic health records\n(ehrs) optimization,\u201d Cureus , vol. 15, no. 7, 2023.\n[20] M. Wornow, A. Lozano, D. Dash, J. Jindal, K. W. Mahaffey, and\nN. H. Shah, \u201cZero-shot clinical trial patient matching with llms,\u201d arXiv\npreprint arXiv:2402.05125 , 2024.\n[21] Y . Li, S. Wang, H. Ding, and H. Chen, \u201cLarge language models in\nfinance: A survey,\u201d in Proceedings of the Fourth ACM International\nConference on AI in Finance , 2023, pp. 374\u2013382.\n[22] H. Yang, X.-Y .", "mimetype": "text/plain", "start_char_idx": 1663, "end_char_idx": 3723, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7228c0de-e674-4cbe-a94c-7ffee87d1438": {"__data__": {"id_": "7228c0de-e674-4cbe-a94c-7ffee87d1438", "embedding": null, "metadata": {"page_label": "9", "file_name": "2405_14748v1.pdf", "Title of this paper": "MultiCast: Zero-Shot Multivariate Time Series Forecasting Using LLMs", "Authors": "Georgios Chatzigeorgakidis, Konstantinos Lentzos, Dimitrios Skoutas", "Date published": "05/23/2024", "URL": "http://arxiv.org/abs/2405.14748v1", "summary": "Predicting future values in multivariate time series is vital across various\ndomains. This work explores the use of large language models (LLMs) for this\ntask. However, LLMs typically handle one-dimensional data. We introduce\nMultiCast, a zero-shot LLM-based approach for multivariate time series\nforecasting. It allows LLMs to receive multivariate time series as input,\nthrough three novel token multiplexing solutions that effectively reduce\ndimensionality while preserving key repetitive patterns. Additionally, a\nquantization scheme helps LLMs to better learn these patterns, while\nsignificantly reducing token use for practical applications. We showcase the\nperformance of our approach in terms of RMSE and execution time against\nstate-of-the-art approaches on three real-world datasets."}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "54339c96-3d24-48c6-ad67-22b6a7fa8e82", "node_type": "4", "metadata": {"page_label": "9", "file_name": "2405_14748v1.pdf", "Title of this paper": "MultiCast: Zero-Shot Multivariate Time Series Forecasting Using LLMs", "Authors": "Georgios Chatzigeorgakidis, Konstantinos Lentzos, Dimitrios Skoutas", "Date published": "05/23/2024", "URL": "http://arxiv.org/abs/2405.14748v1", "summary": "Predicting future values in multivariate time series is vital across various\ndomains. This work explores the use of large language models (LLMs) for this\ntask. However, LLMs typically handle one-dimensional data. We introduce\nMultiCast, a zero-shot LLM-based approach for multivariate time series\nforecasting. It allows LLMs to receive multivariate time series as input,\nthrough three novel token multiplexing solutions that effectively reduce\ndimensionality while preserving key repetitive patterns. Additionally, a\nquantization scheme helps LLMs to better learn these patterns, while\nsignificantly reducing token use for practical applications. We showcase the\nperformance of our approach in terms of RMSE and execution time against\nstate-of-the-art approaches on three real-world datasets."}, "hash": "46a69ffff59fc1e9f4ac7bfac5e5c23578894b130b1025151d7649e14ebbd0a0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "79acdc70-bf45-4526-b97e-293029310334", "node_type": "1", "metadata": {"page_label": "9", "file_name": "2405_14748v1.pdf", "Title of this paper": "MultiCast: Zero-Shot Multivariate Time Series Forecasting Using LLMs", "Authors": "Georgios Chatzigeorgakidis, Konstantinos Lentzos, Dimitrios Skoutas", "Date published": "05/23/2024", "URL": "http://arxiv.org/abs/2405.14748v1", "summary": "Predicting future values in multivariate time series is vital across various\ndomains. This work explores the use of large language models (LLMs) for this\ntask. However, LLMs typically handle one-dimensional data. We introduce\nMultiCast, a zero-shot LLM-based approach for multivariate time series\nforecasting. It allows LLMs to receive multivariate time series as input,\nthrough three novel token multiplexing solutions that effectively reduce\ndimensionality while preserving key repetitive patterns. Additionally, a\nquantization scheme helps LLMs to better learn these patterns, while\nsignificantly reducing token use for practical applications. We showcase the\nperformance of our approach in terms of RMSE and execution time against\nstate-of-the-art approaches on three real-world datasets."}, "hash": "965626718aa15e8c125dd96106b269d3ee2046b9a65181622be3d56a31033ef1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3cfbb976-af02-4be6-ae65-91cd9345fb81", "node_type": "1", "metadata": {}, "hash": "97ecf4634cd49d34410d0041f9835072c960b0f39d857e39c000ce2bb33008e2", "class_name": "RelatedNodeInfo"}}, "text": "15, no. 7, 2023.\n[20] M. Wornow, A. Lozano, D. Dash, J. Jindal, K. W. Mahaffey, and\nN. H. Shah, \u201cZero-shot clinical trial patient matching with llms,\u201d arXiv\npreprint arXiv:2402.05125 , 2024.\n[21] Y . Li, S. Wang, H. Ding, and H. Chen, \u201cLarge language models in\nfinance: A survey,\u201d in Proceedings of the Fourth ACM International\nConference on AI in Finance , 2023, pp. 374\u2013382.\n[22] H. Yang, X.-Y . Liu, and C. D. Wang, \u201cFingpt: Open-source financial\nlarge language models,\u201d arXiv preprint arXiv:2306.06031 , 2023.\n[23] H. Zhao, Z. Liu, Z. Wu, Y . Li, T. Yang, P. Shu, S. Xu, H. Dai, L. Zhao,\nG. Mai et al. , \u201cRevolutionizing finance with llms: An overview of\napplications and insights,\u201d arXiv preprint arXiv:2401.11641 , 2024.\n[24] M. Hosseini, C. A. Gao, D. M. Liebovitz, A. M. Carvalho, F. S. Ahmad,\nY . Luo, N. MacDonald, K. L. Holmes, and A. Kho, \u201cAn exploratory\nsurvey about using chatgpt in education, healthcare, and research,\u201d\nmedRxiv , pp. 2023\u201303, 2023.[25] S. Moore, R. Tong, A. Singh, Z. Liu, X. Hu, Y . Lu, J. Liang, C. Cao,\nH. Khosravi, P. Denny et al. , \u201cEmpowering education with llms-the\nnext-gen interface and content generation,\u201d in International Conference\non Artificial Intelligence in Education . Springer, 2023, pp. 32\u201337.\n[26] Y . Jiang, Z. Pan, X. Zhang, S. Garg, A. Schneider, Y . Nevmyvaka, and\nD. Song, \u201cEmpowering time series analysis with large language models:\nA survey,\u201d 2024.\n[27] X. Zhang, R. R. Chowdhury, R. K. Gupta, and J. Shang, \u201cLarge language\nmodels for time series: A survey,\u201d 2024.\n[28] M. Jin, S. Wang, L. Ma, Z. Chu, J. Y . Zhang, X. Shi, P.-Y . Chen,\nY . Liang, Y .-F. Li, S. Pan, and Q. Wen, \u201cTime-LLM: Time series\nforecasting by reprogramming large language models,\u201d in The Twelfth\nInternational Conference on Learning Representations , 2024. [Online].\nAvailable: https://openreview.net/forum?id=Unb5CVPtae\n[29] J. Shieh and E. J. Keogh, \u201c iSAX: indexing and mining terabyte sized\ntime series,\u201d in SIGKDD , 2008, pp. 623\u2013631.", "mimetype": "text/plain", "start_char_idx": 3326, "end_char_idx": 5299, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3cfbb976-af02-4be6-ae65-91cd9345fb81": {"__data__": {"id_": "3cfbb976-af02-4be6-ae65-91cd9345fb81", "embedding": null, "metadata": {"page_label": "9", "file_name": "2405_14748v1.pdf", "Title of this paper": "MultiCast: Zero-Shot Multivariate Time Series Forecasting Using LLMs", "Authors": "Georgios Chatzigeorgakidis, Konstantinos Lentzos, Dimitrios Skoutas", "Date published": "05/23/2024", "URL": "http://arxiv.org/abs/2405.14748v1", "summary": "Predicting future values in multivariate time series is vital across various\ndomains. This work explores the use of large language models (LLMs) for this\ntask. However, LLMs typically handle one-dimensional data. We introduce\nMultiCast, a zero-shot LLM-based approach for multivariate time series\nforecasting. It allows LLMs to receive multivariate time series as input,\nthrough three novel token multiplexing solutions that effectively reduce\ndimensionality while preserving key repetitive patterns. Additionally, a\nquantization scheme helps LLMs to better learn these patterns, while\nsignificantly reducing token use for practical applications. We showcase the\nperformance of our approach in terms of RMSE and execution time against\nstate-of-the-art approaches on three real-world datasets."}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "54339c96-3d24-48c6-ad67-22b6a7fa8e82", "node_type": "4", "metadata": {"page_label": "9", "file_name": "2405_14748v1.pdf", "Title of this paper": "MultiCast: Zero-Shot Multivariate Time Series Forecasting Using LLMs", "Authors": "Georgios Chatzigeorgakidis, Konstantinos Lentzos, Dimitrios Skoutas", "Date published": "05/23/2024", "URL": "http://arxiv.org/abs/2405.14748v1", "summary": "Predicting future values in multivariate time series is vital across various\ndomains. This work explores the use of large language models (LLMs) for this\ntask. However, LLMs typically handle one-dimensional data. We introduce\nMultiCast, a zero-shot LLM-based approach for multivariate time series\nforecasting. It allows LLMs to receive multivariate time series as input,\nthrough three novel token multiplexing solutions that effectively reduce\ndimensionality while preserving key repetitive patterns. Additionally, a\nquantization scheme helps LLMs to better learn these patterns, while\nsignificantly reducing token use for practical applications. We showcase the\nperformance of our approach in terms of RMSE and execution time against\nstate-of-the-art approaches on three real-world datasets."}, "hash": "46a69ffff59fc1e9f4ac7bfac5e5c23578894b130b1025151d7649e14ebbd0a0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7228c0de-e674-4cbe-a94c-7ffee87d1438", "node_type": "1", "metadata": {"page_label": "9", "file_name": "2405_14748v1.pdf", "Title of this paper": "MultiCast: Zero-Shot Multivariate Time Series Forecasting Using LLMs", "Authors": "Georgios Chatzigeorgakidis, Konstantinos Lentzos, Dimitrios Skoutas", "Date published": "05/23/2024", "URL": "http://arxiv.org/abs/2405.14748v1", "summary": "Predicting future values in multivariate time series is vital across various\ndomains. This work explores the use of large language models (LLMs) for this\ntask. However, LLMs typically handle one-dimensional data. We introduce\nMultiCast, a zero-shot LLM-based approach for multivariate time series\nforecasting. It allows LLMs to receive multivariate time series as input,\nthrough three novel token multiplexing solutions that effectively reduce\ndimensionality while preserving key repetitive patterns. Additionally, a\nquantization scheme helps LLMs to better learn these patterns, while\nsignificantly reducing token use for practical applications. We showcase the\nperformance of our approach in terms of RMSE and execution time against\nstate-of-the-art approaches on three real-world datasets."}, "hash": "5b4280b57ea95f8d4802569878b7644e5d72a6f5eddd62a7c78c3656fa260351", "class_name": "RelatedNodeInfo"}}, "text": "[28] M. Jin, S. Wang, L. Ma, Z. Chu, J. Y . Zhang, X. Shi, P.-Y . Chen,\nY . Liang, Y .-F. Li, S. Pan, and Q. Wen, \u201cTime-LLM: Time series\nforecasting by reprogramming large language models,\u201d in The Twelfth\nInternational Conference on Learning Representations , 2024. [Online].\nAvailable: https://openreview.net/forum?id=Unb5CVPtae\n[29] J. Shieh and E. J. Keogh, \u201c iSAX: indexing and mining terabyte sized\ntime series,\u201d in SIGKDD , 2008, pp. 623\u2013631.\n[30] E. J. Keogh, K. Chakrabarti, M. J. Pazzani, and S. Mehrotra, \u201cDi-\nmensionality reduction for fast similarity search in large time series\ndatabases,\u201d Knowl. Inf. Syst. , vol. 3, no. 3, pp. 263\u2013286, 2001.\n[31] B. Yi and C. Faloutsos, \u201cFast time sequence indexing for arbitrary Lp\nnorms,\u201d in VLDB , 2000, pp. 385\u2013394.\n[32] S. Hochreiter and J. Schmidhuber, \u201cLong short-term memory,\u201d Neural\ncomputation , vol. 9, pp. 1735\u201380, 12 1997.\n[33] S. Alhirmizy and B. Qader, \u201cMultivariate time series forecasting with\nlstm for madrid, spain pollution,\u201d in 2019 international conference on\ncomputing and information science and technology and their applica-\ntions (ICCISTA) . IEEE, 2019, pp. 1\u20135.\n[34] J. Ju and F.-A. Liu, \u201cMultivariate time series data prediction based on\natt-lstm network,\u201d Applied sciences , vol. 11, no. 20, p. 9373, 2021.\n[35] D. P. Kingma and J. Ba, \u201cAdam: A method for stochastic optimization,\u201d\narXiv preprint arXiv:1412.6980 , 2014.\n[36] M. Javaheripi, S. Bubeck, M. Abdin, J. Aneja, S. Bubeck, C. C. T.\nMendes, W. Chen, A. Del Giorno, R. Eldan, S. Gopi et al. , \u201cPhi-2: The\nsurprising power of small language models,\u201d 2023.", "mimetype": "text/plain", "start_char_idx": 4851, "end_char_idx": 6441, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}}, "docstore/metadata": {"c1428036-c081-4445-b8c8-7afd1632f30e": {"doc_hash": "639a88323ff16b5a53d7bfe898d9c9aa20db79feeb0dd898550a82d61a3b74d6", "ref_doc_id": "d48e1395-7828-4f3b-8f36-b268756432e7"}, "5235a6f0-41d6-43ab-a2f1-091a334bf8ac": {"doc_hash": "54c0ab95ab68ceb4d16f5d31d9f8abba1c5bc1d28f7fee999e681069943bda76", "ref_doc_id": "d48e1395-7828-4f3b-8f36-b268756432e7"}, "1930c7ba-fcd1-4f0d-8ff3-5cadb253b660": {"doc_hash": "e2993ac430c821628ee33bcd71c448927cd72f022e62cfb10be39481041a8a4d", "ref_doc_id": "19a31078-f89d-49dc-adb7-173adbb44952"}, "c11d69b3-3f61-4c01-bcc9-cd95e1984e8c": {"doc_hash": "ca79418fa5ed63ab2708db6000bd0fff52c4bac12fc8659da4f6c564fd18d798", "ref_doc_id": "19a31078-f89d-49dc-adb7-173adbb44952"}, "b91a5516-6a4f-403b-91bb-9b8840ee856f": {"doc_hash": "243415512e10611c5f9b00f9585c96ef7a36636f2ba8743de8576ca54b91e783", "ref_doc_id": "19a31078-f89d-49dc-adb7-173adbb44952"}, "6e49a334-f59e-40e0-8d79-76da823feb78": {"doc_hash": "65ca6397fa4c7bd29c88bc672d09110e257a3df6f20a86c989cd6f2ff449538d", "ref_doc_id": "732108b4-c6bc-432e-8958-5d8f781f7957"}, "d309a49d-ac36-4180-b02b-5fe1d8ba350d": {"doc_hash": "ad0eac94128f74d29432f181f9a3bc87b1c5e4257bf1ed10db7fb717693fdffb", "ref_doc_id": "732108b4-c6bc-432e-8958-5d8f781f7957"}, "8bbabe7e-3006-48a4-9f49-508c7470fd99": {"doc_hash": "0525d232c8f797e92dde763496e4c9138428d636f69ef54dd97acc3d94a868b8", "ref_doc_id": "732108b4-c6bc-432e-8958-5d8f781f7957"}, "31e24330-1adb-4395-9de1-5fdbd7970ecd": {"doc_hash": "3d0bccbce765b9a2f22bcbf7200652b9fc8fb2e48b94e803ba4a7dafdd3d8c15", "ref_doc_id": "0865d4e7-44c6-48fc-9fca-a5db256d411b"}, "118f70fc-e5b2-420f-8883-394407df9f8b": {"doc_hash": "a12ca511960e2a144d93303a6d41f45087288d91f90bbe388fee5acb9a457c14", "ref_doc_id": "0865d4e7-44c6-48fc-9fca-a5db256d411b"}, "f20b7855-9bcb-472d-bbfd-367251c7ca03": {"doc_hash": "feef5641476958f227cf04d7564b69a0971e150b93a21ba796d177a233de0cea", "ref_doc_id": "adfdfd5f-2ace-4d7e-8f2f-1991cb419a2e"}, "fbeff2ff-4a57-4df4-aac6-cf0a659e757b": {"doc_hash": "f96335cc2f3af794ec4356629f73d2c385528bd1c29c3a23e885444b6395f050", "ref_doc_id": "adfdfd5f-2ace-4d7e-8f2f-1991cb419a2e"}, "390acbed-d6fc-4a90-b144-bea4c9d268f6": {"doc_hash": "4e2635b7beeab203aa9102e8f4be40d66eecac92d38cafc1fa5fa02afe6c6320", "ref_doc_id": "a3db5c01-9a8f-451e-8696-3fabdf5058c6"}, "424424d2-e089-4003-aaea-f1ee1f9a1713": {"doc_hash": "37e9006e78fe75d33b6ecaed6d4fd144c08f63967b9ce5cd91fbdf1787121b1f", "ref_doc_id": "a3db5c01-9a8f-451e-8696-3fabdf5058c6"}, "89301b3b-8316-49a6-a9f5-4191280a6185": {"doc_hash": "8c85f7167e2fb00d3b23f147c1c2aafaaadfd39a0077dc93dba45b6ef256f924", "ref_doc_id": "e09d5e8b-0c88-453c-bdad-a9ae32acd092"}, "fda47b5a-4199-4831-bb2b-373348f93cd3": {"doc_hash": "14e2f4cabed8e91010e6993f5dc5d067f5cc9c3df09683654a2ffd6e9932ce78", "ref_doc_id": "e09d5e8b-0c88-453c-bdad-a9ae32acd092"}, "f9160b0a-1b8b-4914-90c6-00e3dc2ac67f": {"doc_hash": "72d723a24279b84debbb5581684b8a3ce7055588cc46a2288e9f7e8c8b4ec82e", "ref_doc_id": "1c5d696c-ebec-42f9-bd75-c9243ef6ce94"}, "90084695-289f-4605-9d3d-edef11b81eed": {"doc_hash": "c757eb5a442aa10626a8901de26ca810ab308f276edea7295d8e1aa76f30d631", "ref_doc_id": "54339c96-3d24-48c6-ad67-22b6a7fa8e82"}, "79acdc70-bf45-4526-b97e-293029310334": {"doc_hash": "965626718aa15e8c125dd96106b269d3ee2046b9a65181622be3d56a31033ef1", "ref_doc_id": "54339c96-3d24-48c6-ad67-22b6a7fa8e82"}, "7228c0de-e674-4cbe-a94c-7ffee87d1438": {"doc_hash": "5b4280b57ea95f8d4802569878b7644e5d72a6f5eddd62a7c78c3656fa260351", "ref_doc_id": "54339c96-3d24-48c6-ad67-22b6a7fa8e82"}, "3cfbb976-af02-4be6-ae65-91cd9345fb81": {"doc_hash": "f5188a6baa9ad611727bfaa8f0ae4036896e10062c9613ed4bcde439fa847982", "ref_doc_id": "54339c96-3d24-48c6-ad67-22b6a7fa8e82"}}, "docstore/ref_doc_info": {"d48e1395-7828-4f3b-8f36-b268756432e7": {"node_ids": ["c1428036-c081-4445-b8c8-7afd1632f30e", "5235a6f0-41d6-43ab-a2f1-091a334bf8ac"], "metadata": {"page_label": "1", "file_name": "2405_14748v1.pdf", "Title of this paper": "MultiCast: Zero-Shot Multivariate Time Series Forecasting Using LLMs", "Authors": "Georgios Chatzigeorgakidis, Konstantinos Lentzos, Dimitrios Skoutas", "Date published": "05/23/2024", "URL": "http://arxiv.org/abs/2405.14748v1", "summary": "Predicting future values in multivariate time series is vital across various\ndomains. This work explores the use of large language models (LLMs) for this\ntask. However, LLMs typically handle one-dimensional data. We introduce\nMultiCast, a zero-shot LLM-based approach for multivariate time series\nforecasting. It allows LLMs to receive multivariate time series as input,\nthrough three novel token multiplexing solutions that effectively reduce\ndimensionality while preserving key repetitive patterns. Additionally, a\nquantization scheme helps LLMs to better learn these patterns, while\nsignificantly reducing token use for practical applications. We showcase the\nperformance of our approach in terms of RMSE and execution time against\nstate-of-the-art approaches on three real-world datasets."}}, "19a31078-f89d-49dc-adb7-173adbb44952": {"node_ids": ["1930c7ba-fcd1-4f0d-8ff3-5cadb253b660", "c11d69b3-3f61-4c01-bcc9-cd95e1984e8c", "b91a5516-6a4f-403b-91bb-9b8840ee856f"], "metadata": {"page_label": "2", "file_name": "2405_14748v1.pdf", "Title of this paper": "MultiCast: Zero-Shot Multivariate Time Series Forecasting Using LLMs", "Authors": "Georgios Chatzigeorgakidis, Konstantinos Lentzos, Dimitrios Skoutas", "Date published": "05/23/2024", "URL": "http://arxiv.org/abs/2405.14748v1", "summary": "Predicting future values in multivariate time series is vital across various\ndomains. This work explores the use of large language models (LLMs) for this\ntask. However, LLMs typically handle one-dimensional data. We introduce\nMultiCast, a zero-shot LLM-based approach for multivariate time series\nforecasting. It allows LLMs to receive multivariate time series as input,\nthrough three novel token multiplexing solutions that effectively reduce\ndimensionality while preserving key repetitive patterns. Additionally, a\nquantization scheme helps LLMs to better learn these patterns, while\nsignificantly reducing token use for practical applications. We showcase the\nperformance of our approach in terms of RMSE and execution time against\nstate-of-the-art approaches on three real-world datasets."}}, "732108b4-c6bc-432e-8958-5d8f781f7957": {"node_ids": ["6e49a334-f59e-40e0-8d79-76da823feb78", "d309a49d-ac36-4180-b02b-5fe1d8ba350d", "8bbabe7e-3006-48a4-9f49-508c7470fd99"], "metadata": {"page_label": "3", "file_name": "2405_14748v1.pdf", "Title of this paper": "MultiCast: Zero-Shot Multivariate Time Series Forecasting Using LLMs", "Authors": "Georgios Chatzigeorgakidis, Konstantinos Lentzos, Dimitrios Skoutas", "Date published": "05/23/2024", "URL": "http://arxiv.org/abs/2405.14748v1", "summary": "Predicting future values in multivariate time series is vital across various\ndomains. This work explores the use of large language models (LLMs) for this\ntask. However, LLMs typically handle one-dimensional data. We introduce\nMultiCast, a zero-shot LLM-based approach for multivariate time series\nforecasting. It allows LLMs to receive multivariate time series as input,\nthrough three novel token multiplexing solutions that effectively reduce\ndimensionality while preserving key repetitive patterns. Additionally, a\nquantization scheme helps LLMs to better learn these patterns, while\nsignificantly reducing token use for practical applications. We showcase the\nperformance of our approach in terms of RMSE and execution time against\nstate-of-the-art approaches on three real-world datasets."}}, "0865d4e7-44c6-48fc-9fca-a5db256d411b": {"node_ids": ["31e24330-1adb-4395-9de1-5fdbd7970ecd", "118f70fc-e5b2-420f-8883-394407df9f8b"], "metadata": {"page_label": "4", "file_name": "2405_14748v1.pdf", "Title of this paper": "MultiCast: Zero-Shot Multivariate Time Series Forecasting Using LLMs", "Authors": "Georgios Chatzigeorgakidis, Konstantinos Lentzos, Dimitrios Skoutas", "Date published": "05/23/2024", "URL": "http://arxiv.org/abs/2405.14748v1", "summary": "Predicting future values in multivariate time series is vital across various\ndomains. This work explores the use of large language models (LLMs) for this\ntask. However, LLMs typically handle one-dimensional data. We introduce\nMultiCast, a zero-shot LLM-based approach for multivariate time series\nforecasting. It allows LLMs to receive multivariate time series as input,\nthrough three novel token multiplexing solutions that effectively reduce\ndimensionality while preserving key repetitive patterns. Additionally, a\nquantization scheme helps LLMs to better learn these patterns, while\nsignificantly reducing token use for practical applications. We showcase the\nperformance of our approach in terms of RMSE and execution time against\nstate-of-the-art approaches on three real-world datasets."}}, "adfdfd5f-2ace-4d7e-8f2f-1991cb419a2e": {"node_ids": ["f20b7855-9bcb-472d-bbfd-367251c7ca03", "fbeff2ff-4a57-4df4-aac6-cf0a659e757b"], "metadata": {"page_label": "5", "file_name": "2405_14748v1.pdf", "Title of this paper": "MultiCast: Zero-Shot Multivariate Time Series Forecasting Using LLMs", "Authors": "Georgios Chatzigeorgakidis, Konstantinos Lentzos, Dimitrios Skoutas", "Date published": "05/23/2024", "URL": "http://arxiv.org/abs/2405.14748v1", "summary": "Predicting future values in multivariate time series is vital across various\ndomains. This work explores the use of large language models (LLMs) for this\ntask. However, LLMs typically handle one-dimensional data. We introduce\nMultiCast, a zero-shot LLM-based approach for multivariate time series\nforecasting. It allows LLMs to receive multivariate time series as input,\nthrough three novel token multiplexing solutions that effectively reduce\ndimensionality while preserving key repetitive patterns. Additionally, a\nquantization scheme helps LLMs to better learn these patterns, while\nsignificantly reducing token use for practical applications. We showcase the\nperformance of our approach in terms of RMSE and execution time against\nstate-of-the-art approaches on three real-world datasets."}}, "a3db5c01-9a8f-451e-8696-3fabdf5058c6": {"node_ids": ["390acbed-d6fc-4a90-b144-bea4c9d268f6", "424424d2-e089-4003-aaea-f1ee1f9a1713"], "metadata": {"page_label": "6", "file_name": "2405_14748v1.pdf", "Title of this paper": "MultiCast: Zero-Shot Multivariate Time Series Forecasting Using LLMs", "Authors": "Georgios Chatzigeorgakidis, Konstantinos Lentzos, Dimitrios Skoutas", "Date published": "05/23/2024", "URL": "http://arxiv.org/abs/2405.14748v1", "summary": "Predicting future values in multivariate time series is vital across various\ndomains. This work explores the use of large language models (LLMs) for this\ntask. However, LLMs typically handle one-dimensional data. We introduce\nMultiCast, a zero-shot LLM-based approach for multivariate time series\nforecasting. It allows LLMs to receive multivariate time series as input,\nthrough three novel token multiplexing solutions that effectively reduce\ndimensionality while preserving key repetitive patterns. Additionally, a\nquantization scheme helps LLMs to better learn these patterns, while\nsignificantly reducing token use for practical applications. We showcase the\nperformance of our approach in terms of RMSE and execution time against\nstate-of-the-art approaches on three real-world datasets."}}, "e09d5e8b-0c88-453c-bdad-a9ae32acd092": {"node_ids": ["89301b3b-8316-49a6-a9f5-4191280a6185", "fda47b5a-4199-4831-bb2b-373348f93cd3"], "metadata": {"page_label": "7", "file_name": "2405_14748v1.pdf", "Title of this paper": "MultiCast: Zero-Shot Multivariate Time Series Forecasting Using LLMs", "Authors": "Georgios Chatzigeorgakidis, Konstantinos Lentzos, Dimitrios Skoutas", "Date published": "05/23/2024", "URL": "http://arxiv.org/abs/2405.14748v1", "summary": "Predicting future values in multivariate time series is vital across various\ndomains. This work explores the use of large language models (LLMs) for this\ntask. However, LLMs typically handle one-dimensional data. We introduce\nMultiCast, a zero-shot LLM-based approach for multivariate time series\nforecasting. It allows LLMs to receive multivariate time series as input,\nthrough three novel token multiplexing solutions that effectively reduce\ndimensionality while preserving key repetitive patterns. Additionally, a\nquantization scheme helps LLMs to better learn these patterns, while\nsignificantly reducing token use for practical applications. We showcase the\nperformance of our approach in terms of RMSE and execution time against\nstate-of-the-art approaches on three real-world datasets."}}, "1c5d696c-ebec-42f9-bd75-c9243ef6ce94": {"node_ids": ["f9160b0a-1b8b-4914-90c6-00e3dc2ac67f"], "metadata": {"page_label": "8", "file_name": "2405_14748v1.pdf", "Title of this paper": "MultiCast: Zero-Shot Multivariate Time Series Forecasting Using LLMs", "Authors": "Georgios Chatzigeorgakidis, Konstantinos Lentzos, Dimitrios Skoutas", "Date published": "05/23/2024", "URL": "http://arxiv.org/abs/2405.14748v1", "summary": "Predicting future values in multivariate time series is vital across various\ndomains. This work explores the use of large language models (LLMs) for this\ntask. However, LLMs typically handle one-dimensional data. We introduce\nMultiCast, a zero-shot LLM-based approach for multivariate time series\nforecasting. It allows LLMs to receive multivariate time series as input,\nthrough three novel token multiplexing solutions that effectively reduce\ndimensionality while preserving key repetitive patterns. Additionally, a\nquantization scheme helps LLMs to better learn these patterns, while\nsignificantly reducing token use for practical applications. We showcase the\nperformance of our approach in terms of RMSE and execution time against\nstate-of-the-art approaches on three real-world datasets."}}, "54339c96-3d24-48c6-ad67-22b6a7fa8e82": {"node_ids": ["90084695-289f-4605-9d3d-edef11b81eed", "79acdc70-bf45-4526-b97e-293029310334", "7228c0de-e674-4cbe-a94c-7ffee87d1438", "3cfbb976-af02-4be6-ae65-91cd9345fb81"], "metadata": {"page_label": "9", "file_name": "2405_14748v1.pdf", "Title of this paper": "MultiCast: Zero-Shot Multivariate Time Series Forecasting Using LLMs", "Authors": "Georgios Chatzigeorgakidis, Konstantinos Lentzos, Dimitrios Skoutas", "Date published": "05/23/2024", "URL": "http://arxiv.org/abs/2405.14748v1", "summary": "Predicting future values in multivariate time series is vital across various\ndomains. This work explores the use of large language models (LLMs) for this\ntask. However, LLMs typically handle one-dimensional data. We introduce\nMultiCast, a zero-shot LLM-based approach for multivariate time series\nforecasting. It allows LLMs to receive multivariate time series as input,\nthrough three novel token multiplexing solutions that effectively reduce\ndimensionality while preserving key repetitive patterns. Additionally, a\nquantization scheme helps LLMs to better learn these patterns, while\nsignificantly reducing token use for practical applications. We showcase the\nperformance of our approach in terms of RMSE and execution time against\nstate-of-the-art approaches on three real-world datasets."}}}}