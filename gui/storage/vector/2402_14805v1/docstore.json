{"docstore/data": {"d1b04198-8344-4e96-a8de-ac36bfd57af4": {"__data__": {"id_": "d1b04198-8344-4e96-a8de-ac36bfd57af4", "embedding": null, "metadata": {"page_label": "1", "file_name": "2402_14805v1.pdf", "Title of this paper": "Identifying Multiple Personalities in Large Language Models with External Evaluation", "Authors": "Xiaoyang Song, Yuta Adachi, Jessie Feng, Mouwei Lin, Linhao Yu, Frank Li, Akshat Gupta, Gopala Anumanchipalli, Simerjot Kaur", "Date published": "02/22/2024", "URL": "http://arxiv.org/abs/2402.14805v1", "summary": "As Large Language Models (LLMs) are integrated with human daily applications\nrapidly, many societal and ethical concerns are raised regarding the behavior\nof LLMs. One of the ways to comprehend LLMs' behavior is to analyze their\npersonalities. Many recent studies quantify LLMs' personalities using\nself-assessment tests that are created for humans. Yet many critiques question\nthe applicability and reliability of these self-assessment tests when applied\nto LLMs. In this paper, we investigate LLM personalities using an alternate\npersonality measurement method, which we refer to as the external evaluation\nmethod, where instead of prompting LLMs with multiple-choice questions in the\nLikert scale, we evaluate LLMs' personalities by analyzing their responses\ntoward open-ended situational questions using an external machine learning\nmodel. We first fine-tuned a Llama2-7B model as the MBTI personality predictor\nthat outperforms the state-of-the-art models as the tool to analyze LLMs'\nresponses. Then, we prompt the LLMs with situational questions and ask them to\ngenerate Twitter posts and comments, respectively, in order to assess their\npersonalities when playing two different roles. Using the external personality\nevaluation method, we identify that the obtained personality types for LLMs are\nsignificantly different when generating posts versus comments, whereas humans\nshow a consistent personality profile in these two different situations. This\nshows that LLMs can exhibit different personalities based on different\nscenarios, thus highlighting a fundamental difference between personality in\nLLMs and humans. With our work, we call for a re-evaluation of personality\ndefinition and measurement in LLMs."}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5e945e96-2cdc-4682-ab02-08e1c62537bf", "node_type": "4", "metadata": {"page_label": "1", "file_name": "2402_14805v1.pdf", "Title of this paper": "Identifying Multiple Personalities in Large Language Models with External Evaluation", "Authors": "Xiaoyang Song, Yuta Adachi, Jessie Feng, Mouwei Lin, Linhao Yu, Frank Li, Akshat Gupta, Gopala Anumanchipalli, Simerjot Kaur", "Date published": "02/22/2024", "URL": "http://arxiv.org/abs/2402.14805v1", "summary": "As Large Language Models (LLMs) are integrated with human daily applications\nrapidly, many societal and ethical concerns are raised regarding the behavior\nof LLMs. One of the ways to comprehend LLMs' behavior is to analyze their\npersonalities. Many recent studies quantify LLMs' personalities using\nself-assessment tests that are created for humans. Yet many critiques question\nthe applicability and reliability of these self-assessment tests when applied\nto LLMs. In this paper, we investigate LLM personalities using an alternate\npersonality measurement method, which we refer to as the external evaluation\nmethod, where instead of prompting LLMs with multiple-choice questions in the\nLikert scale, we evaluate LLMs' personalities by analyzing their responses\ntoward open-ended situational questions using an external machine learning\nmodel. We first fine-tuned a Llama2-7B model as the MBTI personality predictor\nthat outperforms the state-of-the-art models as the tool to analyze LLMs'\nresponses. Then, we prompt the LLMs with situational questions and ask them to\ngenerate Twitter posts and comments, respectively, in order to assess their\npersonalities when playing two different roles. Using the external personality\nevaluation method, we identify that the obtained personality types for LLMs are\nsignificantly different when generating posts versus comments, whereas humans\nshow a consistent personality profile in these two different situations. This\nshows that LLMs can exhibit different personalities based on different\nscenarios, thus highlighting a fundamental difference between personality in\nLLMs and humans. With our work, we call for a re-evaluation of personality\ndefinition and measurement in LLMs."}, "hash": "674d9c26d8f649f110929a43124024f37b12a5c0e0a89ec9f676e030089b9e6a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f2cca384-44be-4f99-99a8-de40bc1126c4", "node_type": "1", "metadata": {}, "hash": "9fc47a1985c196227cf55267423af7ef3ac52afedcfcd8a70a30ecaf3462410b", "class_name": "RelatedNodeInfo"}}, "text": "Identifying Multiple Personalities in Large Language Models\nwith External Evaluation\nXiaoyang Song1, Yuta Adachi2\u2217, Jessie Feng2\u2217, Mouwei Lin2\u2217, Linhao Yu2\u2217, Frank Li2\u2217,\nAkshat Gupta3, Gopala Anumanchipalli3, Simerjot Kaur4\n1University of Michigan,2Columbia University,3UC Berkeley,4AI Research, JPMorgan\nxysong@umich.edu, akshat.gupta@berkeley.edu, simerjot.kaur@jpmchase.com\nAbstract\nAs Large Language Models (LLMs) are inte-\ngrated with human daily applications rapidly,\nmany societal and ethical concerns are raised re-\ngarding the behavior of LLMs. One of the ways\nto comprehend LLMs\u2019 behavior is to analyze\ntheir personalities. Many recent studies quan-\ntify LLMs\u2019 personalities using self-assessment\ntests that are created for humans. Yet many\ncritiques question the applicability and relia-\nbility of these self-assessment tests when ap-\nplied to LLMs. In this paper, we investigate\nLLM personalities using an alternate personal-\nity measurement method, which we refer to as\ntheexternal evaluation method, where instead\nof prompting LLMs with multiple-choice ques-\ntions in the Likert scale, we evaluate LLMs\u2019\npersonalities by analyzing their responses to-\nward open-ended situational questions using\nan external machine learning model. We first\nfine-tuned a Llama2-7B model as the MBTI\npersonality predictor that outperforms the state-\nof-the-art models as the tool to analyze LLMs\u2019\nresponses. Then, we prompt the LLMs with\nsituational questions and ask them to generate\nTwitter posts andcomments , respectively, in or-\nder to assess their personalities when playing\ntwo different roles. Using the external person-\nality evaluation method, we identify that the\nobtained personality types for LLMs are signif-\nicantly different when generating posts versus\ncomments, whereas humans show a consistent\npersonality profile in these two different situa-\ntions. This shows that LLMs can exhibit differ-\nent personalities based on different scenarios,\nthus highlighting a fundamental difference be-\ntween personality in LLMs and humans. With\nour work, we call for a re-evaluation of person-\nality definition and measurement in LLMs.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2134, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f2cca384-44be-4f99-99a8-de40bc1126c4": {"__data__": {"id_": "f2cca384-44be-4f99-99a8-de40bc1126c4", "embedding": null, "metadata": {"page_label": "1", "file_name": "2402_14805v1.pdf", "Title of this paper": "Identifying Multiple Personalities in Large Language Models with External Evaluation", "Authors": "Xiaoyang Song, Yuta Adachi, Jessie Feng, Mouwei Lin, Linhao Yu, Frank Li, Akshat Gupta, Gopala Anumanchipalli, Simerjot Kaur", "Date published": "02/22/2024", "URL": "http://arxiv.org/abs/2402.14805v1", "summary": "As Large Language Models (LLMs) are integrated with human daily applications\nrapidly, many societal and ethical concerns are raised regarding the behavior\nof LLMs. One of the ways to comprehend LLMs' behavior is to analyze their\npersonalities. Many recent studies quantify LLMs' personalities using\nself-assessment tests that are created for humans. Yet many critiques question\nthe applicability and reliability of these self-assessment tests when applied\nto LLMs. In this paper, we investigate LLM personalities using an alternate\npersonality measurement method, which we refer to as the external evaluation\nmethod, where instead of prompting LLMs with multiple-choice questions in the\nLikert scale, we evaluate LLMs' personalities by analyzing their responses\ntoward open-ended situational questions using an external machine learning\nmodel. We first fine-tuned a Llama2-7B model as the MBTI personality predictor\nthat outperforms the state-of-the-art models as the tool to analyze LLMs'\nresponses. Then, we prompt the LLMs with situational questions and ask them to\ngenerate Twitter posts and comments, respectively, in order to assess their\npersonalities when playing two different roles. Using the external personality\nevaluation method, we identify that the obtained personality types for LLMs are\nsignificantly different when generating posts versus comments, whereas humans\nshow a consistent personality profile in these two different situations. This\nshows that LLMs can exhibit different personalities based on different\nscenarios, thus highlighting a fundamental difference between personality in\nLLMs and humans. With our work, we call for a re-evaluation of personality\ndefinition and measurement in LLMs."}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5e945e96-2cdc-4682-ab02-08e1c62537bf", "node_type": "4", "metadata": {"page_label": "1", "file_name": "2402_14805v1.pdf", "Title of this paper": "Identifying Multiple Personalities in Large Language Models with External Evaluation", "Authors": "Xiaoyang Song, Yuta Adachi, Jessie Feng, Mouwei Lin, Linhao Yu, Frank Li, Akshat Gupta, Gopala Anumanchipalli, Simerjot Kaur", "Date published": "02/22/2024", "URL": "http://arxiv.org/abs/2402.14805v1", "summary": "As Large Language Models (LLMs) are integrated with human daily applications\nrapidly, many societal and ethical concerns are raised regarding the behavior\nof LLMs. One of the ways to comprehend LLMs' behavior is to analyze their\npersonalities. Many recent studies quantify LLMs' personalities using\nself-assessment tests that are created for humans. Yet many critiques question\nthe applicability and reliability of these self-assessment tests when applied\nto LLMs. In this paper, we investigate LLM personalities using an alternate\npersonality measurement method, which we refer to as the external evaluation\nmethod, where instead of prompting LLMs with multiple-choice questions in the\nLikert scale, we evaluate LLMs' personalities by analyzing their responses\ntoward open-ended situational questions using an external machine learning\nmodel. We first fine-tuned a Llama2-7B model as the MBTI personality predictor\nthat outperforms the state-of-the-art models as the tool to analyze LLMs'\nresponses. Then, we prompt the LLMs with situational questions and ask them to\ngenerate Twitter posts and comments, respectively, in order to assess their\npersonalities when playing two different roles. Using the external personality\nevaluation method, we identify that the obtained personality types for LLMs are\nsignificantly different when generating posts versus comments, whereas humans\nshow a consistent personality profile in these two different situations. This\nshows that LLMs can exhibit different personalities based on different\nscenarios, thus highlighting a fundamental difference between personality in\nLLMs and humans. With our work, we call for a re-evaluation of personality\ndefinition and measurement in LLMs."}, "hash": "674d9c26d8f649f110929a43124024f37b12a5c0e0a89ec9f676e030089b9e6a", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d1b04198-8344-4e96-a8de-ac36bfd57af4", "node_type": "1", "metadata": {"page_label": "1", "file_name": "2402_14805v1.pdf", "Title of this paper": "Identifying Multiple Personalities in Large Language Models with External Evaluation", "Authors": "Xiaoyang Song, Yuta Adachi, Jessie Feng, Mouwei Lin, Linhao Yu, Frank Li, Akshat Gupta, Gopala Anumanchipalli, Simerjot Kaur", "Date published": "02/22/2024", "URL": "http://arxiv.org/abs/2402.14805v1", "summary": "As Large Language Models (LLMs) are integrated with human daily applications\nrapidly, many societal and ethical concerns are raised regarding the behavior\nof LLMs. One of the ways to comprehend LLMs' behavior is to analyze their\npersonalities. Many recent studies quantify LLMs' personalities using\nself-assessment tests that are created for humans. Yet many critiques question\nthe applicability and reliability of these self-assessment tests when applied\nto LLMs. In this paper, we investigate LLM personalities using an alternate\npersonality measurement method, which we refer to as the external evaluation\nmethod, where instead of prompting LLMs with multiple-choice questions in the\nLikert scale, we evaluate LLMs' personalities by analyzing their responses\ntoward open-ended situational questions using an external machine learning\nmodel. We first fine-tuned a Llama2-7B model as the MBTI personality predictor\nthat outperforms the state-of-the-art models as the tool to analyze LLMs'\nresponses. Then, we prompt the LLMs with situational questions and ask them to\ngenerate Twitter posts and comments, respectively, in order to assess their\npersonalities when playing two different roles. Using the external personality\nevaluation method, we identify that the obtained personality types for LLMs are\nsignificantly different when generating posts versus comments, whereas humans\nshow a consistent personality profile in these two different situations. This\nshows that LLMs can exhibit different personalities based on different\nscenarios, thus highlighting a fundamental difference between personality in\nLLMs and humans. With our work, we call for a re-evaluation of personality\ndefinition and measurement in LLMs."}, "hash": "8184d799c5a08651905c3f385b39854114bf8cbd86bca8aeacc9c4568e78f155", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "47c39e18-ad01-4c32-96bb-d5413b3db7f9", "node_type": "1", "metadata": {}, "hash": "4bf587af36ff756f4491f75d5989f41749b294c570c6a74a8c6fb5529c231f90", "class_name": "RelatedNodeInfo"}}, "text": "Then, we prompt the LLMs with\nsituational questions and ask them to generate\nTwitter posts andcomments , respectively, in or-\nder to assess their personalities when playing\ntwo different roles. Using the external person-\nality evaluation method, we identify that the\nobtained personality types for LLMs are signif-\nicantly different when generating posts versus\ncomments, whereas humans show a consistent\npersonality profile in these two different situa-\ntions. This shows that LLMs can exhibit differ-\nent personalities based on different scenarios,\nthus highlighting a fundamental difference be-\ntween personality in LLMs and humans. With\nour work, we call for a re-evaluation of person-\nality definition and measurement in LLMs.\n1 Introduction\n1The evolution of Large Language Models (LLM)\nhas benefited humans in the past few years through\n1\u2217equal contributiontheir unprecedented capacities to understand and\ngenerate human-like languages (Radford et al.,\n2018, 2019; Brown et al., 2020; Ouyang et al.,\n2022; OpenAI, 2023, 2022; Zhang et al., 2022; Tou-\nvron et al., 2023a). For instance, LLMs are now\nbeing deployed as virtual assistants to provide men-\ntal health support (Lai et al., 2023), as online edu-\ncators for common knowledge retrieval (OpenAI,\n2022; Jeon and Lee, 2023), and even as helpers in\nsymbolic music compositions (Agostinelli et al.,\n2023; Imasato et al., 2023). However, this growing\nintegration of LLMs across different social sectors\nof human life raises important concerns about relia-\nbility, safety, and ethics. This dual nature of LLMs\nopens the need to study their behaviors, especially\ntheir behaviors when interacting with humans. Al-\nthough most chat-based models including ChatGPT\n(OpenAI, 2022) and Llama (Touvron et al., 2023b)\nhave undergone safety training to prevent deliver-\ning poisonous and biased information, there is still\nan urgent need to find a proper venue and metrics\nto understand their societal behaviors.\nOne common way to understand the behavior\nof LLMs is to measure their personalities through\nrigorous psychometric studies (Jiang et al., 2022;\nMiotto et al., 2022; Huang et al., 2023; Caron and\nSrivastava, 2022; Karra et al., 2022).", "mimetype": "text/plain", "start_char_idx": 1403, "end_char_idx": 3596, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "47c39e18-ad01-4c32-96bb-d5413b3db7f9": {"__data__": {"id_": "47c39e18-ad01-4c32-96bb-d5413b3db7f9", "embedding": null, "metadata": {"page_label": "1", "file_name": "2402_14805v1.pdf", "Title of this paper": "Identifying Multiple Personalities in Large Language Models with External Evaluation", "Authors": "Xiaoyang Song, Yuta Adachi, Jessie Feng, Mouwei Lin, Linhao Yu, Frank Li, Akshat Gupta, Gopala Anumanchipalli, Simerjot Kaur", "Date published": "02/22/2024", "URL": "http://arxiv.org/abs/2402.14805v1", "summary": "As Large Language Models (LLMs) are integrated with human daily applications\nrapidly, many societal and ethical concerns are raised regarding the behavior\nof LLMs. One of the ways to comprehend LLMs' behavior is to analyze their\npersonalities. Many recent studies quantify LLMs' personalities using\nself-assessment tests that are created for humans. Yet many critiques question\nthe applicability and reliability of these self-assessment tests when applied\nto LLMs. In this paper, we investigate LLM personalities using an alternate\npersonality measurement method, which we refer to as the external evaluation\nmethod, where instead of prompting LLMs with multiple-choice questions in the\nLikert scale, we evaluate LLMs' personalities by analyzing their responses\ntoward open-ended situational questions using an external machine learning\nmodel. We first fine-tuned a Llama2-7B model as the MBTI personality predictor\nthat outperforms the state-of-the-art models as the tool to analyze LLMs'\nresponses. Then, we prompt the LLMs with situational questions and ask them to\ngenerate Twitter posts and comments, respectively, in order to assess their\npersonalities when playing two different roles. Using the external personality\nevaluation method, we identify that the obtained personality types for LLMs are\nsignificantly different when generating posts versus comments, whereas humans\nshow a consistent personality profile in these two different situations. This\nshows that LLMs can exhibit different personalities based on different\nscenarios, thus highlighting a fundamental difference between personality in\nLLMs and humans. With our work, we call for a re-evaluation of personality\ndefinition and measurement in LLMs."}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5e945e96-2cdc-4682-ab02-08e1c62537bf", "node_type": "4", "metadata": {"page_label": "1", "file_name": "2402_14805v1.pdf", "Title of this paper": "Identifying Multiple Personalities in Large Language Models with External Evaluation", "Authors": "Xiaoyang Song, Yuta Adachi, Jessie Feng, Mouwei Lin, Linhao Yu, Frank Li, Akshat Gupta, Gopala Anumanchipalli, Simerjot Kaur", "Date published": "02/22/2024", "URL": "http://arxiv.org/abs/2402.14805v1", "summary": "As Large Language Models (LLMs) are integrated with human daily applications\nrapidly, many societal and ethical concerns are raised regarding the behavior\nof LLMs. One of the ways to comprehend LLMs' behavior is to analyze their\npersonalities. Many recent studies quantify LLMs' personalities using\nself-assessment tests that are created for humans. Yet many critiques question\nthe applicability and reliability of these self-assessment tests when applied\nto LLMs. In this paper, we investigate LLM personalities using an alternate\npersonality measurement method, which we refer to as the external evaluation\nmethod, where instead of prompting LLMs with multiple-choice questions in the\nLikert scale, we evaluate LLMs' personalities by analyzing their responses\ntoward open-ended situational questions using an external machine learning\nmodel. We first fine-tuned a Llama2-7B model as the MBTI personality predictor\nthat outperforms the state-of-the-art models as the tool to analyze LLMs'\nresponses. Then, we prompt the LLMs with situational questions and ask them to\ngenerate Twitter posts and comments, respectively, in order to assess their\npersonalities when playing two different roles. Using the external personality\nevaluation method, we identify that the obtained personality types for LLMs are\nsignificantly different when generating posts versus comments, whereas humans\nshow a consistent personality profile in these two different situations. This\nshows that LLMs can exhibit different personalities based on different\nscenarios, thus highlighting a fundamental difference between personality in\nLLMs and humans. With our work, we call for a re-evaluation of personality\ndefinition and measurement in LLMs."}, "hash": "674d9c26d8f649f110929a43124024f37b12a5c0e0a89ec9f676e030089b9e6a", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f2cca384-44be-4f99-99a8-de40bc1126c4", "node_type": "1", "metadata": {"page_label": "1", "file_name": "2402_14805v1.pdf", "Title of this paper": "Identifying Multiple Personalities in Large Language Models with External Evaluation", "Authors": "Xiaoyang Song, Yuta Adachi, Jessie Feng, Mouwei Lin, Linhao Yu, Frank Li, Akshat Gupta, Gopala Anumanchipalli, Simerjot Kaur", "Date published": "02/22/2024", "URL": "http://arxiv.org/abs/2402.14805v1", "summary": "As Large Language Models (LLMs) are integrated with human daily applications\nrapidly, many societal and ethical concerns are raised regarding the behavior\nof LLMs. One of the ways to comprehend LLMs' behavior is to analyze their\npersonalities. Many recent studies quantify LLMs' personalities using\nself-assessment tests that are created for humans. Yet many critiques question\nthe applicability and reliability of these self-assessment tests when applied\nto LLMs. In this paper, we investigate LLM personalities using an alternate\npersonality measurement method, which we refer to as the external evaluation\nmethod, where instead of prompting LLMs with multiple-choice questions in the\nLikert scale, we evaluate LLMs' personalities by analyzing their responses\ntoward open-ended situational questions using an external machine learning\nmodel. We first fine-tuned a Llama2-7B model as the MBTI personality predictor\nthat outperforms the state-of-the-art models as the tool to analyze LLMs'\nresponses. Then, we prompt the LLMs with situational questions and ask them to\ngenerate Twitter posts and comments, respectively, in order to assess their\npersonalities when playing two different roles. Using the external personality\nevaluation method, we identify that the obtained personality types for LLMs are\nsignificantly different when generating posts versus comments, whereas humans\nshow a consistent personality profile in these two different situations. This\nshows that LLMs can exhibit different personalities based on different\nscenarios, thus highlighting a fundamental difference between personality in\nLLMs and humans. With our work, we call for a re-evaluation of personality\ndefinition and measurement in LLMs."}, "hash": "6937caacc58859fc20afac9fe614fc95093d28b6354c833928a8d045fcfef315", "class_name": "RelatedNodeInfo"}}, "text": "This dual nature of LLMs\nopens the need to study their behaviors, especially\ntheir behaviors when interacting with humans. Al-\nthough most chat-based models including ChatGPT\n(OpenAI, 2022) and Llama (Touvron et al., 2023b)\nhave undergone safety training to prevent deliver-\ning poisonous and biased information, there is still\nan urgent need to find a proper venue and metrics\nto understand their societal behaviors.\nOne common way to understand the behavior\nof LLMs is to measure their personalities through\nrigorous psychometric studies (Jiang et al., 2022;\nMiotto et al., 2022; Huang et al., 2023; Caron and\nSrivastava, 2022; Karra et al., 2022). According\nto the American Psychological Association (APA),\npersonality for humans is defined as \u201cthe endur-\ning characteristic and behavior that comprise a\nperson\u2019s unique adjustment to life\" (Association,\n2023). On the other hand, the exact definition of\nLLM personalities remains an open yet mysteri-\nous question in the field. Nevertheless, many re-\nsearchers made analogies to human personality and\nattempted to study LLM personalities by leverag-\ning the psychometric tests used for humans. For\ninstance, most recent literature prompted LLMs\nwith standardized self-assessment personality test\nquestions and then recorded and analyzed the re-\nsults (Jiang et al., 2023; Karra et al., 2022; MiottoarXiv:2402.14805v1  [cs.CL]  22 Feb 2024", "mimetype": "text/plain", "start_char_idx": 2946, "end_char_idx": 4337, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9c7ee194-9c00-4156-beee-7cdd57f3312e": {"__data__": {"id_": "9c7ee194-9c00-4156-beee-7cdd57f3312e", "embedding": null, "metadata": {"page_label": "2", "file_name": "2402_14805v1.pdf", "Title of this paper": "Identifying Multiple Personalities in Large Language Models with External Evaluation", "Authors": "Xiaoyang Song, Yuta Adachi, Jessie Feng, Mouwei Lin, Linhao Yu, Frank Li, Akshat Gupta, Gopala Anumanchipalli, Simerjot Kaur", "Date published": "02/22/2024", "URL": "http://arxiv.org/abs/2402.14805v1", "summary": "As Large Language Models (LLMs) are integrated with human daily applications\nrapidly, many societal and ethical concerns are raised regarding the behavior\nof LLMs. One of the ways to comprehend LLMs' behavior is to analyze their\npersonalities. Many recent studies quantify LLMs' personalities using\nself-assessment tests that are created for humans. Yet many critiques question\nthe applicability and reliability of these self-assessment tests when applied\nto LLMs. In this paper, we investigate LLM personalities using an alternate\npersonality measurement method, which we refer to as the external evaluation\nmethod, where instead of prompting LLMs with multiple-choice questions in the\nLikert scale, we evaluate LLMs' personalities by analyzing their responses\ntoward open-ended situational questions using an external machine learning\nmodel. We first fine-tuned a Llama2-7B model as the MBTI personality predictor\nthat outperforms the state-of-the-art models as the tool to analyze LLMs'\nresponses. Then, we prompt the LLMs with situational questions and ask them to\ngenerate Twitter posts and comments, respectively, in order to assess their\npersonalities when playing two different roles. Using the external personality\nevaluation method, we identify that the obtained personality types for LLMs are\nsignificantly different when generating posts versus comments, whereas humans\nshow a consistent personality profile in these two different situations. This\nshows that LLMs can exhibit different personalities based on different\nscenarios, thus highlighting a fundamental difference between personality in\nLLMs and humans. With our work, we call for a re-evaluation of personality\ndefinition and measurement in LLMs."}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "043a1305-ea8e-44d4-9f77-3845270e5cd3", "node_type": "4", "metadata": {"page_label": "2", "file_name": "2402_14805v1.pdf", "Title of this paper": "Identifying Multiple Personalities in Large Language Models with External Evaluation", "Authors": "Xiaoyang Song, Yuta Adachi, Jessie Feng, Mouwei Lin, Linhao Yu, Frank Li, Akshat Gupta, Gopala Anumanchipalli, Simerjot Kaur", "Date published": "02/22/2024", "URL": "http://arxiv.org/abs/2402.14805v1", "summary": "As Large Language Models (LLMs) are integrated with human daily applications\nrapidly, many societal and ethical concerns are raised regarding the behavior\nof LLMs. One of the ways to comprehend LLMs' behavior is to analyze their\npersonalities. Many recent studies quantify LLMs' personalities using\nself-assessment tests that are created for humans. Yet many critiques question\nthe applicability and reliability of these self-assessment tests when applied\nto LLMs. In this paper, we investigate LLM personalities using an alternate\npersonality measurement method, which we refer to as the external evaluation\nmethod, where instead of prompting LLMs with multiple-choice questions in the\nLikert scale, we evaluate LLMs' personalities by analyzing their responses\ntoward open-ended situational questions using an external machine learning\nmodel. We first fine-tuned a Llama2-7B model as the MBTI personality predictor\nthat outperforms the state-of-the-art models as the tool to analyze LLMs'\nresponses. Then, we prompt the LLMs with situational questions and ask them to\ngenerate Twitter posts and comments, respectively, in order to assess their\npersonalities when playing two different roles. Using the external personality\nevaluation method, we identify that the obtained personality types for LLMs are\nsignificantly different when generating posts versus comments, whereas humans\nshow a consistent personality profile in these two different situations. This\nshows that LLMs can exhibit different personalities based on different\nscenarios, thus highlighting a fundamental difference between personality in\nLLMs and humans. With our work, we call for a re-evaluation of personality\ndefinition and measurement in LLMs."}, "hash": "e6e96a7f55595dde73b83e637cecf6548747cde0239b5694c664d609e76594cd", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "121c27af-170d-4706-a477-e8f5ec2cc0e0", "node_type": "1", "metadata": {}, "hash": "be4ec5731556959304939b4f17b71b9fcf4e5ca3d475cc4c1272c4e3caac874d", "class_name": "RelatedNodeInfo"}}, "text": "et al., 2022; Bodroza et al., 2023; Safdari et al.,\n2023). However, while these tests are shown to\nbe effective for human personality measurement\n(Digman, 1990), there is evidence that they can not\nbe directly applied to reliably measure personality\nof both base LLMs including GPTs (Radford et al.,\n2018, 2019; Brown et al., 2020) and chat-based\nLLMs like ChatGPT (OpenAI, 2022) and Llama\n(Touvron et al., 2023b). For instance, Song et al.\n(2023) and Gupta et al. (2023) managed to show\nthat the self-assessment personality test results of\nthe same LLM differ significantly as the prompting\ntemplates are changed, which is not surprising as\nLLMs are known to be prone and sensitive to differ-\nent prompts (Sclar et al., 2023; Chen et al., 2023).\nHowever, it has also been shown that even under\nthe same prompt template, the psychometric test\nresults are statistically different when the options\nof those multiple-choice questions are presented\nin different orders (Song et al., 2023; Gupta et al.,\n2023). These observations indicate that prompting\nLLMs with standardized self-assessment is not a\nreliable method to quantify LLMs\u2019 personalities.\nTherefore, an alternative to self-assessment psy-\nchometric tests is desired to better understand and\nanalyze personalities in LLMs.\nIn this paper, we investigate an alternate method\nto measure LLM personalities. To do so, we first\ndevelop a state-of-the-art personality prediction\nmodel. Specifically, we utilized the famous Myers-\nBriggs Type Indicator (MBTI) personality frame-\nwork and fine-tuned a Llama2-7B (Touvron et al.,\n2023b) model on a human personality dataset. The\ndataset contains multiple posts written by a hu-\nman subject and their MBTI personality type. This\nmodel is then used to analyze the personality of\nLLMs under the MBTI framework and compare\nthe results with human counterparts. (Sec. 3.1)\nTo measure LLM personality, we have different\nLLMs write tweets which are used as input for our\npersonality prediction model. While doing this, we\nhave LLMs take two different roles. In the first\nrole, the LLM is asked to write tweets about real-\nworld events based on the event topics which were\nobtained by analyzing news articles.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2196, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "121c27af-170d-4706-a477-e8f5ec2cc0e0": {"__data__": {"id_": "121c27af-170d-4706-a477-e8f5ec2cc0e0", "embedding": null, "metadata": {"page_label": "2", "file_name": "2402_14805v1.pdf", "Title of this paper": "Identifying Multiple Personalities in Large Language Models with External Evaluation", "Authors": "Xiaoyang Song, Yuta Adachi, Jessie Feng, Mouwei Lin, Linhao Yu, Frank Li, Akshat Gupta, Gopala Anumanchipalli, Simerjot Kaur", "Date published": "02/22/2024", "URL": "http://arxiv.org/abs/2402.14805v1", "summary": "As Large Language Models (LLMs) are integrated with human daily applications\nrapidly, many societal and ethical concerns are raised regarding the behavior\nof LLMs. One of the ways to comprehend LLMs' behavior is to analyze their\npersonalities. Many recent studies quantify LLMs' personalities using\nself-assessment tests that are created for humans. Yet many critiques question\nthe applicability and reliability of these self-assessment tests when applied\nto LLMs. In this paper, we investigate LLM personalities using an alternate\npersonality measurement method, which we refer to as the external evaluation\nmethod, where instead of prompting LLMs with multiple-choice questions in the\nLikert scale, we evaluate LLMs' personalities by analyzing their responses\ntoward open-ended situational questions using an external machine learning\nmodel. We first fine-tuned a Llama2-7B model as the MBTI personality predictor\nthat outperforms the state-of-the-art models as the tool to analyze LLMs'\nresponses. Then, we prompt the LLMs with situational questions and ask them to\ngenerate Twitter posts and comments, respectively, in order to assess their\npersonalities when playing two different roles. Using the external personality\nevaluation method, we identify that the obtained personality types for LLMs are\nsignificantly different when generating posts versus comments, whereas humans\nshow a consistent personality profile in these two different situations. This\nshows that LLMs can exhibit different personalities based on different\nscenarios, thus highlighting a fundamental difference between personality in\nLLMs and humans. With our work, we call for a re-evaluation of personality\ndefinition and measurement in LLMs."}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "043a1305-ea8e-44d4-9f77-3845270e5cd3", "node_type": "4", "metadata": {"page_label": "2", "file_name": "2402_14805v1.pdf", "Title of this paper": "Identifying Multiple Personalities in Large Language Models with External Evaluation", "Authors": "Xiaoyang Song, Yuta Adachi, Jessie Feng, Mouwei Lin, Linhao Yu, Frank Li, Akshat Gupta, Gopala Anumanchipalli, Simerjot Kaur", "Date published": "02/22/2024", "URL": "http://arxiv.org/abs/2402.14805v1", "summary": "As Large Language Models (LLMs) are integrated with human daily applications\nrapidly, many societal and ethical concerns are raised regarding the behavior\nof LLMs. One of the ways to comprehend LLMs' behavior is to analyze their\npersonalities. Many recent studies quantify LLMs' personalities using\nself-assessment tests that are created for humans. Yet many critiques question\nthe applicability and reliability of these self-assessment tests when applied\nto LLMs. In this paper, we investigate LLM personalities using an alternate\npersonality measurement method, which we refer to as the external evaluation\nmethod, where instead of prompting LLMs with multiple-choice questions in the\nLikert scale, we evaluate LLMs' personalities by analyzing their responses\ntoward open-ended situational questions using an external machine learning\nmodel. We first fine-tuned a Llama2-7B model as the MBTI personality predictor\nthat outperforms the state-of-the-art models as the tool to analyze LLMs'\nresponses. Then, we prompt the LLMs with situational questions and ask them to\ngenerate Twitter posts and comments, respectively, in order to assess their\npersonalities when playing two different roles. Using the external personality\nevaluation method, we identify that the obtained personality types for LLMs are\nsignificantly different when generating posts versus comments, whereas humans\nshow a consistent personality profile in these two different situations. This\nshows that LLMs can exhibit different personalities based on different\nscenarios, thus highlighting a fundamental difference between personality in\nLLMs and humans. With our work, we call for a re-evaluation of personality\ndefinition and measurement in LLMs."}, "hash": "e6e96a7f55595dde73b83e637cecf6548747cde0239b5694c664d609e76594cd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9c7ee194-9c00-4156-beee-7cdd57f3312e", "node_type": "1", "metadata": {"page_label": "2", "file_name": "2402_14805v1.pdf", "Title of this paper": "Identifying Multiple Personalities in Large Language Models with External Evaluation", "Authors": "Xiaoyang Song, Yuta Adachi, Jessie Feng, Mouwei Lin, Linhao Yu, Frank Li, Akshat Gupta, Gopala Anumanchipalli, Simerjot Kaur", "Date published": "02/22/2024", "URL": "http://arxiv.org/abs/2402.14805v1", "summary": "As Large Language Models (LLMs) are integrated with human daily applications\nrapidly, many societal and ethical concerns are raised regarding the behavior\nof LLMs. One of the ways to comprehend LLMs' behavior is to analyze their\npersonalities. Many recent studies quantify LLMs' personalities using\nself-assessment tests that are created for humans. Yet many critiques question\nthe applicability and reliability of these self-assessment tests when applied\nto LLMs. In this paper, we investigate LLM personalities using an alternate\npersonality measurement method, which we refer to as the external evaluation\nmethod, where instead of prompting LLMs with multiple-choice questions in the\nLikert scale, we evaluate LLMs' personalities by analyzing their responses\ntoward open-ended situational questions using an external machine learning\nmodel. We first fine-tuned a Llama2-7B model as the MBTI personality predictor\nthat outperforms the state-of-the-art models as the tool to analyze LLMs'\nresponses. Then, we prompt the LLMs with situational questions and ask them to\ngenerate Twitter posts and comments, respectively, in order to assess their\npersonalities when playing two different roles. Using the external personality\nevaluation method, we identify that the obtained personality types for LLMs are\nsignificantly different when generating posts versus comments, whereas humans\nshow a consistent personality profile in these two different situations. This\nshows that LLMs can exhibit different personalities based on different\nscenarios, thus highlighting a fundamental difference between personality in\nLLMs and humans. With our work, we call for a re-evaluation of personality\ndefinition and measurement in LLMs."}, "hash": "e2556597549024d5e458a4966107bfd74920e918e1e16a7fa94d23ced91ffd8f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3634128f-155d-4106-96d1-7a0912a718c8", "node_type": "1", "metadata": {}, "hash": "b8e4f26e27cfbc58d1f9960b0a6b14b004d01c92a0246949cc37befa6163415f", "class_name": "RelatedNodeInfo"}}, "text": "Specifically, we utilized the famous Myers-\nBriggs Type Indicator (MBTI) personality frame-\nwork and fine-tuned a Llama2-7B (Touvron et al.,\n2023b) model on a human personality dataset. The\ndataset contains multiple posts written by a hu-\nman subject and their MBTI personality type. This\nmodel is then used to analyze the personality of\nLLMs under the MBTI framework and compare\nthe results with human counterparts. (Sec. 3.1)\nTo measure LLM personality, we have different\nLLMs write tweets which are used as input for our\npersonality prediction model. While doing this, we\nhave LLMs take two different roles. In the first\nrole, the LLM is asked to write tweets about real-\nworld events based on the event topics which were\nobtained by analyzing news articles. This is done\nto prevent data leakage and stop LLMs from repeat-\ning tweets seen previously in pre-training data. In\nthe second role, we ask the LLM to write replies to\nexisting tweets. The tweets are again collected in\nreal time and are not part of the model pre-training\ncorpus. We then evaluate the personality of differ-\nent LLMs based on the tweets generated by themusing our personality prediction model. We per-\nform this analysis for ChatGPT (OpenAI, 2022),\nLlama2-7B-chat, Llama2-13B-chat and Llama2-\n70B-chat models (Touvron et al., 2023b). At the\nsame time, we also repeat the same procedure for\nhuman-written posts and comments to validate the\nproposed personality detection model. To our sur-\nprise, we find that the personality distribution of\nLLMs when writing tweets is completely different\nfrom the distribution when responding to tweets.\nAs defined by APA, personality is supposed to\nbe an enduring characteristic for humans. While\nthis consistency is shown to be true for humans,\nwe show that LLMs exhibit different personalities\nwhile playing different roles. (Sec. 3.2 & Sec. 3.3).\nTo summarize, our paper makes the following\ncontributions:\n\u2022We fine-tune a Llama2-7B for MBTI person-\nality detection that significantly outperforms\nthe state-of-the-art models.\n\u2022We use the fine-tuned personality detection\nmodel on tweets made by humans and show\nthat human personality remains consistent\nacross different roles.\n\u2022We show that LLMs exhibit different person-\nalities across different roles when using the\nexternal evaluation method.", "mimetype": "text/plain", "start_char_idx": 1435, "end_char_idx": 3746, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3634128f-155d-4106-96d1-7a0912a718c8": {"__data__": {"id_": "3634128f-155d-4106-96d1-7a0912a718c8", "embedding": null, "metadata": {"page_label": "2", "file_name": "2402_14805v1.pdf", "Title of this paper": "Identifying Multiple Personalities in Large Language Models with External Evaluation", "Authors": "Xiaoyang Song, Yuta Adachi, Jessie Feng, Mouwei Lin, Linhao Yu, Frank Li, Akshat Gupta, Gopala Anumanchipalli, Simerjot Kaur", "Date published": "02/22/2024", "URL": "http://arxiv.org/abs/2402.14805v1", "summary": "As Large Language Models (LLMs) are integrated with human daily applications\nrapidly, many societal and ethical concerns are raised regarding the behavior\nof LLMs. One of the ways to comprehend LLMs' behavior is to analyze their\npersonalities. Many recent studies quantify LLMs' personalities using\nself-assessment tests that are created for humans. Yet many critiques question\nthe applicability and reliability of these self-assessment tests when applied\nto LLMs. In this paper, we investigate LLM personalities using an alternate\npersonality measurement method, which we refer to as the external evaluation\nmethod, where instead of prompting LLMs with multiple-choice questions in the\nLikert scale, we evaluate LLMs' personalities by analyzing their responses\ntoward open-ended situational questions using an external machine learning\nmodel. We first fine-tuned a Llama2-7B model as the MBTI personality predictor\nthat outperforms the state-of-the-art models as the tool to analyze LLMs'\nresponses. Then, we prompt the LLMs with situational questions and ask them to\ngenerate Twitter posts and comments, respectively, in order to assess their\npersonalities when playing two different roles. Using the external personality\nevaluation method, we identify that the obtained personality types for LLMs are\nsignificantly different when generating posts versus comments, whereas humans\nshow a consistent personality profile in these two different situations. This\nshows that LLMs can exhibit different personalities based on different\nscenarios, thus highlighting a fundamental difference between personality in\nLLMs and humans. With our work, we call for a re-evaluation of personality\ndefinition and measurement in LLMs."}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "043a1305-ea8e-44d4-9f77-3845270e5cd3", "node_type": "4", "metadata": {"page_label": "2", "file_name": "2402_14805v1.pdf", "Title of this paper": "Identifying Multiple Personalities in Large Language Models with External Evaluation", "Authors": "Xiaoyang Song, Yuta Adachi, Jessie Feng, Mouwei Lin, Linhao Yu, Frank Li, Akshat Gupta, Gopala Anumanchipalli, Simerjot Kaur", "Date published": "02/22/2024", "URL": "http://arxiv.org/abs/2402.14805v1", "summary": "As Large Language Models (LLMs) are integrated with human daily applications\nrapidly, many societal and ethical concerns are raised regarding the behavior\nof LLMs. One of the ways to comprehend LLMs' behavior is to analyze their\npersonalities. Many recent studies quantify LLMs' personalities using\nself-assessment tests that are created for humans. Yet many critiques question\nthe applicability and reliability of these self-assessment tests when applied\nto LLMs. In this paper, we investigate LLM personalities using an alternate\npersonality measurement method, which we refer to as the external evaluation\nmethod, where instead of prompting LLMs with multiple-choice questions in the\nLikert scale, we evaluate LLMs' personalities by analyzing their responses\ntoward open-ended situational questions using an external machine learning\nmodel. We first fine-tuned a Llama2-7B model as the MBTI personality predictor\nthat outperforms the state-of-the-art models as the tool to analyze LLMs'\nresponses. Then, we prompt the LLMs with situational questions and ask them to\ngenerate Twitter posts and comments, respectively, in order to assess their\npersonalities when playing two different roles. Using the external personality\nevaluation method, we identify that the obtained personality types for LLMs are\nsignificantly different when generating posts versus comments, whereas humans\nshow a consistent personality profile in these two different situations. This\nshows that LLMs can exhibit different personalities based on different\nscenarios, thus highlighting a fundamental difference between personality in\nLLMs and humans. With our work, we call for a re-evaluation of personality\ndefinition and measurement in LLMs."}, "hash": "e6e96a7f55595dde73b83e637cecf6548747cde0239b5694c664d609e76594cd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "121c27af-170d-4706-a477-e8f5ec2cc0e0", "node_type": "1", "metadata": {"page_label": "2", "file_name": "2402_14805v1.pdf", "Title of this paper": "Identifying Multiple Personalities in Large Language Models with External Evaluation", "Authors": "Xiaoyang Song, Yuta Adachi, Jessie Feng, Mouwei Lin, Linhao Yu, Frank Li, Akshat Gupta, Gopala Anumanchipalli, Simerjot Kaur", "Date published": "02/22/2024", "URL": "http://arxiv.org/abs/2402.14805v1", "summary": "As Large Language Models (LLMs) are integrated with human daily applications\nrapidly, many societal and ethical concerns are raised regarding the behavior\nof LLMs. One of the ways to comprehend LLMs' behavior is to analyze their\npersonalities. Many recent studies quantify LLMs' personalities using\nself-assessment tests that are created for humans. Yet many critiques question\nthe applicability and reliability of these self-assessment tests when applied\nto LLMs. In this paper, we investigate LLM personalities using an alternate\npersonality measurement method, which we refer to as the external evaluation\nmethod, where instead of prompting LLMs with multiple-choice questions in the\nLikert scale, we evaluate LLMs' personalities by analyzing their responses\ntoward open-ended situational questions using an external machine learning\nmodel. We first fine-tuned a Llama2-7B model as the MBTI personality predictor\nthat outperforms the state-of-the-art models as the tool to analyze LLMs'\nresponses. Then, we prompt the LLMs with situational questions and ask them to\ngenerate Twitter posts and comments, respectively, in order to assess their\npersonalities when playing two different roles. Using the external personality\nevaluation method, we identify that the obtained personality types for LLMs are\nsignificantly different when generating posts versus comments, whereas humans\nshow a consistent personality profile in these two different situations. This\nshows that LLMs can exhibit different personalities based on different\nscenarios, thus highlighting a fundamental difference between personality in\nLLMs and humans. With our work, we call for a re-evaluation of personality\ndefinition and measurement in LLMs."}, "hash": "4cc8444bd40ba26f510b9d70271fff4c326327735b6a161d302a874b2b7e8571", "class_name": "RelatedNodeInfo"}}, "text": "To our sur-\nprise, we find that the personality distribution of\nLLMs when writing tweets is completely different\nfrom the distribution when responding to tweets.\nAs defined by APA, personality is supposed to\nbe an enduring characteristic for humans. While\nthis consistency is shown to be true for humans,\nwe show that LLMs exhibit different personalities\nwhile playing different roles. (Sec. 3.2 & Sec. 3.3).\nTo summarize, our paper makes the following\ncontributions:\n\u2022We fine-tune a Llama2-7B for MBTI person-\nality detection that significantly outperforms\nthe state-of-the-art models.\n\u2022We use the fine-tuned personality detection\nmodel on tweets made by humans and show\nthat human personality remains consistent\nacross different roles.\n\u2022We show that LLMs exhibit different person-\nalities across different roles when using the\nexternal evaluation method.\n\u2022With our work, we call for a re-evaluation\nof personality definition and measurement in\nLLMs.\n2 Related Work\n2.1 Personality Theory\nHuman personality is defined as \u201cthe enduring\ncharacteristic and behavior that comprise a per-\nson\u2019s unique adjustment to life\" by APA (Associa-\ntion, 2023), which is typically measured across dif-\nferent effective trait dimensions (Cattell, 1943b,a).\nCentral to human psychological profiling are the\nBig Five personality traits, also referred to as\nthe OCEAN traits, which stands for Openness,\nConscientiousness, Extraversion, Agreeableness,\nandNeuroticism, respectively (Digman, 1990;\nGoldberg, 1990, 1993). The Big Five personal-\nity traits are typically measured through multiple\nchoice self-assessment questions where a situa-\ntional statement is presented and the test-takers\nare requested to choose from an option in Likert\nscales, typically 1 to 5, to reflect the degree of", "mimetype": "text/plain", "start_char_idx": 2890, "end_char_idx": 4661, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "cf4715ba-0c73-457a-b65a-e510d84a8bda": {"__data__": {"id_": "cf4715ba-0c73-457a-b65a-e510d84a8bda", "embedding": null, "metadata": {"page_label": "3", "file_name": "2402_14805v1.pdf", "Title of this paper": "Identifying Multiple Personalities in Large Language Models with External Evaluation", "Authors": "Xiaoyang Song, Yuta Adachi, Jessie Feng, Mouwei Lin, Linhao Yu, Frank Li, Akshat Gupta, Gopala Anumanchipalli, Simerjot Kaur", "Date published": "02/22/2024", "URL": "http://arxiv.org/abs/2402.14805v1", "summary": "As Large Language Models (LLMs) are integrated with human daily applications\nrapidly, many societal and ethical concerns are raised regarding the behavior\nof LLMs. One of the ways to comprehend LLMs' behavior is to analyze their\npersonalities. Many recent studies quantify LLMs' personalities using\nself-assessment tests that are created for humans. Yet many critiques question\nthe applicability and reliability of these self-assessment tests when applied\nto LLMs. In this paper, we investigate LLM personalities using an alternate\npersonality measurement method, which we refer to as the external evaluation\nmethod, where instead of prompting LLMs with multiple-choice questions in the\nLikert scale, we evaluate LLMs' personalities by analyzing their responses\ntoward open-ended situational questions using an external machine learning\nmodel. We first fine-tuned a Llama2-7B model as the MBTI personality predictor\nthat outperforms the state-of-the-art models as the tool to analyze LLMs'\nresponses. Then, we prompt the LLMs with situational questions and ask them to\ngenerate Twitter posts and comments, respectively, in order to assess their\npersonalities when playing two different roles. Using the external personality\nevaluation method, we identify that the obtained personality types for LLMs are\nsignificantly different when generating posts versus comments, whereas humans\nshow a consistent personality profile in these two different situations. This\nshows that LLMs can exhibit different personalities based on different\nscenarios, thus highlighting a fundamental difference between personality in\nLLMs and humans. With our work, we call for a re-evaluation of personality\ndefinition and measurement in LLMs."}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2f2864ad-50f3-4bae-b473-188cd4ecbfa9", "node_type": "4", "metadata": {"page_label": "3", "file_name": "2402_14805v1.pdf", "Title of this paper": "Identifying Multiple Personalities in Large Language Models with External Evaluation", "Authors": "Xiaoyang Song, Yuta Adachi, Jessie Feng, Mouwei Lin, Linhao Yu, Frank Li, Akshat Gupta, Gopala Anumanchipalli, Simerjot Kaur", "Date published": "02/22/2024", "URL": "http://arxiv.org/abs/2402.14805v1", "summary": "As Large Language Models (LLMs) are integrated with human daily applications\nrapidly, many societal and ethical concerns are raised regarding the behavior\nof LLMs. One of the ways to comprehend LLMs' behavior is to analyze their\npersonalities. Many recent studies quantify LLMs' personalities using\nself-assessment tests that are created for humans. Yet many critiques question\nthe applicability and reliability of these self-assessment tests when applied\nto LLMs. In this paper, we investigate LLM personalities using an alternate\npersonality measurement method, which we refer to as the external evaluation\nmethod, where instead of prompting LLMs with multiple-choice questions in the\nLikert scale, we evaluate LLMs' personalities by analyzing their responses\ntoward open-ended situational questions using an external machine learning\nmodel. We first fine-tuned a Llama2-7B model as the MBTI personality predictor\nthat outperforms the state-of-the-art models as the tool to analyze LLMs'\nresponses. Then, we prompt the LLMs with situational questions and ask them to\ngenerate Twitter posts and comments, respectively, in order to assess their\npersonalities when playing two different roles. Using the external personality\nevaluation method, we identify that the obtained personality types for LLMs are\nsignificantly different when generating posts versus comments, whereas humans\nshow a consistent personality profile in these two different situations. This\nshows that LLMs can exhibit different personalities based on different\nscenarios, thus highlighting a fundamental difference between personality in\nLLMs and humans. With our work, we call for a re-evaluation of personality\ndefinition and measurement in LLMs."}, "hash": "b2c5021c128d8dc58803df6aaba5a99854513a1dd779d06e9b6e60bb892675b8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ce2f7c1c-8499-4235-8cc2-e2a7f7fa4423", "node_type": "1", "metadata": {}, "hash": "c9112289487312f09d1094a81a639ee2e19a586d2c0fa3f765a1cfaf8a5ae99c", "class_name": "RelatedNodeInfo"}}, "text": "Figure 1: MBTI trait description (Faust, 2019).\nfitness of the statement to themselves (Johnson,\n2014). Many studies on LLM personality directly\nbase their works on this straightforward test (Jiang\net al., 2023; Song et al., 2023; Caron and Srivastava,\n2022).\nHowever, the result of this test is a distribution\nof scores among all five traits, making it difficult\nto analyze and compare. Instead, we build our\npaper on another famous categorical personality\nframework called the Myers-Briggs Type Indica-\ntor (MBTI), which details 16 distinct types based\non trait combinations across four different dimen-\nsions: Extraverts vs. Introverts (E/I), Sensors vs.\nIntuitives (S/I), Thinkers vs. Feelers (T/F), and\nJudegers vs. Perceivers (J/P). Figure 1 above pro-\nvides detailed information about the trait each di-\nmension measures.\n2.2 LLM Personality Measurement\nMany recent works regarding LLM personal-\nity asked the LLM to perform Multiple-Choice\nQuestion-Answering (MCQA) on those well-\nknown standardized self-assessment personality\ntests (Jiang et al., 2023; Miotto et al., 2022; Caron\nand Srivastava, 2022; Huang et al., 2023; Bodroza\net al., 2023; Safdari et al., 2023; Pan and Zeng,\n2023; Noever and Hyams, 2023). For instance, as\none of the foundation works, Jiang et al. (2023)\nprompted the LLMs with the widely-used IPIP-120\ndataset in psychology (Johnson, 2014) and evalu-\nated their Big Five scores (i.e. OCEAN scores).\nHowever, although these works managed to elab-\norate on how personality can play a role in LLMs,\nthe validity of these self-assessment tests remains\nunchecked, making their conclusions unreliable.\nAs shown by Song et al. (2023) and Gupta et al.\n(2023), the same LLM tends to exhibit significantlydifferent personalities on different self-assessment\ntests, which fails to achieve the crucial consistency\ncriterion in personality definition and highlights the\ninvalidity of self-assessment tests. The origins of\nthese concerns are related to both the difficulty of\nMCQA tasks and the fact that LLMs are sensitive\nto the change in prompts as well as the structure\nof the prompts.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2108, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ce2f7c1c-8499-4235-8cc2-e2a7f7fa4423": {"__data__": {"id_": "ce2f7c1c-8499-4235-8cc2-e2a7f7fa4423", "embedding": null, "metadata": {"page_label": "3", "file_name": "2402_14805v1.pdf", "Title of this paper": "Identifying Multiple Personalities in Large Language Models with External Evaluation", "Authors": "Xiaoyang Song, Yuta Adachi, Jessie Feng, Mouwei Lin, Linhao Yu, Frank Li, Akshat Gupta, Gopala Anumanchipalli, Simerjot Kaur", "Date published": "02/22/2024", "URL": "http://arxiv.org/abs/2402.14805v1", "summary": "As Large Language Models (LLMs) are integrated with human daily applications\nrapidly, many societal and ethical concerns are raised regarding the behavior\nof LLMs. One of the ways to comprehend LLMs' behavior is to analyze their\npersonalities. Many recent studies quantify LLMs' personalities using\nself-assessment tests that are created for humans. Yet many critiques question\nthe applicability and reliability of these self-assessment tests when applied\nto LLMs. In this paper, we investigate LLM personalities using an alternate\npersonality measurement method, which we refer to as the external evaluation\nmethod, where instead of prompting LLMs with multiple-choice questions in the\nLikert scale, we evaluate LLMs' personalities by analyzing their responses\ntoward open-ended situational questions using an external machine learning\nmodel. We first fine-tuned a Llama2-7B model as the MBTI personality predictor\nthat outperforms the state-of-the-art models as the tool to analyze LLMs'\nresponses. Then, we prompt the LLMs with situational questions and ask them to\ngenerate Twitter posts and comments, respectively, in order to assess their\npersonalities when playing two different roles. Using the external personality\nevaluation method, we identify that the obtained personality types for LLMs are\nsignificantly different when generating posts versus comments, whereas humans\nshow a consistent personality profile in these two different situations. This\nshows that LLMs can exhibit different personalities based on different\nscenarios, thus highlighting a fundamental difference between personality in\nLLMs and humans. With our work, we call for a re-evaluation of personality\ndefinition and measurement in LLMs."}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2f2864ad-50f3-4bae-b473-188cd4ecbfa9", "node_type": "4", "metadata": {"page_label": "3", "file_name": "2402_14805v1.pdf", "Title of this paper": "Identifying Multiple Personalities in Large Language Models with External Evaluation", "Authors": "Xiaoyang Song, Yuta Adachi, Jessie Feng, Mouwei Lin, Linhao Yu, Frank Li, Akshat Gupta, Gopala Anumanchipalli, Simerjot Kaur", "Date published": "02/22/2024", "URL": "http://arxiv.org/abs/2402.14805v1", "summary": "As Large Language Models (LLMs) are integrated with human daily applications\nrapidly, many societal and ethical concerns are raised regarding the behavior\nof LLMs. One of the ways to comprehend LLMs' behavior is to analyze their\npersonalities. Many recent studies quantify LLMs' personalities using\nself-assessment tests that are created for humans. Yet many critiques question\nthe applicability and reliability of these self-assessment tests when applied\nto LLMs. In this paper, we investigate LLM personalities using an alternate\npersonality measurement method, which we refer to as the external evaluation\nmethod, where instead of prompting LLMs with multiple-choice questions in the\nLikert scale, we evaluate LLMs' personalities by analyzing their responses\ntoward open-ended situational questions using an external machine learning\nmodel. We first fine-tuned a Llama2-7B model as the MBTI personality predictor\nthat outperforms the state-of-the-art models as the tool to analyze LLMs'\nresponses. Then, we prompt the LLMs with situational questions and ask them to\ngenerate Twitter posts and comments, respectively, in order to assess their\npersonalities when playing two different roles. Using the external personality\nevaluation method, we identify that the obtained personality types for LLMs are\nsignificantly different when generating posts versus comments, whereas humans\nshow a consistent personality profile in these two different situations. This\nshows that LLMs can exhibit different personalities based on different\nscenarios, thus highlighting a fundamental difference between personality in\nLLMs and humans. With our work, we call for a re-evaluation of personality\ndefinition and measurement in LLMs."}, "hash": "b2c5021c128d8dc58803df6aaba5a99854513a1dd779d06e9b6e60bb892675b8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "cf4715ba-0c73-457a-b65a-e510d84a8bda", "node_type": "1", "metadata": {"page_label": "3", "file_name": "2402_14805v1.pdf", "Title of this paper": "Identifying Multiple Personalities in Large Language Models with External Evaluation", "Authors": "Xiaoyang Song, Yuta Adachi, Jessie Feng, Mouwei Lin, Linhao Yu, Frank Li, Akshat Gupta, Gopala Anumanchipalli, Simerjot Kaur", "Date published": "02/22/2024", "URL": "http://arxiv.org/abs/2402.14805v1", "summary": "As Large Language Models (LLMs) are integrated with human daily applications\nrapidly, many societal and ethical concerns are raised regarding the behavior\nof LLMs. One of the ways to comprehend LLMs' behavior is to analyze their\npersonalities. Many recent studies quantify LLMs' personalities using\nself-assessment tests that are created for humans. Yet many critiques question\nthe applicability and reliability of these self-assessment tests when applied\nto LLMs. In this paper, we investigate LLM personalities using an alternate\npersonality measurement method, which we refer to as the external evaluation\nmethod, where instead of prompting LLMs with multiple-choice questions in the\nLikert scale, we evaluate LLMs' personalities by analyzing their responses\ntoward open-ended situational questions using an external machine learning\nmodel. We first fine-tuned a Llama2-7B model as the MBTI personality predictor\nthat outperforms the state-of-the-art models as the tool to analyze LLMs'\nresponses. Then, we prompt the LLMs with situational questions and ask them to\ngenerate Twitter posts and comments, respectively, in order to assess their\npersonalities when playing two different roles. Using the external personality\nevaluation method, we identify that the obtained personality types for LLMs are\nsignificantly different when generating posts versus comments, whereas humans\nshow a consistent personality profile in these two different situations. This\nshows that LLMs can exhibit different personalities based on different\nscenarios, thus highlighting a fundamental difference between personality in\nLLMs and humans. With our work, we call for a re-evaluation of personality\ndefinition and measurement in LLMs."}, "hash": "5cc27868970a22eaadb578416998eb3e6c2a7536691e94d3ad5b115778572f33", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "cd2d26e4-10ce-416e-afd9-ee9cbc4caa46", "node_type": "1", "metadata": {}, "hash": "ee29b7a4831f30356e2e5d5d42f2a2432d0c1360306afc3ac91f6fab200e9a10", "class_name": "RelatedNodeInfo"}}, "text": "(2023)\nprompted the LLMs with the widely-used IPIP-120\ndataset in psychology (Johnson, 2014) and evalu-\nated their Big Five scores (i.e. OCEAN scores).\nHowever, although these works managed to elab-\norate on how personality can play a role in LLMs,\nthe validity of these self-assessment tests remains\nunchecked, making their conclusions unreliable.\nAs shown by Song et al. (2023) and Gupta et al.\n(2023), the same LLM tends to exhibit significantlydifferent personalities on different self-assessment\ntests, which fails to achieve the crucial consistency\ncriterion in personality definition and highlights the\ninvalidity of self-assessment tests. The origins of\nthese concerns are related to both the difficulty of\nMCQA tasks and the fact that LLMs are sensitive\nto the change in prompts as well as the structure\nof the prompts. A well-known example of this is\nthat the LLMs can be manipulated logically via\nChain-of-Thoughts (CoT) (Wei et al., 2022).\nTo the best of our knowledge, there is not much\nliterature that attempts to evaluate LLM personality\nwithout leveraging standardized tests. Driven by\nthis gap, in this work, we attempt to completely\ndiscard self-assessment tests and use an external\npersonality assessment method, where a person-\nality detection model is used to predict LLM per-\nsonality. In addition, unlike previous works that\nconduct tests without validation, we also justify the\npersonality prediction model used for evaluation\nby conducting a validation experiment on humans.\n2.3 MBTI Personality Detection Models\nMost MBTI personality detection models are built\nbased on the public Kaggle dataset2(Tang et al.,\n2023; Yang et al., 2023; Mehta, 2023). The dataset\nconsists of over 8600 entries, where each entry cor-\nresponds to an individual\u2019s MBTI type and includes\nexcerpts from the last 50 posts and comments made\nby the individual on the PersonalityCafe forum3.\nDetailed introduction to example entry, label dis-\ntribution, and the tweet topic distributions can be\nfound in Appendix A.1.\nTo predict MBTI personality types from texts,\nMehta (2023) processed the texts using a BERT\nmodel (Devlin et al., 2018) and then trained an\nMLP to predict the MBTI personality type, while\nYang et al. (2023) utilized a graph convolutional\nneural network to learn the connections between\ndifferent posts made by an individual in order\nto make decent predictions. Furthermore, Tang\net al.", "mimetype": "text/plain", "start_char_idx": 1280, "end_char_idx": 3681, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "cd2d26e4-10ce-416e-afd9-ee9cbc4caa46": {"__data__": {"id_": "cd2d26e4-10ce-416e-afd9-ee9cbc4caa46", "embedding": null, "metadata": {"page_label": "3", "file_name": "2402_14805v1.pdf", "Title of this paper": "Identifying Multiple Personalities in Large Language Models with External Evaluation", "Authors": "Xiaoyang Song, Yuta Adachi, Jessie Feng, Mouwei Lin, Linhao Yu, Frank Li, Akshat Gupta, Gopala Anumanchipalli, Simerjot Kaur", "Date published": "02/22/2024", "URL": "http://arxiv.org/abs/2402.14805v1", "summary": "As Large Language Models (LLMs) are integrated with human daily applications\nrapidly, many societal and ethical concerns are raised regarding the behavior\nof LLMs. One of the ways to comprehend LLMs' behavior is to analyze their\npersonalities. Many recent studies quantify LLMs' personalities using\nself-assessment tests that are created for humans. Yet many critiques question\nthe applicability and reliability of these self-assessment tests when applied\nto LLMs. In this paper, we investigate LLM personalities using an alternate\npersonality measurement method, which we refer to as the external evaluation\nmethod, where instead of prompting LLMs with multiple-choice questions in the\nLikert scale, we evaluate LLMs' personalities by analyzing their responses\ntoward open-ended situational questions using an external machine learning\nmodel. We first fine-tuned a Llama2-7B model as the MBTI personality predictor\nthat outperforms the state-of-the-art models as the tool to analyze LLMs'\nresponses. Then, we prompt the LLMs with situational questions and ask them to\ngenerate Twitter posts and comments, respectively, in order to assess their\npersonalities when playing two different roles. Using the external personality\nevaluation method, we identify that the obtained personality types for LLMs are\nsignificantly different when generating posts versus comments, whereas humans\nshow a consistent personality profile in these two different situations. This\nshows that LLMs can exhibit different personalities based on different\nscenarios, thus highlighting a fundamental difference between personality in\nLLMs and humans. With our work, we call for a re-evaluation of personality\ndefinition and measurement in LLMs."}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2f2864ad-50f3-4bae-b473-188cd4ecbfa9", "node_type": "4", "metadata": {"page_label": "3", "file_name": "2402_14805v1.pdf", "Title of this paper": "Identifying Multiple Personalities in Large Language Models with External Evaluation", "Authors": "Xiaoyang Song, Yuta Adachi, Jessie Feng, Mouwei Lin, Linhao Yu, Frank Li, Akshat Gupta, Gopala Anumanchipalli, Simerjot Kaur", "Date published": "02/22/2024", "URL": "http://arxiv.org/abs/2402.14805v1", "summary": "As Large Language Models (LLMs) are integrated with human daily applications\nrapidly, many societal and ethical concerns are raised regarding the behavior\nof LLMs. One of the ways to comprehend LLMs' behavior is to analyze their\npersonalities. Many recent studies quantify LLMs' personalities using\nself-assessment tests that are created for humans. Yet many critiques question\nthe applicability and reliability of these self-assessment tests when applied\nto LLMs. In this paper, we investigate LLM personalities using an alternate\npersonality measurement method, which we refer to as the external evaluation\nmethod, where instead of prompting LLMs with multiple-choice questions in the\nLikert scale, we evaluate LLMs' personalities by analyzing their responses\ntoward open-ended situational questions using an external machine learning\nmodel. We first fine-tuned a Llama2-7B model as the MBTI personality predictor\nthat outperforms the state-of-the-art models as the tool to analyze LLMs'\nresponses. Then, we prompt the LLMs with situational questions and ask them to\ngenerate Twitter posts and comments, respectively, in order to assess their\npersonalities when playing two different roles. Using the external personality\nevaluation method, we identify that the obtained personality types for LLMs are\nsignificantly different when generating posts versus comments, whereas humans\nshow a consistent personality profile in these two different situations. This\nshows that LLMs can exhibit different personalities based on different\nscenarios, thus highlighting a fundamental difference between personality in\nLLMs and humans. With our work, we call for a re-evaluation of personality\ndefinition and measurement in LLMs."}, "hash": "b2c5021c128d8dc58803df6aaba5a99854513a1dd779d06e9b6e60bb892675b8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ce2f7c1c-8499-4235-8cc2-e2a7f7fa4423", "node_type": "1", "metadata": {"page_label": "3", "file_name": "2402_14805v1.pdf", "Title of this paper": "Identifying Multiple Personalities in Large Language Models with External Evaluation", "Authors": "Xiaoyang Song, Yuta Adachi, Jessie Feng, Mouwei Lin, Linhao Yu, Frank Li, Akshat Gupta, Gopala Anumanchipalli, Simerjot Kaur", "Date published": "02/22/2024", "URL": "http://arxiv.org/abs/2402.14805v1", "summary": "As Large Language Models (LLMs) are integrated with human daily applications\nrapidly, many societal and ethical concerns are raised regarding the behavior\nof LLMs. One of the ways to comprehend LLMs' behavior is to analyze their\npersonalities. Many recent studies quantify LLMs' personalities using\nself-assessment tests that are created for humans. Yet many critiques question\nthe applicability and reliability of these self-assessment tests when applied\nto LLMs. In this paper, we investigate LLM personalities using an alternate\npersonality measurement method, which we refer to as the external evaluation\nmethod, where instead of prompting LLMs with multiple-choice questions in the\nLikert scale, we evaluate LLMs' personalities by analyzing their responses\ntoward open-ended situational questions using an external machine learning\nmodel. We first fine-tuned a Llama2-7B model as the MBTI personality predictor\nthat outperforms the state-of-the-art models as the tool to analyze LLMs'\nresponses. Then, we prompt the LLMs with situational questions and ask them to\ngenerate Twitter posts and comments, respectively, in order to assess their\npersonalities when playing two different roles. Using the external personality\nevaluation method, we identify that the obtained personality types for LLMs are\nsignificantly different when generating posts versus comments, whereas humans\nshow a consistent personality profile in these two different situations. This\nshows that LLMs can exhibit different personalities based on different\nscenarios, thus highlighting a fundamental difference between personality in\nLLMs and humans. With our work, we call for a re-evaluation of personality\ndefinition and measurement in LLMs."}, "hash": "ccf1ae9088bb5632252999de79f878fd5ae1c148c8d6169474ea4f5e05396931", "class_name": "RelatedNodeInfo"}}, "text": "The dataset\nconsists of over 8600 entries, where each entry cor-\nresponds to an individual\u2019s MBTI type and includes\nexcerpts from the last 50 posts and comments made\nby the individual on the PersonalityCafe forum3.\nDetailed introduction to example entry, label dis-\ntribution, and the tweet topic distributions can be\nfound in Appendix A.1.\nTo predict MBTI personality types from texts,\nMehta (2023) processed the texts using a BERT\nmodel (Devlin et al., 2018) and then trained an\nMLP to predict the MBTI personality type, while\nYang et al. (2023) utilized a graph convolutional\nneural network to learn the connections between\ndifferent posts made by an individual in order\nto make decent predictions. Furthermore, Tang\net al. (2023) proposed an attention-based denois-\ning framework (ADF) for MBTI personality detec-\ntion, where they trained the model to effectively\nextract personality signals from noisy and verbose\ntext data. However, a common weakness in these\nworks is that their methods predict the personality\ntype for each MBTI dimension separately. There-\nfore, although their methods achieve an average\n2https://www.kaggle.com/datasets/datasnaek/mbti-type\n3https://www.personalitycafe.com/forums/myers-briggs-\nforum.49/", "mimetype": "text/plain", "start_char_idx": 2955, "end_char_idx": 4185, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e9a29975-4290-4479-9e27-c98f54ff967b": {"__data__": {"id_": "e9a29975-4290-4479-9e27-c98f54ff967b", "embedding": null, "metadata": {"page_label": "4", "file_name": "2402_14805v1.pdf", "Title of this paper": "Identifying Multiple Personalities in Large Language Models with External Evaluation", "Authors": "Xiaoyang Song, Yuta Adachi, Jessie Feng, Mouwei Lin, Linhao Yu, Frank Li, Akshat Gupta, Gopala Anumanchipalli, Simerjot Kaur", "Date published": "02/22/2024", "URL": "http://arxiv.org/abs/2402.14805v1", "summary": "As Large Language Models (LLMs) are integrated with human daily applications\nrapidly, many societal and ethical concerns are raised regarding the behavior\nof LLMs. One of the ways to comprehend LLMs' behavior is to analyze their\npersonalities. Many recent studies quantify LLMs' personalities using\nself-assessment tests that are created for humans. Yet many critiques question\nthe applicability and reliability of these self-assessment tests when applied\nto LLMs. In this paper, we investigate LLM personalities using an alternate\npersonality measurement method, which we refer to as the external evaluation\nmethod, where instead of prompting LLMs with multiple-choice questions in the\nLikert scale, we evaluate LLMs' personalities by analyzing their responses\ntoward open-ended situational questions using an external machine learning\nmodel. We first fine-tuned a Llama2-7B model as the MBTI personality predictor\nthat outperforms the state-of-the-art models as the tool to analyze LLMs'\nresponses. Then, we prompt the LLMs with situational questions and ask them to\ngenerate Twitter posts and comments, respectively, in order to assess their\npersonalities when playing two different roles. Using the external personality\nevaluation method, we identify that the obtained personality types for LLMs are\nsignificantly different when generating posts versus comments, whereas humans\nshow a consistent personality profile in these two different situations. This\nshows that LLMs can exhibit different personalities based on different\nscenarios, thus highlighting a fundamental difference between personality in\nLLMs and humans. With our work, we call for a re-evaluation of personality\ndefinition and measurement in LLMs."}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "58c79f8e-1990-4b21-85e3-c55ea6499dcd", "node_type": "4", "metadata": {"page_label": "4", "file_name": "2402_14805v1.pdf", "Title of this paper": "Identifying Multiple Personalities in Large Language Models with External Evaluation", "Authors": "Xiaoyang Song, Yuta Adachi, Jessie Feng, Mouwei Lin, Linhao Yu, Frank Li, Akshat Gupta, Gopala Anumanchipalli, Simerjot Kaur", "Date published": "02/22/2024", "URL": "http://arxiv.org/abs/2402.14805v1", "summary": "As Large Language Models (LLMs) are integrated with human daily applications\nrapidly, many societal and ethical concerns are raised regarding the behavior\nof LLMs. One of the ways to comprehend LLMs' behavior is to analyze their\npersonalities. Many recent studies quantify LLMs' personalities using\nself-assessment tests that are created for humans. Yet many critiques question\nthe applicability and reliability of these self-assessment tests when applied\nto LLMs. In this paper, we investigate LLM personalities using an alternate\npersonality measurement method, which we refer to as the external evaluation\nmethod, where instead of prompting LLMs with multiple-choice questions in the\nLikert scale, we evaluate LLMs' personalities by analyzing their responses\ntoward open-ended situational questions using an external machine learning\nmodel. We first fine-tuned a Llama2-7B model as the MBTI personality predictor\nthat outperforms the state-of-the-art models as the tool to analyze LLMs'\nresponses. Then, we prompt the LLMs with situational questions and ask them to\ngenerate Twitter posts and comments, respectively, in order to assess their\npersonalities when playing two different roles. Using the external personality\nevaluation method, we identify that the obtained personality types for LLMs are\nsignificantly different when generating posts versus comments, whereas humans\nshow a consistent personality profile in these two different situations. This\nshows that LLMs can exhibit different personalities based on different\nscenarios, thus highlighting a fundamental difference between personality in\nLLMs and humans. With our work, we call for a re-evaluation of personality\ndefinition and measurement in LLMs."}, "hash": "6c6d9dc363c798df08c92902119f2d3a7e957f0ba35b2f8d29779e558ee24685", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "94654449-ec2f-4bcf-af46-24079bc55b20", "node_type": "1", "metadata": {}, "hash": "3a964f58b4f7941d7f1aad4666d12cba68286d7087e4c9326bd8fb7f062ddd4e", "class_name": "RelatedNodeInfo"}}, "text": "of around 70% prediction accuracy for each trait\ndimension, the overall performance remains myste-\nrious when the predictions for each dimension are\naggregated together because the errors and uncer-\ntainties accumulate multiplicatively. In our method,\nwe fine-tuned the popular Llama2-7B (Touvron\net al., 2023b) model as the predictor, which outper-\nformed their models significantly.\n3 Experiments\nIn this paper, we refer to using personality pre-\ndiction model to measure personality as an exter-\nnal personality evaluation method , juxtaposing it\nwith the personality self-assessment methods. The\nexperiments on the proposed external evaluation\nmethod are divided into three stages. Firstly, a\nLlama2-7B (Touvron et al., 2023b) model is fine-\ntuned on the public MBTI datasets as the personal-\nity detection model. Then, to perform external eval-\nuations, we prompted the LLMs with pre-processed\nsituational questions/scenarios and prompted them\nfor open-ended generations based on the input. Fi-\nnally, the collected responses from the LLMs were\nfed into the personality detection model to obtain\nthe evaluation results. In this paper, we base our ex-\nperiments on four popular chat-based LLMs: Chat-\nGPT (OpenAI, 2022) and three versions of Llama2\n(7B, 13B & 70B) (Touvron et al., 2023b). The over-\nall experiment pipeline can be found in Figure 2.\nIn addition, the computational resources used are\nintroduced in Appendix A.2.\n3.1 Stage I: Fine-tuning Llama2-7B-based\nPersonality Detection Model\nIn this work, we utilized the widely used afore-\nmentioned Kaggle MBTI dataset to fine-tune the\npersonality detection model. Although many previ-\nous studies build their models on the same dataset\n(Yang et al., 2023; Tang et al., 2023; Mehta, 2023),\nthere is room for improvement in the overall per-\nformance of their models, which may affect the\nreliability of personality prediction.\nIn particular, we proposed two models: (1) a\nbinary model and (2) a 16-class model. In terms\nof the binary model, we fine-tuned four pre-trained\nLlama2-7B models, and each of them is designed\nto be a binary classifier for each of the four MBTI\ndimensions.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2142, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "94654449-ec2f-4bcf-af46-24079bc55b20": {"__data__": {"id_": "94654449-ec2f-4bcf-af46-24079bc55b20", "embedding": null, "metadata": {"page_label": "4", "file_name": "2402_14805v1.pdf", "Title of this paper": "Identifying Multiple Personalities in Large Language Models with External Evaluation", "Authors": "Xiaoyang Song, Yuta Adachi, Jessie Feng, Mouwei Lin, Linhao Yu, Frank Li, Akshat Gupta, Gopala Anumanchipalli, Simerjot Kaur", "Date published": "02/22/2024", "URL": "http://arxiv.org/abs/2402.14805v1", "summary": "As Large Language Models (LLMs) are integrated with human daily applications\nrapidly, many societal and ethical concerns are raised regarding the behavior\nof LLMs. One of the ways to comprehend LLMs' behavior is to analyze their\npersonalities. Many recent studies quantify LLMs' personalities using\nself-assessment tests that are created for humans. Yet many critiques question\nthe applicability and reliability of these self-assessment tests when applied\nto LLMs. In this paper, we investigate LLM personalities using an alternate\npersonality measurement method, which we refer to as the external evaluation\nmethod, where instead of prompting LLMs with multiple-choice questions in the\nLikert scale, we evaluate LLMs' personalities by analyzing their responses\ntoward open-ended situational questions using an external machine learning\nmodel. We first fine-tuned a Llama2-7B model as the MBTI personality predictor\nthat outperforms the state-of-the-art models as the tool to analyze LLMs'\nresponses. Then, we prompt the LLMs with situational questions and ask them to\ngenerate Twitter posts and comments, respectively, in order to assess their\npersonalities when playing two different roles. Using the external personality\nevaluation method, we identify that the obtained personality types for LLMs are\nsignificantly different when generating posts versus comments, whereas humans\nshow a consistent personality profile in these two different situations. This\nshows that LLMs can exhibit different personalities based on different\nscenarios, thus highlighting a fundamental difference between personality in\nLLMs and humans. With our work, we call for a re-evaluation of personality\ndefinition and measurement in LLMs."}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "58c79f8e-1990-4b21-85e3-c55ea6499dcd", "node_type": "4", "metadata": {"page_label": "4", "file_name": "2402_14805v1.pdf", "Title of this paper": "Identifying Multiple Personalities in Large Language Models with External Evaluation", "Authors": "Xiaoyang Song, Yuta Adachi, Jessie Feng, Mouwei Lin, Linhao Yu, Frank Li, Akshat Gupta, Gopala Anumanchipalli, Simerjot Kaur", "Date published": "02/22/2024", "URL": "http://arxiv.org/abs/2402.14805v1", "summary": "As Large Language Models (LLMs) are integrated with human daily applications\nrapidly, many societal and ethical concerns are raised regarding the behavior\nof LLMs. One of the ways to comprehend LLMs' behavior is to analyze their\npersonalities. Many recent studies quantify LLMs' personalities using\nself-assessment tests that are created for humans. Yet many critiques question\nthe applicability and reliability of these self-assessment tests when applied\nto LLMs. In this paper, we investigate LLM personalities using an alternate\npersonality measurement method, which we refer to as the external evaluation\nmethod, where instead of prompting LLMs with multiple-choice questions in the\nLikert scale, we evaluate LLMs' personalities by analyzing their responses\ntoward open-ended situational questions using an external machine learning\nmodel. We first fine-tuned a Llama2-7B model as the MBTI personality predictor\nthat outperforms the state-of-the-art models as the tool to analyze LLMs'\nresponses. Then, we prompt the LLMs with situational questions and ask them to\ngenerate Twitter posts and comments, respectively, in order to assess their\npersonalities when playing two different roles. Using the external personality\nevaluation method, we identify that the obtained personality types for LLMs are\nsignificantly different when generating posts versus comments, whereas humans\nshow a consistent personality profile in these two different situations. This\nshows that LLMs can exhibit different personalities based on different\nscenarios, thus highlighting a fundamental difference between personality in\nLLMs and humans. With our work, we call for a re-evaluation of personality\ndefinition and measurement in LLMs."}, "hash": "6c6d9dc363c798df08c92902119f2d3a7e957f0ba35b2f8d29779e558ee24685", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e9a29975-4290-4479-9e27-c98f54ff967b", "node_type": "1", "metadata": {"page_label": "4", "file_name": "2402_14805v1.pdf", "Title of this paper": "Identifying Multiple Personalities in Large Language Models with External Evaluation", "Authors": "Xiaoyang Song, Yuta Adachi, Jessie Feng, Mouwei Lin, Linhao Yu, Frank Li, Akshat Gupta, Gopala Anumanchipalli, Simerjot Kaur", "Date published": "02/22/2024", "URL": "http://arxiv.org/abs/2402.14805v1", "summary": "As Large Language Models (LLMs) are integrated with human daily applications\nrapidly, many societal and ethical concerns are raised regarding the behavior\nof LLMs. One of the ways to comprehend LLMs' behavior is to analyze their\npersonalities. Many recent studies quantify LLMs' personalities using\nself-assessment tests that are created for humans. Yet many critiques question\nthe applicability and reliability of these self-assessment tests when applied\nto LLMs. In this paper, we investigate LLM personalities using an alternate\npersonality measurement method, which we refer to as the external evaluation\nmethod, where instead of prompting LLMs with multiple-choice questions in the\nLikert scale, we evaluate LLMs' personalities by analyzing their responses\ntoward open-ended situational questions using an external machine learning\nmodel. We first fine-tuned a Llama2-7B model as the MBTI personality predictor\nthat outperforms the state-of-the-art models as the tool to analyze LLMs'\nresponses. Then, we prompt the LLMs with situational questions and ask them to\ngenerate Twitter posts and comments, respectively, in order to assess their\npersonalities when playing two different roles. Using the external personality\nevaluation method, we identify that the obtained personality types for LLMs are\nsignificantly different when generating posts versus comments, whereas humans\nshow a consistent personality profile in these two different situations. This\nshows that LLMs can exhibit different personalities based on different\nscenarios, thus highlighting a fundamental difference between personality in\nLLMs and humans. With our work, we call for a re-evaluation of personality\ndefinition and measurement in LLMs."}, "hash": "56a0051bcc3fbf269de6ee17da8603a9c2cc295574ef8eb7162be95e4803175d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7bf4ec90-5c9e-4e64-9622-2a969949f78a", "node_type": "1", "metadata": {}, "hash": "0c9c3ddbc3a1cf6d9f0f19812f0174e6718611f0ba3fa333d54afe55bff82b7d", "class_name": "RelatedNodeInfo"}}, "text": "3.1 Stage I: Fine-tuning Llama2-7B-based\nPersonality Detection Model\nIn this work, we utilized the widely used afore-\nmentioned Kaggle MBTI dataset to fine-tune the\npersonality detection model. Although many previ-\nous studies build their models on the same dataset\n(Yang et al., 2023; Tang et al., 2023; Mehta, 2023),\nthere is room for improvement in the overall per-\nformance of their models, which may affect the\nreliability of personality prediction.\nIn particular, we proposed two models: (1) a\nbinary model and (2) a 16-class model. In terms\nof the binary model, we fine-tuned four pre-trained\nLlama2-7B models, and each of them is designed\nto be a binary classifier for each of the four MBTI\ndimensions. The predictions are then aggregated to\nform the final prediction, which is similar to most\nof the previous works (Tang et al., 2023; Yang\net al., 2023; Mehta, 2023). On the other hand, inthe 16-classes model, a pre-trained Llama2-7B is\ndirectly fine-tuned to predict one of the sixteen\nclasses. Aligning with the Kaggle dataset format,\nboth models take 50 posts from an individual as\ninputs at a time to predict personality. The fine-\ntuning process was conducted under the LoRA (Hu\net al., 2021) framework, where the target modules\nto fine-tune are the query (q) and the value (v)\nlayers. We chose the rank to be r= 16 and fine-\ntuned it for only 5 epochs with a learning rate of\n10\u22124and batch size of 8. The training started with a\nwarm-up phase for the first 100 iterations, followed\nby a linear learning rate decay.", "mimetype": "text/plain", "start_char_idx": 1432, "end_char_idx": 2962, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7bf4ec90-5c9e-4e64-9622-2a969949f78a": {"__data__": {"id_": "7bf4ec90-5c9e-4e64-9622-2a969949f78a", "embedding": null, "metadata": {"page_label": "4", "file_name": "2402_14805v1.pdf", "Title of this paper": "Identifying Multiple Personalities in Large Language Models with External Evaluation", "Authors": "Xiaoyang Song, Yuta Adachi, Jessie Feng, Mouwei Lin, Linhao Yu, Frank Li, Akshat Gupta, Gopala Anumanchipalli, Simerjot Kaur", "Date published": "02/22/2024", "URL": "http://arxiv.org/abs/2402.14805v1", "summary": "As Large Language Models (LLMs) are integrated with human daily applications\nrapidly, many societal and ethical concerns are raised regarding the behavior\nof LLMs. One of the ways to comprehend LLMs' behavior is to analyze their\npersonalities. Many recent studies quantify LLMs' personalities using\nself-assessment tests that are created for humans. Yet many critiques question\nthe applicability and reliability of these self-assessment tests when applied\nto LLMs. In this paper, we investigate LLM personalities using an alternate\npersonality measurement method, which we refer to as the external evaluation\nmethod, where instead of prompting LLMs with multiple-choice questions in the\nLikert scale, we evaluate LLMs' personalities by analyzing their responses\ntoward open-ended situational questions using an external machine learning\nmodel. We first fine-tuned a Llama2-7B model as the MBTI personality predictor\nthat outperforms the state-of-the-art models as the tool to analyze LLMs'\nresponses. Then, we prompt the LLMs with situational questions and ask them to\ngenerate Twitter posts and comments, respectively, in order to assess their\npersonalities when playing two different roles. Using the external personality\nevaluation method, we identify that the obtained personality types for LLMs are\nsignificantly different when generating posts versus comments, whereas humans\nshow a consistent personality profile in these two different situations. This\nshows that LLMs can exhibit different personalities based on different\nscenarios, thus highlighting a fundamental difference between personality in\nLLMs and humans. With our work, we call for a re-evaluation of personality\ndefinition and measurement in LLMs."}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "58c79f8e-1990-4b21-85e3-c55ea6499dcd", "node_type": "4", "metadata": {"page_label": "4", "file_name": "2402_14805v1.pdf", "Title of this paper": "Identifying Multiple Personalities in Large Language Models with External Evaluation", "Authors": "Xiaoyang Song, Yuta Adachi, Jessie Feng, Mouwei Lin, Linhao Yu, Frank Li, Akshat Gupta, Gopala Anumanchipalli, Simerjot Kaur", "Date published": "02/22/2024", "URL": "http://arxiv.org/abs/2402.14805v1", "summary": "As Large Language Models (LLMs) are integrated with human daily applications\nrapidly, many societal and ethical concerns are raised regarding the behavior\nof LLMs. One of the ways to comprehend LLMs' behavior is to analyze their\npersonalities. Many recent studies quantify LLMs' personalities using\nself-assessment tests that are created for humans. Yet many critiques question\nthe applicability and reliability of these self-assessment tests when applied\nto LLMs. In this paper, we investigate LLM personalities using an alternate\npersonality measurement method, which we refer to as the external evaluation\nmethod, where instead of prompting LLMs with multiple-choice questions in the\nLikert scale, we evaluate LLMs' personalities by analyzing their responses\ntoward open-ended situational questions using an external machine learning\nmodel. We first fine-tuned a Llama2-7B model as the MBTI personality predictor\nthat outperforms the state-of-the-art models as the tool to analyze LLMs'\nresponses. Then, we prompt the LLMs with situational questions and ask them to\ngenerate Twitter posts and comments, respectively, in order to assess their\npersonalities when playing two different roles. Using the external personality\nevaluation method, we identify that the obtained personality types for LLMs are\nsignificantly different when generating posts versus comments, whereas humans\nshow a consistent personality profile in these two different situations. This\nshows that LLMs can exhibit different personalities based on different\nscenarios, thus highlighting a fundamental difference between personality in\nLLMs and humans. With our work, we call for a re-evaluation of personality\ndefinition and measurement in LLMs."}, "hash": "6c6d9dc363c798df08c92902119f2d3a7e957f0ba35b2f8d29779e558ee24685", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "94654449-ec2f-4bcf-af46-24079bc55b20", "node_type": "1", "metadata": {"page_label": "4", "file_name": "2402_14805v1.pdf", "Title of this paper": "Identifying Multiple Personalities in Large Language Models with External Evaluation", "Authors": "Xiaoyang Song, Yuta Adachi, Jessie Feng, Mouwei Lin, Linhao Yu, Frank Li, Akshat Gupta, Gopala Anumanchipalli, Simerjot Kaur", "Date published": "02/22/2024", "URL": "http://arxiv.org/abs/2402.14805v1", "summary": "As Large Language Models (LLMs) are integrated with human daily applications\nrapidly, many societal and ethical concerns are raised regarding the behavior\nof LLMs. One of the ways to comprehend LLMs' behavior is to analyze their\npersonalities. Many recent studies quantify LLMs' personalities using\nself-assessment tests that are created for humans. Yet many critiques question\nthe applicability and reliability of these self-assessment tests when applied\nto LLMs. In this paper, we investigate LLM personalities using an alternate\npersonality measurement method, which we refer to as the external evaluation\nmethod, where instead of prompting LLMs with multiple-choice questions in the\nLikert scale, we evaluate LLMs' personalities by analyzing their responses\ntoward open-ended situational questions using an external machine learning\nmodel. We first fine-tuned a Llama2-7B model as the MBTI personality predictor\nthat outperforms the state-of-the-art models as the tool to analyze LLMs'\nresponses. Then, we prompt the LLMs with situational questions and ask them to\ngenerate Twitter posts and comments, respectively, in order to assess their\npersonalities when playing two different roles. Using the external personality\nevaluation method, we identify that the obtained personality types for LLMs are\nsignificantly different when generating posts versus comments, whereas humans\nshow a consistent personality profile in these two different situations. This\nshows that LLMs can exhibit different personalities based on different\nscenarios, thus highlighting a fundamental difference between personality in\nLLMs and humans. With our work, we call for a re-evaluation of personality\ndefinition and measurement in LLMs."}, "hash": "a7fdbfb5e632b16fba4b97e48b00bb6bc5d49ae3e4baba2136930040818cbf51", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a9b90eb4-7783-49bf-9b6f-0d297a2508a9", "node_type": "1", "metadata": {}, "hash": "646d2567db95a2f00edc8a3bb468cd50c5a7a2a8ea38fb1467b67a8bb9aa6480", "class_name": "RelatedNodeInfo"}}, "text": "On the other hand, inthe 16-classes model, a pre-trained Llama2-7B is\ndirectly fine-tuned to predict one of the sixteen\nclasses. Aligning with the Kaggle dataset format,\nboth models take 50 posts from an individual as\ninputs at a time to predict personality. The fine-\ntuning process was conducted under the LoRA (Hu\net al., 2021) framework, where the target modules\nto fine-tune are the query (q) and the value (v)\nlayers. We chose the rank to be r= 16 and fine-\ntuned it for only 5 epochs with a learning rate of\n10\u22124and batch size of 8. The training started with a\nwarm-up phase for the first 100 iterations, followed\nby a linear learning rate decay.\nModels Accuracy F1 score Precision Recall\nBERT-Base + MLP\n(Mehta, 2023)73.1 63.2 78.0 65.5\nD-DGCN\n(Yang et al., 2023)78.2 53.5 60.5 49.1\nADF\n(Tang et al., 2023)61.0 38.8 47.5 56.8\nLlama2-7B + FT (B)\n(ours)93.3 91.1 92.3 90.1\nLlama2-7B + FT (16)\n(ours)93.5 91.4 92.0 90.8\nTable 1: Average performance of personality detection\nmodels on each of the four MBTI dimensions. Each\ncell is a numerical average of the performance of each\nbinary model in percentage.\nModels Accuracy F1 score Precision Recall\nBERT-Base + MLP\n(Mehta, 2023)29.4 8.70 14.1 12.0\nD-DGCN\n(Yang et al., 2023)37.6 35.9 35.9 37.6\nADF\n(Tang et al., 2023)14.0 4.10 3.28 5.74\nLlama2-7B + FT (B)\n(ours)81.0 74.3 76.9 73.1\nLlama2-7B + FT (16)\n(ours)81.7 76.9 79.8 75.2\nTable 2: Performance of personality detection models\non 16-class MBTI prediction tasks. The numbers in the\ncell are all percentages.", "mimetype": "text/plain", "start_char_idx": 2309, "end_char_idx": 3823, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a9b90eb4-7783-49bf-9b6f-0d297a2508a9": {"__data__": {"id_": "a9b90eb4-7783-49bf-9b6f-0d297a2508a9", "embedding": null, "metadata": {"page_label": "4", "file_name": "2402_14805v1.pdf", "Title of this paper": "Identifying Multiple Personalities in Large Language Models with External Evaluation", "Authors": "Xiaoyang Song, Yuta Adachi, Jessie Feng, Mouwei Lin, Linhao Yu, Frank Li, Akshat Gupta, Gopala Anumanchipalli, Simerjot Kaur", "Date published": "02/22/2024", "URL": "http://arxiv.org/abs/2402.14805v1", "summary": "As Large Language Models (LLMs) are integrated with human daily applications\nrapidly, many societal and ethical concerns are raised regarding the behavior\nof LLMs. One of the ways to comprehend LLMs' behavior is to analyze their\npersonalities. Many recent studies quantify LLMs' personalities using\nself-assessment tests that are created for humans. Yet many critiques question\nthe applicability and reliability of these self-assessment tests when applied\nto LLMs. In this paper, we investigate LLM personalities using an alternate\npersonality measurement method, which we refer to as the external evaluation\nmethod, where instead of prompting LLMs with multiple-choice questions in the\nLikert scale, we evaluate LLMs' personalities by analyzing their responses\ntoward open-ended situational questions using an external machine learning\nmodel. We first fine-tuned a Llama2-7B model as the MBTI personality predictor\nthat outperforms the state-of-the-art models as the tool to analyze LLMs'\nresponses. Then, we prompt the LLMs with situational questions and ask them to\ngenerate Twitter posts and comments, respectively, in order to assess their\npersonalities when playing two different roles. Using the external personality\nevaluation method, we identify that the obtained personality types for LLMs are\nsignificantly different when generating posts versus comments, whereas humans\nshow a consistent personality profile in these two different situations. This\nshows that LLMs can exhibit different personalities based on different\nscenarios, thus highlighting a fundamental difference between personality in\nLLMs and humans. With our work, we call for a re-evaluation of personality\ndefinition and measurement in LLMs."}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "58c79f8e-1990-4b21-85e3-c55ea6499dcd", "node_type": "4", "metadata": {"page_label": "4", "file_name": "2402_14805v1.pdf", "Title of this paper": "Identifying Multiple Personalities in Large Language Models with External Evaluation", "Authors": "Xiaoyang Song, Yuta Adachi, Jessie Feng, Mouwei Lin, Linhao Yu, Frank Li, Akshat Gupta, Gopala Anumanchipalli, Simerjot Kaur", "Date published": "02/22/2024", "URL": "http://arxiv.org/abs/2402.14805v1", "summary": "As Large Language Models (LLMs) are integrated with human daily applications\nrapidly, many societal and ethical concerns are raised regarding the behavior\nof LLMs. One of the ways to comprehend LLMs' behavior is to analyze their\npersonalities. Many recent studies quantify LLMs' personalities using\nself-assessment tests that are created for humans. Yet many critiques question\nthe applicability and reliability of these self-assessment tests when applied\nto LLMs. In this paper, we investigate LLM personalities using an alternate\npersonality measurement method, which we refer to as the external evaluation\nmethod, where instead of prompting LLMs with multiple-choice questions in the\nLikert scale, we evaluate LLMs' personalities by analyzing their responses\ntoward open-ended situational questions using an external machine learning\nmodel. We first fine-tuned a Llama2-7B model as the MBTI personality predictor\nthat outperforms the state-of-the-art models as the tool to analyze LLMs'\nresponses. Then, we prompt the LLMs with situational questions and ask them to\ngenerate Twitter posts and comments, respectively, in order to assess their\npersonalities when playing two different roles. Using the external personality\nevaluation method, we identify that the obtained personality types for LLMs are\nsignificantly different when generating posts versus comments, whereas humans\nshow a consistent personality profile in these two different situations. This\nshows that LLMs can exhibit different personalities based on different\nscenarios, thus highlighting a fundamental difference between personality in\nLLMs and humans. With our work, we call for a re-evaluation of personality\ndefinition and measurement in LLMs."}, "hash": "6c6d9dc363c798df08c92902119f2d3a7e957f0ba35b2f8d29779e558ee24685", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7bf4ec90-5c9e-4e64-9622-2a969949f78a", "node_type": "1", "metadata": {"page_label": "4", "file_name": "2402_14805v1.pdf", "Title of this paper": "Identifying Multiple Personalities in Large Language Models with External Evaluation", "Authors": "Xiaoyang Song, Yuta Adachi, Jessie Feng, Mouwei Lin, Linhao Yu, Frank Li, Akshat Gupta, Gopala Anumanchipalli, Simerjot Kaur", "Date published": "02/22/2024", "URL": "http://arxiv.org/abs/2402.14805v1", "summary": "As Large Language Models (LLMs) are integrated with human daily applications\nrapidly, many societal and ethical concerns are raised regarding the behavior\nof LLMs. One of the ways to comprehend LLMs' behavior is to analyze their\npersonalities. Many recent studies quantify LLMs' personalities using\nself-assessment tests that are created for humans. Yet many critiques question\nthe applicability and reliability of these self-assessment tests when applied\nto LLMs. In this paper, we investigate LLM personalities using an alternate\npersonality measurement method, which we refer to as the external evaluation\nmethod, where instead of prompting LLMs with multiple-choice questions in the\nLikert scale, we evaluate LLMs' personalities by analyzing their responses\ntoward open-ended situational questions using an external machine learning\nmodel. We first fine-tuned a Llama2-7B model as the MBTI personality predictor\nthat outperforms the state-of-the-art models as the tool to analyze LLMs'\nresponses. Then, we prompt the LLMs with situational questions and ask them to\ngenerate Twitter posts and comments, respectively, in order to assess their\npersonalities when playing two different roles. Using the external personality\nevaluation method, we identify that the obtained personality types for LLMs are\nsignificantly different when generating posts versus comments, whereas humans\nshow a consistent personality profile in these two different situations. This\nshows that LLMs can exhibit different personalities based on different\nscenarios, thus highlighting a fundamental difference between personality in\nLLMs and humans. With our work, we call for a re-evaluation of personality\ndefinition and measurement in LLMs."}, "hash": "d21aaf2060ef63bdd988a587c3d68a720a2df8c2431e796c241a1b74426078b7", "class_name": "RelatedNodeInfo"}}, "text": "Models Accuracy F1 score Precision Recall\nBERT-Base + MLP\n(Mehta, 2023)29.4 8.70 14.1 12.0\nD-DGCN\n(Yang et al., 2023)37.6 35.9 35.9 37.6\nADF\n(Tang et al., 2023)14.0 4.10 3.28 5.74\nLlama2-7B + FT (B)\n(ours)81.0 74.3 76.9 73.1\nLlama2-7B + FT (16)\n(ours)81.7 76.9 79.8 75.2\nTable 2: Performance of personality detection models\non 16-class MBTI prediction tasks. The numbers in the\ncell are all percentages.\nThe testing performance of the proposed person-\nality detection model and the baselines are reported.\nWith the capacities of LLMs to understand texts\nand extract key knowledge, both the fine-tuned bi-\nnary model and 16-classes model outperform the\nstate-of-the-art models significantly in all metrics.\nIn Table 1, we report the average performance of\nthe model for each trait dimension. To do this, the\naccuracy of the model in predicting each of the\nfour dimensions of MBTI personality is calculated\nseparately. Then the accuracy for these four dimen-\nsions is averaged to present the numbers in Table 1,", "mimetype": "text/plain", "start_char_idx": 3420, "end_char_idx": 4429, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "dfe827a6-1755-4688-bf93-82f12621f425": {"__data__": {"id_": "dfe827a6-1755-4688-bf93-82f12621f425", "embedding": null, "metadata": {"page_label": "5", "file_name": "2402_14805v1.pdf", "Title of this paper": "Identifying Multiple Personalities in Large Language Models with External Evaluation", "Authors": "Xiaoyang Song, Yuta Adachi, Jessie Feng, Mouwei Lin, Linhao Yu, Frank Li, Akshat Gupta, Gopala Anumanchipalli, Simerjot Kaur", "Date published": "02/22/2024", "URL": "http://arxiv.org/abs/2402.14805v1", "summary": "As Large Language Models (LLMs) are integrated with human daily applications\nrapidly, many societal and ethical concerns are raised regarding the behavior\nof LLMs. One of the ways to comprehend LLMs' behavior is to analyze their\npersonalities. Many recent studies quantify LLMs' personalities using\nself-assessment tests that are created for humans. Yet many critiques question\nthe applicability and reliability of these self-assessment tests when applied\nto LLMs. In this paper, we investigate LLM personalities using an alternate\npersonality measurement method, which we refer to as the external evaluation\nmethod, where instead of prompting LLMs with multiple-choice questions in the\nLikert scale, we evaluate LLMs' personalities by analyzing their responses\ntoward open-ended situational questions using an external machine learning\nmodel. We first fine-tuned a Llama2-7B model as the MBTI personality predictor\nthat outperforms the state-of-the-art models as the tool to analyze LLMs'\nresponses. Then, we prompt the LLMs with situational questions and ask them to\ngenerate Twitter posts and comments, respectively, in order to assess their\npersonalities when playing two different roles. Using the external personality\nevaluation method, we identify that the obtained personality types for LLMs are\nsignificantly different when generating posts versus comments, whereas humans\nshow a consistent personality profile in these two different situations. This\nshows that LLMs can exhibit different personalities based on different\nscenarios, thus highlighting a fundamental difference between personality in\nLLMs and humans. With our work, we call for a re-evaluation of personality\ndefinition and measurement in LLMs."}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f4512416-7ea8-4ea2-af6c-c57824c81402", "node_type": "4", "metadata": {"page_label": "5", "file_name": "2402_14805v1.pdf", "Title of this paper": "Identifying Multiple Personalities in Large Language Models with External Evaluation", "Authors": "Xiaoyang Song, Yuta Adachi, Jessie Feng, Mouwei Lin, Linhao Yu, Frank Li, Akshat Gupta, Gopala Anumanchipalli, Simerjot Kaur", "Date published": "02/22/2024", "URL": "http://arxiv.org/abs/2402.14805v1", "summary": "As Large Language Models (LLMs) are integrated with human daily applications\nrapidly, many societal and ethical concerns are raised regarding the behavior\nof LLMs. One of the ways to comprehend LLMs' behavior is to analyze their\npersonalities. Many recent studies quantify LLMs' personalities using\nself-assessment tests that are created for humans. Yet many critiques question\nthe applicability and reliability of these self-assessment tests when applied\nto LLMs. In this paper, we investigate LLM personalities using an alternate\npersonality measurement method, which we refer to as the external evaluation\nmethod, where instead of prompting LLMs with multiple-choice questions in the\nLikert scale, we evaluate LLMs' personalities by analyzing their responses\ntoward open-ended situational questions using an external machine learning\nmodel. We first fine-tuned a Llama2-7B model as the MBTI personality predictor\nthat outperforms the state-of-the-art models as the tool to analyze LLMs'\nresponses. Then, we prompt the LLMs with situational questions and ask them to\ngenerate Twitter posts and comments, respectively, in order to assess their\npersonalities when playing two different roles. Using the external personality\nevaluation method, we identify that the obtained personality types for LLMs are\nsignificantly different when generating posts versus comments, whereas humans\nshow a consistent personality profile in these two different situations. This\nshows that LLMs can exhibit different personalities based on different\nscenarios, thus highlighting a fundamental difference between personality in\nLLMs and humans. With our work, we call for a re-evaluation of personality\ndefinition and measurement in LLMs."}, "hash": "b3d561c55d5a2fff8e6d6acd5e63a725cc6c3e91350640222df31f31559e5777", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "698f76d2-1a6c-4e47-8f21-eebd90b9ccba", "node_type": "1", "metadata": {}, "hash": "22e2bab63fb082d617ac72fc32b8b6ff6a2844b067df3fbff6eab8caec715c64", "class_name": "RelatedNodeInfo"}}, "text": "Figure 2: Methodology flowchart.\nwhich is the most common form of evaluation used\nin prior work (Tang et al., 2023; Yang et al., 2023).\nOne drawback of such an evaluation is that it\ndoesn\u2019t take into account the overall accuracy of\nthe model in predicting one of the 16 classes of\npersonality. In Table 2, we take into account the\noverall prediction errors and present the aggregated\n16-class classification performance. The difference\nin performance between Table 1 and Table 2 for\nthe same models highlights the effectiveness of our\nmodel. For instance, while the baseline methods\nachieve around 70% accuracy when averaged over\neach trait, when those predictions are aggregated to-\ngether to predict the actual 16-classes MBTI type,\nthe accuracy drops significantly to around 30%.\nHowever, for our fine-tuned binary model, the pre-\ndiction accuracy for each trait is 93.3%while the\noverall accuracy remains as high as 81.0%. This\nevaluation method helps highlight error accumula-\ntion during the personality evaluation procedure,\nrendering the experimental results more accurate.\nWe observed that the binary model and 16-classes\nmodel yield comparable performance for nearly all\nthe metrics. In later experiments, we chose to stick\nto the binary model as it is often used in previous\nworks (Tang et al., 2023; Yang et al., 2023; Mehta,\n2023), making the results comparable. Complete\nresults for fine-tuned models can be found in Ap-\npendix A.3.\n3.2 Stage II: Collecting LLMs Responses\nToward Open-ended Situational Questions\nIn the second stage, we prompt LLMs with open-\nended situational questions and collect their re-\nsponses as inputs for external evaluation. This is\ndone to collect social media post data generated\nby an LLM which can be used as input for ourpersonality prediction model.\nPrompting Pipeline. To effectively analyze daily\nnews, prompt LLMs, and process their responses,\nwe designed a prompting pipeline. The pipeline\nstarts by analyzing news from different topics and\nsummarizing them into the latest news events be-\nfore prompting the LLMs. We analyzed the latest\nnews articles from November 2023 to make sure\nthat the events were not present in the training data\nof the personality prediction models. These latest\nnews events serve as the basis for prompting the\nLLMs to generate tweets, with the LLMs taking on\nthe role of individuals posting tweets about these\nnews events.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2402, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "698f76d2-1a6c-4e47-8f21-eebd90b9ccba": {"__data__": {"id_": "698f76d2-1a6c-4e47-8f21-eebd90b9ccba", "embedding": null, "metadata": {"page_label": "5", "file_name": "2402_14805v1.pdf", "Title of this paper": "Identifying Multiple Personalities in Large Language Models with External Evaluation", "Authors": "Xiaoyang Song, Yuta Adachi, Jessie Feng, Mouwei Lin, Linhao Yu, Frank Li, Akshat Gupta, Gopala Anumanchipalli, Simerjot Kaur", "Date published": "02/22/2024", "URL": "http://arxiv.org/abs/2402.14805v1", "summary": "As Large Language Models (LLMs) are integrated with human daily applications\nrapidly, many societal and ethical concerns are raised regarding the behavior\nof LLMs. One of the ways to comprehend LLMs' behavior is to analyze their\npersonalities. Many recent studies quantify LLMs' personalities using\nself-assessment tests that are created for humans. Yet many critiques question\nthe applicability and reliability of these self-assessment tests when applied\nto LLMs. In this paper, we investigate LLM personalities using an alternate\npersonality measurement method, which we refer to as the external evaluation\nmethod, where instead of prompting LLMs with multiple-choice questions in the\nLikert scale, we evaluate LLMs' personalities by analyzing their responses\ntoward open-ended situational questions using an external machine learning\nmodel. We first fine-tuned a Llama2-7B model as the MBTI personality predictor\nthat outperforms the state-of-the-art models as the tool to analyze LLMs'\nresponses. Then, we prompt the LLMs with situational questions and ask them to\ngenerate Twitter posts and comments, respectively, in order to assess their\npersonalities when playing two different roles. Using the external personality\nevaluation method, we identify that the obtained personality types for LLMs are\nsignificantly different when generating posts versus comments, whereas humans\nshow a consistent personality profile in these two different situations. This\nshows that LLMs can exhibit different personalities based on different\nscenarios, thus highlighting a fundamental difference between personality in\nLLMs and humans. With our work, we call for a re-evaluation of personality\ndefinition and measurement in LLMs."}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f4512416-7ea8-4ea2-af6c-c57824c81402", "node_type": "4", "metadata": {"page_label": "5", "file_name": "2402_14805v1.pdf", "Title of this paper": "Identifying Multiple Personalities in Large Language Models with External Evaluation", "Authors": "Xiaoyang Song, Yuta Adachi, Jessie Feng, Mouwei Lin, Linhao Yu, Frank Li, Akshat Gupta, Gopala Anumanchipalli, Simerjot Kaur", "Date published": "02/22/2024", "URL": "http://arxiv.org/abs/2402.14805v1", "summary": "As Large Language Models (LLMs) are integrated with human daily applications\nrapidly, many societal and ethical concerns are raised regarding the behavior\nof LLMs. One of the ways to comprehend LLMs' behavior is to analyze their\npersonalities. Many recent studies quantify LLMs' personalities using\nself-assessment tests that are created for humans. Yet many critiques question\nthe applicability and reliability of these self-assessment tests when applied\nto LLMs. In this paper, we investigate LLM personalities using an alternate\npersonality measurement method, which we refer to as the external evaluation\nmethod, where instead of prompting LLMs with multiple-choice questions in the\nLikert scale, we evaluate LLMs' personalities by analyzing their responses\ntoward open-ended situational questions using an external machine learning\nmodel. We first fine-tuned a Llama2-7B model as the MBTI personality predictor\nthat outperforms the state-of-the-art models as the tool to analyze LLMs'\nresponses. Then, we prompt the LLMs with situational questions and ask them to\ngenerate Twitter posts and comments, respectively, in order to assess their\npersonalities when playing two different roles. Using the external personality\nevaluation method, we identify that the obtained personality types for LLMs are\nsignificantly different when generating posts versus comments, whereas humans\nshow a consistent personality profile in these two different situations. This\nshows that LLMs can exhibit different personalities based on different\nscenarios, thus highlighting a fundamental difference between personality in\nLLMs and humans. With our work, we call for a re-evaluation of personality\ndefinition and measurement in LLMs."}, "hash": "b3d561c55d5a2fff8e6d6acd5e63a725cc6c3e91350640222df31f31559e5777", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "dfe827a6-1755-4688-bf93-82f12621f425", "node_type": "1", "metadata": {"page_label": "5", "file_name": "2402_14805v1.pdf", "Title of this paper": "Identifying Multiple Personalities in Large Language Models with External Evaluation", "Authors": "Xiaoyang Song, Yuta Adachi, Jessie Feng, Mouwei Lin, Linhao Yu, Frank Li, Akshat Gupta, Gopala Anumanchipalli, Simerjot Kaur", "Date published": "02/22/2024", "URL": "http://arxiv.org/abs/2402.14805v1", "summary": "As Large Language Models (LLMs) are integrated with human daily applications\nrapidly, many societal and ethical concerns are raised regarding the behavior\nof LLMs. One of the ways to comprehend LLMs' behavior is to analyze their\npersonalities. Many recent studies quantify LLMs' personalities using\nself-assessment tests that are created for humans. Yet many critiques question\nthe applicability and reliability of these self-assessment tests when applied\nto LLMs. In this paper, we investigate LLM personalities using an alternate\npersonality measurement method, which we refer to as the external evaluation\nmethod, where instead of prompting LLMs with multiple-choice questions in the\nLikert scale, we evaluate LLMs' personalities by analyzing their responses\ntoward open-ended situational questions using an external machine learning\nmodel. We first fine-tuned a Llama2-7B model as the MBTI personality predictor\nthat outperforms the state-of-the-art models as the tool to analyze LLMs'\nresponses. Then, we prompt the LLMs with situational questions and ask them to\ngenerate Twitter posts and comments, respectively, in order to assess their\npersonalities when playing two different roles. Using the external personality\nevaluation method, we identify that the obtained personality types for LLMs are\nsignificantly different when generating posts versus comments, whereas humans\nshow a consistent personality profile in these two different situations. This\nshows that LLMs can exhibit different personalities based on different\nscenarios, thus highlighting a fundamental difference between personality in\nLLMs and humans. With our work, we call for a re-evaluation of personality\ndefinition and measurement in LLMs."}, "hash": "0a3dede6e8ef27f8490a44aca8fa86d40e3c09ee99a7c5d61e65ee2b425f3b00", "class_name": "RelatedNodeInfo"}}, "text": "This is\ndone to collect social media post data generated\nby an LLM which can be used as input for ourpersonality prediction model.\nPrompting Pipeline. To effectively analyze daily\nnews, prompt LLMs, and process their responses,\nwe designed a prompting pipeline. The pipeline\nstarts by analyzing news from different topics and\nsummarizing them into the latest news events be-\nfore prompting the LLMs. We analyzed the latest\nnews articles from November 2023 to make sure\nthat the events were not present in the training data\nof the personality prediction models. These latest\nnews events serve as the basis for prompting the\nLLMs to generate tweets, with the LLMs taking on\nthe role of individuals posting tweets about these\nnews events. Additionally, we also gather online\ntweets written by other individuals and prompt the\nLLMs to generate comments in response to these\ntweets. This allows the LLMs to assume differ-\nent roles, specifically that of replying to tweets\nmade by others. To prevent any potential data leak-\nage, these tweets are collected exclusively from the\nmonth of November 2023.\nDepending on whether we want the LLMs to\nwrite posts (i.e. tweets ) about the summarized\nevents or comments (i.e. replies ) to the existing\ntweets, the prompting templates are chosen to be\ndifferent accordingly. For instance, the template\nfor prompting LLMs to write a post is the follow-\ning: \u201c As a user on Twitter. Write a tweet on the\nfollowing contents: [summarized contents] \u201d. It\nis noteworthy that we chose not to engineer the\nprompting templates in order to keep them as sim-\nple as possible, so as to not induce behavior of\nLLMs. The exact prompts used in this work are\nshown in Table 3. We use the simplest prompts\nto elicit generation in order to not bias the model\nthrough prompts for generations.\nIn total, we analyzed several thousand news arti-", "mimetype": "text/plain", "start_char_idx": 1667, "end_char_idx": 3524, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8902a363-6801-4368-99e7-7fe1250bb7a8": {"__data__": {"id_": "8902a363-6801-4368-99e7-7fe1250bb7a8", "embedding": null, "metadata": {"page_label": "6", "file_name": "2402_14805v1.pdf", "Title of this paper": "Identifying Multiple Personalities in Large Language Models with External Evaluation", "Authors": "Xiaoyang Song, Yuta Adachi, Jessie Feng, Mouwei Lin, Linhao Yu, Frank Li, Akshat Gupta, Gopala Anumanchipalli, Simerjot Kaur", "Date published": "02/22/2024", "URL": "http://arxiv.org/abs/2402.14805v1", "summary": "As Large Language Models (LLMs) are integrated with human daily applications\nrapidly, many societal and ethical concerns are raised regarding the behavior\nof LLMs. One of the ways to comprehend LLMs' behavior is to analyze their\npersonalities. Many recent studies quantify LLMs' personalities using\nself-assessment tests that are created for humans. Yet many critiques question\nthe applicability and reliability of these self-assessment tests when applied\nto LLMs. In this paper, we investigate LLM personalities using an alternate\npersonality measurement method, which we refer to as the external evaluation\nmethod, where instead of prompting LLMs with multiple-choice questions in the\nLikert scale, we evaluate LLMs' personalities by analyzing their responses\ntoward open-ended situational questions using an external machine learning\nmodel. We first fine-tuned a Llama2-7B model as the MBTI personality predictor\nthat outperforms the state-of-the-art models as the tool to analyze LLMs'\nresponses. Then, we prompt the LLMs with situational questions and ask them to\ngenerate Twitter posts and comments, respectively, in order to assess their\npersonalities when playing two different roles. Using the external personality\nevaluation method, we identify that the obtained personality types for LLMs are\nsignificantly different when generating posts versus comments, whereas humans\nshow a consistent personality profile in these two different situations. This\nshows that LLMs can exhibit different personalities based on different\nscenarios, thus highlighting a fundamental difference between personality in\nLLMs and humans. With our work, we call for a re-evaluation of personality\ndefinition and measurement in LLMs."}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b7b21648-5a18-432b-9eea-6293512177c5", "node_type": "4", "metadata": {"page_label": "6", "file_name": "2402_14805v1.pdf", "Title of this paper": "Identifying Multiple Personalities in Large Language Models with External Evaluation", "Authors": "Xiaoyang Song, Yuta Adachi, Jessie Feng, Mouwei Lin, Linhao Yu, Frank Li, Akshat Gupta, Gopala Anumanchipalli, Simerjot Kaur", "Date published": "02/22/2024", "URL": "http://arxiv.org/abs/2402.14805v1", "summary": "As Large Language Models (LLMs) are integrated with human daily applications\nrapidly, many societal and ethical concerns are raised regarding the behavior\nof LLMs. One of the ways to comprehend LLMs' behavior is to analyze their\npersonalities. Many recent studies quantify LLMs' personalities using\nself-assessment tests that are created for humans. Yet many critiques question\nthe applicability and reliability of these self-assessment tests when applied\nto LLMs. In this paper, we investigate LLM personalities using an alternate\npersonality measurement method, which we refer to as the external evaluation\nmethod, where instead of prompting LLMs with multiple-choice questions in the\nLikert scale, we evaluate LLMs' personalities by analyzing their responses\ntoward open-ended situational questions using an external machine learning\nmodel. We first fine-tuned a Llama2-7B model as the MBTI personality predictor\nthat outperforms the state-of-the-art models as the tool to analyze LLMs'\nresponses. Then, we prompt the LLMs with situational questions and ask them to\ngenerate Twitter posts and comments, respectively, in order to assess their\npersonalities when playing two different roles. Using the external personality\nevaluation method, we identify that the obtained personality types for LLMs are\nsignificantly different when generating posts versus comments, whereas humans\nshow a consistent personality profile in these two different situations. This\nshows that LLMs can exhibit different personalities based on different\nscenarios, thus highlighting a fundamental difference between personality in\nLLMs and humans. With our work, we call for a re-evaluation of personality\ndefinition and measurement in LLMs."}, "hash": "6d924009e0f2d5e24006b3856d1f539d59a6ccf909f89d74d37a9beb0c0f198b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "71839d5a-e77e-43b6-8960-fc11ef95410d", "node_type": "1", "metadata": {}, "hash": "0ef53541085b42994e3800ebb75c13b359d4fff9806ccda2a97a0481e0882328", "class_name": "RelatedNodeInfo"}}, "text": "Type System Prompt Used User Prompt Used\nPosts Generate a Twitter post As a user on Twitter, write a tweet on the following\ncontents: [summarized contents]\nComments Generate a Twitter comment As a user on Twitter, write a tweet to comment on this\nTweet: [tweet contents]\nTable 3: Prompting templates.\nFigure 3: MBTI distribution of ChatGPT and Llama2 models using external evaluation method for 100 times. In the\nfigure, the first row is the assessment results on the generated posts dataset (P), whereas the second row provides the\nresults on the comments dataset (C). In addition, personality types that appear less than 3 times are merged together\nto form the class \u201cOthers\u201d.\ncles to extract relevant news events and collected\n5000 tweets from 10 popular topics. Subsequently,\nwe tasked the LLMs with generating 4500 posts\nbased on the news events and 5000 comments in\nresponse to the collected tweets. These generated\ntexts were then utilized to study the personalities of\nthe LLMs using our personality detection model. It\nis worth noting that the roles assigned to the LLMs\nvaried depending on whether they were asked to\ngenerate posts or comments, allowing for a diverse\nrange of responses. A detailed description of this\npipeline and downloaded contents can be found in\nAppendix A.4.\n3.3 Stage III: External LLMs Personality\nDetection & Validation\nAfter obtaining the posts and comments datasets\nfor each LLM of interest, we evaluate their person-\nalities using our fine-tuned personality detection\nmodel. The personality detection model is trained\nto take 50 social media posts of a user as input\nand output their personality based on the 16-class\nMBTI personality framework. To minimize errorsdue to sampling and model inaccuracy, we created\n100 sets of 50 generations for each type of tweet\n(posts vs. comments ) generated by the LLM. To\ndo so, we sampled 50 responses with replacements\nfrom our generation dataset 100 times. We then\nreport the predicted MBTI personality distribution\nfor the 100 samples for each role in Figure 3 and\nthe most frequent personality type for each LLM is\nreported as the predicted personality in Table 4.\nModelExternal Evaluation\nPosts Comments\nChatGPT INFJ INFJ\nLlama2-7B ESTJ INFP\nLlama2-13B ESTJ INFP\nLlama2-70B ESTJ INFJ\nTable 4: MBTI personality type obtained by external\nevaluation method. The most frequent MBTI personal-\nity type is reported here.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2397, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "71839d5a-e77e-43b6-8960-fc11ef95410d": {"__data__": {"id_": "71839d5a-e77e-43b6-8960-fc11ef95410d", "embedding": null, "metadata": {"page_label": "6", "file_name": "2402_14805v1.pdf", "Title of this paper": "Identifying Multiple Personalities in Large Language Models with External Evaluation", "Authors": "Xiaoyang Song, Yuta Adachi, Jessie Feng, Mouwei Lin, Linhao Yu, Frank Li, Akshat Gupta, Gopala Anumanchipalli, Simerjot Kaur", "Date published": "02/22/2024", "URL": "http://arxiv.org/abs/2402.14805v1", "summary": "As Large Language Models (LLMs) are integrated with human daily applications\nrapidly, many societal and ethical concerns are raised regarding the behavior\nof LLMs. One of the ways to comprehend LLMs' behavior is to analyze their\npersonalities. Many recent studies quantify LLMs' personalities using\nself-assessment tests that are created for humans. Yet many critiques question\nthe applicability and reliability of these self-assessment tests when applied\nto LLMs. In this paper, we investigate LLM personalities using an alternate\npersonality measurement method, which we refer to as the external evaluation\nmethod, where instead of prompting LLMs with multiple-choice questions in the\nLikert scale, we evaluate LLMs' personalities by analyzing their responses\ntoward open-ended situational questions using an external machine learning\nmodel. We first fine-tuned a Llama2-7B model as the MBTI personality predictor\nthat outperforms the state-of-the-art models as the tool to analyze LLMs'\nresponses. Then, we prompt the LLMs with situational questions and ask them to\ngenerate Twitter posts and comments, respectively, in order to assess their\npersonalities when playing two different roles. Using the external personality\nevaluation method, we identify that the obtained personality types for LLMs are\nsignificantly different when generating posts versus comments, whereas humans\nshow a consistent personality profile in these two different situations. This\nshows that LLMs can exhibit different personalities based on different\nscenarios, thus highlighting a fundamental difference between personality in\nLLMs and humans. With our work, we call for a re-evaluation of personality\ndefinition and measurement in LLMs."}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b7b21648-5a18-432b-9eea-6293512177c5", "node_type": "4", "metadata": {"page_label": "6", "file_name": "2402_14805v1.pdf", "Title of this paper": "Identifying Multiple Personalities in Large Language Models with External Evaluation", "Authors": "Xiaoyang Song, Yuta Adachi, Jessie Feng, Mouwei Lin, Linhao Yu, Frank Li, Akshat Gupta, Gopala Anumanchipalli, Simerjot Kaur", "Date published": "02/22/2024", "URL": "http://arxiv.org/abs/2402.14805v1", "summary": "As Large Language Models (LLMs) are integrated with human daily applications\nrapidly, many societal and ethical concerns are raised regarding the behavior\nof LLMs. One of the ways to comprehend LLMs' behavior is to analyze their\npersonalities. Many recent studies quantify LLMs' personalities using\nself-assessment tests that are created for humans. Yet many critiques question\nthe applicability and reliability of these self-assessment tests when applied\nto LLMs. In this paper, we investigate LLM personalities using an alternate\npersonality measurement method, which we refer to as the external evaluation\nmethod, where instead of prompting LLMs with multiple-choice questions in the\nLikert scale, we evaluate LLMs' personalities by analyzing their responses\ntoward open-ended situational questions using an external machine learning\nmodel. We first fine-tuned a Llama2-7B model as the MBTI personality predictor\nthat outperforms the state-of-the-art models as the tool to analyze LLMs'\nresponses. Then, we prompt the LLMs with situational questions and ask them to\ngenerate Twitter posts and comments, respectively, in order to assess their\npersonalities when playing two different roles. Using the external personality\nevaluation method, we identify that the obtained personality types for LLMs are\nsignificantly different when generating posts versus comments, whereas humans\nshow a consistent personality profile in these two different situations. This\nshows that LLMs can exhibit different personalities based on different\nscenarios, thus highlighting a fundamental difference between personality in\nLLMs and humans. With our work, we call for a re-evaluation of personality\ndefinition and measurement in LLMs."}, "hash": "6d924009e0f2d5e24006b3856d1f539d59a6ccf909f89d74d37a9beb0c0f198b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8902a363-6801-4368-99e7-7fe1250bb7a8", "node_type": "1", "metadata": {"page_label": "6", "file_name": "2402_14805v1.pdf", "Title of this paper": "Identifying Multiple Personalities in Large Language Models with External Evaluation", "Authors": "Xiaoyang Song, Yuta Adachi, Jessie Feng, Mouwei Lin, Linhao Yu, Frank Li, Akshat Gupta, Gopala Anumanchipalli, Simerjot Kaur", "Date published": "02/22/2024", "URL": "http://arxiv.org/abs/2402.14805v1", "summary": "As Large Language Models (LLMs) are integrated with human daily applications\nrapidly, many societal and ethical concerns are raised regarding the behavior\nof LLMs. One of the ways to comprehend LLMs' behavior is to analyze their\npersonalities. Many recent studies quantify LLMs' personalities using\nself-assessment tests that are created for humans. Yet many critiques question\nthe applicability and reliability of these self-assessment tests when applied\nto LLMs. In this paper, we investigate LLM personalities using an alternate\npersonality measurement method, which we refer to as the external evaluation\nmethod, where instead of prompting LLMs with multiple-choice questions in the\nLikert scale, we evaluate LLMs' personalities by analyzing their responses\ntoward open-ended situational questions using an external machine learning\nmodel. We first fine-tuned a Llama2-7B model as the MBTI personality predictor\nthat outperforms the state-of-the-art models as the tool to analyze LLMs'\nresponses. Then, we prompt the LLMs with situational questions and ask them to\ngenerate Twitter posts and comments, respectively, in order to assess their\npersonalities when playing two different roles. Using the external personality\nevaluation method, we identify that the obtained personality types for LLMs are\nsignificantly different when generating posts versus comments, whereas humans\nshow a consistent personality profile in these two different situations. This\nshows that LLMs can exhibit different personalities based on different\nscenarios, thus highlighting a fundamental difference between personality in\nLLMs and humans. With our work, we call for a re-evaluation of personality\ndefinition and measurement in LLMs."}, "hash": "6d2ac28d8823dc124c7e1231b64484b06d0a6543e5e9bc80c830488933684c6f", "class_name": "RelatedNodeInfo"}}, "text": "To minimize errorsdue to sampling and model inaccuracy, we created\n100 sets of 50 generations for each type of tweet\n(posts vs. comments ) generated by the LLM. To\ndo so, we sampled 50 responses with replacements\nfrom our generation dataset 100 times. We then\nreport the predicted MBTI personality distribution\nfor the 100 samples for each role in Figure 3 and\nthe most frequent personality type for each LLM is\nreported as the predicted personality in Table 4.\nModelExternal Evaluation\nPosts Comments\nChatGPT INFJ INFJ\nLlama2-7B ESTJ INFP\nLlama2-13B ESTJ INFP\nLlama2-70B ESTJ INFJ\nTable 4: MBTI personality type obtained by external\nevaluation method. The most frequent MBTI personal-\nity type is reported here.\nWe can observe from Table 4 that for the same\nLLM, the external evaluation results on the gener-\nated posts and comments datasets are very differ-\nent. For instance, for Llama2-7B, the most frequent", "mimetype": "text/plain", "start_char_idx": 1685, "end_char_idx": 2596, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "80bb5150-1f0f-4f9b-afab-43b379bb47ae": {"__data__": {"id_": "80bb5150-1f0f-4f9b-afab-43b379bb47ae", "embedding": null, "metadata": {"page_label": "7", "file_name": "2402_14805v1.pdf", "Title of this paper": "Identifying Multiple Personalities in Large Language Models with External Evaluation", "Authors": "Xiaoyang Song, Yuta Adachi, Jessie Feng, Mouwei Lin, Linhao Yu, Frank Li, Akshat Gupta, Gopala Anumanchipalli, Simerjot Kaur", "Date published": "02/22/2024", "URL": "http://arxiv.org/abs/2402.14805v1", "summary": "As Large Language Models (LLMs) are integrated with human daily applications\nrapidly, many societal and ethical concerns are raised regarding the behavior\nof LLMs. One of the ways to comprehend LLMs' behavior is to analyze their\npersonalities. Many recent studies quantify LLMs' personalities using\nself-assessment tests that are created for humans. Yet many critiques question\nthe applicability and reliability of these self-assessment tests when applied\nto LLMs. In this paper, we investigate LLM personalities using an alternate\npersonality measurement method, which we refer to as the external evaluation\nmethod, where instead of prompting LLMs with multiple-choice questions in the\nLikert scale, we evaluate LLMs' personalities by analyzing their responses\ntoward open-ended situational questions using an external machine learning\nmodel. We first fine-tuned a Llama2-7B model as the MBTI personality predictor\nthat outperforms the state-of-the-art models as the tool to analyze LLMs'\nresponses. Then, we prompt the LLMs with situational questions and ask them to\ngenerate Twitter posts and comments, respectively, in order to assess their\npersonalities when playing two different roles. Using the external personality\nevaluation method, we identify that the obtained personality types for LLMs are\nsignificantly different when generating posts versus comments, whereas humans\nshow a consistent personality profile in these two different situations. This\nshows that LLMs can exhibit different personalities based on different\nscenarios, thus highlighting a fundamental difference between personality in\nLLMs and humans. With our work, we call for a re-evaluation of personality\ndefinition and measurement in LLMs."}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "0460f752-0df5-44ba-8787-2b16ee7aaffc", "node_type": "4", "metadata": {"page_label": "7", "file_name": "2402_14805v1.pdf", "Title of this paper": "Identifying Multiple Personalities in Large Language Models with External Evaluation", "Authors": "Xiaoyang Song, Yuta Adachi, Jessie Feng, Mouwei Lin, Linhao Yu, Frank Li, Akshat Gupta, Gopala Anumanchipalli, Simerjot Kaur", "Date published": "02/22/2024", "URL": "http://arxiv.org/abs/2402.14805v1", "summary": "As Large Language Models (LLMs) are integrated with human daily applications\nrapidly, many societal and ethical concerns are raised regarding the behavior\nof LLMs. One of the ways to comprehend LLMs' behavior is to analyze their\npersonalities. Many recent studies quantify LLMs' personalities using\nself-assessment tests that are created for humans. Yet many critiques question\nthe applicability and reliability of these self-assessment tests when applied\nto LLMs. In this paper, we investigate LLM personalities using an alternate\npersonality measurement method, which we refer to as the external evaluation\nmethod, where instead of prompting LLMs with multiple-choice questions in the\nLikert scale, we evaluate LLMs' personalities by analyzing their responses\ntoward open-ended situational questions using an external machine learning\nmodel. We first fine-tuned a Llama2-7B model as the MBTI personality predictor\nthat outperforms the state-of-the-art models as the tool to analyze LLMs'\nresponses. Then, we prompt the LLMs with situational questions and ask them to\ngenerate Twitter posts and comments, respectively, in order to assess their\npersonalities when playing two different roles. Using the external personality\nevaluation method, we identify that the obtained personality types for LLMs are\nsignificantly different when generating posts versus comments, whereas humans\nshow a consistent personality profile in these two different situations. This\nshows that LLMs can exhibit different personalities based on different\nscenarios, thus highlighting a fundamental difference between personality in\nLLMs and humans. With our work, we call for a re-evaluation of personality\ndefinition and measurement in LLMs."}, "hash": "c4cc39b56a6eae435150ecdade0e6e8c77ba49bba1e6c25ad38c3d05014b64bb", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2079231c-828c-43d2-9bc3-771b971192b0", "node_type": "1", "metadata": {}, "hash": "e33aa64731f9853896322b28499880ba978034986401d6371be160cc0df8dca5", "class_name": "RelatedNodeInfo"}}, "text": "Figure 4: MBTI distribution of 4 celebrities using the external evaluation method for 100 times. In the figure, the\nfirst row is the assessment results on the generated posts dataset (P), whereas the second row provides the results on\nthe comments dataset (C). In addition, personality types that appear less than 3 times are merged together to form\nthe class \u201cOthers\u201d. Complete results for all 8 selected celebrities can be found in Appendix A.5.\nMBTI type is ESTJ when it posts tweets, whereas\nit is INFP on the generated comments. In addi-\ntion, the predicted personality distributions for the\n100 trials in Figure 3 are also significantly differ-\nent between posts and comments. For example,\nin ChatGPT personality evaluation, although the\nmost frequent MBTI type is the same in Table 4\nbetween posts and comments, the predicted per-\nsonality distributions are extremely different, as\ncan be seen in the first column of pie charts in Fig-\nure 3. We can see that INFJ and INFP are nearly\nequally distributed when writing posts, whereas\nINFJ dominates over all other MBTI types when\nwriting comments. In plain words, this indicates\nChatGPT behaves very differently when generat-\ning posts and comments. We can see this very\nclearly for the other Llama2 models, where the\npersonality distribution while writing posts (first\nrow of pie charts in Figure 3) is very different from\nthe personality distribution we get when writing\ncomments (second row in Figure 3). These experi-\nments clearly show that LLMs exhibit different\npersonalities when playing different roles .\nValidation with Human Counterpart. Person-\nality is defined as an enduring characteristic in\nhumans by the American Psychological Associa-\ntion. This means that for humans, the personal-\nity distribution obtained for humans when writing\nposts versus comments is expected to be consistent.\nAs we employed a personality detection model toevaluate LLM personality, it becomes important to\ncheck if the detection model is able to produce sim-\nilar personality distributions when humans take on\nthe two roles of writing posts versus comments. If\nthis is not the case, then the inconsistencies in LLM\npersonalities in the two roles can be attributed to\nthe personality detection model. Furthermore, this\nis crucial to justify the correctness of our later anal-\nysis, which is completely based on this personality\ndetection model.\nTo check this, we conduct the same study with\nhuman counterparts. We randomly downloaded\ntheTwitter posts andcomments from 8 celebrities4\nfrom different domains and applied the personality\ndetection model to these two different datasets.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2630, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2079231c-828c-43d2-9bc3-771b971192b0": {"__data__": {"id_": "2079231c-828c-43d2-9bc3-771b971192b0", "embedding": null, "metadata": {"page_label": "7", "file_name": "2402_14805v1.pdf", "Title of this paper": "Identifying Multiple Personalities in Large Language Models with External Evaluation", "Authors": "Xiaoyang Song, Yuta Adachi, Jessie Feng, Mouwei Lin, Linhao Yu, Frank Li, Akshat Gupta, Gopala Anumanchipalli, Simerjot Kaur", "Date published": "02/22/2024", "URL": "http://arxiv.org/abs/2402.14805v1", "summary": "As Large Language Models (LLMs) are integrated with human daily applications\nrapidly, many societal and ethical concerns are raised regarding the behavior\nof LLMs. One of the ways to comprehend LLMs' behavior is to analyze their\npersonalities. Many recent studies quantify LLMs' personalities using\nself-assessment tests that are created for humans. Yet many critiques question\nthe applicability and reliability of these self-assessment tests when applied\nto LLMs. In this paper, we investigate LLM personalities using an alternate\npersonality measurement method, which we refer to as the external evaluation\nmethod, where instead of prompting LLMs with multiple-choice questions in the\nLikert scale, we evaluate LLMs' personalities by analyzing their responses\ntoward open-ended situational questions using an external machine learning\nmodel. We first fine-tuned a Llama2-7B model as the MBTI personality predictor\nthat outperforms the state-of-the-art models as the tool to analyze LLMs'\nresponses. Then, we prompt the LLMs with situational questions and ask them to\ngenerate Twitter posts and comments, respectively, in order to assess their\npersonalities when playing two different roles. Using the external personality\nevaluation method, we identify that the obtained personality types for LLMs are\nsignificantly different when generating posts versus comments, whereas humans\nshow a consistent personality profile in these two different situations. This\nshows that LLMs can exhibit different personalities based on different\nscenarios, thus highlighting a fundamental difference between personality in\nLLMs and humans. With our work, we call for a re-evaluation of personality\ndefinition and measurement in LLMs."}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "0460f752-0df5-44ba-8787-2b16ee7aaffc", "node_type": "4", "metadata": {"page_label": "7", "file_name": "2402_14805v1.pdf", "Title of this paper": "Identifying Multiple Personalities in Large Language Models with External Evaluation", "Authors": "Xiaoyang Song, Yuta Adachi, Jessie Feng, Mouwei Lin, Linhao Yu, Frank Li, Akshat Gupta, Gopala Anumanchipalli, Simerjot Kaur", "Date published": "02/22/2024", "URL": "http://arxiv.org/abs/2402.14805v1", "summary": "As Large Language Models (LLMs) are integrated with human daily applications\nrapidly, many societal and ethical concerns are raised regarding the behavior\nof LLMs. One of the ways to comprehend LLMs' behavior is to analyze their\npersonalities. Many recent studies quantify LLMs' personalities using\nself-assessment tests that are created for humans. Yet many critiques question\nthe applicability and reliability of these self-assessment tests when applied\nto LLMs. In this paper, we investigate LLM personalities using an alternate\npersonality measurement method, which we refer to as the external evaluation\nmethod, where instead of prompting LLMs with multiple-choice questions in the\nLikert scale, we evaluate LLMs' personalities by analyzing their responses\ntoward open-ended situational questions using an external machine learning\nmodel. We first fine-tuned a Llama2-7B model as the MBTI personality predictor\nthat outperforms the state-of-the-art models as the tool to analyze LLMs'\nresponses. Then, we prompt the LLMs with situational questions and ask them to\ngenerate Twitter posts and comments, respectively, in order to assess their\npersonalities when playing two different roles. Using the external personality\nevaluation method, we identify that the obtained personality types for LLMs are\nsignificantly different when generating posts versus comments, whereas humans\nshow a consistent personality profile in these two different situations. This\nshows that LLMs can exhibit different personalities based on different\nscenarios, thus highlighting a fundamental difference between personality in\nLLMs and humans. With our work, we call for a re-evaluation of personality\ndefinition and measurement in LLMs."}, "hash": "c4cc39b56a6eae435150ecdade0e6e8c77ba49bba1e6c25ad38c3d05014b64bb", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "80bb5150-1f0f-4f9b-afab-43b379bb47ae", "node_type": "1", "metadata": {"page_label": "7", "file_name": "2402_14805v1.pdf", "Title of this paper": "Identifying Multiple Personalities in Large Language Models with External Evaluation", "Authors": "Xiaoyang Song, Yuta Adachi, Jessie Feng, Mouwei Lin, Linhao Yu, Frank Li, Akshat Gupta, Gopala Anumanchipalli, Simerjot Kaur", "Date published": "02/22/2024", "URL": "http://arxiv.org/abs/2402.14805v1", "summary": "As Large Language Models (LLMs) are integrated with human daily applications\nrapidly, many societal and ethical concerns are raised regarding the behavior\nof LLMs. One of the ways to comprehend LLMs' behavior is to analyze their\npersonalities. Many recent studies quantify LLMs' personalities using\nself-assessment tests that are created for humans. Yet many critiques question\nthe applicability and reliability of these self-assessment tests when applied\nto LLMs. In this paper, we investigate LLM personalities using an alternate\npersonality measurement method, which we refer to as the external evaluation\nmethod, where instead of prompting LLMs with multiple-choice questions in the\nLikert scale, we evaluate LLMs' personalities by analyzing their responses\ntoward open-ended situational questions using an external machine learning\nmodel. We first fine-tuned a Llama2-7B model as the MBTI personality predictor\nthat outperforms the state-of-the-art models as the tool to analyze LLMs'\nresponses. Then, we prompt the LLMs with situational questions and ask them to\ngenerate Twitter posts and comments, respectively, in order to assess their\npersonalities when playing two different roles. Using the external personality\nevaluation method, we identify that the obtained personality types for LLMs are\nsignificantly different when generating posts versus comments, whereas humans\nshow a consistent personality profile in these two different situations. This\nshows that LLMs can exhibit different personalities based on different\nscenarios, thus highlighting a fundamental difference between personality in\nLLMs and humans. With our work, we call for a re-evaluation of personality\ndefinition and measurement in LLMs."}, "hash": "1384139126fccd96d210a18e03074d3ee55049ffb2edfc8d8944a2a2dd3dbcd7", "class_name": "RelatedNodeInfo"}}, "text": "This means that for humans, the personal-\nity distribution obtained for humans when writing\nposts versus comments is expected to be consistent.\nAs we employed a personality detection model toevaluate LLM personality, it becomes important to\ncheck if the detection model is able to produce sim-\nilar personality distributions when humans take on\nthe two roles of writing posts versus comments. If\nthis is not the case, then the inconsistencies in LLM\npersonalities in the two roles can be attributed to\nthe personality detection model. Furthermore, this\nis crucial to justify the correctness of our later anal-\nysis, which is completely based on this personality\ndetection model.\nTo check this, we conduct the same study with\nhuman counterparts. We randomly downloaded\ntheTwitter posts andcomments from 8 celebrities4\nfrom different domains and applied the personality\ndetection model to these two different datasets. We\nagain created 100 samples of 50 tweets each and\nfound the personality distribution for these celebri-\nties in the two different roles - while writing posts\nversus comments. The personality distribution re-\nsults for four celebrities can be seen in Figure 4,\nwhile the remaining four can be seen in Figure 5.\nTable 5 presents the results for the eight celebri-\nties. We can see that the most frequent MBTI type,\nthe second most frequent MBTI type, and even the\npredicted distributions (Figure 4) are very akin to\neach other for nearly all the celebrities that we in-\nvestigated. This empirically confirms that humans\ntend to exhibit enduring MBTI personality types\n4Due to proprietary reasons, we have masked the identities\nof the selected celebrities.", "mimetype": "text/plain", "start_char_idx": 1714, "end_char_idx": 3385, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "878da19e-0a9c-44f6-a8c5-dbfeb96c674a": {"__data__": {"id_": "878da19e-0a9c-44f6-a8c5-dbfeb96c674a", "embedding": null, "metadata": {"page_label": "8", "file_name": "2402_14805v1.pdf", "Title of this paper": "Identifying Multiple Personalities in Large Language Models with External Evaluation", "Authors": "Xiaoyang Song, Yuta Adachi, Jessie Feng, Mouwei Lin, Linhao Yu, Frank Li, Akshat Gupta, Gopala Anumanchipalli, Simerjot Kaur", "Date published": "02/22/2024", "URL": "http://arxiv.org/abs/2402.14805v1", "summary": "As Large Language Models (LLMs) are integrated with human daily applications\nrapidly, many societal and ethical concerns are raised regarding the behavior\nof LLMs. One of the ways to comprehend LLMs' behavior is to analyze their\npersonalities. Many recent studies quantify LLMs' personalities using\nself-assessment tests that are created for humans. Yet many critiques question\nthe applicability and reliability of these self-assessment tests when applied\nto LLMs. In this paper, we investigate LLM personalities using an alternate\npersonality measurement method, which we refer to as the external evaluation\nmethod, where instead of prompting LLMs with multiple-choice questions in the\nLikert scale, we evaluate LLMs' personalities by analyzing their responses\ntoward open-ended situational questions using an external machine learning\nmodel. We first fine-tuned a Llama2-7B model as the MBTI personality predictor\nthat outperforms the state-of-the-art models as the tool to analyze LLMs'\nresponses. Then, we prompt the LLMs with situational questions and ask them to\ngenerate Twitter posts and comments, respectively, in order to assess their\npersonalities when playing two different roles. Using the external personality\nevaluation method, we identify that the obtained personality types for LLMs are\nsignificantly different when generating posts versus comments, whereas humans\nshow a consistent personality profile in these two different situations. This\nshows that LLMs can exhibit different personalities based on different\nscenarios, thus highlighting a fundamental difference between personality in\nLLMs and humans. With our work, we call for a re-evaluation of personality\ndefinition and measurement in LLMs."}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a13cb1b0-c145-41ed-bcce-4e94a3791c8b", "node_type": "4", "metadata": {"page_label": "8", "file_name": "2402_14805v1.pdf", "Title of this paper": "Identifying Multiple Personalities in Large Language Models with External Evaluation", "Authors": "Xiaoyang Song, Yuta Adachi, Jessie Feng, Mouwei Lin, Linhao Yu, Frank Li, Akshat Gupta, Gopala Anumanchipalli, Simerjot Kaur", "Date published": "02/22/2024", "URL": "http://arxiv.org/abs/2402.14805v1", "summary": "As Large Language Models (LLMs) are integrated with human daily applications\nrapidly, many societal and ethical concerns are raised regarding the behavior\nof LLMs. One of the ways to comprehend LLMs' behavior is to analyze their\npersonalities. Many recent studies quantify LLMs' personalities using\nself-assessment tests that are created for humans. Yet many critiques question\nthe applicability and reliability of these self-assessment tests when applied\nto LLMs. In this paper, we investigate LLM personalities using an alternate\npersonality measurement method, which we refer to as the external evaluation\nmethod, where instead of prompting LLMs with multiple-choice questions in the\nLikert scale, we evaluate LLMs' personalities by analyzing their responses\ntoward open-ended situational questions using an external machine learning\nmodel. We first fine-tuned a Llama2-7B model as the MBTI personality predictor\nthat outperforms the state-of-the-art models as the tool to analyze LLMs'\nresponses. Then, we prompt the LLMs with situational questions and ask them to\ngenerate Twitter posts and comments, respectively, in order to assess their\npersonalities when playing two different roles. Using the external personality\nevaluation method, we identify that the obtained personality types for LLMs are\nsignificantly different when generating posts versus comments, whereas humans\nshow a consistent personality profile in these two different situations. This\nshows that LLMs can exhibit different personalities based on different\nscenarios, thus highlighting a fundamental difference between personality in\nLLMs and humans. With our work, we call for a re-evaluation of personality\ndefinition and measurement in LLMs."}, "hash": "f6619e698d4c0783c7f4d9e62fa85e6bab917809a75d068da96a4bc99d95e63f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "fb1b99e3-1f08-48bd-9477-6334681d4ba9", "node_type": "1", "metadata": {}, "hash": "aae5ccf41d3174dd53d52fbe96f12539a0ac2fb86dc8fdfebdf7731d584b9f4e", "class_name": "RelatedNodeInfo"}}, "text": "CelebrityExternal Evaluation\nP C\nCelebrity I INTJ (INFJ) INTJ (INFJ)\nCelebrity II INFJ (ENFJ) INFJ (ENFJ)\nCelebrity III INFJ (ENFJ) INFJ (ENFJ)\nCelebrity IV INFJ (ENFJ) INFJ (ENFJ)\nCelebrity V INFJ (INFP) INFJ (INFP)\nCelebrity VI INFJ (INTJ) ENTJ (ENFJ)\nCelebrity VII INFJ (INFP) INFJ (INFP)\nCelebrity VIII INFJ (ISFJ) INFJ (ENFJ)\nTable 5: The most frequent MBTI personality type for\n8 celebrities under external evaluation method using\nposts andcomments . The second most frequent MBTI\npersonality type is reported in the parenthesis.\neven when playing different roles, which echoes\nthe observations in most of the psychology litera-\nture (Green et al., 2019; Association, 2023) as well\nas the personality definition (American Psycholog-\nical Association, 1983). A detailed introduction\nto this validation experiment can be found in Ap-\npendix A.5. Simultaneously, this confirms that our\nfine-tuned model is able to attain similar personal-\nity distributions for humans and rules out its effects\nin the observed inconsistency in LLMs.\nThe above experiments show that LLMs clearly\nexhibit different personalities in different roles,\nwhich is not true for humans. Our experiments\nalso show that the definition of personality as de-\nfined for humans may not be applicable to LLMs,\nas personality in LLMs no longer seems to be an\nenduring characteristic.\n4 Conclusions\nIn this paper, we investigate the personality ex-\nhibited by LLMs using an external personality\nevaluation method, which is an alternative to self-\nassessment tests for personality measurement. To\nperform external personality evaluation, we fine-\ntuned a Llama2-7B model as the third-party agent\nfor personality detection and collected LLMs\u2019 re-\nsponses and attitudes toward open-ended situa-\ntional questions. We also developed a prompt-\ning pipeline to automate the prompts for LLMs\nwith proper content. Our experiments show that\nLLMs exhibit different personalities in different\nroles, which is not true for humans.\nThese observations also shed light on the validity\nof quantifying and defining LLM personality using\nthe same standard for humans. In our work, we see\nthat LLM personality is not an enduring character-\nistic as it is seen in humans.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2217, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "fb1b99e3-1f08-48bd-9477-6334681d4ba9": {"__data__": {"id_": "fb1b99e3-1f08-48bd-9477-6334681d4ba9", "embedding": null, "metadata": {"page_label": "8", "file_name": "2402_14805v1.pdf", "Title of this paper": "Identifying Multiple Personalities in Large Language Models with External Evaluation", "Authors": "Xiaoyang Song, Yuta Adachi, Jessie Feng, Mouwei Lin, Linhao Yu, Frank Li, Akshat Gupta, Gopala Anumanchipalli, Simerjot Kaur", "Date published": "02/22/2024", "URL": "http://arxiv.org/abs/2402.14805v1", "summary": "As Large Language Models (LLMs) are integrated with human daily applications\nrapidly, many societal and ethical concerns are raised regarding the behavior\nof LLMs. One of the ways to comprehend LLMs' behavior is to analyze their\npersonalities. Many recent studies quantify LLMs' personalities using\nself-assessment tests that are created for humans. Yet many critiques question\nthe applicability and reliability of these self-assessment tests when applied\nto LLMs. In this paper, we investigate LLM personalities using an alternate\npersonality measurement method, which we refer to as the external evaluation\nmethod, where instead of prompting LLMs with multiple-choice questions in the\nLikert scale, we evaluate LLMs' personalities by analyzing their responses\ntoward open-ended situational questions using an external machine learning\nmodel. We first fine-tuned a Llama2-7B model as the MBTI personality predictor\nthat outperforms the state-of-the-art models as the tool to analyze LLMs'\nresponses. Then, we prompt the LLMs with situational questions and ask them to\ngenerate Twitter posts and comments, respectively, in order to assess their\npersonalities when playing two different roles. Using the external personality\nevaluation method, we identify that the obtained personality types for LLMs are\nsignificantly different when generating posts versus comments, whereas humans\nshow a consistent personality profile in these two different situations. This\nshows that LLMs can exhibit different personalities based on different\nscenarios, thus highlighting a fundamental difference between personality in\nLLMs and humans. With our work, we call for a re-evaluation of personality\ndefinition and measurement in LLMs."}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a13cb1b0-c145-41ed-bcce-4e94a3791c8b", "node_type": "4", "metadata": {"page_label": "8", "file_name": "2402_14805v1.pdf", "Title of this paper": "Identifying Multiple Personalities in Large Language Models with External Evaluation", "Authors": "Xiaoyang Song, Yuta Adachi, Jessie Feng, Mouwei Lin, Linhao Yu, Frank Li, Akshat Gupta, Gopala Anumanchipalli, Simerjot Kaur", "Date published": "02/22/2024", "URL": "http://arxiv.org/abs/2402.14805v1", "summary": "As Large Language Models (LLMs) are integrated with human daily applications\nrapidly, many societal and ethical concerns are raised regarding the behavior\nof LLMs. One of the ways to comprehend LLMs' behavior is to analyze their\npersonalities. Many recent studies quantify LLMs' personalities using\nself-assessment tests that are created for humans. Yet many critiques question\nthe applicability and reliability of these self-assessment tests when applied\nto LLMs. In this paper, we investigate LLM personalities using an alternate\npersonality measurement method, which we refer to as the external evaluation\nmethod, where instead of prompting LLMs with multiple-choice questions in the\nLikert scale, we evaluate LLMs' personalities by analyzing their responses\ntoward open-ended situational questions using an external machine learning\nmodel. We first fine-tuned a Llama2-7B model as the MBTI personality predictor\nthat outperforms the state-of-the-art models as the tool to analyze LLMs'\nresponses. Then, we prompt the LLMs with situational questions and ask them to\ngenerate Twitter posts and comments, respectively, in order to assess their\npersonalities when playing two different roles. Using the external personality\nevaluation method, we identify that the obtained personality types for LLMs are\nsignificantly different when generating posts versus comments, whereas humans\nshow a consistent personality profile in these two different situations. This\nshows that LLMs can exhibit different personalities based on different\nscenarios, thus highlighting a fundamental difference between personality in\nLLMs and humans. With our work, we call for a re-evaluation of personality\ndefinition and measurement in LLMs."}, "hash": "f6619e698d4c0783c7f4d9e62fa85e6bab917809a75d068da96a4bc99d95e63f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "878da19e-0a9c-44f6-a8c5-dbfeb96c674a", "node_type": "1", "metadata": {"page_label": "8", "file_name": "2402_14805v1.pdf", "Title of this paper": "Identifying Multiple Personalities in Large Language Models with External Evaluation", "Authors": "Xiaoyang Song, Yuta Adachi, Jessie Feng, Mouwei Lin, Linhao Yu, Frank Li, Akshat Gupta, Gopala Anumanchipalli, Simerjot Kaur", "Date published": "02/22/2024", "URL": "http://arxiv.org/abs/2402.14805v1", "summary": "As Large Language Models (LLMs) are integrated with human daily applications\nrapidly, many societal and ethical concerns are raised regarding the behavior\nof LLMs. One of the ways to comprehend LLMs' behavior is to analyze their\npersonalities. Many recent studies quantify LLMs' personalities using\nself-assessment tests that are created for humans. Yet many critiques question\nthe applicability and reliability of these self-assessment tests when applied\nto LLMs. In this paper, we investigate LLM personalities using an alternate\npersonality measurement method, which we refer to as the external evaluation\nmethod, where instead of prompting LLMs with multiple-choice questions in the\nLikert scale, we evaluate LLMs' personalities by analyzing their responses\ntoward open-ended situational questions using an external machine learning\nmodel. We first fine-tuned a Llama2-7B model as the MBTI personality predictor\nthat outperforms the state-of-the-art models as the tool to analyze LLMs'\nresponses. Then, we prompt the LLMs with situational questions and ask them to\ngenerate Twitter posts and comments, respectively, in order to assess their\npersonalities when playing two different roles. Using the external personality\nevaluation method, we identify that the obtained personality types for LLMs are\nsignificantly different when generating posts versus comments, whereas humans\nshow a consistent personality profile in these two different situations. This\nshows that LLMs can exhibit different personalities based on different\nscenarios, thus highlighting a fundamental difference between personality in\nLLMs and humans. With our work, we call for a re-evaluation of personality\ndefinition and measurement in LLMs."}, "hash": "db3282d395d0843d9505f2a6975e7c2eee183860d69d16f548f9473160bde43f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "305b2676-5fc2-45ba-b702-41f269a5060f", "node_type": "1", "metadata": {}, "hash": "f10cbd2d3c1810da487c84a3ca716128ac95a724994ada22580234af567d932f", "class_name": "RelatedNodeInfo"}}, "text": "4 Conclusions\nIn this paper, we investigate the personality ex-\nhibited by LLMs using an external personality\nevaluation method, which is an alternative to self-\nassessment tests for personality measurement. To\nperform external personality evaluation, we fine-\ntuned a Llama2-7B model as the third-party agent\nfor personality detection and collected LLMs\u2019 re-\nsponses and attitudes toward open-ended situa-\ntional questions. We also developed a prompt-\ning pipeline to automate the prompts for LLMs\nwith proper content. Our experiments show that\nLLMs exhibit different personalities in different\nroles, which is not true for humans.\nThese observations also shed light on the validity\nof quantifying and defining LLM personality using\nthe same standard for humans. In our work, we see\nthat LLM personality is not an enduring character-\nistic as it is seen in humans. Additionally, previousworks have shown that self-assessment tests are\nnot the appropriate tool to measure personality in\nLLMs. When we combine our observations with\nthese results, we can conclude that not only do we\nnot have accurate tools to measure LLM personal-\nity, but we also lack the appropriate definition of\npersonality for LLMs. With our work, we caution\nagainst a naive transfer of definition and methods\nused to analyze human personality on LLMs and\ncall for more fundamental work on evaluating LLM\npersonality and behavior, taking into account the\nspecific characteristics of LLMs.\n5 Limitation\nThis paper investigates the validity of applying ex-\nternal evaluation to measuring LLM personalities\nand calls for a re-evaluation of the definition and\nmeasurement of LLM personalities. As we em-\nployed a fine-tuned model as an external agent to\nevaluate personalities and based our analysis on it,\none of the limitations of our work is the accuracy\nof the personality detection model. Although our\nmodel attains state-of-the-art performance and we\nreduce the chances of errors by using 100 samples\nof generations instead of 1, the uncertainties intro-\nduced due to error rates are still inevitable. Further-\nmore, although our work identifies that the current\ndefinition of LLM personality should be reevalu-\nated, this paper does not provide an alternative for\ndefining LLM personality and the measurement\nmethod, which are left for future work.\n6 Acknowledgement\nThis paper was prepared for information purposes\nby the Artificial Intelligence Research group of\nJPMorgan Chase & Co and its affiliates (\u201cJP Mor-\ngan\u201d), and is not a product of the Research De-\npartment of JP Morgan. J.P.", "mimetype": "text/plain", "start_char_idx": 1352, "end_char_idx": 3916, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "305b2676-5fc2-45ba-b702-41f269a5060f": {"__data__": {"id_": "305b2676-5fc2-45ba-b702-41f269a5060f", "embedding": null, "metadata": {"page_label": "8", "file_name": "2402_14805v1.pdf", "Title of this paper": "Identifying Multiple Personalities in Large Language Models with External Evaluation", "Authors": "Xiaoyang Song, Yuta Adachi, Jessie Feng, Mouwei Lin, Linhao Yu, Frank Li, Akshat Gupta, Gopala Anumanchipalli, Simerjot Kaur", "Date published": "02/22/2024", "URL": "http://arxiv.org/abs/2402.14805v1", "summary": "As Large Language Models (LLMs) are integrated with human daily applications\nrapidly, many societal and ethical concerns are raised regarding the behavior\nof LLMs. One of the ways to comprehend LLMs' behavior is to analyze their\npersonalities. Many recent studies quantify LLMs' personalities using\nself-assessment tests that are created for humans. Yet many critiques question\nthe applicability and reliability of these self-assessment tests when applied\nto LLMs. In this paper, we investigate LLM personalities using an alternate\npersonality measurement method, which we refer to as the external evaluation\nmethod, where instead of prompting LLMs with multiple-choice questions in the\nLikert scale, we evaluate LLMs' personalities by analyzing their responses\ntoward open-ended situational questions using an external machine learning\nmodel. We first fine-tuned a Llama2-7B model as the MBTI personality predictor\nthat outperforms the state-of-the-art models as the tool to analyze LLMs'\nresponses. Then, we prompt the LLMs with situational questions and ask them to\ngenerate Twitter posts and comments, respectively, in order to assess their\npersonalities when playing two different roles. Using the external personality\nevaluation method, we identify that the obtained personality types for LLMs are\nsignificantly different when generating posts versus comments, whereas humans\nshow a consistent personality profile in these two different situations. This\nshows that LLMs can exhibit different personalities based on different\nscenarios, thus highlighting a fundamental difference between personality in\nLLMs and humans. With our work, we call for a re-evaluation of personality\ndefinition and measurement in LLMs."}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a13cb1b0-c145-41ed-bcce-4e94a3791c8b", "node_type": "4", "metadata": {"page_label": "8", "file_name": "2402_14805v1.pdf", "Title of this paper": "Identifying Multiple Personalities in Large Language Models with External Evaluation", "Authors": "Xiaoyang Song, Yuta Adachi, Jessie Feng, Mouwei Lin, Linhao Yu, Frank Li, Akshat Gupta, Gopala Anumanchipalli, Simerjot Kaur", "Date published": "02/22/2024", "URL": "http://arxiv.org/abs/2402.14805v1", "summary": "As Large Language Models (LLMs) are integrated with human daily applications\nrapidly, many societal and ethical concerns are raised regarding the behavior\nof LLMs. One of the ways to comprehend LLMs' behavior is to analyze their\npersonalities. Many recent studies quantify LLMs' personalities using\nself-assessment tests that are created for humans. Yet many critiques question\nthe applicability and reliability of these self-assessment tests when applied\nto LLMs. In this paper, we investigate LLM personalities using an alternate\npersonality measurement method, which we refer to as the external evaluation\nmethod, where instead of prompting LLMs with multiple-choice questions in the\nLikert scale, we evaluate LLMs' personalities by analyzing their responses\ntoward open-ended situational questions using an external machine learning\nmodel. We first fine-tuned a Llama2-7B model as the MBTI personality predictor\nthat outperforms the state-of-the-art models as the tool to analyze LLMs'\nresponses. Then, we prompt the LLMs with situational questions and ask them to\ngenerate Twitter posts and comments, respectively, in order to assess their\npersonalities when playing two different roles. Using the external personality\nevaluation method, we identify that the obtained personality types for LLMs are\nsignificantly different when generating posts versus comments, whereas humans\nshow a consistent personality profile in these two different situations. This\nshows that LLMs can exhibit different personalities based on different\nscenarios, thus highlighting a fundamental difference between personality in\nLLMs and humans. With our work, we call for a re-evaluation of personality\ndefinition and measurement in LLMs."}, "hash": "f6619e698d4c0783c7f4d9e62fa85e6bab917809a75d068da96a4bc99d95e63f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "fb1b99e3-1f08-48bd-9477-6334681d4ba9", "node_type": "1", "metadata": {"page_label": "8", "file_name": "2402_14805v1.pdf", "Title of this paper": "Identifying Multiple Personalities in Large Language Models with External Evaluation", "Authors": "Xiaoyang Song, Yuta Adachi, Jessie Feng, Mouwei Lin, Linhao Yu, Frank Li, Akshat Gupta, Gopala Anumanchipalli, Simerjot Kaur", "Date published": "02/22/2024", "URL": "http://arxiv.org/abs/2402.14805v1", "summary": "As Large Language Models (LLMs) are integrated with human daily applications\nrapidly, many societal and ethical concerns are raised regarding the behavior\nof LLMs. One of the ways to comprehend LLMs' behavior is to analyze their\npersonalities. Many recent studies quantify LLMs' personalities using\nself-assessment tests that are created for humans. Yet many critiques question\nthe applicability and reliability of these self-assessment tests when applied\nto LLMs. In this paper, we investigate LLM personalities using an alternate\npersonality measurement method, which we refer to as the external evaluation\nmethod, where instead of prompting LLMs with multiple-choice questions in the\nLikert scale, we evaluate LLMs' personalities by analyzing their responses\ntoward open-ended situational questions using an external machine learning\nmodel. We first fine-tuned a Llama2-7B model as the MBTI personality predictor\nthat outperforms the state-of-the-art models as the tool to analyze LLMs'\nresponses. Then, we prompt the LLMs with situational questions and ask them to\ngenerate Twitter posts and comments, respectively, in order to assess their\npersonalities when playing two different roles. Using the external personality\nevaluation method, we identify that the obtained personality types for LLMs are\nsignificantly different when generating posts versus comments, whereas humans\nshow a consistent personality profile in these two different situations. This\nshows that LLMs can exhibit different personalities based on different\nscenarios, thus highlighting a fundamental difference between personality in\nLLMs and humans. With our work, we call for a re-evaluation of personality\ndefinition and measurement in LLMs."}, "hash": "0fe87a2a98450753106963f9cd873884c8f502d248f9065f3407ed9a4a35ab2c", "class_name": "RelatedNodeInfo"}}, "text": "As we em-\nployed a fine-tuned model as an external agent to\nevaluate personalities and based our analysis on it,\none of the limitations of our work is the accuracy\nof the personality detection model. Although our\nmodel attains state-of-the-art performance and we\nreduce the chances of errors by using 100 samples\nof generations instead of 1, the uncertainties intro-\nduced due to error rates are still inevitable. Further-\nmore, although our work identifies that the current\ndefinition of LLM personality should be reevalu-\nated, this paper does not provide an alternative for\ndefining LLM personality and the measurement\nmethod, which are left for future work.\n6 Acknowledgement\nThis paper was prepared for information purposes\nby the Artificial Intelligence Research group of\nJPMorgan Chase & Co and its affiliates (\u201cJP Mor-\ngan\u201d), and is not a product of the Research De-\npartment of JP Morgan. J.P. Morgan makes no\nrepresentation and warranty whatsoever and dis-\nclaims all liability for the completeness, accuracy\nor reliability of the information contained herein.\nThis document is not intended as investment re-\nsearch or investment advice, or a recommendation,\noffer or solicitation for the purchase or sale of any\nsecurity, financial instrument, financial product or\nservice, or to be used in any way for evaluating the\nmerits of participating in any transaction, and shall\nnot constitute a solicitation under any jurisdiction\nor to any person, if such solicitation under such ju-\nrisdiction or to such person would be unlawful. \u00a9\n2023 JP Morgan Chase & Co. All rights reserved.", "mimetype": "text/plain", "start_char_idx": 3014, "end_char_idx": 4601, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3d2ed0ae-a22b-4e08-b8c1-16f32813a27c": {"__data__": {"id_": "3d2ed0ae-a22b-4e08-b8c1-16f32813a27c", "embedding": null, "metadata": {"page_label": "9", "file_name": "2402_14805v1.pdf", "Title of this paper": "Identifying Multiple Personalities in Large Language Models with External Evaluation", "Authors": "Xiaoyang Song, Yuta Adachi, Jessie Feng, Mouwei Lin, Linhao Yu, Frank Li, Akshat Gupta, Gopala Anumanchipalli, Simerjot Kaur", "Date published": "02/22/2024", "URL": "http://arxiv.org/abs/2402.14805v1", "summary": "As Large Language Models (LLMs) are integrated with human daily applications\nrapidly, many societal and ethical concerns are raised regarding the behavior\nof LLMs. One of the ways to comprehend LLMs' behavior is to analyze their\npersonalities. Many recent studies quantify LLMs' personalities using\nself-assessment tests that are created for humans. Yet many critiques question\nthe applicability and reliability of these self-assessment tests when applied\nto LLMs. In this paper, we investigate LLM personalities using an alternate\npersonality measurement method, which we refer to as the external evaluation\nmethod, where instead of prompting LLMs with multiple-choice questions in the\nLikert scale, we evaluate LLMs' personalities by analyzing their responses\ntoward open-ended situational questions using an external machine learning\nmodel. We first fine-tuned a Llama2-7B model as the MBTI personality predictor\nthat outperforms the state-of-the-art models as the tool to analyze LLMs'\nresponses. Then, we prompt the LLMs with situational questions and ask them to\ngenerate Twitter posts and comments, respectively, in order to assess their\npersonalities when playing two different roles. Using the external personality\nevaluation method, we identify that the obtained personality types for LLMs are\nsignificantly different when generating posts versus comments, whereas humans\nshow a consistent personality profile in these two different situations. This\nshows that LLMs can exhibit different personalities based on different\nscenarios, thus highlighting a fundamental difference between personality in\nLLMs and humans. With our work, we call for a re-evaluation of personality\ndefinition and measurement in LLMs."}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a14878fb-c323-4647-9473-016e9bc34f31", "node_type": "4", "metadata": {"page_label": "9", "file_name": "2402_14805v1.pdf", "Title of this paper": "Identifying Multiple Personalities in Large Language Models with External Evaluation", "Authors": "Xiaoyang Song, Yuta Adachi, Jessie Feng, Mouwei Lin, Linhao Yu, Frank Li, Akshat Gupta, Gopala Anumanchipalli, Simerjot Kaur", "Date published": "02/22/2024", "URL": "http://arxiv.org/abs/2402.14805v1", "summary": "As Large Language Models (LLMs) are integrated with human daily applications\nrapidly, many societal and ethical concerns are raised regarding the behavior\nof LLMs. One of the ways to comprehend LLMs' behavior is to analyze their\npersonalities. Many recent studies quantify LLMs' personalities using\nself-assessment tests that are created for humans. Yet many critiques question\nthe applicability and reliability of these self-assessment tests when applied\nto LLMs. In this paper, we investigate LLM personalities using an alternate\npersonality measurement method, which we refer to as the external evaluation\nmethod, where instead of prompting LLMs with multiple-choice questions in the\nLikert scale, we evaluate LLMs' personalities by analyzing their responses\ntoward open-ended situational questions using an external machine learning\nmodel. We first fine-tuned a Llama2-7B model as the MBTI personality predictor\nthat outperforms the state-of-the-art models as the tool to analyze LLMs'\nresponses. Then, we prompt the LLMs with situational questions and ask them to\ngenerate Twitter posts and comments, respectively, in order to assess their\npersonalities when playing two different roles. Using the external personality\nevaluation method, we identify that the obtained personality types for LLMs are\nsignificantly different when generating posts versus comments, whereas humans\nshow a consistent personality profile in these two different situations. This\nshows that LLMs can exhibit different personalities based on different\nscenarios, thus highlighting a fundamental difference between personality in\nLLMs and humans. With our work, we call for a re-evaluation of personality\ndefinition and measurement in LLMs."}, "hash": "0bc4ef618cd31a1fe5f48f14c7f5bd3eb567a06bbbd0e34ae8211eb67151692a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d1a02ab2-de6c-4ab3-ae4d-57925690cbc4", "node_type": "1", "metadata": {}, "hash": "f7847dcb272252795e21a693f021049b70bf3b87c294b729a2e146d0097fb358", "class_name": "RelatedNodeInfo"}}, "text": "References\nAndrea Agostinelli, Timo I Denk, Zal\u00e1n Borsos,\nJesse Engel, Mauro Verzetti, Antoine Caillon,\nQingqing Huang, Aren Jansen, Adam Roberts, Marco\nTagliasacchi, et al. 2023. Musiclm: Generating mu-\nsic from text. arXiv preprint arXiv:2301.11325 .\nAmerican Psychological Association. 1983. Publica-\ntions Manual . American Psychological Association,\nWashington, DC.\nAmerican Psychological Association. 2023. Definition\nof Personality -https://www.apa.org/topics/\npersonality .\nBojana Bodroza, Bojana M Dinic, and Ljubisa Bojic.\n2023. Personality testing of gpt-3: Limited temporal\nreliability, but highlighted social desirability of gpt-\n3\u2019s personality instruments results. arXiv preprint\narXiv:2306.04308 .\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. Advances in neural information processing\nsystems , 33:1877\u20131901.\nGraham Caron and Shashank Srivastava. 2022. Identi-\nfying and manipulating the personality traits of lan-\nguage models. arXiv preprint arXiv:2212.10276 .\nRaymond B Cattell. 1943a. The description of person-\nality: Basic traits resolved into clusters. The journal\nof abnormal and social psychology , 38(4):476.\nRaymond B Cattell. 1943b. The description of person-\nality. i. foundations of trait measurement. Psycholog-\nical review , 50(6):559.\nBanghao Chen, Zhaofeng Zhang, Nicolas Langren\u00e9,\nand Shengxin Zhu. 2023. Unleashing the potential of\nprompt engineering in large language models: a com-\nprehensive review. arXiv preprint arXiv:2310.14735 .\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2018. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning. arXiv preprint arXiv:1810.04805 .\nJohn M Digman. 1990.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1844, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d1a02ab2-de6c-4ab3-ae4d-57925690cbc4": {"__data__": {"id_": "d1a02ab2-de6c-4ab3-ae4d-57925690cbc4", "embedding": null, "metadata": {"page_label": "9", "file_name": "2402_14805v1.pdf", "Title of this paper": "Identifying Multiple Personalities in Large Language Models with External Evaluation", "Authors": "Xiaoyang Song, Yuta Adachi, Jessie Feng, Mouwei Lin, Linhao Yu, Frank Li, Akshat Gupta, Gopala Anumanchipalli, Simerjot Kaur", "Date published": "02/22/2024", "URL": "http://arxiv.org/abs/2402.14805v1", "summary": "As Large Language Models (LLMs) are integrated with human daily applications\nrapidly, many societal and ethical concerns are raised regarding the behavior\nof LLMs. One of the ways to comprehend LLMs' behavior is to analyze their\npersonalities. Many recent studies quantify LLMs' personalities using\nself-assessment tests that are created for humans. Yet many critiques question\nthe applicability and reliability of these self-assessment tests when applied\nto LLMs. In this paper, we investigate LLM personalities using an alternate\npersonality measurement method, which we refer to as the external evaluation\nmethod, where instead of prompting LLMs with multiple-choice questions in the\nLikert scale, we evaluate LLMs' personalities by analyzing their responses\ntoward open-ended situational questions using an external machine learning\nmodel. We first fine-tuned a Llama2-7B model as the MBTI personality predictor\nthat outperforms the state-of-the-art models as the tool to analyze LLMs'\nresponses. Then, we prompt the LLMs with situational questions and ask them to\ngenerate Twitter posts and comments, respectively, in order to assess their\npersonalities when playing two different roles. Using the external personality\nevaluation method, we identify that the obtained personality types for LLMs are\nsignificantly different when generating posts versus comments, whereas humans\nshow a consistent personality profile in these two different situations. This\nshows that LLMs can exhibit different personalities based on different\nscenarios, thus highlighting a fundamental difference between personality in\nLLMs and humans. With our work, we call for a re-evaluation of personality\ndefinition and measurement in LLMs."}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a14878fb-c323-4647-9473-016e9bc34f31", "node_type": "4", "metadata": {"page_label": "9", "file_name": "2402_14805v1.pdf", "Title of this paper": "Identifying Multiple Personalities in Large Language Models with External Evaluation", "Authors": "Xiaoyang Song, Yuta Adachi, Jessie Feng, Mouwei Lin, Linhao Yu, Frank Li, Akshat Gupta, Gopala Anumanchipalli, Simerjot Kaur", "Date published": "02/22/2024", "URL": "http://arxiv.org/abs/2402.14805v1", "summary": "As Large Language Models (LLMs) are integrated with human daily applications\nrapidly, many societal and ethical concerns are raised regarding the behavior\nof LLMs. One of the ways to comprehend LLMs' behavior is to analyze their\npersonalities. Many recent studies quantify LLMs' personalities using\nself-assessment tests that are created for humans. Yet many critiques question\nthe applicability and reliability of these self-assessment tests when applied\nto LLMs. In this paper, we investigate LLM personalities using an alternate\npersonality measurement method, which we refer to as the external evaluation\nmethod, where instead of prompting LLMs with multiple-choice questions in the\nLikert scale, we evaluate LLMs' personalities by analyzing their responses\ntoward open-ended situational questions using an external machine learning\nmodel. We first fine-tuned a Llama2-7B model as the MBTI personality predictor\nthat outperforms the state-of-the-art models as the tool to analyze LLMs'\nresponses. Then, we prompt the LLMs with situational questions and ask them to\ngenerate Twitter posts and comments, respectively, in order to assess their\npersonalities when playing two different roles. Using the external personality\nevaluation method, we identify that the obtained personality types for LLMs are\nsignificantly different when generating posts versus comments, whereas humans\nshow a consistent personality profile in these two different situations. This\nshows that LLMs can exhibit different personalities based on different\nscenarios, thus highlighting a fundamental difference between personality in\nLLMs and humans. With our work, we call for a re-evaluation of personality\ndefinition and measurement in LLMs."}, "hash": "0bc4ef618cd31a1fe5f48f14c7f5bd3eb567a06bbbd0e34ae8211eb67151692a", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3d2ed0ae-a22b-4e08-b8c1-16f32813a27c", "node_type": "1", "metadata": {"page_label": "9", "file_name": "2402_14805v1.pdf", "Title of this paper": "Identifying Multiple Personalities in Large Language Models with External Evaluation", "Authors": "Xiaoyang Song, Yuta Adachi, Jessie Feng, Mouwei Lin, Linhao Yu, Frank Li, Akshat Gupta, Gopala Anumanchipalli, Simerjot Kaur", "Date published": "02/22/2024", "URL": "http://arxiv.org/abs/2402.14805v1", "summary": "As Large Language Models (LLMs) are integrated with human daily applications\nrapidly, many societal and ethical concerns are raised regarding the behavior\nof LLMs. One of the ways to comprehend LLMs' behavior is to analyze their\npersonalities. Many recent studies quantify LLMs' personalities using\nself-assessment tests that are created for humans. Yet many critiques question\nthe applicability and reliability of these self-assessment tests when applied\nto LLMs. In this paper, we investigate LLM personalities using an alternate\npersonality measurement method, which we refer to as the external evaluation\nmethod, where instead of prompting LLMs with multiple-choice questions in the\nLikert scale, we evaluate LLMs' personalities by analyzing their responses\ntoward open-ended situational questions using an external machine learning\nmodel. We first fine-tuned a Llama2-7B model as the MBTI personality predictor\nthat outperforms the state-of-the-art models as the tool to analyze LLMs'\nresponses. Then, we prompt the LLMs with situational questions and ask them to\ngenerate Twitter posts and comments, respectively, in order to assess their\npersonalities when playing two different roles. Using the external personality\nevaluation method, we identify that the obtained personality types for LLMs are\nsignificantly different when generating posts versus comments, whereas humans\nshow a consistent personality profile in these two different situations. This\nshows that LLMs can exhibit different personalities based on different\nscenarios, thus highlighting a fundamental difference between personality in\nLLMs and humans. With our work, we call for a re-evaluation of personality\ndefinition and measurement in LLMs."}, "hash": "cb90a28a897807c0fc4ede1e837313df569d04de2b21dc24567f264cdb141bc1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a596f26c-17f7-4649-87fc-bd6c3c6c02eb", "node_type": "1", "metadata": {}, "hash": "78ad1ae3f09b1f939c4bf932e7fd368cf3b695dba9a9c4b88c36af52955e6a68", "class_name": "RelatedNodeInfo"}}, "text": "The journal\nof abnormal and social psychology , 38(4):476.\nRaymond B Cattell. 1943b. The description of person-\nality. i. foundations of trait measurement. Psycholog-\nical review , 50(6):559.\nBanghao Chen, Zhaofeng Zhang, Nicolas Langren\u00e9,\nand Shengxin Zhu. 2023. Unleashing the potential of\nprompt engineering in large language models: a com-\nprehensive review. arXiv preprint arXiv:2310.14735 .\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2018. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning. arXiv preprint arXiv:1810.04805 .\nJohn M Digman. 1990. Personality structure: Emer-\ngence of the five-factor model. Annual review of\npsychology , 41(1):417\u2013440.\nKatrina Faust. 2019. Myers-briggs type indicator\n(mbti) overview - https://leadx.org/articles/\nmbti-myers-briggs-type-indicator-overview/ .\nLewis R Goldberg. 1990. An alternative\" description of\npersonality\": the big-five factor structure. Journal of\npersonality and social psychology , 59(6):1216.\nLewis R Goldberg. 1993. The structure of phenotypic\npersonality traits. American psychologist , 48(1):26.Jennifer P Green, Reeshad S Dalal, Kristen L Swigart,\nMelissa A Bleiberg, David M Wallace, and Amber K\nHargrove. 2019. Personality consistency and situa-\ntional influences on behavior. Journal of Manage-\nment , 45(8):3204\u20133234.\nAkshat Gupta, Xiaoyang Song, and Gopala Anu-\nmanchipalli. 2023. Investigating the applicability\nof self-assessment tests for personality measure-\nment of large language models. arXiv preprint\narXiv:2309.08163 .\nAri Holtzman, Jan Buys, Li Du, Maxwell Forbes, and\nYejin Choi. 2019. The curious case of neural text\ndegeneration. arXiv preprint arXiv:1904.09751 .\nEdward J Hu, Yelong Shen, Phillip Wallis, Zeyuan\nAllen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang,\nand Weizhu Chen. 2021.", "mimetype": "text/plain", "start_char_idx": 1237, "end_char_idx": 3058, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a596f26c-17f7-4649-87fc-bd6c3c6c02eb": {"__data__": {"id_": "a596f26c-17f7-4649-87fc-bd6c3c6c02eb", "embedding": null, "metadata": {"page_label": "9", "file_name": "2402_14805v1.pdf", "Title of this paper": "Identifying Multiple Personalities in Large Language Models with External Evaluation", "Authors": "Xiaoyang Song, Yuta Adachi, Jessie Feng, Mouwei Lin, Linhao Yu, Frank Li, Akshat Gupta, Gopala Anumanchipalli, Simerjot Kaur", "Date published": "02/22/2024", "URL": "http://arxiv.org/abs/2402.14805v1", "summary": "As Large Language Models (LLMs) are integrated with human daily applications\nrapidly, many societal and ethical concerns are raised regarding the behavior\nof LLMs. One of the ways to comprehend LLMs' behavior is to analyze their\npersonalities. Many recent studies quantify LLMs' personalities using\nself-assessment tests that are created for humans. Yet many critiques question\nthe applicability and reliability of these self-assessment tests when applied\nto LLMs. In this paper, we investigate LLM personalities using an alternate\npersonality measurement method, which we refer to as the external evaluation\nmethod, where instead of prompting LLMs with multiple-choice questions in the\nLikert scale, we evaluate LLMs' personalities by analyzing their responses\ntoward open-ended situational questions using an external machine learning\nmodel. We first fine-tuned a Llama2-7B model as the MBTI personality predictor\nthat outperforms the state-of-the-art models as the tool to analyze LLMs'\nresponses. Then, we prompt the LLMs with situational questions and ask them to\ngenerate Twitter posts and comments, respectively, in order to assess their\npersonalities when playing two different roles. Using the external personality\nevaluation method, we identify that the obtained personality types for LLMs are\nsignificantly different when generating posts versus comments, whereas humans\nshow a consistent personality profile in these two different situations. This\nshows that LLMs can exhibit different personalities based on different\nscenarios, thus highlighting a fundamental difference between personality in\nLLMs and humans. With our work, we call for a re-evaluation of personality\ndefinition and measurement in LLMs."}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a14878fb-c323-4647-9473-016e9bc34f31", "node_type": "4", "metadata": {"page_label": "9", "file_name": "2402_14805v1.pdf", "Title of this paper": "Identifying Multiple Personalities in Large Language Models with External Evaluation", "Authors": "Xiaoyang Song, Yuta Adachi, Jessie Feng, Mouwei Lin, Linhao Yu, Frank Li, Akshat Gupta, Gopala Anumanchipalli, Simerjot Kaur", "Date published": "02/22/2024", "URL": "http://arxiv.org/abs/2402.14805v1", "summary": "As Large Language Models (LLMs) are integrated with human daily applications\nrapidly, many societal and ethical concerns are raised regarding the behavior\nof LLMs. One of the ways to comprehend LLMs' behavior is to analyze their\npersonalities. Many recent studies quantify LLMs' personalities using\nself-assessment tests that are created for humans. Yet many critiques question\nthe applicability and reliability of these self-assessment tests when applied\nto LLMs. In this paper, we investigate LLM personalities using an alternate\npersonality measurement method, which we refer to as the external evaluation\nmethod, where instead of prompting LLMs with multiple-choice questions in the\nLikert scale, we evaluate LLMs' personalities by analyzing their responses\ntoward open-ended situational questions using an external machine learning\nmodel. We first fine-tuned a Llama2-7B model as the MBTI personality predictor\nthat outperforms the state-of-the-art models as the tool to analyze LLMs'\nresponses. Then, we prompt the LLMs with situational questions and ask them to\ngenerate Twitter posts and comments, respectively, in order to assess their\npersonalities when playing two different roles. Using the external personality\nevaluation method, we identify that the obtained personality types for LLMs are\nsignificantly different when generating posts versus comments, whereas humans\nshow a consistent personality profile in these two different situations. This\nshows that LLMs can exhibit different personalities based on different\nscenarios, thus highlighting a fundamental difference between personality in\nLLMs and humans. With our work, we call for a re-evaluation of personality\ndefinition and measurement in LLMs."}, "hash": "0bc4ef618cd31a1fe5f48f14c7f5bd3eb567a06bbbd0e34ae8211eb67151692a", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d1a02ab2-de6c-4ab3-ae4d-57925690cbc4", "node_type": "1", "metadata": {"page_label": "9", "file_name": "2402_14805v1.pdf", "Title of this paper": "Identifying Multiple Personalities in Large Language Models with External Evaluation", "Authors": "Xiaoyang Song, Yuta Adachi, Jessie Feng, Mouwei Lin, Linhao Yu, Frank Li, Akshat Gupta, Gopala Anumanchipalli, Simerjot Kaur", "Date published": "02/22/2024", "URL": "http://arxiv.org/abs/2402.14805v1", "summary": "As Large Language Models (LLMs) are integrated with human daily applications\nrapidly, many societal and ethical concerns are raised regarding the behavior\nof LLMs. One of the ways to comprehend LLMs' behavior is to analyze their\npersonalities. Many recent studies quantify LLMs' personalities using\nself-assessment tests that are created for humans. Yet many critiques question\nthe applicability and reliability of these self-assessment tests when applied\nto LLMs. In this paper, we investigate LLM personalities using an alternate\npersonality measurement method, which we refer to as the external evaluation\nmethod, where instead of prompting LLMs with multiple-choice questions in the\nLikert scale, we evaluate LLMs' personalities by analyzing their responses\ntoward open-ended situational questions using an external machine learning\nmodel. We first fine-tuned a Llama2-7B model as the MBTI personality predictor\nthat outperforms the state-of-the-art models as the tool to analyze LLMs'\nresponses. Then, we prompt the LLMs with situational questions and ask them to\ngenerate Twitter posts and comments, respectively, in order to assess their\npersonalities when playing two different roles. Using the external personality\nevaluation method, we identify that the obtained personality types for LLMs are\nsignificantly different when generating posts versus comments, whereas humans\nshow a consistent personality profile in these two different situations. This\nshows that LLMs can exhibit different personalities based on different\nscenarios, thus highlighting a fundamental difference between personality in\nLLMs and humans. With our work, we call for a re-evaluation of personality\ndefinition and measurement in LLMs."}, "hash": "40fc30e2591ca0403e915b69c8510405c3ef918ae7357318724340e0fbffa46b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "75c87058-4d99-44e3-8765-945d4cf0094d", "node_type": "1", "metadata": {}, "hash": "c579f8664997ea8d0cc13dc728f69b0c09ac6f652a60d4f14a28f99bc2fe7e73", "class_name": "RelatedNodeInfo"}}, "text": "2019. Personality consistency and situa-\ntional influences on behavior. Journal of Manage-\nment , 45(8):3204\u20133234.\nAkshat Gupta, Xiaoyang Song, and Gopala Anu-\nmanchipalli. 2023. Investigating the applicability\nof self-assessment tests for personality measure-\nment of large language models. arXiv preprint\narXiv:2309.08163 .\nAri Holtzman, Jan Buys, Li Du, Maxwell Forbes, and\nYejin Choi. 2019. The curious case of neural text\ndegeneration. arXiv preprint arXiv:1904.09751 .\nEdward J Hu, Yelong Shen, Phillip Wallis, Zeyuan\nAllen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang,\nand Weizhu Chen. 2021. Lora: Low-rank adap-\ntation of large language models. arXiv preprint\narXiv:2106.09685 .\nJen-tse Huang, Wenxuan Wang, Man Ho Lam, Eric John\nLi, Wenxiang Jiao, and Michael R Lyu. 2023. Chat-\ngpt an enfj, bard an istj: Empirical study on per-\nsonalities of large language models. arXiv preprint\narXiv:2305.19926 .\nNaomi Imasato, Kazuki Miyazawa, Caitlin Duncan, and\nTakayuki Nagai. 2023. Using a language model to\ngenerate music in its symbolic domain while control-\nling its perceived emotion. IEEE Access .\nJaeho Jeon and Seongyong Lee. 2023. Large language\nmodels in education: A focus on the complementary\nrelationship between human teachers and chatgpt.\nEducation and Information Technologies , pages 1\u2013\n20.\nGuangyuan Jiang, Manjie Xu, Song-Chun Zhu, Wen-\njuan Han, Chi Zhang, and Yixin Zhu. 2022. Mpi:\nEvaluating and inducing personality in pre-trained\nlanguage models. arXiv preprint arXiv:2206.07550 .\nGuangyuan Jiang, Manjie Xu, Song-Chun Zhu, Wen-\njuan Han, Chi Zhang, and Yixin Zhu. 2023. Evaluat-\ning and inducing personality in pre-trained language\nmodels.\nJohn A Johnson. 2014. Measuring thirty facets of the\nfive factor model with a 120-item public domain in-\nventory: Development of the ipip-neo-120.", "mimetype": "text/plain", "start_char_idx": 2468, "end_char_idx": 4272, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "75c87058-4d99-44e3-8765-945d4cf0094d": {"__data__": {"id_": "75c87058-4d99-44e3-8765-945d4cf0094d", "embedding": null, "metadata": {"page_label": "9", "file_name": "2402_14805v1.pdf", "Title of this paper": "Identifying Multiple Personalities in Large Language Models with External Evaluation", "Authors": "Xiaoyang Song, Yuta Adachi, Jessie Feng, Mouwei Lin, Linhao Yu, Frank Li, Akshat Gupta, Gopala Anumanchipalli, Simerjot Kaur", "Date published": "02/22/2024", "URL": "http://arxiv.org/abs/2402.14805v1", "summary": "As Large Language Models (LLMs) are integrated with human daily applications\nrapidly, many societal and ethical concerns are raised regarding the behavior\nof LLMs. One of the ways to comprehend LLMs' behavior is to analyze their\npersonalities. Many recent studies quantify LLMs' personalities using\nself-assessment tests that are created for humans. Yet many critiques question\nthe applicability and reliability of these self-assessment tests when applied\nto LLMs. In this paper, we investigate LLM personalities using an alternate\npersonality measurement method, which we refer to as the external evaluation\nmethod, where instead of prompting LLMs with multiple-choice questions in the\nLikert scale, we evaluate LLMs' personalities by analyzing their responses\ntoward open-ended situational questions using an external machine learning\nmodel. We first fine-tuned a Llama2-7B model as the MBTI personality predictor\nthat outperforms the state-of-the-art models as the tool to analyze LLMs'\nresponses. Then, we prompt the LLMs with situational questions and ask them to\ngenerate Twitter posts and comments, respectively, in order to assess their\npersonalities when playing two different roles. Using the external personality\nevaluation method, we identify that the obtained personality types for LLMs are\nsignificantly different when generating posts versus comments, whereas humans\nshow a consistent personality profile in these two different situations. This\nshows that LLMs can exhibit different personalities based on different\nscenarios, thus highlighting a fundamental difference between personality in\nLLMs and humans. With our work, we call for a re-evaluation of personality\ndefinition and measurement in LLMs."}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a14878fb-c323-4647-9473-016e9bc34f31", "node_type": "4", "metadata": {"page_label": "9", "file_name": "2402_14805v1.pdf", "Title of this paper": "Identifying Multiple Personalities in Large Language Models with External Evaluation", "Authors": "Xiaoyang Song, Yuta Adachi, Jessie Feng, Mouwei Lin, Linhao Yu, Frank Li, Akshat Gupta, Gopala Anumanchipalli, Simerjot Kaur", "Date published": "02/22/2024", "URL": "http://arxiv.org/abs/2402.14805v1", "summary": "As Large Language Models (LLMs) are integrated with human daily applications\nrapidly, many societal and ethical concerns are raised regarding the behavior\nof LLMs. One of the ways to comprehend LLMs' behavior is to analyze their\npersonalities. Many recent studies quantify LLMs' personalities using\nself-assessment tests that are created for humans. Yet many critiques question\nthe applicability and reliability of these self-assessment tests when applied\nto LLMs. In this paper, we investigate LLM personalities using an alternate\npersonality measurement method, which we refer to as the external evaluation\nmethod, where instead of prompting LLMs with multiple-choice questions in the\nLikert scale, we evaluate LLMs' personalities by analyzing their responses\ntoward open-ended situational questions using an external machine learning\nmodel. We first fine-tuned a Llama2-7B model as the MBTI personality predictor\nthat outperforms the state-of-the-art models as the tool to analyze LLMs'\nresponses. Then, we prompt the LLMs with situational questions and ask them to\ngenerate Twitter posts and comments, respectively, in order to assess their\npersonalities when playing two different roles. Using the external personality\nevaluation method, we identify that the obtained personality types for LLMs are\nsignificantly different when generating posts versus comments, whereas humans\nshow a consistent personality profile in these two different situations. This\nshows that LLMs can exhibit different personalities based on different\nscenarios, thus highlighting a fundamental difference between personality in\nLLMs and humans. With our work, we call for a re-evaluation of personality\ndefinition and measurement in LLMs."}, "hash": "0bc4ef618cd31a1fe5f48f14c7f5bd3eb567a06bbbd0e34ae8211eb67151692a", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a596f26c-17f7-4649-87fc-bd6c3c6c02eb", "node_type": "1", "metadata": {"page_label": "9", "file_name": "2402_14805v1.pdf", "Title of this paper": "Identifying Multiple Personalities in Large Language Models with External Evaluation", "Authors": "Xiaoyang Song, Yuta Adachi, Jessie Feng, Mouwei Lin, Linhao Yu, Frank Li, Akshat Gupta, Gopala Anumanchipalli, Simerjot Kaur", "Date published": "02/22/2024", "URL": "http://arxiv.org/abs/2402.14805v1", "summary": "As Large Language Models (LLMs) are integrated with human daily applications\nrapidly, many societal and ethical concerns are raised regarding the behavior\nof LLMs. One of the ways to comprehend LLMs' behavior is to analyze their\npersonalities. Many recent studies quantify LLMs' personalities using\nself-assessment tests that are created for humans. Yet many critiques question\nthe applicability and reliability of these self-assessment tests when applied\nto LLMs. In this paper, we investigate LLM personalities using an alternate\npersonality measurement method, which we refer to as the external evaluation\nmethod, where instead of prompting LLMs with multiple-choice questions in the\nLikert scale, we evaluate LLMs' personalities by analyzing their responses\ntoward open-ended situational questions using an external machine learning\nmodel. We first fine-tuned a Llama2-7B model as the MBTI personality predictor\nthat outperforms the state-of-the-art models as the tool to analyze LLMs'\nresponses. Then, we prompt the LLMs with situational questions and ask them to\ngenerate Twitter posts and comments, respectively, in order to assess their\npersonalities when playing two different roles. Using the external personality\nevaluation method, we identify that the obtained personality types for LLMs are\nsignificantly different when generating posts versus comments, whereas humans\nshow a consistent personality profile in these two different situations. This\nshows that LLMs can exhibit different personalities based on different\nscenarios, thus highlighting a fundamental difference between personality in\nLLMs and humans. With our work, we call for a re-evaluation of personality\ndefinition and measurement in LLMs."}, "hash": "9941f93b345e4e9dc486e718c0f6747afbdec1a926c40c68e48a1bc1bfec8d3c", "class_name": "RelatedNodeInfo"}}, "text": "2023. Large language\nmodels in education: A focus on the complementary\nrelationship between human teachers and chatgpt.\nEducation and Information Technologies , pages 1\u2013\n20.\nGuangyuan Jiang, Manjie Xu, Song-Chun Zhu, Wen-\njuan Han, Chi Zhang, and Yixin Zhu. 2022. Mpi:\nEvaluating and inducing personality in pre-trained\nlanguage models. arXiv preprint arXiv:2206.07550 .\nGuangyuan Jiang, Manjie Xu, Song-Chun Zhu, Wen-\njuan Han, Chi Zhang, and Yixin Zhu. 2023. Evaluat-\ning and inducing personality in pre-trained language\nmodels.\nJohn A Johnson. 2014. Measuring thirty facets of the\nfive factor model with a 120-item public domain in-\nventory: Development of the ipip-neo-120. Journal\nof research in personality , 51:78\u201389.\nSaketh Reddy Karra, Theja Tulabandhula, et al. 2022.\nEstimating the personality of white-box language\nmodels. arXiv e-prints , pages arXiv\u20132204.\nTin Lai, Yukun Shi, Zicong Du, Jiajie Wu, Ken Fu,\nYichao Dou, and Ziqi Wang. 2023. Psy-llm: Scal-\ning up global mental health psychological services\nwith ai-based large language models. arXiv preprint\narXiv:2307.11991 .", "mimetype": "text/plain", "start_char_idx": 3595, "end_char_idx": 4684, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "52a55e88-21eb-43e1-96d2-7780dab7d444": {"__data__": {"id_": "52a55e88-21eb-43e1-96d2-7780dab7d444", "embedding": null, "metadata": {"page_label": "10", "file_name": "2402_14805v1.pdf", "Title of this paper": "Identifying Multiple Personalities in Large Language Models with External Evaluation", "Authors": "Xiaoyang Song, Yuta Adachi, Jessie Feng, Mouwei Lin, Linhao Yu, Frank Li, Akshat Gupta, Gopala Anumanchipalli, Simerjot Kaur", "Date published": "02/22/2024", "URL": "http://arxiv.org/abs/2402.14805v1", "summary": "As Large Language Models (LLMs) are integrated with human daily applications\nrapidly, many societal and ethical concerns are raised regarding the behavior\nof LLMs. One of the ways to comprehend LLMs' behavior is to analyze their\npersonalities. Many recent studies quantify LLMs' personalities using\nself-assessment tests that are created for humans. Yet many critiques question\nthe applicability and reliability of these self-assessment tests when applied\nto LLMs. In this paper, we investigate LLM personalities using an alternate\npersonality measurement method, which we refer to as the external evaluation\nmethod, where instead of prompting LLMs with multiple-choice questions in the\nLikert scale, we evaluate LLMs' personalities by analyzing their responses\ntoward open-ended situational questions using an external machine learning\nmodel. We first fine-tuned a Llama2-7B model as the MBTI personality predictor\nthat outperforms the state-of-the-art models as the tool to analyze LLMs'\nresponses. Then, we prompt the LLMs with situational questions and ask them to\ngenerate Twitter posts and comments, respectively, in order to assess their\npersonalities when playing two different roles. Using the external personality\nevaluation method, we identify that the obtained personality types for LLMs are\nsignificantly different when generating posts versus comments, whereas humans\nshow a consistent personality profile in these two different situations. This\nshows that LLMs can exhibit different personalities based on different\nscenarios, thus highlighting a fundamental difference between personality in\nLLMs and humans. With our work, we call for a re-evaluation of personality\ndefinition and measurement in LLMs."}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2fa444ed-af02-49df-9eff-dc61c8867363", "node_type": "4", "metadata": {"page_label": "10", "file_name": "2402_14805v1.pdf", "Title of this paper": "Identifying Multiple Personalities in Large Language Models with External Evaluation", "Authors": "Xiaoyang Song, Yuta Adachi, Jessie Feng, Mouwei Lin, Linhao Yu, Frank Li, Akshat Gupta, Gopala Anumanchipalli, Simerjot Kaur", "Date published": "02/22/2024", "URL": "http://arxiv.org/abs/2402.14805v1", "summary": "As Large Language Models (LLMs) are integrated with human daily applications\nrapidly, many societal and ethical concerns are raised regarding the behavior\nof LLMs. One of the ways to comprehend LLMs' behavior is to analyze their\npersonalities. Many recent studies quantify LLMs' personalities using\nself-assessment tests that are created for humans. Yet many critiques question\nthe applicability and reliability of these self-assessment tests when applied\nto LLMs. In this paper, we investigate LLM personalities using an alternate\npersonality measurement method, which we refer to as the external evaluation\nmethod, where instead of prompting LLMs with multiple-choice questions in the\nLikert scale, we evaluate LLMs' personalities by analyzing their responses\ntoward open-ended situational questions using an external machine learning\nmodel. We first fine-tuned a Llama2-7B model as the MBTI personality predictor\nthat outperforms the state-of-the-art models as the tool to analyze LLMs'\nresponses. Then, we prompt the LLMs with situational questions and ask them to\ngenerate Twitter posts and comments, respectively, in order to assess their\npersonalities when playing two different roles. Using the external personality\nevaluation method, we identify that the obtained personality types for LLMs are\nsignificantly different when generating posts versus comments, whereas humans\nshow a consistent personality profile in these two different situations. This\nshows that LLMs can exhibit different personalities based on different\nscenarios, thus highlighting a fundamental difference between personality in\nLLMs and humans. With our work, we call for a re-evaluation of personality\ndefinition and measurement in LLMs."}, "hash": "c9b214274aca084dc81d5e129b36706f708cd52525a6c93418eedeccb1cf33bc", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3b5e8528-454d-4786-b8e7-af7fcc8ea72d", "node_type": "1", "metadata": {}, "hash": "bfd37f5b8879aacd0b199e309a2d869a9d540bd14e4ae4a5e5ad7d7b00c1bb33", "class_name": "RelatedNodeInfo"}}, "text": "Yash Mehta. 2023. Personality predic-\ntion. https://github.com/yashsmehta/\npersonality-prediction.git . GitHub reposi-\ntory.\nYash Mehta, Samin Fatehi, Amirmohammad Kazameini,\nClemens Stachl, Erik Cambria, and Sauleh Eetemadi.\n2020. Bottom-up and top-down: Predicting person-\nality with psycholinguistic and language model fea-\ntures. In 2020 IEEE International Conference on\nData Mining (ICDM) , pages 1184\u20131189. IEEE.\nMaril\u00f9 Miotto, Nicola Rossberg, and Bennett Klein-\nberg. 2022. Who is gpt-3? an exploration of per-\nsonality, values and demographics. arXiv preprint\narXiv:2209.14338 .\nDavid Noever and Sam Hyams. 2023. Ai text-to-\nbehavior: A study in steerability. arXiv preprint\narXiv:2308.07326 .\nOpenAI. 2022. Chatgpt - https://openai.com/blog/\nchatgpt#OpenAI .\nOpenAI. 2023. Gpt-4 technical report - https://cdn.\nopenai.com/papers/gpt-4.pdf .\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida,\nCarroll Wainwright, Pamela Mishkin, Chong Zhang,\nSandhini Agarwal, Katarina Slama, Alex Ray, et al.\n2022. Training language models to follow instruc-\ntions with human feedback. Advances in Neural\nInformation Processing Systems , 35:27730\u201327744.\nKeyu Pan and Yawen Zeng. 2023. Do llms possess a\npersonality? making the mbti test an amazing eval-\nuation for large language models. arXiv preprint\narXiv:2307.16180 .\nAlec Radford, Karthik Narasimhan, Tim Salimans, Ilya\nSutskever, et al. 2018. Improving language under-\nstanding by generative pre-training.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, Ilya Sutskever, et al. 2019. Language\nmodels are unsupervised multitask learners. OpenAI\nblog, 1(8):9.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1621, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3b5e8528-454d-4786-b8e7-af7fcc8ea72d": {"__data__": {"id_": "3b5e8528-454d-4786-b8e7-af7fcc8ea72d", "embedding": null, "metadata": {"page_label": "10", "file_name": "2402_14805v1.pdf", "Title of this paper": "Identifying Multiple Personalities in Large Language Models with External Evaluation", "Authors": "Xiaoyang Song, Yuta Adachi, Jessie Feng, Mouwei Lin, Linhao Yu, Frank Li, Akshat Gupta, Gopala Anumanchipalli, Simerjot Kaur", "Date published": "02/22/2024", "URL": "http://arxiv.org/abs/2402.14805v1", "summary": "As Large Language Models (LLMs) are integrated with human daily applications\nrapidly, many societal and ethical concerns are raised regarding the behavior\nof LLMs. One of the ways to comprehend LLMs' behavior is to analyze their\npersonalities. Many recent studies quantify LLMs' personalities using\nself-assessment tests that are created for humans. Yet many critiques question\nthe applicability and reliability of these self-assessment tests when applied\nto LLMs. In this paper, we investigate LLM personalities using an alternate\npersonality measurement method, which we refer to as the external evaluation\nmethod, where instead of prompting LLMs with multiple-choice questions in the\nLikert scale, we evaluate LLMs' personalities by analyzing their responses\ntoward open-ended situational questions using an external machine learning\nmodel. We first fine-tuned a Llama2-7B model as the MBTI personality predictor\nthat outperforms the state-of-the-art models as the tool to analyze LLMs'\nresponses. Then, we prompt the LLMs with situational questions and ask them to\ngenerate Twitter posts and comments, respectively, in order to assess their\npersonalities when playing two different roles. Using the external personality\nevaluation method, we identify that the obtained personality types for LLMs are\nsignificantly different when generating posts versus comments, whereas humans\nshow a consistent personality profile in these two different situations. This\nshows that LLMs can exhibit different personalities based on different\nscenarios, thus highlighting a fundamental difference between personality in\nLLMs and humans. With our work, we call for a re-evaluation of personality\ndefinition and measurement in LLMs."}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2fa444ed-af02-49df-9eff-dc61c8867363", "node_type": "4", "metadata": {"page_label": "10", "file_name": "2402_14805v1.pdf", "Title of this paper": "Identifying Multiple Personalities in Large Language Models with External Evaluation", "Authors": "Xiaoyang Song, Yuta Adachi, Jessie Feng, Mouwei Lin, Linhao Yu, Frank Li, Akshat Gupta, Gopala Anumanchipalli, Simerjot Kaur", "Date published": "02/22/2024", "URL": "http://arxiv.org/abs/2402.14805v1", "summary": "As Large Language Models (LLMs) are integrated with human daily applications\nrapidly, many societal and ethical concerns are raised regarding the behavior\nof LLMs. One of the ways to comprehend LLMs' behavior is to analyze their\npersonalities. Many recent studies quantify LLMs' personalities using\nself-assessment tests that are created for humans. Yet many critiques question\nthe applicability and reliability of these self-assessment tests when applied\nto LLMs. In this paper, we investigate LLM personalities using an alternate\npersonality measurement method, which we refer to as the external evaluation\nmethod, where instead of prompting LLMs with multiple-choice questions in the\nLikert scale, we evaluate LLMs' personalities by analyzing their responses\ntoward open-ended situational questions using an external machine learning\nmodel. We first fine-tuned a Llama2-7B model as the MBTI personality predictor\nthat outperforms the state-of-the-art models as the tool to analyze LLMs'\nresponses. Then, we prompt the LLMs with situational questions and ask them to\ngenerate Twitter posts and comments, respectively, in order to assess their\npersonalities when playing two different roles. Using the external personality\nevaluation method, we identify that the obtained personality types for LLMs are\nsignificantly different when generating posts versus comments, whereas humans\nshow a consistent personality profile in these two different situations. This\nshows that LLMs can exhibit different personalities based on different\nscenarios, thus highlighting a fundamental difference between personality in\nLLMs and humans. With our work, we call for a re-evaluation of personality\ndefinition and measurement in LLMs."}, "hash": "c9b214274aca084dc81d5e129b36706f708cd52525a6c93418eedeccb1cf33bc", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "52a55e88-21eb-43e1-96d2-7780dab7d444", "node_type": "1", "metadata": {"page_label": "10", "file_name": "2402_14805v1.pdf", "Title of this paper": "Identifying Multiple Personalities in Large Language Models with External Evaluation", "Authors": "Xiaoyang Song, Yuta Adachi, Jessie Feng, Mouwei Lin, Linhao Yu, Frank Li, Akshat Gupta, Gopala Anumanchipalli, Simerjot Kaur", "Date published": "02/22/2024", "URL": "http://arxiv.org/abs/2402.14805v1", "summary": "As Large Language Models (LLMs) are integrated with human daily applications\nrapidly, many societal and ethical concerns are raised regarding the behavior\nof LLMs. One of the ways to comprehend LLMs' behavior is to analyze their\npersonalities. Many recent studies quantify LLMs' personalities using\nself-assessment tests that are created for humans. Yet many critiques question\nthe applicability and reliability of these self-assessment tests when applied\nto LLMs. In this paper, we investigate LLM personalities using an alternate\npersonality measurement method, which we refer to as the external evaluation\nmethod, where instead of prompting LLMs with multiple-choice questions in the\nLikert scale, we evaluate LLMs' personalities by analyzing their responses\ntoward open-ended situational questions using an external machine learning\nmodel. We first fine-tuned a Llama2-7B model as the MBTI personality predictor\nthat outperforms the state-of-the-art models as the tool to analyze LLMs'\nresponses. Then, we prompt the LLMs with situational questions and ask them to\ngenerate Twitter posts and comments, respectively, in order to assess their\npersonalities when playing two different roles. Using the external personality\nevaluation method, we identify that the obtained personality types for LLMs are\nsignificantly different when generating posts versus comments, whereas humans\nshow a consistent personality profile in these two different situations. This\nshows that LLMs can exhibit different personalities based on different\nscenarios, thus highlighting a fundamental difference between personality in\nLLMs and humans. With our work, we call for a re-evaluation of personality\ndefinition and measurement in LLMs."}, "hash": "69cc4c02e2b75098159623c6522e418865fb5475c0cdba8de6fa8de331411921", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "fca6c576-985e-444b-b18b-ab4a8c8d44bf", "node_type": "1", "metadata": {}, "hash": "34b803e23ccea2f42da6947e7964c000fb7cc540ec08c213d3a8531881067e1e", "class_name": "RelatedNodeInfo"}}, "text": "Training language models to follow instruc-\ntions with human feedback. Advances in Neural\nInformation Processing Systems , 35:27730\u201327744.\nKeyu Pan and Yawen Zeng. 2023. Do llms possess a\npersonality? making the mbti test an amazing eval-\nuation for large language models. arXiv preprint\narXiv:2307.16180 .\nAlec Radford, Karthik Narasimhan, Tim Salimans, Ilya\nSutskever, et al. 2018. Improving language under-\nstanding by generative pre-training.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, Ilya Sutskever, et al. 2019. Language\nmodels are unsupervised multitask learners. OpenAI\nblog, 1(8):9.\nMustafa Safdari, Greg Serapio-Garc\u00eda, Cl\u00e9ment Crepy,\nStephen Fitz, Peter Romero, Luning Sun, Marwa\nAbdulhai, Aleksandra Faust, and Maja Matari \u00b4c. 2023.\nPersonality traits in large language models. arXiv\npreprint arXiv:2307.00184 .\nMelanie Sclar, Yejin Choi, Yulia Tsvetkov, and Alane\nSuhr. 2023. Quantifying language models\u2019 sensitiv-\nity to spurious features in prompt design or: How i\nlearned to start worrying about prompt formatting.\narXiv preprint arXiv:2310.11324 .\nXiaoyang Song, Akshat Gupta, Kiyan Mohebbizadeh,\nShujie Hu, and Anant Singh. 2023. Have large lan-\nguage models developed a personality?: Applicabil-\nity of self-assessment tests in measuring personality\nin llms. arXiv preprint arXiv:2305.14693 .Qirui Tang, Wenkang Jiang, Yihua Du, and Lei Lin.\n2023. An attention-based denoising framework for\npersonality detection in social media texts. arXiv\npreprint arXiv:2311.09945 .\nHugo Touvron, Louis Martin, Kevin Stone, Peter Al-\nbert, Amjad Almahairi, Yasmine Babaei, Nikolay\nBashlykov, Soumya Batra, Prajjwal Bhargava, Shruti\nBhosale, et al. 2023a.", "mimetype": "text/plain", "start_char_idx": 1007, "end_char_idx": 2690, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "fca6c576-985e-444b-b18b-ab4a8c8d44bf": {"__data__": {"id_": "fca6c576-985e-444b-b18b-ab4a8c8d44bf", "embedding": null, "metadata": {"page_label": "10", "file_name": "2402_14805v1.pdf", "Title of this paper": "Identifying Multiple Personalities in Large Language Models with External Evaluation", "Authors": "Xiaoyang Song, Yuta Adachi, Jessie Feng, Mouwei Lin, Linhao Yu, Frank Li, Akshat Gupta, Gopala Anumanchipalli, Simerjot Kaur", "Date published": "02/22/2024", "URL": "http://arxiv.org/abs/2402.14805v1", "summary": "As Large Language Models (LLMs) are integrated with human daily applications\nrapidly, many societal and ethical concerns are raised regarding the behavior\nof LLMs. One of the ways to comprehend LLMs' behavior is to analyze their\npersonalities. Many recent studies quantify LLMs' personalities using\nself-assessment tests that are created for humans. Yet many critiques question\nthe applicability and reliability of these self-assessment tests when applied\nto LLMs. In this paper, we investigate LLM personalities using an alternate\npersonality measurement method, which we refer to as the external evaluation\nmethod, where instead of prompting LLMs with multiple-choice questions in the\nLikert scale, we evaluate LLMs' personalities by analyzing their responses\ntoward open-ended situational questions using an external machine learning\nmodel. We first fine-tuned a Llama2-7B model as the MBTI personality predictor\nthat outperforms the state-of-the-art models as the tool to analyze LLMs'\nresponses. Then, we prompt the LLMs with situational questions and ask them to\ngenerate Twitter posts and comments, respectively, in order to assess their\npersonalities when playing two different roles. Using the external personality\nevaluation method, we identify that the obtained personality types for LLMs are\nsignificantly different when generating posts versus comments, whereas humans\nshow a consistent personality profile in these two different situations. This\nshows that LLMs can exhibit different personalities based on different\nscenarios, thus highlighting a fundamental difference between personality in\nLLMs and humans. With our work, we call for a re-evaluation of personality\ndefinition and measurement in LLMs."}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2fa444ed-af02-49df-9eff-dc61c8867363", "node_type": "4", "metadata": {"page_label": "10", "file_name": "2402_14805v1.pdf", "Title of this paper": "Identifying Multiple Personalities in Large Language Models with External Evaluation", "Authors": "Xiaoyang Song, Yuta Adachi, Jessie Feng, Mouwei Lin, Linhao Yu, Frank Li, Akshat Gupta, Gopala Anumanchipalli, Simerjot Kaur", "Date published": "02/22/2024", "URL": "http://arxiv.org/abs/2402.14805v1", "summary": "As Large Language Models (LLMs) are integrated with human daily applications\nrapidly, many societal and ethical concerns are raised regarding the behavior\nof LLMs. One of the ways to comprehend LLMs' behavior is to analyze their\npersonalities. Many recent studies quantify LLMs' personalities using\nself-assessment tests that are created for humans. Yet many critiques question\nthe applicability and reliability of these self-assessment tests when applied\nto LLMs. In this paper, we investigate LLM personalities using an alternate\npersonality measurement method, which we refer to as the external evaluation\nmethod, where instead of prompting LLMs with multiple-choice questions in the\nLikert scale, we evaluate LLMs' personalities by analyzing their responses\ntoward open-ended situational questions using an external machine learning\nmodel. We first fine-tuned a Llama2-7B model as the MBTI personality predictor\nthat outperforms the state-of-the-art models as the tool to analyze LLMs'\nresponses. Then, we prompt the LLMs with situational questions and ask them to\ngenerate Twitter posts and comments, respectively, in order to assess their\npersonalities when playing two different roles. Using the external personality\nevaluation method, we identify that the obtained personality types for LLMs are\nsignificantly different when generating posts versus comments, whereas humans\nshow a consistent personality profile in these two different situations. This\nshows that LLMs can exhibit different personalities based on different\nscenarios, thus highlighting a fundamental difference between personality in\nLLMs and humans. With our work, we call for a re-evaluation of personality\ndefinition and measurement in LLMs."}, "hash": "c9b214274aca084dc81d5e129b36706f708cd52525a6c93418eedeccb1cf33bc", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3b5e8528-454d-4786-b8e7-af7fcc8ea72d", "node_type": "1", "metadata": {"page_label": "10", "file_name": "2402_14805v1.pdf", "Title of this paper": "Identifying Multiple Personalities in Large Language Models with External Evaluation", "Authors": "Xiaoyang Song, Yuta Adachi, Jessie Feng, Mouwei Lin, Linhao Yu, Frank Li, Akshat Gupta, Gopala Anumanchipalli, Simerjot Kaur", "Date published": "02/22/2024", "URL": "http://arxiv.org/abs/2402.14805v1", "summary": "As Large Language Models (LLMs) are integrated with human daily applications\nrapidly, many societal and ethical concerns are raised regarding the behavior\nof LLMs. One of the ways to comprehend LLMs' behavior is to analyze their\npersonalities. Many recent studies quantify LLMs' personalities using\nself-assessment tests that are created for humans. Yet many critiques question\nthe applicability and reliability of these self-assessment tests when applied\nto LLMs. In this paper, we investigate LLM personalities using an alternate\npersonality measurement method, which we refer to as the external evaluation\nmethod, where instead of prompting LLMs with multiple-choice questions in the\nLikert scale, we evaluate LLMs' personalities by analyzing their responses\ntoward open-ended situational questions using an external machine learning\nmodel. We first fine-tuned a Llama2-7B model as the MBTI personality predictor\nthat outperforms the state-of-the-art models as the tool to analyze LLMs'\nresponses. Then, we prompt the LLMs with situational questions and ask them to\ngenerate Twitter posts and comments, respectively, in order to assess their\npersonalities when playing two different roles. Using the external personality\nevaluation method, we identify that the obtained personality types for LLMs are\nsignificantly different when generating posts versus comments, whereas humans\nshow a consistent personality profile in these two different situations. This\nshows that LLMs can exhibit different personalities based on different\nscenarios, thus highlighting a fundamental difference between personality in\nLLMs and humans. With our work, we call for a re-evaluation of personality\ndefinition and measurement in LLMs."}, "hash": "b9503ea6e1c38cf520eda531c75d3c4306b516e92069891f343ac63e54b28884", "class_name": "RelatedNodeInfo"}}, "text": "2023. Have large lan-\nguage models developed a personality?: Applicabil-\nity of self-assessment tests in measuring personality\nin llms. arXiv preprint arXiv:2305.14693 .Qirui Tang, Wenkang Jiang, Yihua Du, and Lei Lin.\n2023. An attention-based denoising framework for\npersonality detection in social media texts. arXiv\npreprint arXiv:2311.09945 .\nHugo Touvron, Louis Martin, Kevin Stone, Peter Al-\nbert, Amjad Almahairi, Yasmine Babaei, Nikolay\nBashlykov, Soumya Batra, Prajjwal Bhargava, Shruti\nBhosale, et al. 2023a. Llama 2: Open founda-\ntion and fine-tuned chat models. arXiv preprint\narXiv:2307.09288 .\nHugo Touvron, Louis Martin, Kevin Stone, Peter Al-\nbert, Amjad Almahairi, Yasmine Babaei, Nikolay\nBashlykov, Soumya Batra, Prajjwal Bhargava, Shruti\nBhosale, et al. 2023b. Llama 2: Open founda-\ntion and fine-tuned chat models. arXiv preprint\narXiv:2307.09288 .\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou,\net al. 2022. Chain-of-thought prompting elicits rea-\nsoning in large language models. Advances in Neural\nInformation Processing Systems , 35:24824\u201324837.\nT. Yang, J. Deng, X. Quan, and Q. Wang. 2023. Orders\nare unwanted: Dynamic deep graph convolutional\nnetwork for personality detection. In Proceedings\nof the AAAI Conference on Artificial Intelligence ,\nvolume 37, pages 13896\u201313904.\nSusan Zhang, Stephen Roller, Naman Goyal, Mikel\nArtetxe, Moya Chen, Shuohui Chen, Christopher De-\nwan, Mona Diab, Xian Li, Xi Victoria Lin, et al. 2022.\nOpt: Open pre-trained transformer language models.\narXiv preprint arXiv:2205.01068 .", "mimetype": "text/plain", "start_char_idx": 2172, "end_char_idx": 3760, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9d8a0017-36bd-43c3-9c38-19033e41956a": {"__data__": {"id_": "9d8a0017-36bd-43c3-9c38-19033e41956a", "embedding": null, "metadata": {"page_label": "11", "file_name": "2402_14805v1.pdf", "Title of this paper": "Identifying Multiple Personalities in Large Language Models with External Evaluation", "Authors": "Xiaoyang Song, Yuta Adachi, Jessie Feng, Mouwei Lin, Linhao Yu, Frank Li, Akshat Gupta, Gopala Anumanchipalli, Simerjot Kaur", "Date published": "02/22/2024", "URL": "http://arxiv.org/abs/2402.14805v1", "summary": "As Large Language Models (LLMs) are integrated with human daily applications\nrapidly, many societal and ethical concerns are raised regarding the behavior\nof LLMs. One of the ways to comprehend LLMs' behavior is to analyze their\npersonalities. Many recent studies quantify LLMs' personalities using\nself-assessment tests that are created for humans. Yet many critiques question\nthe applicability and reliability of these self-assessment tests when applied\nto LLMs. In this paper, we investigate LLM personalities using an alternate\npersonality measurement method, which we refer to as the external evaluation\nmethod, where instead of prompting LLMs with multiple-choice questions in the\nLikert scale, we evaluate LLMs' personalities by analyzing their responses\ntoward open-ended situational questions using an external machine learning\nmodel. We first fine-tuned a Llama2-7B model as the MBTI personality predictor\nthat outperforms the state-of-the-art models as the tool to analyze LLMs'\nresponses. Then, we prompt the LLMs with situational questions and ask them to\ngenerate Twitter posts and comments, respectively, in order to assess their\npersonalities when playing two different roles. Using the external personality\nevaluation method, we identify that the obtained personality types for LLMs are\nsignificantly different when generating posts versus comments, whereas humans\nshow a consistent personality profile in these two different situations. This\nshows that LLMs can exhibit different personalities based on different\nscenarios, thus highlighting a fundamental difference between personality in\nLLMs and humans. With our work, we call for a re-evaluation of personality\ndefinition and measurement in LLMs."}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e6068ed1-1725-4b6b-bbab-094aaf2467f9", "node_type": "4", "metadata": {"page_label": "11", "file_name": "2402_14805v1.pdf", "Title of this paper": "Identifying Multiple Personalities in Large Language Models with External Evaluation", "Authors": "Xiaoyang Song, Yuta Adachi, Jessie Feng, Mouwei Lin, Linhao Yu, Frank Li, Akshat Gupta, Gopala Anumanchipalli, Simerjot Kaur", "Date published": "02/22/2024", "URL": "http://arxiv.org/abs/2402.14805v1", "summary": "As Large Language Models (LLMs) are integrated with human daily applications\nrapidly, many societal and ethical concerns are raised regarding the behavior\nof LLMs. One of the ways to comprehend LLMs' behavior is to analyze their\npersonalities. Many recent studies quantify LLMs' personalities using\nself-assessment tests that are created for humans. Yet many critiques question\nthe applicability and reliability of these self-assessment tests when applied\nto LLMs. In this paper, we investigate LLM personalities using an alternate\npersonality measurement method, which we refer to as the external evaluation\nmethod, where instead of prompting LLMs with multiple-choice questions in the\nLikert scale, we evaluate LLMs' personalities by analyzing their responses\ntoward open-ended situational questions using an external machine learning\nmodel. We first fine-tuned a Llama2-7B model as the MBTI personality predictor\nthat outperforms the state-of-the-art models as the tool to analyze LLMs'\nresponses. Then, we prompt the LLMs with situational questions and ask them to\ngenerate Twitter posts and comments, respectively, in order to assess their\npersonalities when playing two different roles. Using the external personality\nevaluation method, we identify that the obtained personality types for LLMs are\nsignificantly different when generating posts versus comments, whereas humans\nshow a consistent personality profile in these two different situations. This\nshows that LLMs can exhibit different personalities based on different\nscenarios, thus highlighting a fundamental difference between personality in\nLLMs and humans. With our work, we call for a re-evaluation of personality\ndefinition and measurement in LLMs."}, "hash": "21e3dd20d861e61f065738ae5342820fe8947bc23a6bbec364f1efbb8f56a9ce", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3c58ba9a-e2d2-49b9-807d-142f2f774a68", "node_type": "1", "metadata": {}, "hash": "63d7d45c0d5ec42dc67064dcb42cfdf153d30d64e94bbb82e1f26edcca32ec69", "class_name": "RelatedNodeInfo"}}, "text": "A Appendix\nA.1 Kaggle MBTI Dataset\nThe public Kaggle MBTI dataset is comprised of\n8675 entries, each corresponding to an individual\u2019s\nMBTI type. In particular, every data entry includes\nexcerpts from the last 50 posts or comments made\nby the individual on the PersonalityCafe forum,\nseparated by the special character \u201c |||\u201d. Two exam-\nple data entries are provided in Table 8. Note that\nthis dataset is anonymous upon creation.\nA.2 Computational Resources\nIn this work, all the experiments are done with\nNvidia RTX A6000 GPU. In particular, we hosted\nLlama2-7B, Llama2-13B, and Llama2-70B on 1, 2,\nand 4 GPU cards, respectively, where Llama2-70B\nwas hosted in mixed precisions. For the ChatGPT\nexperiment, we directly use OpenAI APIs5.\nA.3 Finetuning Experiments Details\nIn this section, we report the experimental details\nand complete results for both binary and 16-class\nmodels to reproduce Table 1. For baseline models,\nas the models are trained on the same dataset, we\nkindly refer the audience to the original papers for\nthe performance of their models (Yang et al., 2023;\nMehta et al., 2020; Tang et al., 2023). In partic-\nular, in the fine-tuning experiments, the original\nMBTI dataset is divided into training, evaluation,\nand heldout testing subsets in an 81:9:10 ratio. Fur-\nthermore, for the binary model, the model for each\ntrait shared the same set of hyperparameters.\nMetrics E/I N/S T/F P/J Average\nAccuracy ( %)94.01 96.08 92.97 89.98 93.26\nF1 (%) 90.57 91.48 92.91 89.38 91.09\nPrecision ( %)93.16 93.44 93.13 89.41 92.29\nRecall ( %) 88.57 89.76 92.78 89.36 90.10\nTable 6: Performance of personality detection models\non each of the four MBTI dimensions for the binary\nmodel.\nA.4 Prompting Pipeline & Downloaded Topics\nIn principle, we want to feed LLMs with real-world\ntopics, and tweets and collect their responses and\nattitudes toward them. We analyzed daily news\nto obtain a list of real-world topics.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1921, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3c58ba9a-e2d2-49b9-807d-142f2f774a68": {"__data__": {"id_": "3c58ba9a-e2d2-49b9-807d-142f2f774a68", "embedding": null, "metadata": {"page_label": "11", "file_name": "2402_14805v1.pdf", "Title of this paper": "Identifying Multiple Personalities in Large Language Models with External Evaluation", "Authors": "Xiaoyang Song, Yuta Adachi, Jessie Feng, Mouwei Lin, Linhao Yu, Frank Li, Akshat Gupta, Gopala Anumanchipalli, Simerjot Kaur", "Date published": "02/22/2024", "URL": "http://arxiv.org/abs/2402.14805v1", "summary": "As Large Language Models (LLMs) are integrated with human daily applications\nrapidly, many societal and ethical concerns are raised regarding the behavior\nof LLMs. One of the ways to comprehend LLMs' behavior is to analyze their\npersonalities. Many recent studies quantify LLMs' personalities using\nself-assessment tests that are created for humans. Yet many critiques question\nthe applicability and reliability of these self-assessment tests when applied\nto LLMs. In this paper, we investigate LLM personalities using an alternate\npersonality measurement method, which we refer to as the external evaluation\nmethod, where instead of prompting LLMs with multiple-choice questions in the\nLikert scale, we evaluate LLMs' personalities by analyzing their responses\ntoward open-ended situational questions using an external machine learning\nmodel. We first fine-tuned a Llama2-7B model as the MBTI personality predictor\nthat outperforms the state-of-the-art models as the tool to analyze LLMs'\nresponses. Then, we prompt the LLMs with situational questions and ask them to\ngenerate Twitter posts and comments, respectively, in order to assess their\npersonalities when playing two different roles. Using the external personality\nevaluation method, we identify that the obtained personality types for LLMs are\nsignificantly different when generating posts versus comments, whereas humans\nshow a consistent personality profile in these two different situations. This\nshows that LLMs can exhibit different personalities based on different\nscenarios, thus highlighting a fundamental difference between personality in\nLLMs and humans. With our work, we call for a re-evaluation of personality\ndefinition and measurement in LLMs."}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e6068ed1-1725-4b6b-bbab-094aaf2467f9", "node_type": "4", "metadata": {"page_label": "11", "file_name": "2402_14805v1.pdf", "Title of this paper": "Identifying Multiple Personalities in Large Language Models with External Evaluation", "Authors": "Xiaoyang Song, Yuta Adachi, Jessie Feng, Mouwei Lin, Linhao Yu, Frank Li, Akshat Gupta, Gopala Anumanchipalli, Simerjot Kaur", "Date published": "02/22/2024", "URL": "http://arxiv.org/abs/2402.14805v1", "summary": "As Large Language Models (LLMs) are integrated with human daily applications\nrapidly, many societal and ethical concerns are raised regarding the behavior\nof LLMs. One of the ways to comprehend LLMs' behavior is to analyze their\npersonalities. Many recent studies quantify LLMs' personalities using\nself-assessment tests that are created for humans. Yet many critiques question\nthe applicability and reliability of these self-assessment tests when applied\nto LLMs. In this paper, we investigate LLM personalities using an alternate\npersonality measurement method, which we refer to as the external evaluation\nmethod, where instead of prompting LLMs with multiple-choice questions in the\nLikert scale, we evaluate LLMs' personalities by analyzing their responses\ntoward open-ended situational questions using an external machine learning\nmodel. We first fine-tuned a Llama2-7B model as the MBTI personality predictor\nthat outperforms the state-of-the-art models as the tool to analyze LLMs'\nresponses. Then, we prompt the LLMs with situational questions and ask them to\ngenerate Twitter posts and comments, respectively, in order to assess their\npersonalities when playing two different roles. Using the external personality\nevaluation method, we identify that the obtained personality types for LLMs are\nsignificantly different when generating posts versus comments, whereas humans\nshow a consistent personality profile in these two different situations. This\nshows that LLMs can exhibit different personalities based on different\nscenarios, thus highlighting a fundamental difference between personality in\nLLMs and humans. With our work, we call for a re-evaluation of personality\ndefinition and measurement in LLMs."}, "hash": "21e3dd20d861e61f065738ae5342820fe8947bc23a6bbec364f1efbb8f56a9ce", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9d8a0017-36bd-43c3-9c38-19033e41956a", "node_type": "1", "metadata": {"page_label": "11", "file_name": "2402_14805v1.pdf", "Title of this paper": "Identifying Multiple Personalities in Large Language Models with External Evaluation", "Authors": "Xiaoyang Song, Yuta Adachi, Jessie Feng, Mouwei Lin, Linhao Yu, Frank Li, Akshat Gupta, Gopala Anumanchipalli, Simerjot Kaur", "Date published": "02/22/2024", "URL": "http://arxiv.org/abs/2402.14805v1", "summary": "As Large Language Models (LLMs) are integrated with human daily applications\nrapidly, many societal and ethical concerns are raised regarding the behavior\nof LLMs. One of the ways to comprehend LLMs' behavior is to analyze their\npersonalities. Many recent studies quantify LLMs' personalities using\nself-assessment tests that are created for humans. Yet many critiques question\nthe applicability and reliability of these self-assessment tests when applied\nto LLMs. In this paper, we investigate LLM personalities using an alternate\npersonality measurement method, which we refer to as the external evaluation\nmethod, where instead of prompting LLMs with multiple-choice questions in the\nLikert scale, we evaluate LLMs' personalities by analyzing their responses\ntoward open-ended situational questions using an external machine learning\nmodel. We first fine-tuned a Llama2-7B model as the MBTI personality predictor\nthat outperforms the state-of-the-art models as the tool to analyze LLMs'\nresponses. Then, we prompt the LLMs with situational questions and ask them to\ngenerate Twitter posts and comments, respectively, in order to assess their\npersonalities when playing two different roles. Using the external personality\nevaluation method, we identify that the obtained personality types for LLMs are\nsignificantly different when generating posts versus comments, whereas humans\nshow a consistent personality profile in these two different situations. This\nshows that LLMs can exhibit different personalities based on different\nscenarios, thus highlighting a fundamental difference between personality in\nLLMs and humans. With our work, we call for a re-evaluation of personality\ndefinition and measurement in LLMs."}, "hash": "45903cbb3b2461740a720fc36dd91299e75d8d8b99b8fe925bcc4899e813ac14", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "cad614db-d4a4-4874-911c-c88c3728e5aa", "node_type": "1", "metadata": {}, "hash": "2e9d374595831400841e713e074e5a740a1853ec9eb44bff63626abf4670114f", "class_name": "RelatedNodeInfo"}}, "text": "Metrics E/I N/S T/F P/J Average\nAccuracy ( %)94.01 96.08 92.97 89.98 93.26\nF1 (%) 90.57 91.48 92.91 89.38 91.09\nPrecision ( %)93.16 93.44 93.13 89.41 92.29\nRecall ( %) 88.57 89.76 92.78 89.36 90.10\nTable 6: Performance of personality detection models\non each of the four MBTI dimensions for the binary\nmodel.\nA.4 Prompting Pipeline & Downloaded Topics\nIn principle, we want to feed LLMs with real-world\ntopics, and tweets and collect their responses and\nattitudes toward them. We analyzed daily news\nto obtain a list of real-world topics. In terms of\nthe topics for tweets, we downloaded 5000 tweets\ncontaining the following 10 different topics: Bit-\ncoin, NFL, Music, Oscars, Travel, Fashion, Food,\n5https://platform.openai.com/Metrics E/I N/S T/F P/J Average\nAccuracy ( %)93.09 96.31 93.55 90.90 93.46\nF1 (%) 89.49 92.10 93.50 90.29 91.35\nPrecision ( %)90.33 93.37 93.65 90.64 92.00\nRecall ( %) 88.71 90.93 93.40 89.99 90.76\nTable 7: Performance of personality detection models\non each of the four MBTI dimensions for the 16-class\nmodel. Note that the numbers are calculated by extract-\ning each dimension from the predictions.\nFitness, Gaming, and Technology, where they are\nnearly evenly distributed.\nThe templates that we used to generate Twitter\nposts and comments are provided in Table 3. As for\nthe parameters for generation, we utilized a nucleus\nsampling (Holtzman et al., 2019) with the temper-\nature and top_p set to 0.2 and 0.95, respectively.\nIn addition, the maximum length of generated con-\ntents is set to 200. The same configuration is used\nfor all LLMs that we investigated.\nA.5 Validation Experiment on Human\nCounterpart\nIn the validation experiment, we downloaded the\npublic Twitter posts and comments made by 8\ncelebrities from different domains and use the same\npersonality evaluation procedure as what we have\ndone for LLMs.", "mimetype": "text/plain", "start_char_idx": 1383, "end_char_idx": 3231, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "cad614db-d4a4-4874-911c-c88c3728e5aa": {"__data__": {"id_": "cad614db-d4a4-4874-911c-c88c3728e5aa", "embedding": null, "metadata": {"page_label": "11", "file_name": "2402_14805v1.pdf", "Title of this paper": "Identifying Multiple Personalities in Large Language Models with External Evaluation", "Authors": "Xiaoyang Song, Yuta Adachi, Jessie Feng, Mouwei Lin, Linhao Yu, Frank Li, Akshat Gupta, Gopala Anumanchipalli, Simerjot Kaur", "Date published": "02/22/2024", "URL": "http://arxiv.org/abs/2402.14805v1", "summary": "As Large Language Models (LLMs) are integrated with human daily applications\nrapidly, many societal and ethical concerns are raised regarding the behavior\nof LLMs. One of the ways to comprehend LLMs' behavior is to analyze their\npersonalities. Many recent studies quantify LLMs' personalities using\nself-assessment tests that are created for humans. Yet many critiques question\nthe applicability and reliability of these self-assessment tests when applied\nto LLMs. In this paper, we investigate LLM personalities using an alternate\npersonality measurement method, which we refer to as the external evaluation\nmethod, where instead of prompting LLMs with multiple-choice questions in the\nLikert scale, we evaluate LLMs' personalities by analyzing their responses\ntoward open-ended situational questions using an external machine learning\nmodel. We first fine-tuned a Llama2-7B model as the MBTI personality predictor\nthat outperforms the state-of-the-art models as the tool to analyze LLMs'\nresponses. Then, we prompt the LLMs with situational questions and ask them to\ngenerate Twitter posts and comments, respectively, in order to assess their\npersonalities when playing two different roles. Using the external personality\nevaluation method, we identify that the obtained personality types for LLMs are\nsignificantly different when generating posts versus comments, whereas humans\nshow a consistent personality profile in these two different situations. This\nshows that LLMs can exhibit different personalities based on different\nscenarios, thus highlighting a fundamental difference between personality in\nLLMs and humans. With our work, we call for a re-evaluation of personality\ndefinition and measurement in LLMs."}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e6068ed1-1725-4b6b-bbab-094aaf2467f9", "node_type": "4", "metadata": {"page_label": "11", "file_name": "2402_14805v1.pdf", "Title of this paper": "Identifying Multiple Personalities in Large Language Models with External Evaluation", "Authors": "Xiaoyang Song, Yuta Adachi, Jessie Feng, Mouwei Lin, Linhao Yu, Frank Li, Akshat Gupta, Gopala Anumanchipalli, Simerjot Kaur", "Date published": "02/22/2024", "URL": "http://arxiv.org/abs/2402.14805v1", "summary": "As Large Language Models (LLMs) are integrated with human daily applications\nrapidly, many societal and ethical concerns are raised regarding the behavior\nof LLMs. One of the ways to comprehend LLMs' behavior is to analyze their\npersonalities. Many recent studies quantify LLMs' personalities using\nself-assessment tests that are created for humans. Yet many critiques question\nthe applicability and reliability of these self-assessment tests when applied\nto LLMs. In this paper, we investigate LLM personalities using an alternate\npersonality measurement method, which we refer to as the external evaluation\nmethod, where instead of prompting LLMs with multiple-choice questions in the\nLikert scale, we evaluate LLMs' personalities by analyzing their responses\ntoward open-ended situational questions using an external machine learning\nmodel. We first fine-tuned a Llama2-7B model as the MBTI personality predictor\nthat outperforms the state-of-the-art models as the tool to analyze LLMs'\nresponses. Then, we prompt the LLMs with situational questions and ask them to\ngenerate Twitter posts and comments, respectively, in order to assess their\npersonalities when playing two different roles. Using the external personality\nevaluation method, we identify that the obtained personality types for LLMs are\nsignificantly different when generating posts versus comments, whereas humans\nshow a consistent personality profile in these two different situations. This\nshows that LLMs can exhibit different personalities based on different\nscenarios, thus highlighting a fundamental difference between personality in\nLLMs and humans. With our work, we call for a re-evaluation of personality\ndefinition and measurement in LLMs."}, "hash": "21e3dd20d861e61f065738ae5342820fe8947bc23a6bbec364f1efbb8f56a9ce", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3c58ba9a-e2d2-49b9-807d-142f2f774a68", "node_type": "1", "metadata": {"page_label": "11", "file_name": "2402_14805v1.pdf", "Title of this paper": "Identifying Multiple Personalities in Large Language Models with External Evaluation", "Authors": "Xiaoyang Song, Yuta Adachi, Jessie Feng, Mouwei Lin, Linhao Yu, Frank Li, Akshat Gupta, Gopala Anumanchipalli, Simerjot Kaur", "Date published": "02/22/2024", "URL": "http://arxiv.org/abs/2402.14805v1", "summary": "As Large Language Models (LLMs) are integrated with human daily applications\nrapidly, many societal and ethical concerns are raised regarding the behavior\nof LLMs. One of the ways to comprehend LLMs' behavior is to analyze their\npersonalities. Many recent studies quantify LLMs' personalities using\nself-assessment tests that are created for humans. Yet many critiques question\nthe applicability and reliability of these self-assessment tests when applied\nto LLMs. In this paper, we investigate LLM personalities using an alternate\npersonality measurement method, which we refer to as the external evaluation\nmethod, where instead of prompting LLMs with multiple-choice questions in the\nLikert scale, we evaluate LLMs' personalities by analyzing their responses\ntoward open-ended situational questions using an external machine learning\nmodel. We first fine-tuned a Llama2-7B model as the MBTI personality predictor\nthat outperforms the state-of-the-art models as the tool to analyze LLMs'\nresponses. Then, we prompt the LLMs with situational questions and ask them to\ngenerate Twitter posts and comments, respectively, in order to assess their\npersonalities when playing two different roles. Using the external personality\nevaluation method, we identify that the obtained personality types for LLMs are\nsignificantly different when generating posts versus comments, whereas humans\nshow a consistent personality profile in these two different situations. This\nshows that LLMs can exhibit different personalities based on different\nscenarios, thus highlighting a fundamental difference between personality in\nLLMs and humans. With our work, we call for a re-evaluation of personality\ndefinition and measurement in LLMs."}, "hash": "3c35c35ced3c22250a98f7c07a265bc8b9678f770cff8b39d1bffe320c6c97d1", "class_name": "RelatedNodeInfo"}}, "text": "Note that the numbers are calculated by extract-\ning each dimension from the predictions.\nFitness, Gaming, and Technology, where they are\nnearly evenly distributed.\nThe templates that we used to generate Twitter\nposts and comments are provided in Table 3. As for\nthe parameters for generation, we utilized a nucleus\nsampling (Holtzman et al., 2019) with the temper-\nature and top_p set to 0.2 and 0.95, respectively.\nIn addition, the maximum length of generated con-\ntents is set to 200. The same configuration is used\nfor all LLMs that we investigated.\nA.5 Validation Experiment on Human\nCounterpart\nIn the validation experiment, we downloaded the\npublic Twitter posts and comments made by 8\ncelebrities from different domains and use the same\npersonality evaluation procedure as what we have\ndone for LLMs. Specifically, we created 100 sam-\nples with 50 tweets in each for every celebrity and\nrepeated the external evaluation method for 100\ntimes. The obtained MBTI distributions and the\nmost frequent MBTI types are reported in Figure 5\nand Table 5, respectively, where the results echo the\nfact in psychology literature where personality is a\nconsistent characteristic for humans (Association,\n2023; Green et al., 2019).", "mimetype": "text/plain", "start_char_idx": 2423, "end_char_idx": 3647, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "be34ed48-0337-462e-9a7f-7dc2cbe1ecc1": {"__data__": {"id_": "be34ed48-0337-462e-9a7f-7dc2cbe1ecc1", "embedding": null, "metadata": {"page_label": "12", "file_name": "2402_14805v1.pdf", "Title of this paper": "Identifying Multiple Personalities in Large Language Models with External Evaluation", "Authors": "Xiaoyang Song, Yuta Adachi, Jessie Feng, Mouwei Lin, Linhao Yu, Frank Li, Akshat Gupta, Gopala Anumanchipalli, Simerjot Kaur", "Date published": "02/22/2024", "URL": "http://arxiv.org/abs/2402.14805v1", "summary": "As Large Language Models (LLMs) are integrated with human daily applications\nrapidly, many societal and ethical concerns are raised regarding the behavior\nof LLMs. One of the ways to comprehend LLMs' behavior is to analyze their\npersonalities. Many recent studies quantify LLMs' personalities using\nself-assessment tests that are created for humans. Yet many critiques question\nthe applicability and reliability of these self-assessment tests when applied\nto LLMs. In this paper, we investigate LLM personalities using an alternate\npersonality measurement method, which we refer to as the external evaluation\nmethod, where instead of prompting LLMs with multiple-choice questions in the\nLikert scale, we evaluate LLMs' personalities by analyzing their responses\ntoward open-ended situational questions using an external machine learning\nmodel. We first fine-tuned a Llama2-7B model as the MBTI personality predictor\nthat outperforms the state-of-the-art models as the tool to analyze LLMs'\nresponses. Then, we prompt the LLMs with situational questions and ask them to\ngenerate Twitter posts and comments, respectively, in order to assess their\npersonalities when playing two different roles. Using the external personality\nevaluation method, we identify that the obtained personality types for LLMs are\nsignificantly different when generating posts versus comments, whereas humans\nshow a consistent personality profile in these two different situations. This\nshows that LLMs can exhibit different personalities based on different\nscenarios, thus highlighting a fundamental difference between personality in\nLLMs and humans. With our work, we call for a re-evaluation of personality\ndefinition and measurement in LLMs."}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7861777b-c4f4-4694-9c79-d3c0f7650736", "node_type": "4", "metadata": {"page_label": "12", "file_name": "2402_14805v1.pdf", "Title of this paper": "Identifying Multiple Personalities in Large Language Models with External Evaluation", "Authors": "Xiaoyang Song, Yuta Adachi, Jessie Feng, Mouwei Lin, Linhao Yu, Frank Li, Akshat Gupta, Gopala Anumanchipalli, Simerjot Kaur", "Date published": "02/22/2024", "URL": "http://arxiv.org/abs/2402.14805v1", "summary": "As Large Language Models (LLMs) are integrated with human daily applications\nrapidly, many societal and ethical concerns are raised regarding the behavior\nof LLMs. One of the ways to comprehend LLMs' behavior is to analyze their\npersonalities. Many recent studies quantify LLMs' personalities using\nself-assessment tests that are created for humans. Yet many critiques question\nthe applicability and reliability of these self-assessment tests when applied\nto LLMs. In this paper, we investigate LLM personalities using an alternate\npersonality measurement method, which we refer to as the external evaluation\nmethod, where instead of prompting LLMs with multiple-choice questions in the\nLikert scale, we evaluate LLMs' personalities by analyzing their responses\ntoward open-ended situational questions using an external machine learning\nmodel. We first fine-tuned a Llama2-7B model as the MBTI personality predictor\nthat outperforms the state-of-the-art models as the tool to analyze LLMs'\nresponses. Then, we prompt the LLMs with situational questions and ask them to\ngenerate Twitter posts and comments, respectively, in order to assess their\npersonalities when playing two different roles. Using the external personality\nevaluation method, we identify that the obtained personality types for LLMs are\nsignificantly different when generating posts versus comments, whereas humans\nshow a consistent personality profile in these two different situations. This\nshows that LLMs can exhibit different personalities based on different\nscenarios, thus highlighting a fundamental difference between personality in\nLLMs and humans. With our work, we call for a re-evaluation of personality\ndefinition and measurement in LLMs."}, "hash": "d3137332546ec0b2e7263c7d33b87eb8739f8280c1af7fe38c4a4b31178f8f79", "class_name": "RelatedNodeInfo"}}, "text": "Type Contents (50 posts or comments on PersonalityCafe)\nENTP I am finding the lack of me in these posts very alarming |||This + Lack of Balance and\nHand-Eye Coordination. |||...\nINTJ 2% still means about 1/50 people. I\u2019ve probably seen 1-2 others today. I never understood\nfascination by virtue of rarity. |||I collect shoes. I do so because I like status and nothing\ncommunicates such a thing as much as a pair of Jordans...\n... ...\nTable 8: Two example data entries of the public Kaggle MBTI dataset.\nFigure 5: MBTI distribution of 8 selected celebrities using external evaluation method for 100 times. In the figure,\nthe first row is the assessment results on the generated posts dataset (P), whereas the second row provides the results\non the comments dataset (C). In addition, personality types that appear less than 3 times are merged together to\nform the class \u201cOthers\u201d.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 877, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}}, "docstore/metadata": {"d1b04198-8344-4e96-a8de-ac36bfd57af4": {"doc_hash": "8184d799c5a08651905c3f385b39854114bf8cbd86bca8aeacc9c4568e78f155", "ref_doc_id": "5e945e96-2cdc-4682-ab02-08e1c62537bf"}, "f2cca384-44be-4f99-99a8-de40bc1126c4": {"doc_hash": "6937caacc58859fc20afac9fe614fc95093d28b6354c833928a8d045fcfef315", "ref_doc_id": "5e945e96-2cdc-4682-ab02-08e1c62537bf"}, "47c39e18-ad01-4c32-96bb-d5413b3db7f9": {"doc_hash": "489c869a51f397b329fed192df6c0a7b9765c0a1c62513d59499e45651d12809", "ref_doc_id": "5e945e96-2cdc-4682-ab02-08e1c62537bf"}, "9c7ee194-9c00-4156-beee-7cdd57f3312e": {"doc_hash": "e2556597549024d5e458a4966107bfd74920e918e1e16a7fa94d23ced91ffd8f", "ref_doc_id": "043a1305-ea8e-44d4-9f77-3845270e5cd3"}, "121c27af-170d-4706-a477-e8f5ec2cc0e0": {"doc_hash": "4cc8444bd40ba26f510b9d70271fff4c326327735b6a161d302a874b2b7e8571", "ref_doc_id": "043a1305-ea8e-44d4-9f77-3845270e5cd3"}, "3634128f-155d-4106-96d1-7a0912a718c8": {"doc_hash": "18e8e4ce40fcb90bcf488caa0123c27fc6c3f824ed33eaeb49bd682bb7065cbe", "ref_doc_id": "043a1305-ea8e-44d4-9f77-3845270e5cd3"}, "cf4715ba-0c73-457a-b65a-e510d84a8bda": {"doc_hash": "5cc27868970a22eaadb578416998eb3e6c2a7536691e94d3ad5b115778572f33", "ref_doc_id": "2f2864ad-50f3-4bae-b473-188cd4ecbfa9"}, "ce2f7c1c-8499-4235-8cc2-e2a7f7fa4423": {"doc_hash": "ccf1ae9088bb5632252999de79f878fd5ae1c148c8d6169474ea4f5e05396931", "ref_doc_id": "2f2864ad-50f3-4bae-b473-188cd4ecbfa9"}, "cd2d26e4-10ce-416e-afd9-ee9cbc4caa46": {"doc_hash": "0342b47d3293317a9d1131890eca937b64dc4888d2a0c0fba0ec36ed8fbcf5bc", "ref_doc_id": "2f2864ad-50f3-4bae-b473-188cd4ecbfa9"}, "e9a29975-4290-4479-9e27-c98f54ff967b": {"doc_hash": "56a0051bcc3fbf269de6ee17da8603a9c2cc295574ef8eb7162be95e4803175d", "ref_doc_id": "58c79f8e-1990-4b21-85e3-c55ea6499dcd"}, "94654449-ec2f-4bcf-af46-24079bc55b20": {"doc_hash": "a7fdbfb5e632b16fba4b97e48b00bb6bc5d49ae3e4baba2136930040818cbf51", "ref_doc_id": "58c79f8e-1990-4b21-85e3-c55ea6499dcd"}, "7bf4ec90-5c9e-4e64-9622-2a969949f78a": {"doc_hash": "d21aaf2060ef63bdd988a587c3d68a720a2df8c2431e796c241a1b74426078b7", "ref_doc_id": "58c79f8e-1990-4b21-85e3-c55ea6499dcd"}, "a9b90eb4-7783-49bf-9b6f-0d297a2508a9": {"doc_hash": "8529eea4dc9d1f2f7bbd4406e691797bfda6b369fb7237a9db7aa9fe2b184f99", "ref_doc_id": "58c79f8e-1990-4b21-85e3-c55ea6499dcd"}, "dfe827a6-1755-4688-bf93-82f12621f425": {"doc_hash": "0a3dede6e8ef27f8490a44aca8fa86d40e3c09ee99a7c5d61e65ee2b425f3b00", "ref_doc_id": "f4512416-7ea8-4ea2-af6c-c57824c81402"}, "698f76d2-1a6c-4e47-8f21-eebd90b9ccba": {"doc_hash": "2bfbb8df9d58ba5d2700ecaba40083a1d07e9b5d96cb1ba355f1357a9b6dffa5", "ref_doc_id": "f4512416-7ea8-4ea2-af6c-c57824c81402"}, "8902a363-6801-4368-99e7-7fe1250bb7a8": {"doc_hash": "6d2ac28d8823dc124c7e1231b64484b06d0a6543e5e9bc80c830488933684c6f", "ref_doc_id": "b7b21648-5a18-432b-9eea-6293512177c5"}, "71839d5a-e77e-43b6-8960-fc11ef95410d": {"doc_hash": "29b32eee764790c6709e235f0d9833e0a7a9ac038fad0c63eb3edb40fb60e3ed", "ref_doc_id": "b7b21648-5a18-432b-9eea-6293512177c5"}, "80bb5150-1f0f-4f9b-afab-43b379bb47ae": {"doc_hash": "1384139126fccd96d210a18e03074d3ee55049ffb2edfc8d8944a2a2dd3dbcd7", "ref_doc_id": "0460f752-0df5-44ba-8787-2b16ee7aaffc"}, "2079231c-828c-43d2-9bc3-771b971192b0": {"doc_hash": "87c6d3ec9e8bf493b9fb8407cf0f8b19533e7c22b8abf886d7acc32809a95b86", "ref_doc_id": "0460f752-0df5-44ba-8787-2b16ee7aaffc"}, "878da19e-0a9c-44f6-a8c5-dbfeb96c674a": {"doc_hash": "db3282d395d0843d9505f2a6975e7c2eee183860d69d16f548f9473160bde43f", "ref_doc_id": "a13cb1b0-c145-41ed-bcce-4e94a3791c8b"}, "fb1b99e3-1f08-48bd-9477-6334681d4ba9": {"doc_hash": "0fe87a2a98450753106963f9cd873884c8f502d248f9065f3407ed9a4a35ab2c", "ref_doc_id": "a13cb1b0-c145-41ed-bcce-4e94a3791c8b"}, "305b2676-5fc2-45ba-b702-41f269a5060f": {"doc_hash": "cfb9219dbd1570e29bf9a3a3f736fa21b503b600ce9c6d615843c11a076dc384", "ref_doc_id": "a13cb1b0-c145-41ed-bcce-4e94a3791c8b"}, "3d2ed0ae-a22b-4e08-b8c1-16f32813a27c": {"doc_hash": "cb90a28a897807c0fc4ede1e837313df569d04de2b21dc24567f264cdb141bc1", "ref_doc_id": "a14878fb-c323-4647-9473-016e9bc34f31"}, "d1a02ab2-de6c-4ab3-ae4d-57925690cbc4": {"doc_hash": "40fc30e2591ca0403e915b69c8510405c3ef918ae7357318724340e0fbffa46b", "ref_doc_id": "a14878fb-c323-4647-9473-016e9bc34f31"}, "a596f26c-17f7-4649-87fc-bd6c3c6c02eb": {"doc_hash": "9941f93b345e4e9dc486e718c0f6747afbdec1a926c40c68e48a1bc1bfec8d3c", "ref_doc_id": "a14878fb-c323-4647-9473-016e9bc34f31"}, "75c87058-4d99-44e3-8765-945d4cf0094d": {"doc_hash": "780caa2c751a66dd9747a76ec39dd29f2b21fd78e742f342d731973a3015c324", "ref_doc_id": "a14878fb-c323-4647-9473-016e9bc34f31"}, "52a55e88-21eb-43e1-96d2-7780dab7d444": {"doc_hash": "69cc4c02e2b75098159623c6522e418865fb5475c0cdba8de6fa8de331411921", "ref_doc_id": "2fa444ed-af02-49df-9eff-dc61c8867363"}, "3b5e8528-454d-4786-b8e7-af7fcc8ea72d": {"doc_hash": "b9503ea6e1c38cf520eda531c75d3c4306b516e92069891f343ac63e54b28884", "ref_doc_id": "2fa444ed-af02-49df-9eff-dc61c8867363"}, "fca6c576-985e-444b-b18b-ab4a8c8d44bf": {"doc_hash": "16e1276d4897b16252846a186f5cc60c9f5808f9681e3d437afdff34ee376659", "ref_doc_id": "2fa444ed-af02-49df-9eff-dc61c8867363"}, "9d8a0017-36bd-43c3-9c38-19033e41956a": {"doc_hash": "45903cbb3b2461740a720fc36dd91299e75d8d8b99b8fe925bcc4899e813ac14", "ref_doc_id": "e6068ed1-1725-4b6b-bbab-094aaf2467f9"}, "3c58ba9a-e2d2-49b9-807d-142f2f774a68": {"doc_hash": "3c35c35ced3c22250a98f7c07a265bc8b9678f770cff8b39d1bffe320c6c97d1", "ref_doc_id": "e6068ed1-1725-4b6b-bbab-094aaf2467f9"}, "cad614db-d4a4-4874-911c-c88c3728e5aa": {"doc_hash": "7a2061c826f250e94c3ef95af48b922b96dec70c829db7eccb7504c50a874c0c", "ref_doc_id": "e6068ed1-1725-4b6b-bbab-094aaf2467f9"}, "be34ed48-0337-462e-9a7f-7dc2cbe1ecc1": {"doc_hash": "d3137332546ec0b2e7263c7d33b87eb8739f8280c1af7fe38c4a4b31178f8f79", "ref_doc_id": "7861777b-c4f4-4694-9c79-d3c0f7650736"}}, "docstore/ref_doc_info": {"5e945e96-2cdc-4682-ab02-08e1c62537bf": {"node_ids": ["d1b04198-8344-4e96-a8de-ac36bfd57af4", "f2cca384-44be-4f99-99a8-de40bc1126c4", "47c39e18-ad01-4c32-96bb-d5413b3db7f9"], "metadata": {"page_label": "1", "file_name": "2402_14805v1.pdf", "Title of this paper": "Identifying Multiple Personalities in Large Language Models with External Evaluation", "Authors": "Xiaoyang Song, Yuta Adachi, Jessie Feng, Mouwei Lin, Linhao Yu, Frank Li, Akshat Gupta, Gopala Anumanchipalli, Simerjot Kaur", "Date published": "02/22/2024", "URL": "http://arxiv.org/abs/2402.14805v1", "summary": "As Large Language Models (LLMs) are integrated with human daily applications\nrapidly, many societal and ethical concerns are raised regarding the behavior\nof LLMs. One of the ways to comprehend LLMs' behavior is to analyze their\npersonalities. Many recent studies quantify LLMs' personalities using\nself-assessment tests that are created for humans. Yet many critiques question\nthe applicability and reliability of these self-assessment tests when applied\nto LLMs. In this paper, we investigate LLM personalities using an alternate\npersonality measurement method, which we refer to as the external evaluation\nmethod, where instead of prompting LLMs with multiple-choice questions in the\nLikert scale, we evaluate LLMs' personalities by analyzing their responses\ntoward open-ended situational questions using an external machine learning\nmodel. We first fine-tuned a Llama2-7B model as the MBTI personality predictor\nthat outperforms the state-of-the-art models as the tool to analyze LLMs'\nresponses. Then, we prompt the LLMs with situational questions and ask them to\ngenerate Twitter posts and comments, respectively, in order to assess their\npersonalities when playing two different roles. Using the external personality\nevaluation method, we identify that the obtained personality types for LLMs are\nsignificantly different when generating posts versus comments, whereas humans\nshow a consistent personality profile in these two different situations. This\nshows that LLMs can exhibit different personalities based on different\nscenarios, thus highlighting a fundamental difference between personality in\nLLMs and humans. With our work, we call for a re-evaluation of personality\ndefinition and measurement in LLMs."}}, "043a1305-ea8e-44d4-9f77-3845270e5cd3": {"node_ids": ["9c7ee194-9c00-4156-beee-7cdd57f3312e", "121c27af-170d-4706-a477-e8f5ec2cc0e0", "3634128f-155d-4106-96d1-7a0912a718c8"], "metadata": {"page_label": "2", "file_name": "2402_14805v1.pdf", "Title of this paper": "Identifying Multiple Personalities in Large Language Models with External Evaluation", "Authors": "Xiaoyang Song, Yuta Adachi, Jessie Feng, Mouwei Lin, Linhao Yu, Frank Li, Akshat Gupta, Gopala Anumanchipalli, Simerjot Kaur", "Date published": "02/22/2024", "URL": "http://arxiv.org/abs/2402.14805v1", "summary": "As Large Language Models (LLMs) are integrated with human daily applications\nrapidly, many societal and ethical concerns are raised regarding the behavior\nof LLMs. One of the ways to comprehend LLMs' behavior is to analyze their\npersonalities. Many recent studies quantify LLMs' personalities using\nself-assessment tests that are created for humans. Yet many critiques question\nthe applicability and reliability of these self-assessment tests when applied\nto LLMs. In this paper, we investigate LLM personalities using an alternate\npersonality measurement method, which we refer to as the external evaluation\nmethod, where instead of prompting LLMs with multiple-choice questions in the\nLikert scale, we evaluate LLMs' personalities by analyzing their responses\ntoward open-ended situational questions using an external machine learning\nmodel. We first fine-tuned a Llama2-7B model as the MBTI personality predictor\nthat outperforms the state-of-the-art models as the tool to analyze LLMs'\nresponses. Then, we prompt the LLMs with situational questions and ask them to\ngenerate Twitter posts and comments, respectively, in order to assess their\npersonalities when playing two different roles. Using the external personality\nevaluation method, we identify that the obtained personality types for LLMs are\nsignificantly different when generating posts versus comments, whereas humans\nshow a consistent personality profile in these two different situations. This\nshows that LLMs can exhibit different personalities based on different\nscenarios, thus highlighting a fundamental difference between personality in\nLLMs and humans. With our work, we call for a re-evaluation of personality\ndefinition and measurement in LLMs."}}, "2f2864ad-50f3-4bae-b473-188cd4ecbfa9": {"node_ids": ["cf4715ba-0c73-457a-b65a-e510d84a8bda", "ce2f7c1c-8499-4235-8cc2-e2a7f7fa4423", "cd2d26e4-10ce-416e-afd9-ee9cbc4caa46"], "metadata": {"page_label": "3", "file_name": "2402_14805v1.pdf", "Title of this paper": "Identifying Multiple Personalities in Large Language Models with External Evaluation", "Authors": "Xiaoyang Song, Yuta Adachi, Jessie Feng, Mouwei Lin, Linhao Yu, Frank Li, Akshat Gupta, Gopala Anumanchipalli, Simerjot Kaur", "Date published": "02/22/2024", "URL": "http://arxiv.org/abs/2402.14805v1", "summary": "As Large Language Models (LLMs) are integrated with human daily applications\nrapidly, many societal and ethical concerns are raised regarding the behavior\nof LLMs. One of the ways to comprehend LLMs' behavior is to analyze their\npersonalities. Many recent studies quantify LLMs' personalities using\nself-assessment tests that are created for humans. Yet many critiques question\nthe applicability and reliability of these self-assessment tests when applied\nto LLMs. In this paper, we investigate LLM personalities using an alternate\npersonality measurement method, which we refer to as the external evaluation\nmethod, where instead of prompting LLMs with multiple-choice questions in the\nLikert scale, we evaluate LLMs' personalities by analyzing their responses\ntoward open-ended situational questions using an external machine learning\nmodel. We first fine-tuned a Llama2-7B model as the MBTI personality predictor\nthat outperforms the state-of-the-art models as the tool to analyze LLMs'\nresponses. Then, we prompt the LLMs with situational questions and ask them to\ngenerate Twitter posts and comments, respectively, in order to assess their\npersonalities when playing two different roles. Using the external personality\nevaluation method, we identify that the obtained personality types for LLMs are\nsignificantly different when generating posts versus comments, whereas humans\nshow a consistent personality profile in these two different situations. This\nshows that LLMs can exhibit different personalities based on different\nscenarios, thus highlighting a fundamental difference between personality in\nLLMs and humans. With our work, we call for a re-evaluation of personality\ndefinition and measurement in LLMs."}}, "58c79f8e-1990-4b21-85e3-c55ea6499dcd": {"node_ids": ["e9a29975-4290-4479-9e27-c98f54ff967b", "94654449-ec2f-4bcf-af46-24079bc55b20", "7bf4ec90-5c9e-4e64-9622-2a969949f78a", "a9b90eb4-7783-49bf-9b6f-0d297a2508a9"], "metadata": {"page_label": "4", "file_name": "2402_14805v1.pdf", "Title of this paper": "Identifying Multiple Personalities in Large Language Models with External Evaluation", "Authors": "Xiaoyang Song, Yuta Adachi, Jessie Feng, Mouwei Lin, Linhao Yu, Frank Li, Akshat Gupta, Gopala Anumanchipalli, Simerjot Kaur", "Date published": "02/22/2024", "URL": "http://arxiv.org/abs/2402.14805v1", "summary": "As Large Language Models (LLMs) are integrated with human daily applications\nrapidly, many societal and ethical concerns are raised regarding the behavior\nof LLMs. One of the ways to comprehend LLMs' behavior is to analyze their\npersonalities. Many recent studies quantify LLMs' personalities using\nself-assessment tests that are created for humans. Yet many critiques question\nthe applicability and reliability of these self-assessment tests when applied\nto LLMs. In this paper, we investigate LLM personalities using an alternate\npersonality measurement method, which we refer to as the external evaluation\nmethod, where instead of prompting LLMs with multiple-choice questions in the\nLikert scale, we evaluate LLMs' personalities by analyzing their responses\ntoward open-ended situational questions using an external machine learning\nmodel. We first fine-tuned a Llama2-7B model as the MBTI personality predictor\nthat outperforms the state-of-the-art models as the tool to analyze LLMs'\nresponses. Then, we prompt the LLMs with situational questions and ask them to\ngenerate Twitter posts and comments, respectively, in order to assess their\npersonalities when playing two different roles. Using the external personality\nevaluation method, we identify that the obtained personality types for LLMs are\nsignificantly different when generating posts versus comments, whereas humans\nshow a consistent personality profile in these two different situations. This\nshows that LLMs can exhibit different personalities based on different\nscenarios, thus highlighting a fundamental difference between personality in\nLLMs and humans. With our work, we call for a re-evaluation of personality\ndefinition and measurement in LLMs."}}, "f4512416-7ea8-4ea2-af6c-c57824c81402": {"node_ids": ["dfe827a6-1755-4688-bf93-82f12621f425", "698f76d2-1a6c-4e47-8f21-eebd90b9ccba"], "metadata": {"page_label": "5", "file_name": "2402_14805v1.pdf", "Title of this paper": "Identifying Multiple Personalities in Large Language Models with External Evaluation", "Authors": "Xiaoyang Song, Yuta Adachi, Jessie Feng, Mouwei Lin, Linhao Yu, Frank Li, Akshat Gupta, Gopala Anumanchipalli, Simerjot Kaur", "Date published": "02/22/2024", "URL": "http://arxiv.org/abs/2402.14805v1", "summary": "As Large Language Models (LLMs) are integrated with human daily applications\nrapidly, many societal and ethical concerns are raised regarding the behavior\nof LLMs. One of the ways to comprehend LLMs' behavior is to analyze their\npersonalities. Many recent studies quantify LLMs' personalities using\nself-assessment tests that are created for humans. Yet many critiques question\nthe applicability and reliability of these self-assessment tests when applied\nto LLMs. In this paper, we investigate LLM personalities using an alternate\npersonality measurement method, which we refer to as the external evaluation\nmethod, where instead of prompting LLMs with multiple-choice questions in the\nLikert scale, we evaluate LLMs' personalities by analyzing their responses\ntoward open-ended situational questions using an external machine learning\nmodel. We first fine-tuned a Llama2-7B model as the MBTI personality predictor\nthat outperforms the state-of-the-art models as the tool to analyze LLMs'\nresponses. Then, we prompt the LLMs with situational questions and ask them to\ngenerate Twitter posts and comments, respectively, in order to assess their\npersonalities when playing two different roles. Using the external personality\nevaluation method, we identify that the obtained personality types for LLMs are\nsignificantly different when generating posts versus comments, whereas humans\nshow a consistent personality profile in these two different situations. This\nshows that LLMs can exhibit different personalities based on different\nscenarios, thus highlighting a fundamental difference between personality in\nLLMs and humans. With our work, we call for a re-evaluation of personality\ndefinition and measurement in LLMs."}}, "b7b21648-5a18-432b-9eea-6293512177c5": {"node_ids": ["8902a363-6801-4368-99e7-7fe1250bb7a8", "71839d5a-e77e-43b6-8960-fc11ef95410d"], "metadata": {"page_label": "6", "file_name": "2402_14805v1.pdf", "Title of this paper": "Identifying Multiple Personalities in Large Language Models with External Evaluation", "Authors": "Xiaoyang Song, Yuta Adachi, Jessie Feng, Mouwei Lin, Linhao Yu, Frank Li, Akshat Gupta, Gopala Anumanchipalli, Simerjot Kaur", "Date published": "02/22/2024", "URL": "http://arxiv.org/abs/2402.14805v1", "summary": "As Large Language Models (LLMs) are integrated with human daily applications\nrapidly, many societal and ethical concerns are raised regarding the behavior\nof LLMs. One of the ways to comprehend LLMs' behavior is to analyze their\npersonalities. Many recent studies quantify LLMs' personalities using\nself-assessment tests that are created for humans. Yet many critiques question\nthe applicability and reliability of these self-assessment tests when applied\nto LLMs. In this paper, we investigate LLM personalities using an alternate\npersonality measurement method, which we refer to as the external evaluation\nmethod, where instead of prompting LLMs with multiple-choice questions in the\nLikert scale, we evaluate LLMs' personalities by analyzing their responses\ntoward open-ended situational questions using an external machine learning\nmodel. We first fine-tuned a Llama2-7B model as the MBTI personality predictor\nthat outperforms the state-of-the-art models as the tool to analyze LLMs'\nresponses. Then, we prompt the LLMs with situational questions and ask them to\ngenerate Twitter posts and comments, respectively, in order to assess their\npersonalities when playing two different roles. Using the external personality\nevaluation method, we identify that the obtained personality types for LLMs are\nsignificantly different when generating posts versus comments, whereas humans\nshow a consistent personality profile in these two different situations. This\nshows that LLMs can exhibit different personalities based on different\nscenarios, thus highlighting a fundamental difference between personality in\nLLMs and humans. With our work, we call for a re-evaluation of personality\ndefinition and measurement in LLMs."}}, "0460f752-0df5-44ba-8787-2b16ee7aaffc": {"node_ids": ["80bb5150-1f0f-4f9b-afab-43b379bb47ae", "2079231c-828c-43d2-9bc3-771b971192b0"], "metadata": {"page_label": "7", "file_name": "2402_14805v1.pdf", "Title of this paper": "Identifying Multiple Personalities in Large Language Models with External Evaluation", "Authors": "Xiaoyang Song, Yuta Adachi, Jessie Feng, Mouwei Lin, Linhao Yu, Frank Li, Akshat Gupta, Gopala Anumanchipalli, Simerjot Kaur", "Date published": "02/22/2024", "URL": "http://arxiv.org/abs/2402.14805v1", "summary": "As Large Language Models (LLMs) are integrated with human daily applications\nrapidly, many societal and ethical concerns are raised regarding the behavior\nof LLMs. One of the ways to comprehend LLMs' behavior is to analyze their\npersonalities. Many recent studies quantify LLMs' personalities using\nself-assessment tests that are created for humans. Yet many critiques question\nthe applicability and reliability of these self-assessment tests when applied\nto LLMs. In this paper, we investigate LLM personalities using an alternate\npersonality measurement method, which we refer to as the external evaluation\nmethod, where instead of prompting LLMs with multiple-choice questions in the\nLikert scale, we evaluate LLMs' personalities by analyzing their responses\ntoward open-ended situational questions using an external machine learning\nmodel. We first fine-tuned a Llama2-7B model as the MBTI personality predictor\nthat outperforms the state-of-the-art models as the tool to analyze LLMs'\nresponses. Then, we prompt the LLMs with situational questions and ask them to\ngenerate Twitter posts and comments, respectively, in order to assess their\npersonalities when playing two different roles. Using the external personality\nevaluation method, we identify that the obtained personality types for LLMs are\nsignificantly different when generating posts versus comments, whereas humans\nshow a consistent personality profile in these two different situations. This\nshows that LLMs can exhibit different personalities based on different\nscenarios, thus highlighting a fundamental difference between personality in\nLLMs and humans. With our work, we call for a re-evaluation of personality\ndefinition and measurement in LLMs."}}, "a13cb1b0-c145-41ed-bcce-4e94a3791c8b": {"node_ids": ["878da19e-0a9c-44f6-a8c5-dbfeb96c674a", "fb1b99e3-1f08-48bd-9477-6334681d4ba9", "305b2676-5fc2-45ba-b702-41f269a5060f"], "metadata": {"page_label": "8", "file_name": "2402_14805v1.pdf", "Title of this paper": "Identifying Multiple Personalities in Large Language Models with External Evaluation", "Authors": "Xiaoyang Song, Yuta Adachi, Jessie Feng, Mouwei Lin, Linhao Yu, Frank Li, Akshat Gupta, Gopala Anumanchipalli, Simerjot Kaur", "Date published": "02/22/2024", "URL": "http://arxiv.org/abs/2402.14805v1", "summary": "As Large Language Models (LLMs) are integrated with human daily applications\nrapidly, many societal and ethical concerns are raised regarding the behavior\nof LLMs. One of the ways to comprehend LLMs' behavior is to analyze their\npersonalities. Many recent studies quantify LLMs' personalities using\nself-assessment tests that are created for humans. Yet many critiques question\nthe applicability and reliability of these self-assessment tests when applied\nto LLMs. In this paper, we investigate LLM personalities using an alternate\npersonality measurement method, which we refer to as the external evaluation\nmethod, where instead of prompting LLMs with multiple-choice questions in the\nLikert scale, we evaluate LLMs' personalities by analyzing their responses\ntoward open-ended situational questions using an external machine learning\nmodel. We first fine-tuned a Llama2-7B model as the MBTI personality predictor\nthat outperforms the state-of-the-art models as the tool to analyze LLMs'\nresponses. Then, we prompt the LLMs with situational questions and ask them to\ngenerate Twitter posts and comments, respectively, in order to assess their\npersonalities when playing two different roles. Using the external personality\nevaluation method, we identify that the obtained personality types for LLMs are\nsignificantly different when generating posts versus comments, whereas humans\nshow a consistent personality profile in these two different situations. This\nshows that LLMs can exhibit different personalities based on different\nscenarios, thus highlighting a fundamental difference between personality in\nLLMs and humans. With our work, we call for a re-evaluation of personality\ndefinition and measurement in LLMs."}}, "a14878fb-c323-4647-9473-016e9bc34f31": {"node_ids": ["3d2ed0ae-a22b-4e08-b8c1-16f32813a27c", "d1a02ab2-de6c-4ab3-ae4d-57925690cbc4", "a596f26c-17f7-4649-87fc-bd6c3c6c02eb", "75c87058-4d99-44e3-8765-945d4cf0094d"], "metadata": {"page_label": "9", "file_name": "2402_14805v1.pdf", "Title of this paper": "Identifying Multiple Personalities in Large Language Models with External Evaluation", "Authors": "Xiaoyang Song, Yuta Adachi, Jessie Feng, Mouwei Lin, Linhao Yu, Frank Li, Akshat Gupta, Gopala Anumanchipalli, Simerjot Kaur", "Date published": "02/22/2024", "URL": "http://arxiv.org/abs/2402.14805v1", "summary": "As Large Language Models (LLMs) are integrated with human daily applications\nrapidly, many societal and ethical concerns are raised regarding the behavior\nof LLMs. One of the ways to comprehend LLMs' behavior is to analyze their\npersonalities. Many recent studies quantify LLMs' personalities using\nself-assessment tests that are created for humans. Yet many critiques question\nthe applicability and reliability of these self-assessment tests when applied\nto LLMs. In this paper, we investigate LLM personalities using an alternate\npersonality measurement method, which we refer to as the external evaluation\nmethod, where instead of prompting LLMs with multiple-choice questions in the\nLikert scale, we evaluate LLMs' personalities by analyzing their responses\ntoward open-ended situational questions using an external machine learning\nmodel. We first fine-tuned a Llama2-7B model as the MBTI personality predictor\nthat outperforms the state-of-the-art models as the tool to analyze LLMs'\nresponses. Then, we prompt the LLMs with situational questions and ask them to\ngenerate Twitter posts and comments, respectively, in order to assess their\npersonalities when playing two different roles. Using the external personality\nevaluation method, we identify that the obtained personality types for LLMs are\nsignificantly different when generating posts versus comments, whereas humans\nshow a consistent personality profile in these two different situations. This\nshows that LLMs can exhibit different personalities based on different\nscenarios, thus highlighting a fundamental difference between personality in\nLLMs and humans. With our work, we call for a re-evaluation of personality\ndefinition and measurement in LLMs."}}, "2fa444ed-af02-49df-9eff-dc61c8867363": {"node_ids": ["52a55e88-21eb-43e1-96d2-7780dab7d444", "3b5e8528-454d-4786-b8e7-af7fcc8ea72d", "fca6c576-985e-444b-b18b-ab4a8c8d44bf"], "metadata": {"page_label": "10", "file_name": "2402_14805v1.pdf", "Title of this paper": "Identifying Multiple Personalities in Large Language Models with External Evaluation", "Authors": "Xiaoyang Song, Yuta Adachi, Jessie Feng, Mouwei Lin, Linhao Yu, Frank Li, Akshat Gupta, Gopala Anumanchipalli, Simerjot Kaur", "Date published": "02/22/2024", "URL": "http://arxiv.org/abs/2402.14805v1", "summary": "As Large Language Models (LLMs) are integrated with human daily applications\nrapidly, many societal and ethical concerns are raised regarding the behavior\nof LLMs. One of the ways to comprehend LLMs' behavior is to analyze their\npersonalities. Many recent studies quantify LLMs' personalities using\nself-assessment tests that are created for humans. Yet many critiques question\nthe applicability and reliability of these self-assessment tests when applied\nto LLMs. In this paper, we investigate LLM personalities using an alternate\npersonality measurement method, which we refer to as the external evaluation\nmethod, where instead of prompting LLMs with multiple-choice questions in the\nLikert scale, we evaluate LLMs' personalities by analyzing their responses\ntoward open-ended situational questions using an external machine learning\nmodel. We first fine-tuned a Llama2-7B model as the MBTI personality predictor\nthat outperforms the state-of-the-art models as the tool to analyze LLMs'\nresponses. Then, we prompt the LLMs with situational questions and ask them to\ngenerate Twitter posts and comments, respectively, in order to assess their\npersonalities when playing two different roles. Using the external personality\nevaluation method, we identify that the obtained personality types for LLMs are\nsignificantly different when generating posts versus comments, whereas humans\nshow a consistent personality profile in these two different situations. This\nshows that LLMs can exhibit different personalities based on different\nscenarios, thus highlighting a fundamental difference between personality in\nLLMs and humans. With our work, we call for a re-evaluation of personality\ndefinition and measurement in LLMs."}}, "e6068ed1-1725-4b6b-bbab-094aaf2467f9": {"node_ids": ["9d8a0017-36bd-43c3-9c38-19033e41956a", "3c58ba9a-e2d2-49b9-807d-142f2f774a68", "cad614db-d4a4-4874-911c-c88c3728e5aa"], "metadata": {"page_label": "11", "file_name": "2402_14805v1.pdf", "Title of this paper": "Identifying Multiple Personalities in Large Language Models with External Evaluation", "Authors": "Xiaoyang Song, Yuta Adachi, Jessie Feng, Mouwei Lin, Linhao Yu, Frank Li, Akshat Gupta, Gopala Anumanchipalli, Simerjot Kaur", "Date published": "02/22/2024", "URL": "http://arxiv.org/abs/2402.14805v1", "summary": "As Large Language Models (LLMs) are integrated with human daily applications\nrapidly, many societal and ethical concerns are raised regarding the behavior\nof LLMs. One of the ways to comprehend LLMs' behavior is to analyze their\npersonalities. Many recent studies quantify LLMs' personalities using\nself-assessment tests that are created for humans. Yet many critiques question\nthe applicability and reliability of these self-assessment tests when applied\nto LLMs. In this paper, we investigate LLM personalities using an alternate\npersonality measurement method, which we refer to as the external evaluation\nmethod, where instead of prompting LLMs with multiple-choice questions in the\nLikert scale, we evaluate LLMs' personalities by analyzing their responses\ntoward open-ended situational questions using an external machine learning\nmodel. We first fine-tuned a Llama2-7B model as the MBTI personality predictor\nthat outperforms the state-of-the-art models as the tool to analyze LLMs'\nresponses. Then, we prompt the LLMs with situational questions and ask them to\ngenerate Twitter posts and comments, respectively, in order to assess their\npersonalities when playing two different roles. Using the external personality\nevaluation method, we identify that the obtained personality types for LLMs are\nsignificantly different when generating posts versus comments, whereas humans\nshow a consistent personality profile in these two different situations. This\nshows that LLMs can exhibit different personalities based on different\nscenarios, thus highlighting a fundamental difference between personality in\nLLMs and humans. With our work, we call for a re-evaluation of personality\ndefinition and measurement in LLMs."}}, "7861777b-c4f4-4694-9c79-d3c0f7650736": {"node_ids": ["be34ed48-0337-462e-9a7f-7dc2cbe1ecc1"], "metadata": {"page_label": "12", "file_name": "2402_14805v1.pdf", "Title of this paper": "Identifying Multiple Personalities in Large Language Models with External Evaluation", "Authors": "Xiaoyang Song, Yuta Adachi, Jessie Feng, Mouwei Lin, Linhao Yu, Frank Li, Akshat Gupta, Gopala Anumanchipalli, Simerjot Kaur", "Date published": "02/22/2024", "URL": "http://arxiv.org/abs/2402.14805v1", "summary": "As Large Language Models (LLMs) are integrated with human daily applications\nrapidly, many societal and ethical concerns are raised regarding the behavior\nof LLMs. One of the ways to comprehend LLMs' behavior is to analyze their\npersonalities. Many recent studies quantify LLMs' personalities using\nself-assessment tests that are created for humans. Yet many critiques question\nthe applicability and reliability of these self-assessment tests when applied\nto LLMs. In this paper, we investigate LLM personalities using an alternate\npersonality measurement method, which we refer to as the external evaluation\nmethod, where instead of prompting LLMs with multiple-choice questions in the\nLikert scale, we evaluate LLMs' personalities by analyzing their responses\ntoward open-ended situational questions using an external machine learning\nmodel. We first fine-tuned a Llama2-7B model as the MBTI personality predictor\nthat outperforms the state-of-the-art models as the tool to analyze LLMs'\nresponses. Then, we prompt the LLMs with situational questions and ask them to\ngenerate Twitter posts and comments, respectively, in order to assess their\npersonalities when playing two different roles. Using the external personality\nevaluation method, we identify that the obtained personality types for LLMs are\nsignificantly different when generating posts versus comments, whereas humans\nshow a consistent personality profile in these two different situations. This\nshows that LLMs can exhibit different personalities based on different\nscenarios, thus highlighting a fundamental difference between personality in\nLLMs and humans. With our work, we call for a re-evaluation of personality\ndefinition and measurement in LLMs."}}}}