{"docstore/data": {"43acfff8-6996-460d-9f6e-18fc0ab55610": {"__data__": {"id_": "43acfff8-6996-460d-9f6e-18fc0ab55610", "embedding": null, "metadata": {"page_label": "1", "file_name": "2310_10049v1.pdf", "Title of this paper": "FATE-LLM: A Industrial Grade Federated Learning Framework for Large Language Models", "Authors": "Tao Fan, Yan Kang, Guoqiang Ma, Weijing Chen, Wenbin Wei, Lixin Fan, Qiang Yang", "Date published": "10/16/2023", "URL": "http://arxiv.org/abs/2310.10049v1", "summary": "Large Language Models (LLMs), such as ChatGPT, LLaMA, GLM, and PaLM, have\nexhibited remarkable performances across various tasks in recent years.\nHowever, LLMs face two main challenges in real-world applications. One\nchallenge is that training LLMs consumes vast computing resources, preventing\nLLMs from being adopted by small and medium-sized enterprises with limited\ncomputing resources. Another is that training LLM requires a large amount of\nhigh-quality data, which are often scattered among enterprises. To address\nthese challenges, we propose FATE-LLM, an industrial-grade federated learning\nframework for large language models. FATE-LLM (1) facilitates federated\nlearning for large language models (coined FedLLM); (2) promotes efficient\ntraining of FedLLM using parameter-efficient fine-tuning methods; (3) protects\nthe intellectual property of LLMs; (4) preserves data privacy during training\nand inference through privacy-preserving mechanisms. We release the code of\nFATE-LLM at https://github.com/FederatedAI/FATE-LLM to facilitate the research\nof FedLLM and enable a broad range of industrial applications."}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "767daaf0-f94b-44da-9c3c-ec3c5b9c7af1", "node_type": "4", "metadata": {"page_label": "1", "file_name": "2310_10049v1.pdf", "Title of this paper": "FATE-LLM: A Industrial Grade Federated Learning Framework for Large Language Models", "Authors": "Tao Fan, Yan Kang, Guoqiang Ma, Weijing Chen, Wenbin Wei, Lixin Fan, Qiang Yang", "Date published": "10/16/2023", "URL": "http://arxiv.org/abs/2310.10049v1", "summary": "Large Language Models (LLMs), such as ChatGPT, LLaMA, GLM, and PaLM, have\nexhibited remarkable performances across various tasks in recent years.\nHowever, LLMs face two main challenges in real-world applications. One\nchallenge is that training LLMs consumes vast computing resources, preventing\nLLMs from being adopted by small and medium-sized enterprises with limited\ncomputing resources. Another is that training LLM requires a large amount of\nhigh-quality data, which are often scattered among enterprises. To address\nthese challenges, we propose FATE-LLM, an industrial-grade federated learning\nframework for large language models. FATE-LLM (1) facilitates federated\nlearning for large language models (coined FedLLM); (2) promotes efficient\ntraining of FedLLM using parameter-efficient fine-tuning methods; (3) protects\nthe intellectual property of LLMs; (4) preserves data privacy during training\nand inference through privacy-preserving mechanisms. We release the code of\nFATE-LLM at https://github.com/FederatedAI/FATE-LLM to facilitate the research\nof FedLLM and enable a broad range of industrial applications."}, "hash": "9729038b3b3069af2ef8974046c28c030001bb477413b3fbfa44ed469c77b9e5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "43e8970f-eaab-4f0b-bf3c-4f2e0edbf721", "node_type": "1", "metadata": {}, "hash": "0bef361a4808c57c9ca3d619d20f2315ec43788488b3e756d565e3b8af249f72", "class_name": "RelatedNodeInfo"}}, "text": "FATE-LLM: A Industrial Grade Federated Learning Framework for Large\nLanguage Models\nTao Fan1,2\u2217,Yan Kang2,Guoqiang Ma2,Weijing Chen2,Wenbin Wei2,Lixin Fan2,Qiang\nYang1,2\n1Hong Kong University of Science and Technology, China\n2WeBank, China\ntfanac@cse.ust.hk, yangkang@webank.com, zotrseeewma@webank.com, weijingchen@webank.com,\nsagewei@webank.com, lixinfan@webank.com, qyang@cse.ust.hk\nAbstract\nLarge Language Models (LLMs), such as Chat-\nGPT, LLaMA, GLM, and PaLM, have exhibited re-\nmarkable performances across various tasks in re-\ncent years. However, LLMs face two main chal-\nlenges in real-world applications. One challenge is\nthat training LLMs consumes vast computing re-\nsources, preventing LLMs from being adopted by\nsmall and medium-sized enterprises with limited\ncomputing resources. Another is that training LLM\nrequires a large amount of high-quality data, which\nare often scattered among enterprises.\nTo address these challenges, we propose FATE-\nLLM, an industrial-grade federated learning frame-\nwork for large language models. FATE-LLM (1)\nfacilitates federated learning for large language\nmodels (coined FedLLM); (2) promotes efficient\ntraining of FedLLM using parameter-efficient fine-\ntuning methods; (3) protects the intellectual prop-\nerty of LLMs; (4) preserves data privacy during\ntraining and inference through privacy-preserving\nmechanisms. We release the code of FATE-LLM at\nhttps://github.com/FederatedAI/FATE-LLM to fa-\ncilitate the research of FedLLM and enable a broad\nrange of industrial applications.\n1 Introduction\nIn recent few years, the advent of large language models\n(LLMs) [Yang et al. , 2023b; Zhou et al. , 2023 ]has been re-\nshaping the field of artificial intelligence. In particular, the\nmost advanced LLMs, such as ChatGPT [OpenAI, 2022 ],\nGPT-4 [OpenAI, 2023 ], and PaLM [Chowdhery et al. , 2022 ]\nthat boast billions of parameters have gained considerable at-\ntention due to their remarkable performance in a variety of\nnatural language generation tasks. Many open-sourced LLMs\nwith high performance have been released, and the public\u2019s\nenthusiasm for research and application of LLMs has been\nstimulated.\nHowever, grounding LLMs in real-world applications faces\nmany challenges. The two main challenges are (i) training\nLLMs consumes vast computing resources, which prevents\n\u2217Corresponding Author\nFigure 1: Large Language Models are federated on FATE .", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2402, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "43e8970f-eaab-4f0b-bf3c-4f2e0edbf721": {"__data__": {"id_": "43e8970f-eaab-4f0b-bf3c-4f2e0edbf721", "embedding": null, "metadata": {"page_label": "1", "file_name": "2310_10049v1.pdf", "Title of this paper": "FATE-LLM: A Industrial Grade Federated Learning Framework for Large Language Models", "Authors": "Tao Fan, Yan Kang, Guoqiang Ma, Weijing Chen, Wenbin Wei, Lixin Fan, Qiang Yang", "Date published": "10/16/2023", "URL": "http://arxiv.org/abs/2310.10049v1", "summary": "Large Language Models (LLMs), such as ChatGPT, LLaMA, GLM, and PaLM, have\nexhibited remarkable performances across various tasks in recent years.\nHowever, LLMs face two main challenges in real-world applications. One\nchallenge is that training LLMs consumes vast computing resources, preventing\nLLMs from being adopted by small and medium-sized enterprises with limited\ncomputing resources. Another is that training LLM requires a large amount of\nhigh-quality data, which are often scattered among enterprises. To address\nthese challenges, we propose FATE-LLM, an industrial-grade federated learning\nframework for large language models. FATE-LLM (1) facilitates federated\nlearning for large language models (coined FedLLM); (2) promotes efficient\ntraining of FedLLM using parameter-efficient fine-tuning methods; (3) protects\nthe intellectual property of LLMs; (4) preserves data privacy during training\nand inference through privacy-preserving mechanisms. We release the code of\nFATE-LLM at https://github.com/FederatedAI/FATE-LLM to facilitate the research\nof FedLLM and enable a broad range of industrial applications."}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "767daaf0-f94b-44da-9c3c-ec3c5b9c7af1", "node_type": "4", "metadata": {"page_label": "1", "file_name": "2310_10049v1.pdf", "Title of this paper": "FATE-LLM: A Industrial Grade Federated Learning Framework for Large Language Models", "Authors": "Tao Fan, Yan Kang, Guoqiang Ma, Weijing Chen, Wenbin Wei, Lixin Fan, Qiang Yang", "Date published": "10/16/2023", "URL": "http://arxiv.org/abs/2310.10049v1", "summary": "Large Language Models (LLMs), such as ChatGPT, LLaMA, GLM, and PaLM, have\nexhibited remarkable performances across various tasks in recent years.\nHowever, LLMs face two main challenges in real-world applications. One\nchallenge is that training LLMs consumes vast computing resources, preventing\nLLMs from being adopted by small and medium-sized enterprises with limited\ncomputing resources. Another is that training LLM requires a large amount of\nhigh-quality data, which are often scattered among enterprises. To address\nthese challenges, we propose FATE-LLM, an industrial-grade federated learning\nframework for large language models. FATE-LLM (1) facilitates federated\nlearning for large language models (coined FedLLM); (2) promotes efficient\ntraining of FedLLM using parameter-efficient fine-tuning methods; (3) protects\nthe intellectual property of LLMs; (4) preserves data privacy during training\nand inference through privacy-preserving mechanisms. We release the code of\nFATE-LLM at https://github.com/FederatedAI/FATE-LLM to facilitate the research\nof FedLLM and enable a broad range of industrial applications."}, "hash": "9729038b3b3069af2ef8974046c28c030001bb477413b3fbfa44ed469c77b9e5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "43acfff8-6996-460d-9f6e-18fc0ab55610", "node_type": "1", "metadata": {"page_label": "1", "file_name": "2310_10049v1.pdf", "Title of this paper": "FATE-LLM: A Industrial Grade Federated Learning Framework for Large Language Models", "Authors": "Tao Fan, Yan Kang, Guoqiang Ma, Weijing Chen, Wenbin Wei, Lixin Fan, Qiang Yang", "Date published": "10/16/2023", "URL": "http://arxiv.org/abs/2310.10049v1", "summary": "Large Language Models (LLMs), such as ChatGPT, LLaMA, GLM, and PaLM, have\nexhibited remarkable performances across various tasks in recent years.\nHowever, LLMs face two main challenges in real-world applications. One\nchallenge is that training LLMs consumes vast computing resources, preventing\nLLMs from being adopted by small and medium-sized enterprises with limited\ncomputing resources. Another is that training LLM requires a large amount of\nhigh-quality data, which are often scattered among enterprises. To address\nthese challenges, we propose FATE-LLM, an industrial-grade federated learning\nframework for large language models. FATE-LLM (1) facilitates federated\nlearning for large language models (coined FedLLM); (2) promotes efficient\ntraining of FedLLM using parameter-efficient fine-tuning methods; (3) protects\nthe intellectual property of LLMs; (4) preserves data privacy during training\nand inference through privacy-preserving mechanisms. We release the code of\nFATE-LLM at https://github.com/FederatedAI/FATE-LLM to facilitate the research\nof FedLLM and enable a broad range of industrial applications."}, "hash": "14123e192bc882c08bf551ecd4ce4f589da0ca584de3482f6adf3a1b720ea02d", "class_name": "RelatedNodeInfo"}}, "text": ", 2023 ]has been re-\nshaping the field of artificial intelligence. In particular, the\nmost advanced LLMs, such as ChatGPT [OpenAI, 2022 ],\nGPT-4 [OpenAI, 2023 ], and PaLM [Chowdhery et al. , 2022 ]\nthat boast billions of parameters have gained considerable at-\ntention due to their remarkable performance in a variety of\nnatural language generation tasks. Many open-sourced LLMs\nwith high performance have been released, and the public\u2019s\nenthusiasm for research and application of LLMs has been\nstimulated.\nHowever, grounding LLMs in real-world applications faces\nmany challenges. The two main challenges are (i) training\nLLMs consumes vast computing resources, which prevents\n\u2217Corresponding Author\nFigure 1: Large Language Models are federated on FATE .\nLLMs from being adopted by small and medium-sized com-\npanies with limited computing resources; (ii) training LLMs\nrequires a large amount of public data, which may run out\nsoon [Villalobos et al. , 2022 ].\nFederated learning (FL) [McMahan et al. , 2017 ] [Yang et\nal., 2019 ], a privacy-preserving collaborative machine learn-\ning paradigm, is a promising approach to deal with these two\nchallenges. For one thing, FL enables many companies with\ndifferent computing resources to collaboratively train pow-\nerful machine learning models such that the computational\nburden of training large models can be alleviated. For an-\nother, massive high-quality data are scattered among compa-\nnies that are typically isolated from each other, and FL can\nexploit these data silos in a privacy-preserving way.\nIn this work, we propose FATE-LLM, built upon FATE\n(Federated AI Technology Enabler) [Liuet al. , 2021b ], to fa-\ncilitate federated learning for large language models. More\nspecifically, FATE-LLM (1) enables federated learning for\nboth homogeneous and heterogeneous large language mod-\nels (FedLLM); (2) promotes efficient training of FedLLM\nthrough parameter-efficient fine-tuning methods, such as\nLoRA [Huet al. , 2021 ]and P-Tuning-v2 [Liuet al. , 2021a ];\n(3) protects the intellectual property of LLMs using fed-\nerated intellectual property protection approach [Liet al. ,\n2022 ]; (4) protects data privacy during training and infer-\nence through privacy-preserving mechanisms. We release\nthe code of FATE-LLM at https://github.com/FederatedAI/\nFATE-LLM to promote the research of FedLLM and enable\na broad range of industrial applications.arXiv:2310.10049v1  [cs.LG]  16 Oct 2023", "mimetype": "text/plain", "start_char_idx": 1648, "end_char_idx": 4089, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6116b8d1-1056-4b81-9fc2-b26583cc9f1c": {"__data__": {"id_": "6116b8d1-1056-4b81-9fc2-b26583cc9f1c", "embedding": null, "metadata": {"page_label": "2", "file_name": "2310_10049v1.pdf", "Title of this paper": "FATE-LLM: A Industrial Grade Federated Learning Framework for Large Language Models", "Authors": "Tao Fan, Yan Kang, Guoqiang Ma, Weijing Chen, Wenbin Wei, Lixin Fan, Qiang Yang", "Date published": "10/16/2023", "URL": "http://arxiv.org/abs/2310.10049v1", "summary": "Large Language Models (LLMs), such as ChatGPT, LLaMA, GLM, and PaLM, have\nexhibited remarkable performances across various tasks in recent years.\nHowever, LLMs face two main challenges in real-world applications. One\nchallenge is that training LLMs consumes vast computing resources, preventing\nLLMs from being adopted by small and medium-sized enterprises with limited\ncomputing resources. Another is that training LLM requires a large amount of\nhigh-quality data, which are often scattered among enterprises. To address\nthese challenges, we propose FATE-LLM, an industrial-grade federated learning\nframework for large language models. FATE-LLM (1) facilitates federated\nlearning for large language models (coined FedLLM); (2) promotes efficient\ntraining of FedLLM using parameter-efficient fine-tuning methods; (3) protects\nthe intellectual property of LLMs; (4) preserves data privacy during training\nand inference through privacy-preserving mechanisms. We release the code of\nFATE-LLM at https://github.com/FederatedAI/FATE-LLM to facilitate the research\nof FedLLM and enable a broad range of industrial applications."}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b812a356-755a-4b58-8304-98030e45c6ed", "node_type": "4", "metadata": {"page_label": "2", "file_name": "2310_10049v1.pdf", "Title of this paper": "FATE-LLM: A Industrial Grade Federated Learning Framework for Large Language Models", "Authors": "Tao Fan, Yan Kang, Guoqiang Ma, Weijing Chen, Wenbin Wei, Lixin Fan, Qiang Yang", "Date published": "10/16/2023", "URL": "http://arxiv.org/abs/2310.10049v1", "summary": "Large Language Models (LLMs), such as ChatGPT, LLaMA, GLM, and PaLM, have\nexhibited remarkable performances across various tasks in recent years.\nHowever, LLMs face two main challenges in real-world applications. One\nchallenge is that training LLMs consumes vast computing resources, preventing\nLLMs from being adopted by small and medium-sized enterprises with limited\ncomputing resources. Another is that training LLM requires a large amount of\nhigh-quality data, which are often scattered among enterprises. To address\nthese challenges, we propose FATE-LLM, an industrial-grade federated learning\nframework for large language models. FATE-LLM (1) facilitates federated\nlearning for large language models (coined FedLLM); (2) promotes efficient\ntraining of FedLLM using parameter-efficient fine-tuning methods; (3) protects\nthe intellectual property of LLMs; (4) preserves data privacy during training\nand inference through privacy-preserving mechanisms. We release the code of\nFATE-LLM at https://github.com/FederatedAI/FATE-LLM to facilitate the research\nof FedLLM and enable a broad range of industrial applications."}, "hash": "76aea2df5808e495111215b0258b9217c02c0f51514ffbcb42f621d9aa05ef2b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "320b32a3-ea3c-4677-aa7f-3b9f531744eb", "node_type": "1", "metadata": {}, "hash": "c2c1806108fa4727bd545589e0a69a3ac6e480191592cdaa03e8b1e3ac344545", "class_name": "RelatedNodeInfo"}}, "text": "2 Related Work\nIn this section, we briefly review related work regarding large\nlanguage models and federated learning.\n2.1 Large Language Models\nThe advancements in large language models(LLMs) have led\nto significant advances in a variety of NLP tasks. A great\nexample of LLMs application is ChatGPT [OpenAI, 2022 ].\nChatGPT is fine-tuned from the generative pretrained trans-\nformer GPT-3.5, which was trained on a blend of text and\ncode. ChatGPT applies reinforcement learning from human\nfeedback (RLHF), which has become a promising way to\nalign LLMs with a human\u2019s intent. LLMs are generally di-\nvided into two categories: encoder-decoder or encoder-only\nlarge language models and decoder-only large language mod-\nels[Yang et al. , 2023b ]. Bert [Devlin et al. , 2018 ]is the\nrepresentative of encoder-only large language models. GPTs\n[Radford et al. , 2018 ]is the representative of decoder-only\nlarge language models. At the early stage of LLMs devel-\nopment, decoder-only LLMs were not as popular as encoder-\nonly and encoder-decoder LLMs. However, after 2021, with\nthe introduction of GPT-3 [Brown et al. , 2020 ], decoder-only\nLLMs experienced a significant boom. At the same time, af-\nter the initial explosion brought about by BERT [Devlin et al. ,\n2018 ], encoder-only LLMs gradually began to fade away. Re-\ncently, many decoder-only LLMs have been released, such as\nLLaMA [Touvron et al. , 2023 ], OPT [Zhang et al. , 2022a ],\nPaLM [Chowdhery et al. , 2022 ], and BLOOM [Scao et al. ,\n2022 ]. These LLMs demonstrated reasonable few-/zero-shot\nperformance via prompting and in-context learning.\n2.2 Federated Learning\nFederated learning (FL) [McMahan et al. , 2017 ] [Yang et\nal., 2019; Liu et al. , 2022 ]is a distributed machine learn-\ning paradigm that enables clients (devices or organizations)\nto train a machine learning model collaboratively without ex-\nposing clients\u2019 data. Unlike traditional centralized machine\nlearning techniques, data are fixed locally rather than being\ngathered in a central server, which exists many of the sys-\ntemic privacy risks and costs [Kairouz et al. , 2021 ]. Hence,\nFL is a promising approach to deal with this data isolation\nchallenge. To enhance data privacy, federated learning uses\na variety of secure computing protocols.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2279, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "320b32a3-ea3c-4677-aa7f-3b9f531744eb": {"__data__": {"id_": "320b32a3-ea3c-4677-aa7f-3b9f531744eb", "embedding": null, "metadata": {"page_label": "2", "file_name": "2310_10049v1.pdf", "Title of this paper": "FATE-LLM: A Industrial Grade Federated Learning Framework for Large Language Models", "Authors": "Tao Fan, Yan Kang, Guoqiang Ma, Weijing Chen, Wenbin Wei, Lixin Fan, Qiang Yang", "Date published": "10/16/2023", "URL": "http://arxiv.org/abs/2310.10049v1", "summary": "Large Language Models (LLMs), such as ChatGPT, LLaMA, GLM, and PaLM, have\nexhibited remarkable performances across various tasks in recent years.\nHowever, LLMs face two main challenges in real-world applications. One\nchallenge is that training LLMs consumes vast computing resources, preventing\nLLMs from being adopted by small and medium-sized enterprises with limited\ncomputing resources. Another is that training LLM requires a large amount of\nhigh-quality data, which are often scattered among enterprises. To address\nthese challenges, we propose FATE-LLM, an industrial-grade federated learning\nframework for large language models. FATE-LLM (1) facilitates federated\nlearning for large language models (coined FedLLM); (2) promotes efficient\ntraining of FedLLM using parameter-efficient fine-tuning methods; (3) protects\nthe intellectual property of LLMs; (4) preserves data privacy during training\nand inference through privacy-preserving mechanisms. We release the code of\nFATE-LLM at https://github.com/FederatedAI/FATE-LLM to facilitate the research\nof FedLLM and enable a broad range of industrial applications."}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b812a356-755a-4b58-8304-98030e45c6ed", "node_type": "4", "metadata": {"page_label": "2", "file_name": "2310_10049v1.pdf", "Title of this paper": "FATE-LLM: A Industrial Grade Federated Learning Framework for Large Language Models", "Authors": "Tao Fan, Yan Kang, Guoqiang Ma, Weijing Chen, Wenbin Wei, Lixin Fan, Qiang Yang", "Date published": "10/16/2023", "URL": "http://arxiv.org/abs/2310.10049v1", "summary": "Large Language Models (LLMs), such as ChatGPT, LLaMA, GLM, and PaLM, have\nexhibited remarkable performances across various tasks in recent years.\nHowever, LLMs face two main challenges in real-world applications. One\nchallenge is that training LLMs consumes vast computing resources, preventing\nLLMs from being adopted by small and medium-sized enterprises with limited\ncomputing resources. Another is that training LLM requires a large amount of\nhigh-quality data, which are often scattered among enterprises. To address\nthese challenges, we propose FATE-LLM, an industrial-grade federated learning\nframework for large language models. FATE-LLM (1) facilitates federated\nlearning for large language models (coined FedLLM); (2) promotes efficient\ntraining of FedLLM using parameter-efficient fine-tuning methods; (3) protects\nthe intellectual property of LLMs; (4) preserves data privacy during training\nand inference through privacy-preserving mechanisms. We release the code of\nFATE-LLM at https://github.com/FederatedAI/FATE-LLM to facilitate the research\nof FedLLM and enable a broad range of industrial applications."}, "hash": "76aea2df5808e495111215b0258b9217c02c0f51514ffbcb42f621d9aa05ef2b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6116b8d1-1056-4b81-9fc2-b26583cc9f1c", "node_type": "1", "metadata": {"page_label": "2", "file_name": "2310_10049v1.pdf", "Title of this paper": "FATE-LLM: A Industrial Grade Federated Learning Framework for Large Language Models", "Authors": "Tao Fan, Yan Kang, Guoqiang Ma, Weijing Chen, Wenbin Wei, Lixin Fan, Qiang Yang", "Date published": "10/16/2023", "URL": "http://arxiv.org/abs/2310.10049v1", "summary": "Large Language Models (LLMs), such as ChatGPT, LLaMA, GLM, and PaLM, have\nexhibited remarkable performances across various tasks in recent years.\nHowever, LLMs face two main challenges in real-world applications. One\nchallenge is that training LLMs consumes vast computing resources, preventing\nLLMs from being adopted by small and medium-sized enterprises with limited\ncomputing resources. Another is that training LLM requires a large amount of\nhigh-quality data, which are often scattered among enterprises. To address\nthese challenges, we propose FATE-LLM, an industrial-grade federated learning\nframework for large language models. FATE-LLM (1) facilitates federated\nlearning for large language models (coined FedLLM); (2) promotes efficient\ntraining of FedLLM using parameter-efficient fine-tuning methods; (3) protects\nthe intellectual property of LLMs; (4) preserves data privacy during training\nand inference through privacy-preserving mechanisms. We release the code of\nFATE-LLM at https://github.com/FederatedAI/FATE-LLM to facilitate the research\nof FedLLM and enable a broad range of industrial applications."}, "hash": "f43c4e084b85504cd4fbf2aaa21d8488f23d8e8066abbe986e5e94be7b5ba353", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4e435b07-1d69-439d-a558-18d619201098", "node_type": "1", "metadata": {}, "hash": "47972913029549af527b2b7a3d2f177f15565e20c891a4b81a92645f54fbc9ef", "class_name": "RelatedNodeInfo"}}, "text": ",\n2022 ]. These LLMs demonstrated reasonable few-/zero-shot\nperformance via prompting and in-context learning.\n2.2 Federated Learning\nFederated learning (FL) [McMahan et al. , 2017 ] [Yang et\nal., 2019; Liu et al. , 2022 ]is a distributed machine learn-\ning paradigm that enables clients (devices or organizations)\nto train a machine learning model collaboratively without ex-\nposing clients\u2019 data. Unlike traditional centralized machine\nlearning techniques, data are fixed locally rather than being\ngathered in a central server, which exists many of the sys-\ntemic privacy risks and costs [Kairouz et al. , 2021 ]. Hence,\nFL is a promising approach to deal with this data isolation\nchallenge. To enhance data privacy, federated learning uses\na variety of secure computing protocols. The most popular\nprotocols are Homomorphic Encryption (HE) [Paillier, 1999 ],\nMulti-Party Computation(MPC) [Shamir, 1979 ] [Damg \u02daardet\nal., 2012 ], and Differential Privacy (DP) [Dwork et al. , 2014 ].\nIn recent years, the literature has presented various algorithms\nin the FL setting. [Hardy et al. , 2017 ]proposed vertical logis-\ntic regression (VLR) using homomorphic encryption (HE) to\nprotect data privacy. [Chen et al. , 2021 ]further enhanced the\nprivacy-preserving capability of VLR by employing a hybrid\nstrategy combining HE and secret sharing (SS). [Cheng et al. ,\n2021 ]proposed the SecureBoost, a VFL version of XGBoost,\nthat leverages HE to protect the parameters exchanged among\nparties. [Kang et al. , 2022 ]applied a semi-supervised learn-\ning method to estimate missing features and labels for further\ntraining. [McMahan et al. , 2017 ]proposed Secure Aggrega-\ntion to enhance data protection.3 FATE-LLM System Design\nWe introduce the FATE-LLM system, including its compo-\nnents, architecture, and roadmap.\n3.1 Overview of FATE-LLM system\nFATE-LLM1was open-sourced as a submodule of FATE, and\nit contains three components: Communication-Efficient Hub,\nFedLLM Model Hub, and FedLLM Privacy Hub. Figure 2\noverviews the FATE-LLM system.\nFigure 2: Components of the FATE-LLM system .\nThe Communication-Efficient Hub integrates a variety\nof communication-efficient methods into FedLLM to re-\nduce the communication cost for training LLMs, includ-\ning parameter-efficiency fine-tuning (PEFT) [Zhang et al. ,\n2022b ]methods (e.g., Adapter Tuning [Cai et al. , 2022 ]\nand Prompt Tuning [Zhao et al. , 2022 ], Knowledge Distilla-\ntion(KD) [Wuet al.", "mimetype": "text/plain", "start_char_idx": 1496, "end_char_idx": 3939, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4e435b07-1d69-439d-a558-18d619201098": {"__data__": {"id_": "4e435b07-1d69-439d-a558-18d619201098", "embedding": null, "metadata": {"page_label": "2", "file_name": "2310_10049v1.pdf", "Title of this paper": "FATE-LLM: A Industrial Grade Federated Learning Framework for Large Language Models", "Authors": "Tao Fan, Yan Kang, Guoqiang Ma, Weijing Chen, Wenbin Wei, Lixin Fan, Qiang Yang", "Date published": "10/16/2023", "URL": "http://arxiv.org/abs/2310.10049v1", "summary": "Large Language Models (LLMs), such as ChatGPT, LLaMA, GLM, and PaLM, have\nexhibited remarkable performances across various tasks in recent years.\nHowever, LLMs face two main challenges in real-world applications. One\nchallenge is that training LLMs consumes vast computing resources, preventing\nLLMs from being adopted by small and medium-sized enterprises with limited\ncomputing resources. Another is that training LLM requires a large amount of\nhigh-quality data, which are often scattered among enterprises. To address\nthese challenges, we propose FATE-LLM, an industrial-grade federated learning\nframework for large language models. FATE-LLM (1) facilitates federated\nlearning for large language models (coined FedLLM); (2) promotes efficient\ntraining of FedLLM using parameter-efficient fine-tuning methods; (3) protects\nthe intellectual property of LLMs; (4) preserves data privacy during training\nand inference through privacy-preserving mechanisms. We release the code of\nFATE-LLM at https://github.com/FederatedAI/FATE-LLM to facilitate the research\nof FedLLM and enable a broad range of industrial applications."}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b812a356-755a-4b58-8304-98030e45c6ed", "node_type": "4", "metadata": {"page_label": "2", "file_name": "2310_10049v1.pdf", "Title of this paper": "FATE-LLM: A Industrial Grade Federated Learning Framework for Large Language Models", "Authors": "Tao Fan, Yan Kang, Guoqiang Ma, Weijing Chen, Wenbin Wei, Lixin Fan, Qiang Yang", "Date published": "10/16/2023", "URL": "http://arxiv.org/abs/2310.10049v1", "summary": "Large Language Models (LLMs), such as ChatGPT, LLaMA, GLM, and PaLM, have\nexhibited remarkable performances across various tasks in recent years.\nHowever, LLMs face two main challenges in real-world applications. One\nchallenge is that training LLMs consumes vast computing resources, preventing\nLLMs from being adopted by small and medium-sized enterprises with limited\ncomputing resources. Another is that training LLM requires a large amount of\nhigh-quality data, which are often scattered among enterprises. To address\nthese challenges, we propose FATE-LLM, an industrial-grade federated learning\nframework for large language models. FATE-LLM (1) facilitates federated\nlearning for large language models (coined FedLLM); (2) promotes efficient\ntraining of FedLLM using parameter-efficient fine-tuning methods; (3) protects\nthe intellectual property of LLMs; (4) preserves data privacy during training\nand inference through privacy-preserving mechanisms. We release the code of\nFATE-LLM at https://github.com/FederatedAI/FATE-LLM to facilitate the research\nof FedLLM and enable a broad range of industrial applications."}, "hash": "76aea2df5808e495111215b0258b9217c02c0f51514ffbcb42f621d9aa05ef2b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "320b32a3-ea3c-4677-aa7f-3b9f531744eb", "node_type": "1", "metadata": {"page_label": "2", "file_name": "2310_10049v1.pdf", "Title of this paper": "FATE-LLM: A Industrial Grade Federated Learning Framework for Large Language Models", "Authors": "Tao Fan, Yan Kang, Guoqiang Ma, Weijing Chen, Wenbin Wei, Lixin Fan, Qiang Yang", "Date published": "10/16/2023", "URL": "http://arxiv.org/abs/2310.10049v1", "summary": "Large Language Models (LLMs), such as ChatGPT, LLaMA, GLM, and PaLM, have\nexhibited remarkable performances across various tasks in recent years.\nHowever, LLMs face two main challenges in real-world applications. One\nchallenge is that training LLMs consumes vast computing resources, preventing\nLLMs from being adopted by small and medium-sized enterprises with limited\ncomputing resources. Another is that training LLM requires a large amount of\nhigh-quality data, which are often scattered among enterprises. To address\nthese challenges, we propose FATE-LLM, an industrial-grade federated learning\nframework for large language models. FATE-LLM (1) facilitates federated\nlearning for large language models (coined FedLLM); (2) promotes efficient\ntraining of FedLLM using parameter-efficient fine-tuning methods; (3) protects\nthe intellectual property of LLMs; (4) preserves data privacy during training\nand inference through privacy-preserving mechanisms. We release the code of\nFATE-LLM at https://github.com/FederatedAI/FATE-LLM to facilitate the research\nof FedLLM and enable a broad range of industrial applications."}, "hash": "8aaa0046ae5ab2fc9e147111761b22fb9e7fcdc4a3ef4faf10cbacefd81e2899", "class_name": "RelatedNodeInfo"}}, "text": "3.1 Overview of FATE-LLM system\nFATE-LLM1was open-sourced as a submodule of FATE, and\nit contains three components: Communication-Efficient Hub,\nFedLLM Model Hub, and FedLLM Privacy Hub. Figure 2\noverviews the FATE-LLM system.\nFigure 2: Components of the FATE-LLM system .\nThe Communication-Efficient Hub integrates a variety\nof communication-efficient methods into FedLLM to re-\nduce the communication cost for training LLMs, includ-\ning parameter-efficiency fine-tuning (PEFT) [Zhang et al. ,\n2022b ]methods (e.g., Adapter Tuning [Cai et al. , 2022 ]\nand Prompt Tuning [Zhao et al. , 2022 ], Knowledge Distilla-\ntion(KD) [Wuet al. , 2022 ], and Model Quantization [Zhang\net al. , 2018 ]. More specifically, [Zhang et al. , 2022b ]pro-\nposed PETuning methods that can reduce the communication\noverhead by 1\u223c2orders of magnitude under the FL set-\nting compared with full fine-tuning. They also found that\nPETuning methods can bring down local model adaptation\ncosts for clients in FL systems. These results imply that FL\nclients (e.g., devices) with limited storage capacity can bene-\nfit from PETuning methods since these methods enable shar-\ning an LLM across different tasks and maintaining a few pa-\nrameters for each task, reducing the storage requirement.\nThe FedLLM Model Hub integrates a variety of main-\nstream LLMs, including BERT [Devlin et al. , 2018 ],\nGPTs [Radford et al. , 2018 ], ChatGLM-6B [Duet al. , 2022 ],\nLLaMA [Touvron et al. , 2023 ], BLOOM [Scao et al. , 2022 ],\nand Baichuan [Yang et al. , 2023a ]. These LLMs have dif-\nferent architectures and sizes and can be applied in different\nscenarios.\nThe FedLLM Trainer Hub offers a variety of training\nmethods for different federated LLMs learning scenarios,\nincluding FedHomoLLM, FedHeteroLLM, FedCoLLM, and\nFedOST.\nIn FL, clients may have sufficient computing resources\nto train LLMs of the same size. However, in many het-\nerogeneous scenarios, clients are likely to have quite differ-\nent computing or data resources so that they can afford to\ntrain LLMs of quite different sizes. FATE-LLM offers Fed-\nerated Homogeneous LLMs (FedHomoLLM) and Federated\n1FATE-LLM was open-sourced in April 2023 in the FATE Com-\nmunity and is running on the infrastructure of FATE.", "mimetype": "text/plain", "start_char_idx": 3307, "end_char_idx": 5545, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "719c4cbe-79d0-4e39-94b9-07deda9c7041": {"__data__": {"id_": "719c4cbe-79d0-4e39-94b9-07deda9c7041", "embedding": null, "metadata": {"page_label": "3", "file_name": "2310_10049v1.pdf", "Title of this paper": "FATE-LLM: A Industrial Grade Federated Learning Framework for Large Language Models", "Authors": "Tao Fan, Yan Kang, Guoqiang Ma, Weijing Chen, Wenbin Wei, Lixin Fan, Qiang Yang", "Date published": "10/16/2023", "URL": "http://arxiv.org/abs/2310.10049v1", "summary": "Large Language Models (LLMs), such as ChatGPT, LLaMA, GLM, and PaLM, have\nexhibited remarkable performances across various tasks in recent years.\nHowever, LLMs face two main challenges in real-world applications. One\nchallenge is that training LLMs consumes vast computing resources, preventing\nLLMs from being adopted by small and medium-sized enterprises with limited\ncomputing resources. Another is that training LLM requires a large amount of\nhigh-quality data, which are often scattered among enterprises. To address\nthese challenges, we propose FATE-LLM, an industrial-grade federated learning\nframework for large language models. FATE-LLM (1) facilitates federated\nlearning for large language models (coined FedLLM); (2) promotes efficient\ntraining of FedLLM using parameter-efficient fine-tuning methods; (3) protects\nthe intellectual property of LLMs; (4) preserves data privacy during training\nand inference through privacy-preserving mechanisms. We release the code of\nFATE-LLM at https://github.com/FederatedAI/FATE-LLM to facilitate the research\nof FedLLM and enable a broad range of industrial applications."}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1b8839fa-acf9-4a7d-93ec-66295ad759b6", "node_type": "4", "metadata": {"page_label": "3", "file_name": "2310_10049v1.pdf", "Title of this paper": "FATE-LLM: A Industrial Grade Federated Learning Framework for Large Language Models", "Authors": "Tao Fan, Yan Kang, Guoqiang Ma, Weijing Chen, Wenbin Wei, Lixin Fan, Qiang Yang", "Date published": "10/16/2023", "URL": "http://arxiv.org/abs/2310.10049v1", "summary": "Large Language Models (LLMs), such as ChatGPT, LLaMA, GLM, and PaLM, have\nexhibited remarkable performances across various tasks in recent years.\nHowever, LLMs face two main challenges in real-world applications. One\nchallenge is that training LLMs consumes vast computing resources, preventing\nLLMs from being adopted by small and medium-sized enterprises with limited\ncomputing resources. Another is that training LLM requires a large amount of\nhigh-quality data, which are often scattered among enterprises. To address\nthese challenges, we propose FATE-LLM, an industrial-grade federated learning\nframework for large language models. FATE-LLM (1) facilitates federated\nlearning for large language models (coined FedLLM); (2) promotes efficient\ntraining of FedLLM using parameter-efficient fine-tuning methods; (3) protects\nthe intellectual property of LLMs; (4) preserves data privacy during training\nand inference through privacy-preserving mechanisms. We release the code of\nFATE-LLM at https://github.com/FederatedAI/FATE-LLM to facilitate the research\nof FedLLM and enable a broad range of industrial applications."}, "hash": "e80d8a9267c87f09e24e1492838383dd4d4973409b0fb6b650ca29c4a1937fb0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "065cab32-39be-485b-a77d-c955b06df5a2", "node_type": "1", "metadata": {}, "hash": "8a16684f7a794dc2644152534e68c6a0fec79e6176f2be8481e8d613c3993088", "class_name": "RelatedNodeInfo"}}, "text": "(a)FedHomoLLM (Federated homogeneous LLMs): Clients have\nLLMs with the same architecture leverage PEFT to train their\nLLMs.\n(b)FedHeteroLLM (Federated Heterogeneous LLMs): Clients\nhave LLMs with different architecture leverage knowledge distil-\nlation and PEFT to train their LLMs.\n(c)FedCoLLM (Federated Co-tuning LLMs): Not only clients but\nalso the server owns LLMs. They leverage PEFT and knowledge\ndistillation to fine-tune their LLMs.\n(d)FedOST (Federated OffSite-Tuning): Clients transfer their\nknowledge to the LLM hosted by the server through offsite-tuning\nin a federated way.\nFigure 3: FATE-LLM Trainers. FATE-LLM offers four trainers for four different federated LLM learning scenarios.\nHeterogeneous LLMs (FedHeteroLLM) to support both sce-\nnarios. FedHomoLLM leverages PEFT techniques to train\nclients\u2019 LLMs with the same architecture and size (illustrated\nin Figure 3(a)). FedHeteroLLM leverages knowledge distil-\nlation (KD) [Shen et al. , 2020 ]and PEFT techniques to deal\nwith the FL scenario where FL clients own LLMs of different\nsizes (illustrated in Figure 3(b)). Specifically, each client in\nFedHeteroLLM leverages KD to learn a mentee model from\nits local pre-trained LLM. Then, all clients send adaptor or\nprompt parameters to the server for secure aggregation. Next,\nthe server dispatches the aggregated model to all clients for\nthe next round of training.\nInitializing clients with an LLM distilled from a larger one\nhosted by the server enables federated LLMs to obtain a bet-\nter global model more efficiently than starting clients\u2019 models\nfrom random initialization [Wang et al. , 2023 ]. On the other\nhand, the domain knowledge captured by clients\u2019 local LLMs\nallows the server\u2019s larger LLM to continue to evolve. FATE\noffers the FedCoLLM (Federated Co-tuning LLM) frame-\nwork to co-evolve the LLMs of the server and clients. Figure\n3(c) illustrates the FedCoLLM. Specifically, in FedCoLLM,\neach client having a LLaMa-7B model conducts federated\nlearning applying PEFT techniques. On the server side, theserver distills the knowledge between its LLaMa-65B model\nand the aggregated LLaMa-7B mode to co-evolve models on\nboth sides.\n[Xiao et al. , 2023 ]proposed Offsite-Tuning, a privacy-\npreserving and efficient transfer learning framework that can\nadapt an LLM to downstream tasks without access to the\nLLM\u2019s full weights.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2354, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "065cab32-39be-485b-a77d-c955b06df5a2": {"__data__": {"id_": "065cab32-39be-485b-a77d-c955b06df5a2", "embedding": null, "metadata": {"page_label": "3", "file_name": "2310_10049v1.pdf", "Title of this paper": "FATE-LLM: A Industrial Grade Federated Learning Framework for Large Language Models", "Authors": "Tao Fan, Yan Kang, Guoqiang Ma, Weijing Chen, Wenbin Wei, Lixin Fan, Qiang Yang", "Date published": "10/16/2023", "URL": "http://arxiv.org/abs/2310.10049v1", "summary": "Large Language Models (LLMs), such as ChatGPT, LLaMA, GLM, and PaLM, have\nexhibited remarkable performances across various tasks in recent years.\nHowever, LLMs face two main challenges in real-world applications. One\nchallenge is that training LLMs consumes vast computing resources, preventing\nLLMs from being adopted by small and medium-sized enterprises with limited\ncomputing resources. Another is that training LLM requires a large amount of\nhigh-quality data, which are often scattered among enterprises. To address\nthese challenges, we propose FATE-LLM, an industrial-grade federated learning\nframework for large language models. FATE-LLM (1) facilitates federated\nlearning for large language models (coined FedLLM); (2) promotes efficient\ntraining of FedLLM using parameter-efficient fine-tuning methods; (3) protects\nthe intellectual property of LLMs; (4) preserves data privacy during training\nand inference through privacy-preserving mechanisms. We release the code of\nFATE-LLM at https://github.com/FederatedAI/FATE-LLM to facilitate the research\nof FedLLM and enable a broad range of industrial applications."}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1b8839fa-acf9-4a7d-93ec-66295ad759b6", "node_type": "4", "metadata": {"page_label": "3", "file_name": "2310_10049v1.pdf", "Title of this paper": "FATE-LLM: A Industrial Grade Federated Learning Framework for Large Language Models", "Authors": "Tao Fan, Yan Kang, Guoqiang Ma, Weijing Chen, Wenbin Wei, Lixin Fan, Qiang Yang", "Date published": "10/16/2023", "URL": "http://arxiv.org/abs/2310.10049v1", "summary": "Large Language Models (LLMs), such as ChatGPT, LLaMA, GLM, and PaLM, have\nexhibited remarkable performances across various tasks in recent years.\nHowever, LLMs face two main challenges in real-world applications. One\nchallenge is that training LLMs consumes vast computing resources, preventing\nLLMs from being adopted by small and medium-sized enterprises with limited\ncomputing resources. Another is that training LLM requires a large amount of\nhigh-quality data, which are often scattered among enterprises. To address\nthese challenges, we propose FATE-LLM, an industrial-grade federated learning\nframework for large language models. FATE-LLM (1) facilitates federated\nlearning for large language models (coined FedLLM); (2) promotes efficient\ntraining of FedLLM using parameter-efficient fine-tuning methods; (3) protects\nthe intellectual property of LLMs; (4) preserves data privacy during training\nand inference through privacy-preserving mechanisms. We release the code of\nFATE-LLM at https://github.com/FederatedAI/FATE-LLM to facilitate the research\nof FedLLM and enable a broad range of industrial applications."}, "hash": "e80d8a9267c87f09e24e1492838383dd4d4973409b0fb6b650ca29c4a1937fb0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "719c4cbe-79d0-4e39-94b9-07deda9c7041", "node_type": "1", "metadata": {"page_label": "3", "file_name": "2310_10049v1.pdf", "Title of this paper": "FATE-LLM: A Industrial Grade Federated Learning Framework for Large Language Models", "Authors": "Tao Fan, Yan Kang, Guoqiang Ma, Weijing Chen, Wenbin Wei, Lixin Fan, Qiang Yang", "Date published": "10/16/2023", "URL": "http://arxiv.org/abs/2310.10049v1", "summary": "Large Language Models (LLMs), such as ChatGPT, LLaMA, GLM, and PaLM, have\nexhibited remarkable performances across various tasks in recent years.\nHowever, LLMs face two main challenges in real-world applications. One\nchallenge is that training LLMs consumes vast computing resources, preventing\nLLMs from being adopted by small and medium-sized enterprises with limited\ncomputing resources. Another is that training LLM requires a large amount of\nhigh-quality data, which are often scattered among enterprises. To address\nthese challenges, we propose FATE-LLM, an industrial-grade federated learning\nframework for large language models. FATE-LLM (1) facilitates federated\nlearning for large language models (coined FedLLM); (2) promotes efficient\ntraining of FedLLM using parameter-efficient fine-tuning methods; (3) protects\nthe intellectual property of LLMs; (4) preserves data privacy during training\nand inference through privacy-preserving mechanisms. We release the code of\nFATE-LLM at https://github.com/FederatedAI/FATE-LLM to facilitate the research\nof FedLLM and enable a broad range of industrial applications."}, "hash": "de9807c5daba6981174055a1a62bdde8516d92fa75bdbe72428d9135a29c889f", "class_name": "RelatedNodeInfo"}}, "text": "FATE\noffers the FedCoLLM (Federated Co-tuning LLM) frame-\nwork to co-evolve the LLMs of the server and clients. Figure\n3(c) illustrates the FedCoLLM. Specifically, in FedCoLLM,\neach client having a LLaMa-7B model conducts federated\nlearning applying PEFT techniques. On the server side, theserver distills the knowledge between its LLaMa-65B model\nand the aggregated LLaMa-7B mode to co-evolve models on\nboth sides.\n[Xiao et al. , 2023 ]proposed Offsite-Tuning, a privacy-\npreserving and efficient transfer learning framework that can\nadapt an LLM to downstream tasks without access to the\nLLM\u2019s full weights. More specifically, in Offsite-Tuning,\nthe server sends two adaptors and an emulator of its LLM\nto a client, which in turn finetunes adaptors with the help of\nthe frozen emulator using its domain-specific data. Next, the\nclient sends adaptors back to the server, which then plugs\nthem into its LLM to form an adapted LLM for the client.\nThe Offsite-Tuning has the potential to protect the client\u2019s\ndata privacy and the server\u2019s model property.\nFATE-LLM offers the FedOST (Federated OffSite-Tuning)\nthat extends the Offsite-Tuning framework to the federated\nlearning setting (see Figure 3(d)). In FedOST, multiple clients\ncollaboratively train two global adaptors that adapt the LLM\nto all clients. FedOST brings two additional benefits than\nOffsite-Tuning: (1) FedOST enhances data privacy by adopt-\ning secure aggregation, and (2) it adapts an LLM to clients\nthat did not even participate in the FL because of the general-\nization of the FL global model.", "mimetype": "text/plain", "start_char_idx": 1745, "end_char_idx": 3309, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "888fcd67-9acf-4500-9b52-ebb34688c2da": {"__data__": {"id_": "888fcd67-9acf-4500-9b52-ebb34688c2da", "embedding": null, "metadata": {"page_label": "4", "file_name": "2310_10049v1.pdf", "Title of this paper": "FATE-LLM: A Industrial Grade Federated Learning Framework for Large Language Models", "Authors": "Tao Fan, Yan Kang, Guoqiang Ma, Weijing Chen, Wenbin Wei, Lixin Fan, Qiang Yang", "Date published": "10/16/2023", "URL": "http://arxiv.org/abs/2310.10049v1", "summary": "Large Language Models (LLMs), such as ChatGPT, LLaMA, GLM, and PaLM, have\nexhibited remarkable performances across various tasks in recent years.\nHowever, LLMs face two main challenges in real-world applications. One\nchallenge is that training LLMs consumes vast computing resources, preventing\nLLMs from being adopted by small and medium-sized enterprises with limited\ncomputing resources. Another is that training LLM requires a large amount of\nhigh-quality data, which are often scattered among enterprises. To address\nthese challenges, we propose FATE-LLM, an industrial-grade federated learning\nframework for large language models. FATE-LLM (1) facilitates federated\nlearning for large language models (coined FedLLM); (2) promotes efficient\ntraining of FedLLM using parameter-efficient fine-tuning methods; (3) protects\nthe intellectual property of LLMs; (4) preserves data privacy during training\nand inference through privacy-preserving mechanisms. We release the code of\nFATE-LLM at https://github.com/FederatedAI/FATE-LLM to facilitate the research\nof FedLLM and enable a broad range of industrial applications."}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e4b7e2c9-78ec-4562-8341-4365b109bf5b", "node_type": "4", "metadata": {"page_label": "4", "file_name": "2310_10049v1.pdf", "Title of this paper": "FATE-LLM: A Industrial Grade Federated Learning Framework for Large Language Models", "Authors": "Tao Fan, Yan Kang, Guoqiang Ma, Weijing Chen, Wenbin Wei, Lixin Fan, Qiang Yang", "Date published": "10/16/2023", "URL": "http://arxiv.org/abs/2310.10049v1", "summary": "Large Language Models (LLMs), such as ChatGPT, LLaMA, GLM, and PaLM, have\nexhibited remarkable performances across various tasks in recent years.\nHowever, LLMs face two main challenges in real-world applications. One\nchallenge is that training LLMs consumes vast computing resources, preventing\nLLMs from being adopted by small and medium-sized enterprises with limited\ncomputing resources. Another is that training LLM requires a large amount of\nhigh-quality data, which are often scattered among enterprises. To address\nthese challenges, we propose FATE-LLM, an industrial-grade federated learning\nframework for large language models. FATE-LLM (1) facilitates federated\nlearning for large language models (coined FedLLM); (2) promotes efficient\ntraining of FedLLM using parameter-efficient fine-tuning methods; (3) protects\nthe intellectual property of LLMs; (4) preserves data privacy during training\nand inference through privacy-preserving mechanisms. We release the code of\nFATE-LLM at https://github.com/FederatedAI/FATE-LLM to facilitate the research\nof FedLLM and enable a broad range of industrial applications."}, "hash": "31d886b05a9dbd5e37ee8a57bf95b17f14369a7a06776352e9da511eeaa3b933", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "09007ec3-8f0c-4075-9a16-d8ea9400d11d", "node_type": "1", "metadata": {}, "hash": "b0a6daaf08fbef395ae1890a81177a403705abdfb812d280169ddf57a93cbc33", "class_name": "RelatedNodeInfo"}}, "text": "The FedLLM Privacy Hub integrates various privacy and\nsecurity protection technologies, including federated intellec-\ntual property protection (FedIPR) [Liet al. , 2022 ], secure ag-\ngregation (SecureAgg) [McMahan et al. , 2017 ], Differential\nPrivacy (DP) and Multi-Party Computation (MPC) to protect\ndata privacy and model security. Specifically, FedIPR [Liet\nal., 2022 ]proposed a federated deep neural network owner-\nship verification scheme that enables private watermarks to\nbe embedded into private DNN models during FL training\n(see Figure 4) such that each client can independently verify\nthe existence of embedded watermarks and claim its owner-\nship of the federated model without disclosing private train-\ning data and watermark information. FedIPR can be applied\nto FedLLM to verify the IP ownership of the federated LLMs.\nSecureAgg, DP, and MPC can be applied to FedLLM during\ntraining and fine-tuning to protect clients\u2019 data privacy.\nFigure 4: FedIPR! [Liet al. , 2022 ]. Private watermarks are gen-\nerated and embedded into the trainable parameters (i.e., adaptors or\nprompts) of local large language models. Then, trainable parameters\nare aggregated through FedAvg.\n3.2 Architecture of FATE-LLM\nFATE-LLM is running on the infrastructure of FATE, which\nconsists of FATE-Flow, Eggroll, and OSX as the main com-\nponents. FATE-Flow is a task scheduling engine for the\nmulti-party federated learning end-to-end pipeline, Eggroll\nis the distributed computing engine, and OSX (open site ex-\nchange) is the multi-party federated communication engine.\nFATE-LLM Algorithm Hub and LLM Optim Lib Hub are\ntailored to perform FedLLM. FATE-LLM Algorithm Hub in-\ncludes Communication-Efficient Hub, FedLLM Model Hub,\nand FedLLM Privacy Hub (see Figure 2). LLM Optim\nLib Hub includes DeepSpeed and Megatron-LM. As of June\n2023, FATE has integrated DeepSpeed into Eggroll, which\ncan manage the GPUs cluster well and dispatch DeepSpeed\nLLMs tasks. Figure 5 shows the architecture of FATE-LLM.\n3.3 RoadMap of FATE-LLM\nWe present the roadmap of FATE-LLM in Figure 6. As\nof June 2023, three versions of FTE-LLM have been re-\nleased: FATE-LLM 1.0, FATE-LLM 1.1, and FATE-LLM\n1.2. The three versions integrate Bert, GPT-2, ChatGLM-6B,\nand LLaMA, consecutively, and adopt FedIPR and privacy-\npreserving techniques to protect data privacy and model own-\nership.4 Experiments\nWe conduct experiments on the scenario in which each client\nowns a ChatGLM-6B [Duet al.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2454, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "09007ec3-8f0c-4075-9a16-d8ea9400d11d": {"__data__": {"id_": "09007ec3-8f0c-4075-9a16-d8ea9400d11d", "embedding": null, "metadata": {"page_label": "4", "file_name": "2310_10049v1.pdf", "Title of this paper": "FATE-LLM: A Industrial Grade Federated Learning Framework for Large Language Models", "Authors": "Tao Fan, Yan Kang, Guoqiang Ma, Weijing Chen, Wenbin Wei, Lixin Fan, Qiang Yang", "Date published": "10/16/2023", "URL": "http://arxiv.org/abs/2310.10049v1", "summary": "Large Language Models (LLMs), such as ChatGPT, LLaMA, GLM, and PaLM, have\nexhibited remarkable performances across various tasks in recent years.\nHowever, LLMs face two main challenges in real-world applications. One\nchallenge is that training LLMs consumes vast computing resources, preventing\nLLMs from being adopted by small and medium-sized enterprises with limited\ncomputing resources. Another is that training LLM requires a large amount of\nhigh-quality data, which are often scattered among enterprises. To address\nthese challenges, we propose FATE-LLM, an industrial-grade federated learning\nframework for large language models. FATE-LLM (1) facilitates federated\nlearning for large language models (coined FedLLM); (2) promotes efficient\ntraining of FedLLM using parameter-efficient fine-tuning methods; (3) protects\nthe intellectual property of LLMs; (4) preserves data privacy during training\nand inference through privacy-preserving mechanisms. We release the code of\nFATE-LLM at https://github.com/FederatedAI/FATE-LLM to facilitate the research\nof FedLLM and enable a broad range of industrial applications."}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e4b7e2c9-78ec-4562-8341-4365b109bf5b", "node_type": "4", "metadata": {"page_label": "4", "file_name": "2310_10049v1.pdf", "Title of this paper": "FATE-LLM: A Industrial Grade Federated Learning Framework for Large Language Models", "Authors": "Tao Fan, Yan Kang, Guoqiang Ma, Weijing Chen, Wenbin Wei, Lixin Fan, Qiang Yang", "Date published": "10/16/2023", "URL": "http://arxiv.org/abs/2310.10049v1", "summary": "Large Language Models (LLMs), such as ChatGPT, LLaMA, GLM, and PaLM, have\nexhibited remarkable performances across various tasks in recent years.\nHowever, LLMs face two main challenges in real-world applications. One\nchallenge is that training LLMs consumes vast computing resources, preventing\nLLMs from being adopted by small and medium-sized enterprises with limited\ncomputing resources. Another is that training LLM requires a large amount of\nhigh-quality data, which are often scattered among enterprises. To address\nthese challenges, we propose FATE-LLM, an industrial-grade federated learning\nframework for large language models. FATE-LLM (1) facilitates federated\nlearning for large language models (coined FedLLM); (2) promotes efficient\ntraining of FedLLM using parameter-efficient fine-tuning methods; (3) protects\nthe intellectual property of LLMs; (4) preserves data privacy during training\nand inference through privacy-preserving mechanisms. We release the code of\nFATE-LLM at https://github.com/FederatedAI/FATE-LLM to facilitate the research\nof FedLLM and enable a broad range of industrial applications."}, "hash": "31d886b05a9dbd5e37ee8a57bf95b17f14369a7a06776352e9da511eeaa3b933", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "888fcd67-9acf-4500-9b52-ebb34688c2da", "node_type": "1", "metadata": {"page_label": "4", "file_name": "2310_10049v1.pdf", "Title of this paper": "FATE-LLM: A Industrial Grade Federated Learning Framework for Large Language Models", "Authors": "Tao Fan, Yan Kang, Guoqiang Ma, Weijing Chen, Wenbin Wei, Lixin Fan, Qiang Yang", "Date published": "10/16/2023", "URL": "http://arxiv.org/abs/2310.10049v1", "summary": "Large Language Models (LLMs), such as ChatGPT, LLaMA, GLM, and PaLM, have\nexhibited remarkable performances across various tasks in recent years.\nHowever, LLMs face two main challenges in real-world applications. One\nchallenge is that training LLMs consumes vast computing resources, preventing\nLLMs from being adopted by small and medium-sized enterprises with limited\ncomputing resources. Another is that training LLM requires a large amount of\nhigh-quality data, which are often scattered among enterprises. To address\nthese challenges, we propose FATE-LLM, an industrial-grade federated learning\nframework for large language models. FATE-LLM (1) facilitates federated\nlearning for large language models (coined FedLLM); (2) promotes efficient\ntraining of FedLLM using parameter-efficient fine-tuning methods; (3) protects\nthe intellectual property of LLMs; (4) preserves data privacy during training\nand inference through privacy-preserving mechanisms. We release the code of\nFATE-LLM at https://github.com/FederatedAI/FATE-LLM to facilitate the research\nof FedLLM and enable a broad range of industrial applications."}, "hash": "d57af6e66a620b2ae0b3c184a879e6e011448d6be33a8b11926911b5da7e2341", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3c08a56c-f2ca-488b-9d17-d00d9239e364", "node_type": "1", "metadata": {}, "hash": "38dd3207fb554c047e15feadd6c8ea6d717ed357aed9ec01c94719513956d3c3", "class_name": "RelatedNodeInfo"}}, "text": "Figure 5 shows the architecture of FATE-LLM.\n3.3 RoadMap of FATE-LLM\nWe present the roadmap of FATE-LLM in Figure 6. As\nof June 2023, three versions of FTE-LLM have been re-\nleased: FATE-LLM 1.0, FATE-LLM 1.1, and FATE-LLM\n1.2. The three versions integrate Bert, GPT-2, ChatGLM-6B,\nand LLaMA, consecutively, and adopt FedIPR and privacy-\npreserving techniques to protect data privacy and model own-\nership.4 Experiments\nWe conduct experiments on the scenario in which each client\nowns a ChatGLM-6B [Duet al. , 2022 ]model, and all clients\nwant to fine-tune their models collaboratively through feder-\nated learning. Since fine-tuning all parameters of ChatGLM-\n6B involves huge computational and communication costs,\nall clients leverage a PETuning method to only fine-tune a\nsmall portion of the ChatGLM-6B parameters through feder-\nated learning.\nWe leverage our FedLLM modules to conduct these exper-\niments using both LoRA [Huet al. , 2021 ]andP-Tuning-v2\n[Liuet al. , 2021a ]. Figure 7 illustrates this scenario we con-\nduct our experiments on.\n4.1 Experimental Setup\nWe detail the experimental setup, including the dataset, FL\nsetting, and baselines.\nDataset and setting . We conduct experiments on Adver-\ntiseGen [Shao et al. , 2019 ], a dataset for advertising text gen-\neration. We simulate the FL setting with 2 clients and ran-\ndomly split the AdvertiseGen dataset such that each client\nhas 57K samples. Each client is assigned 8 NVIDIA V100\nand trained on DeepSpeed. We set the FL training epoch to 5\nand run the experiments in the LAN network environment.\nBaselines . We adopt two types of baselines. One is cen-\ntralized , in which data of all clients are centralized to conduct\nfine-tuning (either LoRA or P-Tuning-v2) on a ChatGLM-6B\nmodel. The another is that each client uses local data to fine-\ntune its local ChatGLM-6B model.\nEvaluation metrics . We adopt Rouge-1, Rouge-2, Rouge-\nl[Lin, 2004 ]and BLEU-4 [Papineni et al. , 2002 ]to evaluate\nthe performance of fine-tined LLMs.\n4.2 Experiment Results\nModel Performance\nThe experimental results for FedLLM using LoRA and P-\nTuning-v2 are reported in Table 1 and Table 2, respectively,\nwhich show that LoRA Federated and P-Tuning-v2 Feder-\nated generally outperform their individual client counterparts\nacross all performance metrics, demonstrating that federated\nlearning help enhance the fine-tuning performance for each\nclient.", "mimetype": "text/plain", "start_char_idx": 1947, "end_char_idx": 4346, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3c08a56c-f2ca-488b-9d17-d00d9239e364": {"__data__": {"id_": "3c08a56c-f2ca-488b-9d17-d00d9239e364", "embedding": null, "metadata": {"page_label": "4", "file_name": "2310_10049v1.pdf", "Title of this paper": "FATE-LLM: A Industrial Grade Federated Learning Framework for Large Language Models", "Authors": "Tao Fan, Yan Kang, Guoqiang Ma, Weijing Chen, Wenbin Wei, Lixin Fan, Qiang Yang", "Date published": "10/16/2023", "URL": "http://arxiv.org/abs/2310.10049v1", "summary": "Large Language Models (LLMs), such as ChatGPT, LLaMA, GLM, and PaLM, have\nexhibited remarkable performances across various tasks in recent years.\nHowever, LLMs face two main challenges in real-world applications. One\nchallenge is that training LLMs consumes vast computing resources, preventing\nLLMs from being adopted by small and medium-sized enterprises with limited\ncomputing resources. Another is that training LLM requires a large amount of\nhigh-quality data, which are often scattered among enterprises. To address\nthese challenges, we propose FATE-LLM, an industrial-grade federated learning\nframework for large language models. FATE-LLM (1) facilitates federated\nlearning for large language models (coined FedLLM); (2) promotes efficient\ntraining of FedLLM using parameter-efficient fine-tuning methods; (3) protects\nthe intellectual property of LLMs; (4) preserves data privacy during training\nand inference through privacy-preserving mechanisms. We release the code of\nFATE-LLM at https://github.com/FederatedAI/FATE-LLM to facilitate the research\nof FedLLM and enable a broad range of industrial applications."}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e4b7e2c9-78ec-4562-8341-4365b109bf5b", "node_type": "4", "metadata": {"page_label": "4", "file_name": "2310_10049v1.pdf", "Title of this paper": "FATE-LLM: A Industrial Grade Federated Learning Framework for Large Language Models", "Authors": "Tao Fan, Yan Kang, Guoqiang Ma, Weijing Chen, Wenbin Wei, Lixin Fan, Qiang Yang", "Date published": "10/16/2023", "URL": "http://arxiv.org/abs/2310.10049v1", "summary": "Large Language Models (LLMs), such as ChatGPT, LLaMA, GLM, and PaLM, have\nexhibited remarkable performances across various tasks in recent years.\nHowever, LLMs face two main challenges in real-world applications. One\nchallenge is that training LLMs consumes vast computing resources, preventing\nLLMs from being adopted by small and medium-sized enterprises with limited\ncomputing resources. Another is that training LLM requires a large amount of\nhigh-quality data, which are often scattered among enterprises. To address\nthese challenges, we propose FATE-LLM, an industrial-grade federated learning\nframework for large language models. FATE-LLM (1) facilitates federated\nlearning for large language models (coined FedLLM); (2) promotes efficient\ntraining of FedLLM using parameter-efficient fine-tuning methods; (3) protects\nthe intellectual property of LLMs; (4) preserves data privacy during training\nand inference through privacy-preserving mechanisms. We release the code of\nFATE-LLM at https://github.com/FederatedAI/FATE-LLM to facilitate the research\nof FedLLM and enable a broad range of industrial applications."}, "hash": "31d886b05a9dbd5e37ee8a57bf95b17f14369a7a06776352e9da511eeaa3b933", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "09007ec3-8f0c-4075-9a16-d8ea9400d11d", "node_type": "1", "metadata": {"page_label": "4", "file_name": "2310_10049v1.pdf", "Title of this paper": "FATE-LLM: A Industrial Grade Federated Learning Framework for Large Language Models", "Authors": "Tao Fan, Yan Kang, Guoqiang Ma, Weijing Chen, Wenbin Wei, Lixin Fan, Qiang Yang", "Date published": "10/16/2023", "URL": "http://arxiv.org/abs/2310.10049v1", "summary": "Large Language Models (LLMs), such as ChatGPT, LLaMA, GLM, and PaLM, have\nexhibited remarkable performances across various tasks in recent years.\nHowever, LLMs face two main challenges in real-world applications. One\nchallenge is that training LLMs consumes vast computing resources, preventing\nLLMs from being adopted by small and medium-sized enterprises with limited\ncomputing resources. Another is that training LLM requires a large amount of\nhigh-quality data, which are often scattered among enterprises. To address\nthese challenges, we propose FATE-LLM, an industrial-grade federated learning\nframework for large language models. FATE-LLM (1) facilitates federated\nlearning for large language models (coined FedLLM); (2) promotes efficient\ntraining of FedLLM using parameter-efficient fine-tuning methods; (3) protects\nthe intellectual property of LLMs; (4) preserves data privacy during training\nand inference through privacy-preserving mechanisms. We release the code of\nFATE-LLM at https://github.com/FederatedAI/FATE-LLM to facilitate the research\nof FedLLM and enable a broad range of industrial applications."}, "hash": "eea538f05deaac79545c4578eacf0da18cb60675cbba7c6e651bed2e2f1c89ad", "class_name": "RelatedNodeInfo"}}, "text": "The another is that each client uses local data to fine-\ntune its local ChatGLM-6B model.\nEvaluation metrics . We adopt Rouge-1, Rouge-2, Rouge-\nl[Lin, 2004 ]and BLEU-4 [Papineni et al. , 2002 ]to evaluate\nthe performance of fine-tined LLMs.\n4.2 Experiment Results\nModel Performance\nThe experimental results for FedLLM using LoRA and P-\nTuning-v2 are reported in Table 1 and Table 2, respectively,\nwhich show that LoRA Federated and P-Tuning-v2 Feder-\nated generally outperform their individual client counterparts\nacross all performance metrics, demonstrating that federated\nlearning help enhance the fine-tuning performance for each\nclient. From Table 1 and Table 2, we also observe that the\nperformance of LoRA and P-Tuning-v2 federated fine-tuning\nare generally worse than their centralized counterparts across\nall performance metrics, indicating that there has room to im-\nprove federated fine-tuning methods.\nFigure 5: Architecture of the FATE-LLM system .", "mimetype": "text/plain", "start_char_idx": 3704, "end_char_idx": 4666, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5a67e0e5-e8b3-4569-9489-fc5a1899695e": {"__data__": {"id_": "5a67e0e5-e8b3-4569-9489-fc5a1899695e", "embedding": null, "metadata": {"page_label": "5", "file_name": "2310_10049v1.pdf", "Title of this paper": "FATE-LLM: A Industrial Grade Federated Learning Framework for Large Language Models", "Authors": "Tao Fan, Yan Kang, Guoqiang Ma, Weijing Chen, Wenbin Wei, Lixin Fan, Qiang Yang", "Date published": "10/16/2023", "URL": "http://arxiv.org/abs/2310.10049v1", "summary": "Large Language Models (LLMs), such as ChatGPT, LLaMA, GLM, and PaLM, have\nexhibited remarkable performances across various tasks in recent years.\nHowever, LLMs face two main challenges in real-world applications. One\nchallenge is that training LLMs consumes vast computing resources, preventing\nLLMs from being adopted by small and medium-sized enterprises with limited\ncomputing resources. Another is that training LLM requires a large amount of\nhigh-quality data, which are often scattered among enterprises. To address\nthese challenges, we propose FATE-LLM, an industrial-grade federated learning\nframework for large language models. FATE-LLM (1) facilitates federated\nlearning for large language models (coined FedLLM); (2) promotes efficient\ntraining of FedLLM using parameter-efficient fine-tuning methods; (3) protects\nthe intellectual property of LLMs; (4) preserves data privacy during training\nand inference through privacy-preserving mechanisms. We release the code of\nFATE-LLM at https://github.com/FederatedAI/FATE-LLM to facilitate the research\nof FedLLM and enable a broad range of industrial applications."}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f865cc78-890f-4f46-9953-7e3633ecafc3", "node_type": "4", "metadata": {"page_label": "5", "file_name": "2310_10049v1.pdf", "Title of this paper": "FATE-LLM: A Industrial Grade Federated Learning Framework for Large Language Models", "Authors": "Tao Fan, Yan Kang, Guoqiang Ma, Weijing Chen, Wenbin Wei, Lixin Fan, Qiang Yang", "Date published": "10/16/2023", "URL": "http://arxiv.org/abs/2310.10049v1", "summary": "Large Language Models (LLMs), such as ChatGPT, LLaMA, GLM, and PaLM, have\nexhibited remarkable performances across various tasks in recent years.\nHowever, LLMs face two main challenges in real-world applications. One\nchallenge is that training LLMs consumes vast computing resources, preventing\nLLMs from being adopted by small and medium-sized enterprises with limited\ncomputing resources. Another is that training LLM requires a large amount of\nhigh-quality data, which are often scattered among enterprises. To address\nthese challenges, we propose FATE-LLM, an industrial-grade federated learning\nframework for large language models. FATE-LLM (1) facilitates federated\nlearning for large language models (coined FedLLM); (2) promotes efficient\ntraining of FedLLM using parameter-efficient fine-tuning methods; (3) protects\nthe intellectual property of LLMs; (4) preserves data privacy during training\nand inference through privacy-preserving mechanisms. We release the code of\nFATE-LLM at https://github.com/FederatedAI/FATE-LLM to facilitate the research\nof FedLLM and enable a broad range of industrial applications."}, "hash": "9dc9895ee79e8c9e334cee92a955493f0d747515720908f587f20e4f6592147a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "574016cd-f196-433b-9fb7-72d1be23d22b", "node_type": "1", "metadata": {}, "hash": "19b4dd1e1c59f16f1c1076022f35610d7a118844bbc61ee9304829c7b7383a13", "class_name": "RelatedNodeInfo"}}, "text": "Metrics LoRA Federated LoRA Centralized LoRA Client-1 LoRA Client-2\nRouge-1 32.331 32.384 31.824 31.764\nRouge-2 7.740 8.150 7.849 7.765\nRouge- l 25.600 25.830 25.408 25.404\nBLEU-4 8.344 8.730 8.340 8.366\nTable 1: FedLLM fune-tuning ChatGLM-6B using LoRA.\nMetrics P-Tuning-v2 Federated P-Tuning-v2 Centralized P-Tuning-v2 Client-1 P-Tuning-v2 Client-2\nRouge-1 32.227 32.184 31.362 31.18\nRouge-2 7.644 8.048 7.472 7.478\nRouge- l 25.853 26.010 25.454 25.227\nBLEU-4 8.490 8.851 8.329 8.221\nTable 2: FedLLM fine-tuning ChatGLM-6B using P-Tuning-v2.\nFigure 6: RoadMap of FATE-LLM .\nFigure 7: Multiple clients leverage LoRA or P-Tuning-v2 to fine-\ntine their local ChatGLM-6B models through federated learning.\nCommunication Cost\nWe investigate the communication cost for FedLLM using\nLoRA and P-Tuning-v2 in terms of the size of parameters\nto be fine-tuned. Table 3 reports the results, and it shows\nthat FedLLM using LoRA consumes 0.058% communication\ncost of FedLLM fine-tuning all parameters, while FedLLM\nusing P-Tuning-v2 accounts for 0.475% communication cost\nof FedLLM fine-tuning all parameters.\nMethods Model Size (MB) Param Percent (%)\nLoRA 3.6 0.058\nP-Tuning-v2 29.3 0.475\nFine-tune All 6173 100\nTable 3: Comparison of communication cost for FedLLM fine-\ntuning all parameters of ChatGLM-6B, fine-tuning ChatGLM-6B\nusing LoRA and P-Tuning-v2. Model Size denotes the size of pa-\nrameters to be fine-tuned. Param Percent denotes the ratio of param-\neters to be fine-tuned to all parameters.5 Conclusions and Future Work\nWe proposed FATE-LLM, an industrial-grade federated\nlearning framework for large language models(FedLLM). As\nan open-sourced software, FATE-LLM encourages collabo-\nration among the research and industry communities and ex-\npects to receive increasing feedback on its use.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1794, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "574016cd-f196-433b-9fb7-72d1be23d22b": {"__data__": {"id_": "574016cd-f196-433b-9fb7-72d1be23d22b", "embedding": null, "metadata": {"page_label": "5", "file_name": "2310_10049v1.pdf", "Title of this paper": "FATE-LLM: A Industrial Grade Federated Learning Framework for Large Language Models", "Authors": "Tao Fan, Yan Kang, Guoqiang Ma, Weijing Chen, Wenbin Wei, Lixin Fan, Qiang Yang", "Date published": "10/16/2023", "URL": "http://arxiv.org/abs/2310.10049v1", "summary": "Large Language Models (LLMs), such as ChatGPT, LLaMA, GLM, and PaLM, have\nexhibited remarkable performances across various tasks in recent years.\nHowever, LLMs face two main challenges in real-world applications. One\nchallenge is that training LLMs consumes vast computing resources, preventing\nLLMs from being adopted by small and medium-sized enterprises with limited\ncomputing resources. Another is that training LLM requires a large amount of\nhigh-quality data, which are often scattered among enterprises. To address\nthese challenges, we propose FATE-LLM, an industrial-grade federated learning\nframework for large language models. FATE-LLM (1) facilitates federated\nlearning for large language models (coined FedLLM); (2) promotes efficient\ntraining of FedLLM using parameter-efficient fine-tuning methods; (3) protects\nthe intellectual property of LLMs; (4) preserves data privacy during training\nand inference through privacy-preserving mechanisms. We release the code of\nFATE-LLM at https://github.com/FederatedAI/FATE-LLM to facilitate the research\nof FedLLM and enable a broad range of industrial applications."}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f865cc78-890f-4f46-9953-7e3633ecafc3", "node_type": "4", "metadata": {"page_label": "5", "file_name": "2310_10049v1.pdf", "Title of this paper": "FATE-LLM: A Industrial Grade Federated Learning Framework for Large Language Models", "Authors": "Tao Fan, Yan Kang, Guoqiang Ma, Weijing Chen, Wenbin Wei, Lixin Fan, Qiang Yang", "Date published": "10/16/2023", "URL": "http://arxiv.org/abs/2310.10049v1", "summary": "Large Language Models (LLMs), such as ChatGPT, LLaMA, GLM, and PaLM, have\nexhibited remarkable performances across various tasks in recent years.\nHowever, LLMs face two main challenges in real-world applications. One\nchallenge is that training LLMs consumes vast computing resources, preventing\nLLMs from being adopted by small and medium-sized enterprises with limited\ncomputing resources. Another is that training LLM requires a large amount of\nhigh-quality data, which are often scattered among enterprises. To address\nthese challenges, we propose FATE-LLM, an industrial-grade federated learning\nframework for large language models. FATE-LLM (1) facilitates federated\nlearning for large language models (coined FedLLM); (2) promotes efficient\ntraining of FedLLM using parameter-efficient fine-tuning methods; (3) protects\nthe intellectual property of LLMs; (4) preserves data privacy during training\nand inference through privacy-preserving mechanisms. We release the code of\nFATE-LLM at https://github.com/FederatedAI/FATE-LLM to facilitate the research\nof FedLLM and enable a broad range of industrial applications."}, "hash": "9dc9895ee79e8c9e334cee92a955493f0d747515720908f587f20e4f6592147a", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5a67e0e5-e8b3-4569-9489-fc5a1899695e", "node_type": "1", "metadata": {"page_label": "5", "file_name": "2310_10049v1.pdf", "Title of this paper": "FATE-LLM: A Industrial Grade Federated Learning Framework for Large Language Models", "Authors": "Tao Fan, Yan Kang, Guoqiang Ma, Weijing Chen, Wenbin Wei, Lixin Fan, Qiang Yang", "Date published": "10/16/2023", "URL": "http://arxiv.org/abs/2310.10049v1", "summary": "Large Language Models (LLMs), such as ChatGPT, LLaMA, GLM, and PaLM, have\nexhibited remarkable performances across various tasks in recent years.\nHowever, LLMs face two main challenges in real-world applications. One\nchallenge is that training LLMs consumes vast computing resources, preventing\nLLMs from being adopted by small and medium-sized enterprises with limited\ncomputing resources. Another is that training LLM requires a large amount of\nhigh-quality data, which are often scattered among enterprises. To address\nthese challenges, we propose FATE-LLM, an industrial-grade federated learning\nframework for large language models. FATE-LLM (1) facilitates federated\nlearning for large language models (coined FedLLM); (2) promotes efficient\ntraining of FedLLM using parameter-efficient fine-tuning methods; (3) protects\nthe intellectual property of LLMs; (4) preserves data privacy during training\nand inference through privacy-preserving mechanisms. We release the code of\nFATE-LLM at https://github.com/FederatedAI/FATE-LLM to facilitate the research\nof FedLLM and enable a broad range of industrial applications."}, "hash": "a124a50e1eec82726e65fc0527e9b08bf47bb182590a7ed603b2aa2955030957", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "554f3385-27d1-42d3-9b11-cb7e1f162bb2", "node_type": "1", "metadata": {}, "hash": "5b183a35d633238762de22154bfe9d3f6c73d2122a66db9a2cda80eacdf096df", "class_name": "RelatedNodeInfo"}}, "text": "Methods Model Size (MB) Param Percent (%)\nLoRA 3.6 0.058\nP-Tuning-v2 29.3 0.475\nFine-tune All 6173 100\nTable 3: Comparison of communication cost for FedLLM fine-\ntuning all parameters of ChatGLM-6B, fine-tuning ChatGLM-6B\nusing LoRA and P-Tuning-v2. Model Size denotes the size of pa-\nrameters to be fine-tuned. Param Percent denotes the ratio of param-\neters to be fine-tuned to all parameters.5 Conclusions and Future Work\nWe proposed FATE-LLM, an industrial-grade federated\nlearning framework for large language models(FedLLM). As\nan open-sourced software, FATE-LLM encourages collabo-\nration among the research and industry communities and ex-\npects to receive increasing feedback on its use.\nIn the future, we may consider research directions: (1) rec-\noncile LLMs of different model architectures during FL fine-\ntuning; (2) fine-tune private LLMs of one party using private\ndata of another party without compromising the data privacy\nand model ownership; (3) protect the privacy of user prompts\nefficiently in the inference stage; (4) apply FedLLM to verti-\ncal federated learning [Liuet al. , 2022 ].\nReferences\n[Brown et al. , 2020 ]Tom Brown, Benjamin Mann, Nick Ry-\nder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhari-\nwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry,\nAmanda Askell, et al. Language models are few-shot\nlearners. Advances in neural information processing sys-\ntems, 33:1877\u20131901, 2020.\n[Caiet al. , 2022 ]Dongqi Cai, Yaozong Wu, Shangguang\nWang, Felix Xiaozhu Lin, and Mengwei Xu. Aut-\nofednlp: An efficient fednlp framework. arXiv preprint\narXiv:2205.10162 , 2022.\n[Chen et al. , 2021 ]Chaochao Chen, Jun Zhou, Li Wang,\nXibin Wu, Wenjing Fang, Jin Tan, Lei Wang, Alex X Liu,\nHao Wang, and Cheng Hong. When homomorphic en-\ncryption marries secret sharing: Secure large-scale sparse\nlogistic regression and applications in risk control. In Pro-\nceedings of the 27th ACM SIGKDD Conference on Knowl-\nedge Discovery & Data Mining , pages 2652\u20132662, 2021.\n[Cheng et al. , 2021 ]Kewei Cheng, Tao Fan, Yilun Jin, Yang\nLiu, Tianjian Chen, Dimitrios Papadopoulos, and Qiang\nYang. Secureboost: A lossless federated learning frame-\nwork. IEEE Intelligent Systems , 36(6):87\u201398, 2021.", "mimetype": "text/plain", "start_char_idx": 1098, "end_char_idx": 3303, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "554f3385-27d1-42d3-9b11-cb7e1f162bb2": {"__data__": {"id_": "554f3385-27d1-42d3-9b11-cb7e1f162bb2", "embedding": null, "metadata": {"page_label": "5", "file_name": "2310_10049v1.pdf", "Title of this paper": "FATE-LLM: A Industrial Grade Federated Learning Framework for Large Language Models", "Authors": "Tao Fan, Yan Kang, Guoqiang Ma, Weijing Chen, Wenbin Wei, Lixin Fan, Qiang Yang", "Date published": "10/16/2023", "URL": "http://arxiv.org/abs/2310.10049v1", "summary": "Large Language Models (LLMs), such as ChatGPT, LLaMA, GLM, and PaLM, have\nexhibited remarkable performances across various tasks in recent years.\nHowever, LLMs face two main challenges in real-world applications. One\nchallenge is that training LLMs consumes vast computing resources, preventing\nLLMs from being adopted by small and medium-sized enterprises with limited\ncomputing resources. Another is that training LLM requires a large amount of\nhigh-quality data, which are often scattered among enterprises. To address\nthese challenges, we propose FATE-LLM, an industrial-grade federated learning\nframework for large language models. FATE-LLM (1) facilitates federated\nlearning for large language models (coined FedLLM); (2) promotes efficient\ntraining of FedLLM using parameter-efficient fine-tuning methods; (3) protects\nthe intellectual property of LLMs; (4) preserves data privacy during training\nand inference through privacy-preserving mechanisms. We release the code of\nFATE-LLM at https://github.com/FederatedAI/FATE-LLM to facilitate the research\nof FedLLM and enable a broad range of industrial applications."}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f865cc78-890f-4f46-9953-7e3633ecafc3", "node_type": "4", "metadata": {"page_label": "5", "file_name": "2310_10049v1.pdf", "Title of this paper": "FATE-LLM: A Industrial Grade Federated Learning Framework for Large Language Models", "Authors": "Tao Fan, Yan Kang, Guoqiang Ma, Weijing Chen, Wenbin Wei, Lixin Fan, Qiang Yang", "Date published": "10/16/2023", "URL": "http://arxiv.org/abs/2310.10049v1", "summary": "Large Language Models (LLMs), such as ChatGPT, LLaMA, GLM, and PaLM, have\nexhibited remarkable performances across various tasks in recent years.\nHowever, LLMs face two main challenges in real-world applications. One\nchallenge is that training LLMs consumes vast computing resources, preventing\nLLMs from being adopted by small and medium-sized enterprises with limited\ncomputing resources. Another is that training LLM requires a large amount of\nhigh-quality data, which are often scattered among enterprises. To address\nthese challenges, we propose FATE-LLM, an industrial-grade federated learning\nframework for large language models. FATE-LLM (1) facilitates federated\nlearning for large language models (coined FedLLM); (2) promotes efficient\ntraining of FedLLM using parameter-efficient fine-tuning methods; (3) protects\nthe intellectual property of LLMs; (4) preserves data privacy during training\nand inference through privacy-preserving mechanisms. We release the code of\nFATE-LLM at https://github.com/FederatedAI/FATE-LLM to facilitate the research\nof FedLLM and enable a broad range of industrial applications."}, "hash": "9dc9895ee79e8c9e334cee92a955493f0d747515720908f587f20e4f6592147a", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "574016cd-f196-433b-9fb7-72d1be23d22b", "node_type": "1", "metadata": {"page_label": "5", "file_name": "2310_10049v1.pdf", "Title of this paper": "FATE-LLM: A Industrial Grade Federated Learning Framework for Large Language Models", "Authors": "Tao Fan, Yan Kang, Guoqiang Ma, Weijing Chen, Wenbin Wei, Lixin Fan, Qiang Yang", "Date published": "10/16/2023", "URL": "http://arxiv.org/abs/2310.10049v1", "summary": "Large Language Models (LLMs), such as ChatGPT, LLaMA, GLM, and PaLM, have\nexhibited remarkable performances across various tasks in recent years.\nHowever, LLMs face two main challenges in real-world applications. One\nchallenge is that training LLMs consumes vast computing resources, preventing\nLLMs from being adopted by small and medium-sized enterprises with limited\ncomputing resources. Another is that training LLM requires a large amount of\nhigh-quality data, which are often scattered among enterprises. To address\nthese challenges, we propose FATE-LLM, an industrial-grade federated learning\nframework for large language models. FATE-LLM (1) facilitates federated\nlearning for large language models (coined FedLLM); (2) promotes efficient\ntraining of FedLLM using parameter-efficient fine-tuning methods; (3) protects\nthe intellectual property of LLMs; (4) preserves data privacy during training\nand inference through privacy-preserving mechanisms. We release the code of\nFATE-LLM at https://github.com/FederatedAI/FATE-LLM to facilitate the research\nof FedLLM and enable a broad range of industrial applications."}, "hash": "b557d8a39562ff907f713763e8bdcc18716e121b0a1e0088e8061c9e0a5f68f1", "class_name": "RelatedNodeInfo"}}, "text": "[Chen et al. , 2021 ]Chaochao Chen, Jun Zhou, Li Wang,\nXibin Wu, Wenjing Fang, Jin Tan, Lei Wang, Alex X Liu,\nHao Wang, and Cheng Hong. When homomorphic en-\ncryption marries secret sharing: Secure large-scale sparse\nlogistic regression and applications in risk control. In Pro-\nceedings of the 27th ACM SIGKDD Conference on Knowl-\nedge Discovery & Data Mining , pages 2652\u20132662, 2021.\n[Cheng et al. , 2021 ]Kewei Cheng, Tao Fan, Yilun Jin, Yang\nLiu, Tianjian Chen, Dimitrios Papadopoulos, and Qiang\nYang. Secureboost: A lossless federated learning frame-\nwork. IEEE Intelligent Systems , 36(6):87\u201398, 2021.\n[Chowdhery et al. , 2022 ]Aakanksha Chowdhery, Sharan\nNarang, Jacob Devlin, Maarten Bosma, Gaurav Mishra,\nAdam Roberts, Paul Barham, Hyung Won Chung, Charles", "mimetype": "text/plain", "start_char_idx": 2697, "end_char_idx": 3461, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ee3a13bb-b63f-4c17-8c8b-2567dd440a34": {"__data__": {"id_": "ee3a13bb-b63f-4c17-8c8b-2567dd440a34", "embedding": null, "metadata": {"page_label": "6", "file_name": "2310_10049v1.pdf", "Title of this paper": "FATE-LLM: A Industrial Grade Federated Learning Framework for Large Language Models", "Authors": "Tao Fan, Yan Kang, Guoqiang Ma, Weijing Chen, Wenbin Wei, Lixin Fan, Qiang Yang", "Date published": "10/16/2023", "URL": "http://arxiv.org/abs/2310.10049v1", "summary": "Large Language Models (LLMs), such as ChatGPT, LLaMA, GLM, and PaLM, have\nexhibited remarkable performances across various tasks in recent years.\nHowever, LLMs face two main challenges in real-world applications. One\nchallenge is that training LLMs consumes vast computing resources, preventing\nLLMs from being adopted by small and medium-sized enterprises with limited\ncomputing resources. Another is that training LLM requires a large amount of\nhigh-quality data, which are often scattered among enterprises. To address\nthese challenges, we propose FATE-LLM, an industrial-grade federated learning\nframework for large language models. FATE-LLM (1) facilitates federated\nlearning for large language models (coined FedLLM); (2) promotes efficient\ntraining of FedLLM using parameter-efficient fine-tuning methods; (3) protects\nthe intellectual property of LLMs; (4) preserves data privacy during training\nand inference through privacy-preserving mechanisms. We release the code of\nFATE-LLM at https://github.com/FederatedAI/FATE-LLM to facilitate the research\nof FedLLM and enable a broad range of industrial applications."}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1bfd8547-1f5f-4fe4-9d6f-7b7d0f5b0eb0", "node_type": "4", "metadata": {"page_label": "6", "file_name": "2310_10049v1.pdf", "Title of this paper": "FATE-LLM: A Industrial Grade Federated Learning Framework for Large Language Models", "Authors": "Tao Fan, Yan Kang, Guoqiang Ma, Weijing Chen, Wenbin Wei, Lixin Fan, Qiang Yang", "Date published": "10/16/2023", "URL": "http://arxiv.org/abs/2310.10049v1", "summary": "Large Language Models (LLMs), such as ChatGPT, LLaMA, GLM, and PaLM, have\nexhibited remarkable performances across various tasks in recent years.\nHowever, LLMs face two main challenges in real-world applications. One\nchallenge is that training LLMs consumes vast computing resources, preventing\nLLMs from being adopted by small and medium-sized enterprises with limited\ncomputing resources. Another is that training LLM requires a large amount of\nhigh-quality data, which are often scattered among enterprises. To address\nthese challenges, we propose FATE-LLM, an industrial-grade federated learning\nframework for large language models. FATE-LLM (1) facilitates federated\nlearning for large language models (coined FedLLM); (2) promotes efficient\ntraining of FedLLM using parameter-efficient fine-tuning methods; (3) protects\nthe intellectual property of LLMs; (4) preserves data privacy during training\nand inference through privacy-preserving mechanisms. We release the code of\nFATE-LLM at https://github.com/FederatedAI/FATE-LLM to facilitate the research\nof FedLLM and enable a broad range of industrial applications."}, "hash": "9e598658a005179913eb1e4d7cbc3bc1412a8d737da3018a74c628dbe9a2f7c7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "decd9340-49d8-4788-a620-38cb561dd2d8", "node_type": "1", "metadata": {}, "hash": "4c27e22f204935bc0263de472d6868b02e0ea9efa6eefa3edbe64d011de8dd4a", "class_name": "RelatedNodeInfo"}}, "text": "Sutton, Sebastian Gehrmann, et al. Palm: Scal-\ning language modeling with pathways. arXiv preprint\narXiv:2204.02311 , 2022.\n[Damg \u02daardet al. , 2012 ]Ivan Damg \u02daard, Valerio Pastro, Nigel\nSmart, and Sarah Zakarias. Multiparty computation from\nsomewhat homomorphic encryption. In Annual Cryptol-\nogy Conference , pages 643\u2013662. Springer, 2012.\n[Devlin et al. , 2018 ]Jacob Devlin, Ming-Wei Chang, Ken-\nton Lee, and Kristina Toutanova. Bert: Pre-training of\ndeep bidirectional transformers for language understand-\ning. arXiv preprint arXiv:1810.04805 , 2018.\n[Duet al. , 2022 ]Zhengxiao Du, Yujie Qian, Xiao Liu, Ming\nDing, Jiezhong Qiu, Zhilin Yang, and Jie Tang. Glm: Gen-\neral language model pretraining with autoregressive blank\ninfilling. In Proceedings of the 60th Annual Meeting of\nthe Association for Computational Linguistics (Volume 1:\nLong Papers) , pages 320\u2013335, 2022.\n[Dwork et al. , 2014 ]Cynthia Dwork, Aaron Roth, et al. The\nalgorithmic foundations of differential privacy. Founda-\ntions and Trends\u00ae in Theoretical Computer Science , 9(3\u2013\n4):211\u2013407, 2014.\n[Hardy et al. , 2017 ]Stephen Hardy, Wilko Henecka,\nHamish Ivey-Law, Richard Nock, Giorgio Patrini, Guil-\nlaume Smith, and Brian Thorne. Private federated learning\non vertically partitioned data via entity resolution and\nadditively homomorphic encryption. arXiv preprint\narXiv:1711.10677 , 2017.\n[Huet al. , 2021 ]Edward J Hu, Yelong Shen, Phillip Wallis,\nZeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang,\nand Weizhu Chen. Lora: Low-rank adaptation of large\nlanguage models. arXiv preprint arXiv:2106.09685 , 2021.\n[Kairouz et al. , 2021 ]Peter Kairouz, H Brendan McMa-\nhan, Brendan Avent, Aur \u00b4elien Bellet, Mehdi Bennis, Ar-\njun Nitin Bhagoji, Kallista Bonawitz, Zachary Charles,\nGraham Cormode, Rachel Cummings, et al. Advances\nand open problems in federated learning. Foundations and\nTrends\u00ae in Machine Learning , 14(1\u20132):1\u2013210, 2021.\n[Kang et al. , 2022 ]Yan Kang, Yuanqin He, Jiahuan Luo, Tao\nFan, Yang Liu, and Qiang Yang.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2002, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "decd9340-49d8-4788-a620-38cb561dd2d8": {"__data__": {"id_": "decd9340-49d8-4788-a620-38cb561dd2d8", "embedding": null, "metadata": {"page_label": "6", "file_name": "2310_10049v1.pdf", "Title of this paper": "FATE-LLM: A Industrial Grade Federated Learning Framework for Large Language Models", "Authors": "Tao Fan, Yan Kang, Guoqiang Ma, Weijing Chen, Wenbin Wei, Lixin Fan, Qiang Yang", "Date published": "10/16/2023", "URL": "http://arxiv.org/abs/2310.10049v1", "summary": "Large Language Models (LLMs), such as ChatGPT, LLaMA, GLM, and PaLM, have\nexhibited remarkable performances across various tasks in recent years.\nHowever, LLMs face two main challenges in real-world applications. One\nchallenge is that training LLMs consumes vast computing resources, preventing\nLLMs from being adopted by small and medium-sized enterprises with limited\ncomputing resources. Another is that training LLM requires a large amount of\nhigh-quality data, which are often scattered among enterprises. To address\nthese challenges, we propose FATE-LLM, an industrial-grade federated learning\nframework for large language models. FATE-LLM (1) facilitates federated\nlearning for large language models (coined FedLLM); (2) promotes efficient\ntraining of FedLLM using parameter-efficient fine-tuning methods; (3) protects\nthe intellectual property of LLMs; (4) preserves data privacy during training\nand inference through privacy-preserving mechanisms. We release the code of\nFATE-LLM at https://github.com/FederatedAI/FATE-LLM to facilitate the research\nof FedLLM and enable a broad range of industrial applications."}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1bfd8547-1f5f-4fe4-9d6f-7b7d0f5b0eb0", "node_type": "4", "metadata": {"page_label": "6", "file_name": "2310_10049v1.pdf", "Title of this paper": "FATE-LLM: A Industrial Grade Federated Learning Framework for Large Language Models", "Authors": "Tao Fan, Yan Kang, Guoqiang Ma, Weijing Chen, Wenbin Wei, Lixin Fan, Qiang Yang", "Date published": "10/16/2023", "URL": "http://arxiv.org/abs/2310.10049v1", "summary": "Large Language Models (LLMs), such as ChatGPT, LLaMA, GLM, and PaLM, have\nexhibited remarkable performances across various tasks in recent years.\nHowever, LLMs face two main challenges in real-world applications. One\nchallenge is that training LLMs consumes vast computing resources, preventing\nLLMs from being adopted by small and medium-sized enterprises with limited\ncomputing resources. Another is that training LLM requires a large amount of\nhigh-quality data, which are often scattered among enterprises. To address\nthese challenges, we propose FATE-LLM, an industrial-grade federated learning\nframework for large language models. FATE-LLM (1) facilitates federated\nlearning for large language models (coined FedLLM); (2) promotes efficient\ntraining of FedLLM using parameter-efficient fine-tuning methods; (3) protects\nthe intellectual property of LLMs; (4) preserves data privacy during training\nand inference through privacy-preserving mechanisms. We release the code of\nFATE-LLM at https://github.com/FederatedAI/FATE-LLM to facilitate the research\nof FedLLM and enable a broad range of industrial applications."}, "hash": "9e598658a005179913eb1e4d7cbc3bc1412a8d737da3018a74c628dbe9a2f7c7", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ee3a13bb-b63f-4c17-8c8b-2567dd440a34", "node_type": "1", "metadata": {"page_label": "6", "file_name": "2310_10049v1.pdf", "Title of this paper": "FATE-LLM: A Industrial Grade Federated Learning Framework for Large Language Models", "Authors": "Tao Fan, Yan Kang, Guoqiang Ma, Weijing Chen, Wenbin Wei, Lixin Fan, Qiang Yang", "Date published": "10/16/2023", "URL": "http://arxiv.org/abs/2310.10049v1", "summary": "Large Language Models (LLMs), such as ChatGPT, LLaMA, GLM, and PaLM, have\nexhibited remarkable performances across various tasks in recent years.\nHowever, LLMs face two main challenges in real-world applications. One\nchallenge is that training LLMs consumes vast computing resources, preventing\nLLMs from being adopted by small and medium-sized enterprises with limited\ncomputing resources. Another is that training LLM requires a large amount of\nhigh-quality data, which are often scattered among enterprises. To address\nthese challenges, we propose FATE-LLM, an industrial-grade federated learning\nframework for large language models. FATE-LLM (1) facilitates federated\nlearning for large language models (coined FedLLM); (2) promotes efficient\ntraining of FedLLM using parameter-efficient fine-tuning methods; (3) protects\nthe intellectual property of LLMs; (4) preserves data privacy during training\nand inference through privacy-preserving mechanisms. We release the code of\nFATE-LLM at https://github.com/FederatedAI/FATE-LLM to facilitate the research\nof FedLLM and enable a broad range of industrial applications."}, "hash": "7d6db25b982f6e3b37833c1e24659fe38bece1f58a483f33b52778c1d34616ad", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "12cda149-f185-431f-976f-5fb9d4933fad", "node_type": "1", "metadata": {}, "hash": "d2b2f704ec4a6b94bb5c0f5b599100ebd7e3f241bef9e35625881333ec2bd8f7", "class_name": "RelatedNodeInfo"}}, "text": "Lora: Low-rank adaptation of large\nlanguage models. arXiv preprint arXiv:2106.09685 , 2021.\n[Kairouz et al. , 2021 ]Peter Kairouz, H Brendan McMa-\nhan, Brendan Avent, Aur \u00b4elien Bellet, Mehdi Bennis, Ar-\njun Nitin Bhagoji, Kallista Bonawitz, Zachary Charles,\nGraham Cormode, Rachel Cummings, et al. Advances\nand open problems in federated learning. Foundations and\nTrends\u00ae in Machine Learning , 14(1\u20132):1\u2013210, 2021.\n[Kang et al. , 2022 ]Yan Kang, Yuanqin He, Jiahuan Luo, Tao\nFan, Yang Liu, and Qiang Yang. Privacy-preserving feder-\nated adversarial domain adaptation over feature groups for\ninterpretability. IEEE Transactions on Big Data , 2022.\n[Liet al. , 2022 ]Bowen Li, Lixin Fan, Hanlin Gu, Jie Li, and\nQiang Yang. Fedipr: Ownership verification for federated\ndeep neural network models. IEEE Transactions on Pat-\ntern Analysis and Machine Intelligence , 2022.\n[Lin, 2004 ]Chin-Yew Lin. Rouge: A package for automatic\nevaluation of summaries. In Text summarization branches\nout, pages 74\u201381, 2004.\n[Liuet al. , 2021a ]Xiao Liu, Kaixuan Ji, Yicheng Fu,\nWeng Lam Tam, Zhengxiao Du, Zhilin Yang, and Jie Tang.\nP-tuning v2: Prompt tuning can be comparable to fine-\ntuning universally across scales and tasks. arXiv preprint\narXiv:2110.07602 , 2021.\n[Liuet al. , 2021b ]Yang Liu, Tao Fan, Tianjian Chen, Qian\nXu, and Qiang Yang. Fate: An industrial grade platformfor collaborative learning with data protection. J. Mach.\nLearn. Res. , 22(226):1\u20136, 2021.\n[Liuet al. , 2022 ]Yang Liu, Yan Kang, Tianyuan Zou, Yan-\nhong Pu, Yuanqin He, Xiaozhou Ye, Ye Ouyang, Ya-Qin\nZhang, and Qiang Yang. Vertical federated learning. arXiv\npreprint arXiv:2211.12814 , 2022.\n[McMahan et al. , 2017 ]Brendan McMahan, Eider Moore,\nDaniel Ramage, Seth Hampson, and Blaise Aguera y Ar-\ncas. Communication-efficient learning of deep networks\nfrom decentralized data. In Artificial intelligence and\nstatistics , pages 1273\u20131282. PMLR, 2017.\n[OpenAI, 2022 ]OpenAI. Chatgpt. 2022.\n[OpenAI, 2023 ]OpenAI.", "mimetype": "text/plain", "start_char_idx": 1496, "end_char_idx": 3474, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "12cda149-f185-431f-976f-5fb9d4933fad": {"__data__": {"id_": "12cda149-f185-431f-976f-5fb9d4933fad", "embedding": null, "metadata": {"page_label": "6", "file_name": "2310_10049v1.pdf", "Title of this paper": "FATE-LLM: A Industrial Grade Federated Learning Framework for Large Language Models", "Authors": "Tao Fan, Yan Kang, Guoqiang Ma, Weijing Chen, Wenbin Wei, Lixin Fan, Qiang Yang", "Date published": "10/16/2023", "URL": "http://arxiv.org/abs/2310.10049v1", "summary": "Large Language Models (LLMs), such as ChatGPT, LLaMA, GLM, and PaLM, have\nexhibited remarkable performances across various tasks in recent years.\nHowever, LLMs face two main challenges in real-world applications. One\nchallenge is that training LLMs consumes vast computing resources, preventing\nLLMs from being adopted by small and medium-sized enterprises with limited\ncomputing resources. Another is that training LLM requires a large amount of\nhigh-quality data, which are often scattered among enterprises. To address\nthese challenges, we propose FATE-LLM, an industrial-grade federated learning\nframework for large language models. FATE-LLM (1) facilitates federated\nlearning for large language models (coined FedLLM); (2) promotes efficient\ntraining of FedLLM using parameter-efficient fine-tuning methods; (3) protects\nthe intellectual property of LLMs; (4) preserves data privacy during training\nand inference through privacy-preserving mechanisms. We release the code of\nFATE-LLM at https://github.com/FederatedAI/FATE-LLM to facilitate the research\nof FedLLM and enable a broad range of industrial applications."}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1bfd8547-1f5f-4fe4-9d6f-7b7d0f5b0eb0", "node_type": "4", "metadata": {"page_label": "6", "file_name": "2310_10049v1.pdf", "Title of this paper": "FATE-LLM: A Industrial Grade Federated Learning Framework for Large Language Models", "Authors": "Tao Fan, Yan Kang, Guoqiang Ma, Weijing Chen, Wenbin Wei, Lixin Fan, Qiang Yang", "Date published": "10/16/2023", "URL": "http://arxiv.org/abs/2310.10049v1", "summary": "Large Language Models (LLMs), such as ChatGPT, LLaMA, GLM, and PaLM, have\nexhibited remarkable performances across various tasks in recent years.\nHowever, LLMs face two main challenges in real-world applications. One\nchallenge is that training LLMs consumes vast computing resources, preventing\nLLMs from being adopted by small and medium-sized enterprises with limited\ncomputing resources. Another is that training LLM requires a large amount of\nhigh-quality data, which are often scattered among enterprises. To address\nthese challenges, we propose FATE-LLM, an industrial-grade federated learning\nframework for large language models. FATE-LLM (1) facilitates federated\nlearning for large language models (coined FedLLM); (2) promotes efficient\ntraining of FedLLM using parameter-efficient fine-tuning methods; (3) protects\nthe intellectual property of LLMs; (4) preserves data privacy during training\nand inference through privacy-preserving mechanisms. We release the code of\nFATE-LLM at https://github.com/FederatedAI/FATE-LLM to facilitate the research\nof FedLLM and enable a broad range of industrial applications."}, "hash": "9e598658a005179913eb1e4d7cbc3bc1412a8d737da3018a74c628dbe9a2f7c7", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "decd9340-49d8-4788-a620-38cb561dd2d8", "node_type": "1", "metadata": {"page_label": "6", "file_name": "2310_10049v1.pdf", "Title of this paper": "FATE-LLM: A Industrial Grade Federated Learning Framework for Large Language Models", "Authors": "Tao Fan, Yan Kang, Guoqiang Ma, Weijing Chen, Wenbin Wei, Lixin Fan, Qiang Yang", "Date published": "10/16/2023", "URL": "http://arxiv.org/abs/2310.10049v1", "summary": "Large Language Models (LLMs), such as ChatGPT, LLaMA, GLM, and PaLM, have\nexhibited remarkable performances across various tasks in recent years.\nHowever, LLMs face two main challenges in real-world applications. One\nchallenge is that training LLMs consumes vast computing resources, preventing\nLLMs from being adopted by small and medium-sized enterprises with limited\ncomputing resources. Another is that training LLM requires a large amount of\nhigh-quality data, which are often scattered among enterprises. To address\nthese challenges, we propose FATE-LLM, an industrial-grade federated learning\nframework for large language models. FATE-LLM (1) facilitates federated\nlearning for large language models (coined FedLLM); (2) promotes efficient\ntraining of FedLLM using parameter-efficient fine-tuning methods; (3) protects\nthe intellectual property of LLMs; (4) preserves data privacy during training\nand inference through privacy-preserving mechanisms. We release the code of\nFATE-LLM at https://github.com/FederatedAI/FATE-LLM to facilitate the research\nof FedLLM and enable a broad range of industrial applications."}, "hash": "b1e065bdfaaf7b2886c62a2628a404e29bf9db42a1b62eb010af5ffbcf624349", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ff2b67a9-2340-4ecf-b42d-09fe0c4bb545", "node_type": "1", "metadata": {}, "hash": "0638eb297a9fb66401784e3ece4fdf9ad52eb4e19ba604dc5a4ddb2b8b8a5391", "class_name": "RelatedNodeInfo"}}, "text": "[Liuet al. , 2022 ]Yang Liu, Yan Kang, Tianyuan Zou, Yan-\nhong Pu, Yuanqin He, Xiaozhou Ye, Ye Ouyang, Ya-Qin\nZhang, and Qiang Yang. Vertical federated learning. arXiv\npreprint arXiv:2211.12814 , 2022.\n[McMahan et al. , 2017 ]Brendan McMahan, Eider Moore,\nDaniel Ramage, Seth Hampson, and Blaise Aguera y Ar-\ncas. Communication-efficient learning of deep networks\nfrom decentralized data. In Artificial intelligence and\nstatistics , pages 1273\u20131282. PMLR, 2017.\n[OpenAI, 2022 ]OpenAI. Chatgpt. 2022.\n[OpenAI, 2023 ]OpenAI. Gpt-4. 2023.\n[Paillier, 1999 ]Pascal Paillier. Public-key cryptosystems\nbased on composite degree residuosity classes. In Interna-\ntional conference on the theory and applications of cryp-\ntographic techniques , pages 223\u2013238. Springer, 1999.\n[Papineni et al. , 2002 ]Kishore Papineni, Salim Roukos,\nTodd Ward, and Wei-Jing Zhu. Bleu: a method for au-\ntomatic evaluation of machine translation. In Proceedings\nof the 40th annual meeting of the Association for Compu-\ntational Linguistics , pages 311\u2013318, 2002.\n[Radford et al. , 2018 ]Alec Radford, Karthik Narasimhan,\nTim Salimans, Ilya Sutskever, et al. Improving language\nunderstanding by generative pre-training. 2018.\n[Scao et al. , 2022 ]Teven Le Scao, Angela Fan, Christo-\npher Akiki, Ellie Pavlick, Suzana Ili \u00b4c, Daniel Hesslow,\nRoman Castagn \u00b4e, Alexandra Sasha Luccioni, Franc \u00b8ois\nYvon, Matthias Gall \u00b4e, et al. Bloom: A 176b-parameter\nopen-access multilingual language model. arXiv preprint\narXiv:2211.05100 , 2022.\n[Shamir, 1979 ]Adi Shamir. How to share a secret. Commu-\nnications of the ACM , 22(11):612\u2013613, 1979.\n[Shao et al. , 2019 ]Zhihong Shao, Minlie Huang, Jiangtao\nWen, Wenfei Xu, and Xiaoyan Zhu. Long and diverse\ntext generation with planning-based hierarchical varia-\ntional model. arXiv preprint arXiv:1908.06605 , 2019.\n[Shen et al.", "mimetype": "text/plain", "start_char_idx": 2952, "end_char_idx": 4786, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ff2b67a9-2340-4ecf-b42d-09fe0c4bb545": {"__data__": {"id_": "ff2b67a9-2340-4ecf-b42d-09fe0c4bb545", "embedding": null, "metadata": {"page_label": "6", "file_name": "2310_10049v1.pdf", "Title of this paper": "FATE-LLM: A Industrial Grade Federated Learning Framework for Large Language Models", "Authors": "Tao Fan, Yan Kang, Guoqiang Ma, Weijing Chen, Wenbin Wei, Lixin Fan, Qiang Yang", "Date published": "10/16/2023", "URL": "http://arxiv.org/abs/2310.10049v1", "summary": "Large Language Models (LLMs), such as ChatGPT, LLaMA, GLM, and PaLM, have\nexhibited remarkable performances across various tasks in recent years.\nHowever, LLMs face two main challenges in real-world applications. One\nchallenge is that training LLMs consumes vast computing resources, preventing\nLLMs from being adopted by small and medium-sized enterprises with limited\ncomputing resources. Another is that training LLM requires a large amount of\nhigh-quality data, which are often scattered among enterprises. To address\nthese challenges, we propose FATE-LLM, an industrial-grade federated learning\nframework for large language models. FATE-LLM (1) facilitates federated\nlearning for large language models (coined FedLLM); (2) promotes efficient\ntraining of FedLLM using parameter-efficient fine-tuning methods; (3) protects\nthe intellectual property of LLMs; (4) preserves data privacy during training\nand inference through privacy-preserving mechanisms. We release the code of\nFATE-LLM at https://github.com/FederatedAI/FATE-LLM to facilitate the research\nof FedLLM and enable a broad range of industrial applications."}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1bfd8547-1f5f-4fe4-9d6f-7b7d0f5b0eb0", "node_type": "4", "metadata": {"page_label": "6", "file_name": "2310_10049v1.pdf", "Title of this paper": "FATE-LLM: A Industrial Grade Federated Learning Framework for Large Language Models", "Authors": "Tao Fan, Yan Kang, Guoqiang Ma, Weijing Chen, Wenbin Wei, Lixin Fan, Qiang Yang", "Date published": "10/16/2023", "URL": "http://arxiv.org/abs/2310.10049v1", "summary": "Large Language Models (LLMs), such as ChatGPT, LLaMA, GLM, and PaLM, have\nexhibited remarkable performances across various tasks in recent years.\nHowever, LLMs face two main challenges in real-world applications. One\nchallenge is that training LLMs consumes vast computing resources, preventing\nLLMs from being adopted by small and medium-sized enterprises with limited\ncomputing resources. Another is that training LLM requires a large amount of\nhigh-quality data, which are often scattered among enterprises. To address\nthese challenges, we propose FATE-LLM, an industrial-grade federated learning\nframework for large language models. FATE-LLM (1) facilitates federated\nlearning for large language models (coined FedLLM); (2) promotes efficient\ntraining of FedLLM using parameter-efficient fine-tuning methods; (3) protects\nthe intellectual property of LLMs; (4) preserves data privacy during training\nand inference through privacy-preserving mechanisms. We release the code of\nFATE-LLM at https://github.com/FederatedAI/FATE-LLM to facilitate the research\nof FedLLM and enable a broad range of industrial applications."}, "hash": "9e598658a005179913eb1e4d7cbc3bc1412a8d737da3018a74c628dbe9a2f7c7", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "12cda149-f185-431f-976f-5fb9d4933fad", "node_type": "1", "metadata": {"page_label": "6", "file_name": "2310_10049v1.pdf", "Title of this paper": "FATE-LLM: A Industrial Grade Federated Learning Framework for Large Language Models", "Authors": "Tao Fan, Yan Kang, Guoqiang Ma, Weijing Chen, Wenbin Wei, Lixin Fan, Qiang Yang", "Date published": "10/16/2023", "URL": "http://arxiv.org/abs/2310.10049v1", "summary": "Large Language Models (LLMs), such as ChatGPT, LLaMA, GLM, and PaLM, have\nexhibited remarkable performances across various tasks in recent years.\nHowever, LLMs face two main challenges in real-world applications. One\nchallenge is that training LLMs consumes vast computing resources, preventing\nLLMs from being adopted by small and medium-sized enterprises with limited\ncomputing resources. Another is that training LLM requires a large amount of\nhigh-quality data, which are often scattered among enterprises. To address\nthese challenges, we propose FATE-LLM, an industrial-grade federated learning\nframework for large language models. FATE-LLM (1) facilitates federated\nlearning for large language models (coined FedLLM); (2) promotes efficient\ntraining of FedLLM using parameter-efficient fine-tuning methods; (3) protects\nthe intellectual property of LLMs; (4) preserves data privacy during training\nand inference through privacy-preserving mechanisms. We release the code of\nFATE-LLM at https://github.com/FederatedAI/FATE-LLM to facilitate the research\nof FedLLM and enable a broad range of industrial applications."}, "hash": "6499e411c9b39e352dbaa72043b466578067b41c55502497ad4ab1d7ca341fcb", "class_name": "RelatedNodeInfo"}}, "text": "Bloom: A 176b-parameter\nopen-access multilingual language model. arXiv preprint\narXiv:2211.05100 , 2022.\n[Shamir, 1979 ]Adi Shamir. How to share a secret. Commu-\nnications of the ACM , 22(11):612\u2013613, 1979.\n[Shao et al. , 2019 ]Zhihong Shao, Minlie Huang, Jiangtao\nWen, Wenfei Xu, and Xiaoyan Zhu. Long and diverse\ntext generation with planning-based hierarchical varia-\ntional model. arXiv preprint arXiv:1908.06605 , 2019.\n[Shen et al. , 2020 ]Tao Shen, Jie Zhang, Xinkang Jia,\nFengda Zhang, Gang Huang, Pan Zhou, Kun Kuang, Fei\nWu, and Chao Wu. Federated mutual learning. arXiv\npreprint arXiv:2006.16765 , 2020.\n[Touvron et al. , 2023 ]Hugo Touvron, Thibaut Lavril, Gau-\ntier Izacard, Xavier Martinet, Marie-Anne Lachaux, Tim-\noth\u00b4ee Lacroix, Baptiste Rozi `ere, Naman Goyal, Eric Ham-\nbro, Faisal Azhar, et al. Llama: Open and efficient founda-\ntion language models. arXiv preprint arXiv:2302.13971 ,\n2023.\n[Villalobos et al. , 2022 ]Pablo Villalobos, Jaime Sevilla,\nLennart Heim, Tamay Besiroglu, Marius Hobbhahn, and\nAnson Ho. Will we run out of data? an analysis of the lim-\nits of scaling datasets in machine learning. arXiv preprint\narXiv:2211.04325 , 2022.\n[Wang et al. , 2023 ]Boxin Wang, Yibo Jacky Zhang, Yuan\nCao, Bo Li, H Brendan McMahan, Sewoong Oh, Zheng", "mimetype": "text/plain", "start_char_idx": 4349, "end_char_idx": 5620, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "32469866-1865-47e5-8644-8684428e21d9": {"__data__": {"id_": "32469866-1865-47e5-8644-8684428e21d9", "embedding": null, "metadata": {"page_label": "7", "file_name": "2310_10049v1.pdf", "Title of this paper": "FATE-LLM: A Industrial Grade Federated Learning Framework for Large Language Models", "Authors": "Tao Fan, Yan Kang, Guoqiang Ma, Weijing Chen, Wenbin Wei, Lixin Fan, Qiang Yang", "Date published": "10/16/2023", "URL": "http://arxiv.org/abs/2310.10049v1", "summary": "Large Language Models (LLMs), such as ChatGPT, LLaMA, GLM, and PaLM, have\nexhibited remarkable performances across various tasks in recent years.\nHowever, LLMs face two main challenges in real-world applications. One\nchallenge is that training LLMs consumes vast computing resources, preventing\nLLMs from being adopted by small and medium-sized enterprises with limited\ncomputing resources. Another is that training LLM requires a large amount of\nhigh-quality data, which are often scattered among enterprises. To address\nthese challenges, we propose FATE-LLM, an industrial-grade federated learning\nframework for large language models. FATE-LLM (1) facilitates federated\nlearning for large language models (coined FedLLM); (2) promotes efficient\ntraining of FedLLM using parameter-efficient fine-tuning methods; (3) protects\nthe intellectual property of LLMs; (4) preserves data privacy during training\nand inference through privacy-preserving mechanisms. We release the code of\nFATE-LLM at https://github.com/FederatedAI/FATE-LLM to facilitate the research\nof FedLLM and enable a broad range of industrial applications."}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "0a5a9e5a-3186-437c-a36f-1c56340f0fcb", "node_type": "4", "metadata": {"page_label": "7", "file_name": "2310_10049v1.pdf", "Title of this paper": "FATE-LLM: A Industrial Grade Federated Learning Framework for Large Language Models", "Authors": "Tao Fan, Yan Kang, Guoqiang Ma, Weijing Chen, Wenbin Wei, Lixin Fan, Qiang Yang", "Date published": "10/16/2023", "URL": "http://arxiv.org/abs/2310.10049v1", "summary": "Large Language Models (LLMs), such as ChatGPT, LLaMA, GLM, and PaLM, have\nexhibited remarkable performances across various tasks in recent years.\nHowever, LLMs face two main challenges in real-world applications. One\nchallenge is that training LLMs consumes vast computing resources, preventing\nLLMs from being adopted by small and medium-sized enterprises with limited\ncomputing resources. Another is that training LLM requires a large amount of\nhigh-quality data, which are often scattered among enterprises. To address\nthese challenges, we propose FATE-LLM, an industrial-grade federated learning\nframework for large language models. FATE-LLM (1) facilitates federated\nlearning for large language models (coined FedLLM); (2) promotes efficient\ntraining of FedLLM using parameter-efficient fine-tuning methods; (3) protects\nthe intellectual property of LLMs; (4) preserves data privacy during training\nand inference through privacy-preserving mechanisms. We release the code of\nFATE-LLM at https://github.com/FederatedAI/FATE-LLM to facilitate the research\nof FedLLM and enable a broad range of industrial applications."}, "hash": "ac6c38170750b2c5dae8a836611a1c1d673b61e49cdaf4d7ece84869a47f38f1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "76b15b7a-86fc-40fd-a70c-23d02621a13e", "node_type": "1", "metadata": {}, "hash": "204e937ff36a0d6661590799ac3a5e562aa1de346b22de90b1d53cb7ce8ec8b0", "class_name": "RelatedNodeInfo"}}, "text": "Xu, and Manzil Zaheer. Can public large language models\nhelp private cross-device federated learning? Workshop on\nChallenges in Deployable Generative AI at International\nConference on Machine Learning (ICML) , 2023.\n[Wuet al. , 2022 ]Chuhan Wu, Fangzhao Wu, Lingjuan Lyu,\nYongfeng Huang, and Xing Xie. Communication-efficient\nfederated learning via knowledge distillation. Nature com-\nmunications , 13(1):2032, 2022.\n[Xiao et al. , 2023 ]Guangxuan Xiao, Ji Lin, and Song Han.\nOffsite-tuning: Transfer learning without full model.\narXiv preprint arXiv:2302.04870 , 2023.\n[Yang et al. , 2019 ]Qiang Yang, Yang Liu, Yong Cheng, Yan\nKang, Tianjian Chen, and Han Yu. Federated learning.\nSynthesis Lectures on Artificial Intelligence and Machine\nLearning , 13(3):1\u2013207, 2019.\n[Yang et al. , 2023a ]Aiyuan Yang, Bin Xiao, Bingning\nWang, Borong Zhang, Chao Yin, Chenxu Lv, Da Pan, Dian\nWang, Dong Yan, Fan Yang, et al. Baichuan 2: Open large-\nscale language models. arXiv preprint arXiv:2309.10305 ,\n2023.\n[Yang et al. , 2023b ]Jingfeng Yang, Hongye Jin, Ruixiang\nTang, Xiaotian Han, Qizhang Feng, Haoming Jiang, Bing\nYin, and Xia Hu. Harnessing the power of llms in prac-\ntice: A survey on chatgpt and beyond. arXiv preprint\narXiv:2304.13712 , 2023.\n[Zhang et al. , 2018 ]Dongqing Zhang, Jiaolong Yang,\nDongqiangzi Ye, and Gang Hua. Lq-nets: Learned\nquantization for highly accurate and compact deep neural\nnetworks. In Proceedings of the European conference on\ncomputer vision (ECCV) , pages 365\u2013382, 2018.\n[Zhang et al. , 2022a ]Susan Zhang, Stephen Roller, Na-\nman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen,\nChristopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin,\net al. Opt: Open pre-trained transformer language models.\narXiv preprint arXiv:2205.01068 , 2022.\n[Zhang et al. , 2022b ]Zhuo Zhang, Yuanhang Yang, Yong\nDai, Lizhen Qu, and Zenglin Xu. When federated learning\nmeets pre-trained language models\u2019 parameter-efficient\ntuning methods. arXiv preprint arXiv:2212.10025 , 2022.\n[Zhao et al.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1996, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "76b15b7a-86fc-40fd-a70c-23d02621a13e": {"__data__": {"id_": "76b15b7a-86fc-40fd-a70c-23d02621a13e", "embedding": null, "metadata": {"page_label": "7", "file_name": "2310_10049v1.pdf", "Title of this paper": "FATE-LLM: A Industrial Grade Federated Learning Framework for Large Language Models", "Authors": "Tao Fan, Yan Kang, Guoqiang Ma, Weijing Chen, Wenbin Wei, Lixin Fan, Qiang Yang", "Date published": "10/16/2023", "URL": "http://arxiv.org/abs/2310.10049v1", "summary": "Large Language Models (LLMs), such as ChatGPT, LLaMA, GLM, and PaLM, have\nexhibited remarkable performances across various tasks in recent years.\nHowever, LLMs face two main challenges in real-world applications. One\nchallenge is that training LLMs consumes vast computing resources, preventing\nLLMs from being adopted by small and medium-sized enterprises with limited\ncomputing resources. Another is that training LLM requires a large amount of\nhigh-quality data, which are often scattered among enterprises. To address\nthese challenges, we propose FATE-LLM, an industrial-grade federated learning\nframework for large language models. FATE-LLM (1) facilitates federated\nlearning for large language models (coined FedLLM); (2) promotes efficient\ntraining of FedLLM using parameter-efficient fine-tuning methods; (3) protects\nthe intellectual property of LLMs; (4) preserves data privacy during training\nand inference through privacy-preserving mechanisms. We release the code of\nFATE-LLM at https://github.com/FederatedAI/FATE-LLM to facilitate the research\nof FedLLM and enable a broad range of industrial applications."}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "0a5a9e5a-3186-437c-a36f-1c56340f0fcb", "node_type": "4", "metadata": {"page_label": "7", "file_name": "2310_10049v1.pdf", "Title of this paper": "FATE-LLM: A Industrial Grade Federated Learning Framework for Large Language Models", "Authors": "Tao Fan, Yan Kang, Guoqiang Ma, Weijing Chen, Wenbin Wei, Lixin Fan, Qiang Yang", "Date published": "10/16/2023", "URL": "http://arxiv.org/abs/2310.10049v1", "summary": "Large Language Models (LLMs), such as ChatGPT, LLaMA, GLM, and PaLM, have\nexhibited remarkable performances across various tasks in recent years.\nHowever, LLMs face two main challenges in real-world applications. One\nchallenge is that training LLMs consumes vast computing resources, preventing\nLLMs from being adopted by small and medium-sized enterprises with limited\ncomputing resources. Another is that training LLM requires a large amount of\nhigh-quality data, which are often scattered among enterprises. To address\nthese challenges, we propose FATE-LLM, an industrial-grade federated learning\nframework for large language models. FATE-LLM (1) facilitates federated\nlearning for large language models (coined FedLLM); (2) promotes efficient\ntraining of FedLLM using parameter-efficient fine-tuning methods; (3) protects\nthe intellectual property of LLMs; (4) preserves data privacy during training\nand inference through privacy-preserving mechanisms. We release the code of\nFATE-LLM at https://github.com/FederatedAI/FATE-LLM to facilitate the research\nof FedLLM and enable a broad range of industrial applications."}, "hash": "ac6c38170750b2c5dae8a836611a1c1d673b61e49cdaf4d7ece84869a47f38f1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "32469866-1865-47e5-8644-8684428e21d9", "node_type": "1", "metadata": {"page_label": "7", "file_name": "2310_10049v1.pdf", "Title of this paper": "FATE-LLM: A Industrial Grade Federated Learning Framework for Large Language Models", "Authors": "Tao Fan, Yan Kang, Guoqiang Ma, Weijing Chen, Wenbin Wei, Lixin Fan, Qiang Yang", "Date published": "10/16/2023", "URL": "http://arxiv.org/abs/2310.10049v1", "summary": "Large Language Models (LLMs), such as ChatGPT, LLaMA, GLM, and PaLM, have\nexhibited remarkable performances across various tasks in recent years.\nHowever, LLMs face two main challenges in real-world applications. One\nchallenge is that training LLMs consumes vast computing resources, preventing\nLLMs from being adopted by small and medium-sized enterprises with limited\ncomputing resources. Another is that training LLM requires a large amount of\nhigh-quality data, which are often scattered among enterprises. To address\nthese challenges, we propose FATE-LLM, an industrial-grade federated learning\nframework for large language models. FATE-LLM (1) facilitates federated\nlearning for large language models (coined FedLLM); (2) promotes efficient\ntraining of FedLLM using parameter-efficient fine-tuning methods; (3) protects\nthe intellectual property of LLMs; (4) preserves data privacy during training\nand inference through privacy-preserving mechanisms. We release the code of\nFATE-LLM at https://github.com/FederatedAI/FATE-LLM to facilitate the research\nof FedLLM and enable a broad range of industrial applications."}, "hash": "adcf3247f268eb8f11c3fdb6cbfae553a241cf9d6a66874fa7745619c479a22c", "class_name": "RelatedNodeInfo"}}, "text": "In Proceedings of the European conference on\ncomputer vision (ECCV) , pages 365\u2013382, 2018.\n[Zhang et al. , 2022a ]Susan Zhang, Stephen Roller, Na-\nman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen,\nChristopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin,\net al. Opt: Open pre-trained transformer language models.\narXiv preprint arXiv:2205.01068 , 2022.\n[Zhang et al. , 2022b ]Zhuo Zhang, Yuanhang Yang, Yong\nDai, Lizhen Qu, and Zenglin Xu. When federated learning\nmeets pre-trained language models\u2019 parameter-efficient\ntuning methods. arXiv preprint arXiv:2212.10025 , 2022.\n[Zhao et al. , 2022 ]Haodong Zhao, Wei Du, Fangqi Li, Peix-\nuan Li, and Gongshen Liu. Reduce communication costs\nand preserve privacy: Prompt tuning method in federated\nlearning. arXiv preprint arXiv:2208.12268 , 2022.\n[Zhou et al. , 2023 ]Ce Zhou, Qian Li, Chen Li, Jun Yu, Yixin\nLiu, Guangjing Wang, Kai Zhang, Cheng Ji, Qiben Yan,\nLifang He, et al. A comprehensive survey on pretrained\nfoundation models: A history from bert to chatgpt. arXiv\npreprint arXiv:2302.09419 , 2023.", "mimetype": "text/plain", "start_char_idx": 1410, "end_char_idx": 2463, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}}, "docstore/metadata": {"43acfff8-6996-460d-9f6e-18fc0ab55610": {"doc_hash": "14123e192bc882c08bf551ecd4ce4f589da0ca584de3482f6adf3a1b720ea02d", "ref_doc_id": "767daaf0-f94b-44da-9c3c-ec3c5b9c7af1"}, "43e8970f-eaab-4f0b-bf3c-4f2e0edbf721": {"doc_hash": "e0287a0f20d2e57de07ec4ec4019b82627a16e0ca7033eacd9202e3f96dab557", "ref_doc_id": "767daaf0-f94b-44da-9c3c-ec3c5b9c7af1"}, "6116b8d1-1056-4b81-9fc2-b26583cc9f1c": {"doc_hash": "f43c4e084b85504cd4fbf2aaa21d8488f23d8e8066abbe986e5e94be7b5ba353", "ref_doc_id": "b812a356-755a-4b58-8304-98030e45c6ed"}, "320b32a3-ea3c-4677-aa7f-3b9f531744eb": {"doc_hash": "8aaa0046ae5ab2fc9e147111761b22fb9e7fcdc4a3ef4faf10cbacefd81e2899", "ref_doc_id": "b812a356-755a-4b58-8304-98030e45c6ed"}, "4e435b07-1d69-439d-a558-18d619201098": {"doc_hash": "32996099e9d326a284758e93ee27eb693d7aeafe1be127066ba3dea74ed29738", "ref_doc_id": "b812a356-755a-4b58-8304-98030e45c6ed"}, "719c4cbe-79d0-4e39-94b9-07deda9c7041": {"doc_hash": "de9807c5daba6981174055a1a62bdde8516d92fa75bdbe72428d9135a29c889f", "ref_doc_id": "1b8839fa-acf9-4a7d-93ec-66295ad759b6"}, "065cab32-39be-485b-a77d-c955b06df5a2": {"doc_hash": "26311dac79a70d33092dad53fd4621d31b0247b599c607a1fe46074a0065fa1d", "ref_doc_id": "1b8839fa-acf9-4a7d-93ec-66295ad759b6"}, "888fcd67-9acf-4500-9b52-ebb34688c2da": {"doc_hash": "d57af6e66a620b2ae0b3c184a879e6e011448d6be33a8b11926911b5da7e2341", "ref_doc_id": "e4b7e2c9-78ec-4562-8341-4365b109bf5b"}, "09007ec3-8f0c-4075-9a16-d8ea9400d11d": {"doc_hash": "eea538f05deaac79545c4578eacf0da18cb60675cbba7c6e651bed2e2f1c89ad", "ref_doc_id": "e4b7e2c9-78ec-4562-8341-4365b109bf5b"}, "3c08a56c-f2ca-488b-9d17-d00d9239e364": {"doc_hash": "d48630d573ad8af96c92e65b816a929d3e50b138894def7415a156835808eee0", "ref_doc_id": "e4b7e2c9-78ec-4562-8341-4365b109bf5b"}, "5a67e0e5-e8b3-4569-9489-fc5a1899695e": {"doc_hash": "a124a50e1eec82726e65fc0527e9b08bf47bb182590a7ed603b2aa2955030957", "ref_doc_id": "f865cc78-890f-4f46-9953-7e3633ecafc3"}, "574016cd-f196-433b-9fb7-72d1be23d22b": {"doc_hash": "b557d8a39562ff907f713763e8bdcc18716e121b0a1e0088e8061c9e0a5f68f1", "ref_doc_id": "f865cc78-890f-4f46-9953-7e3633ecafc3"}, "554f3385-27d1-42d3-9b11-cb7e1f162bb2": {"doc_hash": "720256d5f0272430c61fb106cd597bb756e1f4104751ae4b303aec7f46cd996a", "ref_doc_id": "f865cc78-890f-4f46-9953-7e3633ecafc3"}, "ee3a13bb-b63f-4c17-8c8b-2567dd440a34": {"doc_hash": "7d6db25b982f6e3b37833c1e24659fe38bece1f58a483f33b52778c1d34616ad", "ref_doc_id": "1bfd8547-1f5f-4fe4-9d6f-7b7d0f5b0eb0"}, "decd9340-49d8-4788-a620-38cb561dd2d8": {"doc_hash": "b1e065bdfaaf7b2886c62a2628a404e29bf9db42a1b62eb010af5ffbcf624349", "ref_doc_id": "1bfd8547-1f5f-4fe4-9d6f-7b7d0f5b0eb0"}, "12cda149-f185-431f-976f-5fb9d4933fad": {"doc_hash": "6499e411c9b39e352dbaa72043b466578067b41c55502497ad4ab1d7ca341fcb", "ref_doc_id": "1bfd8547-1f5f-4fe4-9d6f-7b7d0f5b0eb0"}, "ff2b67a9-2340-4ecf-b42d-09fe0c4bb545": {"doc_hash": "0c28f440c10e2abc45be2634cd5456fe0be3b18cf2a539fe063a4a3c8fd8a62c", "ref_doc_id": "1bfd8547-1f5f-4fe4-9d6f-7b7d0f5b0eb0"}, "32469866-1865-47e5-8644-8684428e21d9": {"doc_hash": "adcf3247f268eb8f11c3fdb6cbfae553a241cf9d6a66874fa7745619c479a22c", "ref_doc_id": "0a5a9e5a-3186-437c-a36f-1c56340f0fcb"}, "76b15b7a-86fc-40fd-a70c-23d02621a13e": {"doc_hash": "eba0e3d95b2ba4e3786a2b74b6c733155e036aa9b4bbf21c7c03e7e6b9f9bd30", "ref_doc_id": "0a5a9e5a-3186-437c-a36f-1c56340f0fcb"}}, "docstore/ref_doc_info": {"767daaf0-f94b-44da-9c3c-ec3c5b9c7af1": {"node_ids": ["43acfff8-6996-460d-9f6e-18fc0ab55610", "43e8970f-eaab-4f0b-bf3c-4f2e0edbf721"], "metadata": {"page_label": "1", "file_name": "2310_10049v1.pdf", "Title of this paper": "FATE-LLM: A Industrial Grade Federated Learning Framework for Large Language Models", "Authors": "Tao Fan, Yan Kang, Guoqiang Ma, Weijing Chen, Wenbin Wei, Lixin Fan, Qiang Yang", "Date published": "10/16/2023", "URL": "http://arxiv.org/abs/2310.10049v1", "summary": "Large Language Models (LLMs), such as ChatGPT, LLaMA, GLM, and PaLM, have\nexhibited remarkable performances across various tasks in recent years.\nHowever, LLMs face two main challenges in real-world applications. One\nchallenge is that training LLMs consumes vast computing resources, preventing\nLLMs from being adopted by small and medium-sized enterprises with limited\ncomputing resources. Another is that training LLM requires a large amount of\nhigh-quality data, which are often scattered among enterprises. To address\nthese challenges, we propose FATE-LLM, an industrial-grade federated learning\nframework for large language models. FATE-LLM (1) facilitates federated\nlearning for large language models (coined FedLLM); (2) promotes efficient\ntraining of FedLLM using parameter-efficient fine-tuning methods; (3) protects\nthe intellectual property of LLMs; (4) preserves data privacy during training\nand inference through privacy-preserving mechanisms. We release the code of\nFATE-LLM at https://github.com/FederatedAI/FATE-LLM to facilitate the research\nof FedLLM and enable a broad range of industrial applications."}}, "b812a356-755a-4b58-8304-98030e45c6ed": {"node_ids": ["6116b8d1-1056-4b81-9fc2-b26583cc9f1c", "320b32a3-ea3c-4677-aa7f-3b9f531744eb", "4e435b07-1d69-439d-a558-18d619201098"], "metadata": {"page_label": "2", "file_name": "2310_10049v1.pdf", "Title of this paper": "FATE-LLM: A Industrial Grade Federated Learning Framework for Large Language Models", "Authors": "Tao Fan, Yan Kang, Guoqiang Ma, Weijing Chen, Wenbin Wei, Lixin Fan, Qiang Yang", "Date published": "10/16/2023", "URL": "http://arxiv.org/abs/2310.10049v1", "summary": "Large Language Models (LLMs), such as ChatGPT, LLaMA, GLM, and PaLM, have\nexhibited remarkable performances across various tasks in recent years.\nHowever, LLMs face two main challenges in real-world applications. One\nchallenge is that training LLMs consumes vast computing resources, preventing\nLLMs from being adopted by small and medium-sized enterprises with limited\ncomputing resources. Another is that training LLM requires a large amount of\nhigh-quality data, which are often scattered among enterprises. To address\nthese challenges, we propose FATE-LLM, an industrial-grade federated learning\nframework for large language models. FATE-LLM (1) facilitates federated\nlearning for large language models (coined FedLLM); (2) promotes efficient\ntraining of FedLLM using parameter-efficient fine-tuning methods; (3) protects\nthe intellectual property of LLMs; (4) preserves data privacy during training\nand inference through privacy-preserving mechanisms. We release the code of\nFATE-LLM at https://github.com/FederatedAI/FATE-LLM to facilitate the research\nof FedLLM and enable a broad range of industrial applications."}}, "1b8839fa-acf9-4a7d-93ec-66295ad759b6": {"node_ids": ["719c4cbe-79d0-4e39-94b9-07deda9c7041", "065cab32-39be-485b-a77d-c955b06df5a2"], "metadata": {"page_label": "3", "file_name": "2310_10049v1.pdf", "Title of this paper": "FATE-LLM: A Industrial Grade Federated Learning Framework for Large Language Models", "Authors": "Tao Fan, Yan Kang, Guoqiang Ma, Weijing Chen, Wenbin Wei, Lixin Fan, Qiang Yang", "Date published": "10/16/2023", "URL": "http://arxiv.org/abs/2310.10049v1", "summary": "Large Language Models (LLMs), such as ChatGPT, LLaMA, GLM, and PaLM, have\nexhibited remarkable performances across various tasks in recent years.\nHowever, LLMs face two main challenges in real-world applications. One\nchallenge is that training LLMs consumes vast computing resources, preventing\nLLMs from being adopted by small and medium-sized enterprises with limited\ncomputing resources. Another is that training LLM requires a large amount of\nhigh-quality data, which are often scattered among enterprises. To address\nthese challenges, we propose FATE-LLM, an industrial-grade federated learning\nframework for large language models. FATE-LLM (1) facilitates federated\nlearning for large language models (coined FedLLM); (2) promotes efficient\ntraining of FedLLM using parameter-efficient fine-tuning methods; (3) protects\nthe intellectual property of LLMs; (4) preserves data privacy during training\nand inference through privacy-preserving mechanisms. We release the code of\nFATE-LLM at https://github.com/FederatedAI/FATE-LLM to facilitate the research\nof FedLLM and enable a broad range of industrial applications."}}, "e4b7e2c9-78ec-4562-8341-4365b109bf5b": {"node_ids": ["888fcd67-9acf-4500-9b52-ebb34688c2da", "09007ec3-8f0c-4075-9a16-d8ea9400d11d", "3c08a56c-f2ca-488b-9d17-d00d9239e364"], "metadata": {"page_label": "4", "file_name": "2310_10049v1.pdf", "Title of this paper": "FATE-LLM: A Industrial Grade Federated Learning Framework for Large Language Models", "Authors": "Tao Fan, Yan Kang, Guoqiang Ma, Weijing Chen, Wenbin Wei, Lixin Fan, Qiang Yang", "Date published": "10/16/2023", "URL": "http://arxiv.org/abs/2310.10049v1", "summary": "Large Language Models (LLMs), such as ChatGPT, LLaMA, GLM, and PaLM, have\nexhibited remarkable performances across various tasks in recent years.\nHowever, LLMs face two main challenges in real-world applications. One\nchallenge is that training LLMs consumes vast computing resources, preventing\nLLMs from being adopted by small and medium-sized enterprises with limited\ncomputing resources. Another is that training LLM requires a large amount of\nhigh-quality data, which are often scattered among enterprises. To address\nthese challenges, we propose FATE-LLM, an industrial-grade federated learning\nframework for large language models. FATE-LLM (1) facilitates federated\nlearning for large language models (coined FedLLM); (2) promotes efficient\ntraining of FedLLM using parameter-efficient fine-tuning methods; (3) protects\nthe intellectual property of LLMs; (4) preserves data privacy during training\nand inference through privacy-preserving mechanisms. We release the code of\nFATE-LLM at https://github.com/FederatedAI/FATE-LLM to facilitate the research\nof FedLLM and enable a broad range of industrial applications."}}, "f865cc78-890f-4f46-9953-7e3633ecafc3": {"node_ids": ["5a67e0e5-e8b3-4569-9489-fc5a1899695e", "574016cd-f196-433b-9fb7-72d1be23d22b", "554f3385-27d1-42d3-9b11-cb7e1f162bb2"], "metadata": {"page_label": "5", "file_name": "2310_10049v1.pdf", "Title of this paper": "FATE-LLM: A Industrial Grade Federated Learning Framework for Large Language Models", "Authors": "Tao Fan, Yan Kang, Guoqiang Ma, Weijing Chen, Wenbin Wei, Lixin Fan, Qiang Yang", "Date published": "10/16/2023", "URL": "http://arxiv.org/abs/2310.10049v1", "summary": "Large Language Models (LLMs), such as ChatGPT, LLaMA, GLM, and PaLM, have\nexhibited remarkable performances across various tasks in recent years.\nHowever, LLMs face two main challenges in real-world applications. One\nchallenge is that training LLMs consumes vast computing resources, preventing\nLLMs from being adopted by small and medium-sized enterprises with limited\ncomputing resources. Another is that training LLM requires a large amount of\nhigh-quality data, which are often scattered among enterprises. To address\nthese challenges, we propose FATE-LLM, an industrial-grade federated learning\nframework for large language models. FATE-LLM (1) facilitates federated\nlearning for large language models (coined FedLLM); (2) promotes efficient\ntraining of FedLLM using parameter-efficient fine-tuning methods; (3) protects\nthe intellectual property of LLMs; (4) preserves data privacy during training\nand inference through privacy-preserving mechanisms. We release the code of\nFATE-LLM at https://github.com/FederatedAI/FATE-LLM to facilitate the research\nof FedLLM and enable a broad range of industrial applications."}}, "1bfd8547-1f5f-4fe4-9d6f-7b7d0f5b0eb0": {"node_ids": ["ee3a13bb-b63f-4c17-8c8b-2567dd440a34", "decd9340-49d8-4788-a620-38cb561dd2d8", "12cda149-f185-431f-976f-5fb9d4933fad", "ff2b67a9-2340-4ecf-b42d-09fe0c4bb545"], "metadata": {"page_label": "6", "file_name": "2310_10049v1.pdf", "Title of this paper": "FATE-LLM: A Industrial Grade Federated Learning Framework for Large Language Models", "Authors": "Tao Fan, Yan Kang, Guoqiang Ma, Weijing Chen, Wenbin Wei, Lixin Fan, Qiang Yang", "Date published": "10/16/2023", "URL": "http://arxiv.org/abs/2310.10049v1", "summary": "Large Language Models (LLMs), such as ChatGPT, LLaMA, GLM, and PaLM, have\nexhibited remarkable performances across various tasks in recent years.\nHowever, LLMs face two main challenges in real-world applications. One\nchallenge is that training LLMs consumes vast computing resources, preventing\nLLMs from being adopted by small and medium-sized enterprises with limited\ncomputing resources. Another is that training LLM requires a large amount of\nhigh-quality data, which are often scattered among enterprises. To address\nthese challenges, we propose FATE-LLM, an industrial-grade federated learning\nframework for large language models. FATE-LLM (1) facilitates federated\nlearning for large language models (coined FedLLM); (2) promotes efficient\ntraining of FedLLM using parameter-efficient fine-tuning methods; (3) protects\nthe intellectual property of LLMs; (4) preserves data privacy during training\nand inference through privacy-preserving mechanisms. We release the code of\nFATE-LLM at https://github.com/FederatedAI/FATE-LLM to facilitate the research\nof FedLLM and enable a broad range of industrial applications."}}, "0a5a9e5a-3186-437c-a36f-1c56340f0fcb": {"node_ids": ["32469866-1865-47e5-8644-8684428e21d9", "76b15b7a-86fc-40fd-a70c-23d02621a13e"], "metadata": {"page_label": "7", "file_name": "2310_10049v1.pdf", "Title of this paper": "FATE-LLM: A Industrial Grade Federated Learning Framework for Large Language Models", "Authors": "Tao Fan, Yan Kang, Guoqiang Ma, Weijing Chen, Wenbin Wei, Lixin Fan, Qiang Yang", "Date published": "10/16/2023", "URL": "http://arxiv.org/abs/2310.10049v1", "summary": "Large Language Models (LLMs), such as ChatGPT, LLaMA, GLM, and PaLM, have\nexhibited remarkable performances across various tasks in recent years.\nHowever, LLMs face two main challenges in real-world applications. One\nchallenge is that training LLMs consumes vast computing resources, preventing\nLLMs from being adopted by small and medium-sized enterprises with limited\ncomputing resources. Another is that training LLM requires a large amount of\nhigh-quality data, which are often scattered among enterprises. To address\nthese challenges, we propose FATE-LLM, an industrial-grade federated learning\nframework for large language models. FATE-LLM (1) facilitates federated\nlearning for large language models (coined FedLLM); (2) promotes efficient\ntraining of FedLLM using parameter-efficient fine-tuning methods; (3) protects\nthe intellectual property of LLMs; (4) preserves data privacy during training\nand inference through privacy-preserving mechanisms. We release the code of\nFATE-LLM at https://github.com/FederatedAI/FATE-LLM to facilitate the research\nof FedLLM and enable a broad range of industrial applications."}}}}